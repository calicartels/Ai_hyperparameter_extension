{"repo_name": "roscopecoltran/scraper", "file_path": ".staging/meta-engines/xlinkBook/analysis/tfidf/analyze.py", "content": "#!/usr/bin/env python\n\"\"\"\nReads txt files of all papers and computes tfidf vectors for all papers.\nDumps results to file tfidf.p\n\"\"\"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport cPickle as pickle\nimport urllib2\nimport shutil\nimport time\nimport os\nimport sys\nimport random\nimport numpy as np\nsys.path.append(\"../..\")\nfrom record import PaperRecord\n\n# read database\n#db = pickle.load(open('db.p', 'rb'))\n\n# read all text files for all papers into memory\ntxts = []\npids = []\n'''\nn=0\nfor pid,j in db.iteritems():\n  n+=1\n  idvv = '%sv%d' % (j['_rawid'], j['_version'])\n  fname = os.path.join('txt', idvv) + '.pdf.txt'\n  if os.path.isfile(fname): # some pdfs dont translate to txt\n    txt = open(fname, 'r').read()\n    if len(txt) > 100: # way too short and suspicious\n      txts.append(txt) # todo later: maybe filter or something some of them\n      pids.append(idvv)\n      print 'read %d/%d (%s) with %d chars' % (n, len(db), idvv, len(txt))\n    else:\n      print 'skipped %d/%d (%s) with %d chars: suspicious!' % (n, len(db), idvv, len(txt))\n'''\nfiles = os.listdir('../../db/eecs/papers/arxiv/')\nfor item in files:\n    f = open('../../db/eecs/papers/arxiv/' + item, 'rU')\n    lines = f.readlines()\n    f.close()\n    for line in lines:\n        record = PaperRecord(line)\n        summary = record.get_title() + ' ' + record.get_summary()\n        txts.append(summary)\n        url = record.get_url()\n        pid = url[url.rfind('/') + 1 : ].replace('.pdf', '').strip()\n        pids.append(pid)\n        if pid.find('v') != -1:\n            print pid\n# compute tfidf vectors with scikits\nv = TfidfVectorizer(input='content', \n        encoding='utf-8', decode_error='replace', strip_accents='unicode', \n        lowercase=True, analyzer='word', stop_words='english', \n        token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]+\\b',\n        ngram_range=(1, 2), max_features = 20000, \n        norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n\nX = v.fit_transform(txts)\nprint v.vocabulary_\nprint X.shape\n\n# write full matrix out\nout = {}\nout['X'] = X # this one is heavy!\nprint('writing tfidf.p')\npickle.dump(out, open(\"tfidf.p\", \"wb\"))\n\n# writing lighter metadata information into a separate (smaller) file\nout = {}\nout['vocab'] = v.vocabulary_\nout['idf'] = v._tfidf.idf_\nout['pids'] = pids # a full idvv string (id and version number)\nout['ptoi'] = { x:i for i,x in enumerate(pids) } # pid to ix in X mapping\nprint('writing tfidf_meta.p')\npickle.dump(out, open(\"tfidf_meta.p\", \"wb\"))\n\nprint 'precomputing nearest neighbor queries in batches...'\nX = X.todense() # originally it's a sparse matrix\nsim_dict = {}\nbatch_size = 500\nfor i in xrange(0,len(pids),batch_size):\n  i1 = min(len(pids), i+batch_size)\n  xquery = X[i:i1] # BxD\n  #print xquery\n  ds = -np.asarray(np.dot(X, xquery.T)) #NxD * DxB => NxB\n  IX = np.argsort(ds, axis=0) # NxB\n  for j in xrange(i1-i):\n    sim_dict[pids[i+j]] = [pids[q] for q in list(IX[:50,j])]\n  print '%d/%d...' % (i, len(pids))\n\nprint('writing sim_dict.p')\npickle.dump(sim_dict, open(\"sim_dict.p\", \"wb\"))\n", "framework": "tensorflow"}
{"repo_name": "Honkl/general-ai", "file_path": "Controller/reinforcement/ddpg/actor_network_bn.py", "content": "\"\"\"\nThe MIT License (MIT)\n\nCopyright (c) 2016 Flood Sung\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nThis library is used and modified in General Artificial Intelligence for Game Playing project by Jan Kluj.\nSee the original license above.\n\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\nimport numpy as np\nimport math\n\n# Hyper Parameters\nLAYER1_SIZE = 400\nLAYER2_SIZE = 300\nLEARNING_RATE = 1e-4\nTAU = 0.001\n\n\nclass ActorNetwork:\n    \"\"\"docstring for ActorNetwork\"\"\"\n\n    def __init__(self, sess, state_dim, action_dim):\n        self.sess = sess\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        # create actor network\n        self.state_input, self.action_output, self.net, self.is_training = self.create_network(state_dim, action_dim)\n\n        # create target actor network\n        self.target_state_input, self.target_action_output, self.target_update, self.target_is_training = self.create_target_network(\n            state_dim, action_dim, self.net)\n\n        # define training rules\n        self.create_training_method()\n\n        self.sess.run(tf.global_variables_initializer())\n\n        self.update_target()\n\n        # self.load_network()\n\n    def get_parameters(self):\n        data = {}\n        data[\"layers\"] = [LAYER1_SIZE, LAYER2_SIZE]\n        data[\"learning_rate\"] = LEARNING_RATE\n        data[\"tau\"] = TAU\n        return data\n\n    def create_training_method(self):\n        self.q_gradient_input = tf.placeholder(\"float\", [None, self.action_dim])\n        self.parameters_gradients = tf.gradients(self.action_output, self.net, -self.q_gradient_input)\n        self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE).apply_gradients(zip(self.parameters_gradients, self.net))\n\n    def create_network(self, state_dim, action_dim):\n        layer1_size = LAYER1_SIZE\n        layer2_size = LAYER2_SIZE\n\n        state_input = tf.placeholder(\"float\", [None, state_dim])\n        is_training = tf.placeholder(tf.bool)\n\n        W1 = self.variable([state_dim, layer1_size], state_dim)\n        b1 = self.variable([layer1_size], state_dim)\n        W2 = self.variable([layer1_size, layer2_size], layer1_size)\n        b2 = self.variable([layer2_size], layer1_size)\n        W3 = tf.Variable(tf.random_uniform([layer2_size, action_dim], -3e-3, 3e-3))\n        b3 = tf.Variable(tf.random_uniform([action_dim], -3e-3, 3e-3))\n\n        layer0_bn = self.batch_norm_layer(state_input, training_phase=is_training, scope_bn='batch_norm_0',\n                                          activation=tf.identity)\n        layer1 = tf.matmul(layer0_bn, W1) + b1\n        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, W2) + b2\n        layer2_bn = self.batch_norm_layer(layer2, training_phase=is_training, scope_bn='batch_norm_2',\n                                          activation=tf.nn.relu)\n\n        action_output = tf.tanh(tf.matmul(layer2_bn, W3) + b3)\n\n        return state_input, action_output, [W1, b1, W2, b2, W3, b3], is_training\n\n    def create_target_network(self, state_dim, action_dim, net):\n        state_input = tf.placeholder(\"float\", [None, state_dim])\n        is_training = tf.placeholder(tf.bool)\n        ema = tf.train.ExponentialMovingAverage(decay=1 - TAU)\n        target_update = ema.apply(net)\n        target_net = [ema.average(x) for x in net]\n\n        layer0_bn = self.batch_norm_layer(state_input, training_phase=is_training, scope_bn='target_batch_norm_0',\n                                          activation=tf.identity)\n\n        layer1 = tf.matmul(layer0_bn, target_net[0]) + target_net[1]\n        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='target_batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, target_net[2]) + target_net[3]\n        layer2_bn = self.batch_norm_layer(layer2, training_phase=is_training, scope_bn='target_batch_norm_2',\n                                          activation=tf.nn.relu)\n\n        action_output = tf.tanh(tf.matmul(layer2_bn, target_net[4]) + target_net[5])\n\n        return state_input, action_output, target_update, is_training\n\n    def update_target(self):\n        self.sess.run(self.target_update)\n\n    def train(self, q_gradient_batch, state_batch):\n        self.sess.run(self.optimizer, feed_dict={\n            self.q_gradient_input: q_gradient_batch,\n            self.state_input: state_batch,\n            self.is_training: True\n        })\n\n    def actions(self, state_batch):\n        return self.sess.run(self.action_output, feed_dict={\n            self.state_input: state_batch,\n            self.is_training: True\n        })\n\n    def action(self, state):\n        return self.sess.run(self.action_output, feed_dict={\n            self.state_input: [state],\n            self.is_training: False\n        })[0]\n\n    def target_actions(self, state_batch):\n        return self.sess.run(self.target_action_output, feed_dict={\n            self.target_state_input: state_batch,\n            self.target_is_training: True\n        })\n\n    # f fan-in size\n    def variable(self, shape, f):\n        return tf.Variable(tf.random_uniform(shape, -1 / math.sqrt(f), 1 / math.sqrt(f)))\n\n    def batch_norm_layer(self, x, training_phase, scope_bn, activation=None):\n        return tf.cond(training_phase,\n                       lambda: tf.contrib.layers.batch_norm(x, activation_fn=activation, center=True, scale=True,\n                                                            updates_collections=None, is_training=True, reuse=None,\n                                                            scope=scope_bn, decay=0.9, epsilon=1e-5),\n                       lambda: tf.contrib.layers.batch_norm(x, activation_fn=activation, center=True, scale=True,\n                                                            updates_collections=None, is_training=False, reuse=True,\n                                                            scope=scope_bn, decay=0.9, epsilon=1e-5))\n\n\n'''\n\tdef load_network(self):\n\t\tself.saver = tf.train.Saver()\n\t\tcheckpoint = tf.train.get_checkpoint_state(\"saved_actor_networks\")\n\t\tif checkpoint and checkpoint.model_checkpoint_path:\n\t\t\tself.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n\t\t\tprint \"Successfully loaded:\", checkpoint.model_checkpoint_path\n\t\telse:\n\t\t\tprint \"Could not find old network weights\"\n\tdef save_network(self,time_step):\n\t\tprint 'save actor-network...',time_step\n\t\tself.saver.save(self.sess, 'saved_actor_networks/' + 'actor-network', global_step = time_step)\n\n'''\n", "framework": "tensorflow"}
{"repo_name": "nlholdem/icodoom", "file_path": "ICO1/agent/agent1.py", "content": "from __future__ import print_function\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport ops as my_ops\nimport os\nimport re\nimport itertools as it\n\nclass Agent:\n\n    def __init__(self, sess, args):\n        '''Agent - powered by neural nets, can infer, act, train, test.\n        '''\n        self.sess = sess\n        \n        # input data properties\n        self.state_imgs_shape = args['state_imgs_shape']\n        self.state_meas_shape = args['state_meas_shape']\n        self.meas_for_net = args['meas_for_net']\n        self.meas_for_manual = args['meas_for_manual']\n        self.resolution = args['resolution']\n\n        # preprocessing\n        self.preprocess_input_images = args['preprocess_input_images']\n\n        # net parameters\n        self.conv_params = args['conv_params']\n        self.fc_img_params = args['fc_img_params']\n        self.fc_meas_params = args['fc_meas_params']\n        self.fc_joint_params = args['fc_joint_params']      \n        self.target_dim = args['target_dim']\n\n        self.n_ffnet_input = args['n_ffnet_input']\n        self.n_ffnet_hidden = args['n_ffnet_hidden']\n        self.n_ffnet_output = args['n_ffnet_output']\n        self.learning_rate = args['learning_rate']\n        self.momentum = args['momentum']\n        self.ext_ffnet_output = np.zeros(self.n_ffnet_output)\n        self.ext_covnet_output = np.zeros(self.n_ffnet_output)\n        self.ext_fcnet_output = np.zeros(self.n_ffnet_output)\n        print (\"ffnet_inputs: \", args['n_ffnet_input'])\n        print (\"ffnet_hidden: \", args['n_ffnet_hidden'])\n        print (\"ext_ffnet_output: \", self.ext_ffnet_output.shape)\n\n        self.ffnet_input = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_input])\n        self.ffnet_target = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_output])\n        self.covnet_input = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_input])\n        self.covnet_target = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_output])\n        self.fcnet_input = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_input])\n        self.fcnet_target = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_output])\n\n        self.build_model()\n        self.epoch = 20\n        self.iter = 1\n        \n    def make_convnet(self):\n        n_ffnet_inputs = self.n_ffnet_input\n        n_ffnet_outputs = self.n_ffnet_output\n\n        print(\"COVNET: Inputs: \", n_ffnet_inputs, \" outputs: \", n_ffnet_outputs)\n\n        with tf.name_scope('reshape'):\n            x_image = tf.reshape(self.covnet_input, [-1, self.resolution[0], self.resolution[1], 1])\n\n        with tf.name_scope('conv1'):\n            W_conv1 = my_ops.weight_variable([5, 5, 1, 32])\n            b_conv1 = my_ops.bias_variable([32])\n            h_conv1 = tf.nn.relu(my_ops.conv2d(x_image, W_conv1) + b_conv1)\n\n        with tf.name_scope('pool1'):\n            h_pool1 = my_ops.max_pool_2x2(h_conv1)\n\n        with tf.name_scope('conv2'):\n            W_conv2 = my_ops.weight_variable([5, 5, 32, 64])\n            b_conv2 = my_ops.bias_variable([64])\n            h_conv2 = tf.nn.relu(my_ops.conv2d(h_pool1, W_conv2) + b_conv2)\n\n        with tf.name_scope('pool2'):\n            h_pool2 = my_ops.max_pool_2x2(h_conv2)\n\n        with tf.name_scope('fc1'):\n            W_fc1 = my_ops.weight_variable([int(self.resolution[0]/4) * int(self.resolution[1]/4) * 64, 64])\n            b_fc1 = my_ops.bias_variable([64])\n\n            h_pool2_flat = tf.reshape(h_pool2, [-1, int(self.resolution[0]/4) * int(self.resolution[1]/4) * 64])\n            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n        # single output:\n        with tf.name_scope('fc2'):\n            W_fc2 = my_ops.weight_variable([64, 1])\n            b_fc2 = my_ops.bias_variable([1])\n\n        self.y_conv = tf.tanh(tf.matmul(h_fc1, W_fc2) + b_fc2)\n        self.covloss = tf.squared_difference(self.y_conv, self.covnet_target)\n        self.covnet_train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.covloss)\n        self.covaccuracy = tf.reduce_mean(self.covloss)\n\n    def make_fcnet(self):\n        n_ffnet_inputs = self.n_ffnet_input\n        n_ffnet_outputs = self.n_ffnet_output\n\n        print(\"FCNET: Inputs: \", n_ffnet_inputs, \" outputs: \", n_ffnet_outputs)\n\n        W_fc1 = my_ops.weight_variable([n_ffnet_inputs, 8], 0.003)\n        b_fc1 = my_ops.bias_variable([8])\n\n        W_fc2 = my_ops.weight_variable([8, 2], 0.003)\n        b_fc2 = my_ops.bias_variable([2])\n\n        W_fc3 = my_ops.weight_variable([2, 1], 0.003)\n        b_fc3 = my_ops.bias_variable([1])\n\n        h1 = tf.tanh(tf.matmul(self.fcnet_input, W_fc1) + b_fc1)\n        h2 = tf.tanh(tf.matmul(h1, W_fc2) + b_fc2)\n        self.y_fc = tf.tanh(tf.matmul(h2, W_fc3) + b_fc3)\n\n        self.fcloss = tf.squared_difference(self.y_fc, self.fcnet_target)\n        self.fcnet_train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.fcloss)\n        self.fcaccuracy = tf.reduce_mean(self.fcloss)\n\n    def make_ffnet(self):\n\n\n        n_ffnet_inputs = self.n_ffnet_input\n        n_ffnet_outputs = self.n_ffnet_output\n        print (\"FFNET: in: \", n_ffnet_inputs, \" hid: \", self.n_ffnet_hidden, \" out: \", n_ffnet_outputs)\n\n\n        W_layer1 = my_ops.weight_variable([n_ffnet_inputs, self.n_ffnet_hidden[0]])\n        b_layer1 = my_ops.bias_variable([self.n_ffnet_hidden[0]])\n\n        W_layer2 = my_ops.weight_variable([self.n_ffnet_hidden[0], self.n_ffnet_hidden[1]])\n        b_layer2 = my_ops.bias_variable([self.n_ffnet_hidden[1]])\n\n        W_layer3 = my_ops.weight_variable([self.n_ffnet_hidden[1], n_ffnet_outputs])\n        b_layer3 = my_ops.bias_variable([n_ffnet_outputs])\n\n        h_1 = tf.nn.relu(tf.matmul(self.ffnet_input, W_layer1) + b_layer1)\n        h_2 = tf.nn.relu(tf.matmul(h_1, W_layer2) + b_layer2)\n\n        # dropout\n        #print(\"output shape: \", self.ffnet_output.get_shape(), \"target shape: \", self.ffnet_target.get_shape())\n        #print(\"W3: \", W_layer3.get_shape(), \" bias3: \", b_layer3.get_shape())\n\n        self.ffnet_output = tf.matmul(h_2, W_layer3) + b_layer3\n        #print(\"output shape: \", self.ffnet_output.get_shape(), \"target shape: \", self.ffnet_target.get_shape())\n        #print(\"W3: \", W_layer3.get_shape(), \" bias3: \", b_layer3.get_shape())\n\n        self.loss = tf.squared_difference(self.ffnet_output, self.ffnet_target)\n\n        self.ffnet_train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n\n        self.accuracy = tf.reduce_mean(self.loss)\n#        sess.run(tf.global_variables_initializer())\n\n    def make_net(self, input_images, input_measurements, input_actions, reuse=False):\n        if reuse:\n            tf.get_variable_scope().reuse_variables()\n        \n        self.fc_val_params = np.copy(self.fc_joint_params)\n        self.fc_val_params['out_dims'][-1] = self.target_dim\n        self.fc_adv_params = np.copy(self.fc_joint_params)\n        self.fc_adv_params['out_dims'][-1] = len(self.net_discrete_actions) * self.target_dim\n        print(len(self.net_discrete_actions) * self.target_dim)\n        p_img_conv = my_ops.conv_encoder(input_images, self.conv_params, 'p_img_conv', msra_coeff=0.9)\n        print (\"Conv Params: \", self.conv_params)\n\n        p_img_fc = my_ops.fc_net(my_ops.flatten(p_img_conv), self.fc_img_params, 'p_img_fc', msra_coeff=0.9)\n        print (\"img_params\", self.fc_img_params)\n        p_meas_fc = my_ops.fc_net(input_measurements, self.fc_meas_params, 'p_meas_fc', msra_coeff=0.9)\n        print (\"meas_params\", self.fc_meas_params)\n        p_val_fc = my_ops.fc_net(tf.concat(1, [p_img_fc,p_meas_fc]), self.fc_val_params, 'p_val_fc', last_linear=True, msra_coeff=0.9)\n        print (\"val_params\", self.fc_val_params)\n        p_adv_fc = my_ops.fc_net(tf.concat(1, [p_img_fc,p_meas_fc]), self.fc_adv_params, 'p_adv_fc', last_linear=True, msra_coeff=0.9)\n        print (\"adv_params\", self.fc_adv_params)\n\n        p_adv_fc_nomean = p_adv_fc - tf.reduce_mean(p_adv_fc, reduction_indices=1, keep_dims=True)  \n        \n        self.pred_all_nomean = tf.reshape(p_adv_fc_nomean, [-1, len(self.net_discrete_actions), self.target_dim])\n        self.pred_all = self.pred_all_nomean + tf.reshape(p_val_fc, [-1, 1, self.target_dim])\n        self.pred_relevant = tf.boolean_mask(self.pred_all, tf.cast(input_actions, tf.bool))\n        print (\"make_net: input_actions: \", input_actions)\n        print (\"make_net: pred_all: \", self.pred_all)\n        print (\"make_net: pred_relevant: \", self.pred_relevant)\n\n    def build_model(self):\n\n        #self.make_ffnet()\n        #self.make_convnet()\n        self.make_fcnet()\n        self.saver = tf.train.Saver()\n        tf.initialize_all_variables().run(session=self.sess)\n    \n    def act(self, state_imgs, state_meas, objective):\n        return self.postprocess_actions(self.act_net(state_imgs, state_meas, objective), self.act_manual(state_meas)), None # last output should be predictions, but we omit these for now\n\n    def act_ffnet(self, in_image, in_meas, target):\n\n        net_inputs = in_image\n        net_targets = target\n\n        with self.sess.as_default():\n            self.ext_ffnet_output, hack = self.sess.run([self.ffnet_output, self.ffnet_train_step], feed_dict={\n                self.ffnet_input: net_inputs,\n                self.ffnet_target: net_targets\n            })\n\n            if self.iter % self.epoch == 0:\n                print (\"LOSS: \", self.accuracy.eval(feed_dict={\n                    self.ffnet_input: net_inputs,\n                    self.ffnet_target: net_targets\n                }))\n\n        self.iter = self.iter+1\n\n    def act_covnet(self, in_image, in_meas, target):\n\n        net_inputs = in_image\n        net_targets = target\n\n        with self.sess.as_default():\n            self.sess.run([self.covnet_train_step], feed_dict={\n                self.covnet_input: net_inputs,\n                self.covnet_target: net_targets\n            })\n            self.ext_covnet_output = self.sess.run([self.y_conv], feed_dict={\n                self.covnet_input: net_inputs,\n                self.covnet_target: net_targets\n            })[0]\n\n            if self.iter % self.epoch == 0:\n                print (\"LOSS: \", self.covaccuracy.eval(feed_dict={\n                    self.covnet_input: net_inputs,\n                    self.covnet_target: net_targets\n                }))\n\n        self.iter = self.iter+1\n\n    def act_fcnet(self, in_image, in_meas, target):\n\n        net_inputs = in_image\n        net_targets = target\n\n        if (in_meas[1] > 0):\n\n            with self.sess.as_default():\n                self.sess.run([self.fcnet_train_step], feed_dict={\n                    self.fcnet_input: net_inputs,\n                    self.fcnet_target: net_targets\n                })\n                self.ext_fcnet_output = self.sess.run([self.y_fc], feed_dict={\n                    self.fcnet_input: net_inputs,\n                    self.fcnet_target: net_targets\n                })[0]\n\n                if self.iter % self.epoch == 0:\n                    print (\"LOSS: \", self.fcaccuracy.eval(feed_dict={\n                        self.fcnet_input: net_inputs,\n                        self.fcnet_target: net_targets\n                    }))\n\n            self.iter = self.iter+1\n\n\n    def act_net(self, state_imgs, state_meas, objective):\n        #Act given a state and objective\n        predictions = self.sess.run(self.pred_all, feed_dict={self.input_images: state_imgs, \n                                                            self.input_measurements: state_meas[:,self.meas_for_net]})\n        #print (predictions)\n\n        objectives = np.sum(predictions[:,:,objective[0]]*objective[1][None,None,:], axis=2)    \n        curr_action = np.argmax(objectives, axis=1)\n#        print (\" ** ACTION \", curr_action)\n        return curr_action\n\n    # act_manual is a purely hard-coded method to handle weapons-switching\n    def act_manual(self, state_meas):\n        if len(self.meas_for_manual) == 0:\n            return []\n        else:\n            assert(len(self.meas_for_manual) == 13) # expected to be [AMMO2 AMMO3 AMMO4 AMMO5 AMMO6 AMMO7 WEAPON2 WEAPON3 WEAPON4 WEAPON5 WEAPON6 WEAPON7 SELECTED_WEAPON]\n            assert(self.num_manual_controls == 6) # expected to be [SELECT_WEAPON2 SELECT_WEAPON3 SELECT_WEAPON4 SELECT_WEAPON5 SELECT_WEAPON6 SELECT_WEAPON7]\n\n            curr_act = np.zeros((state_meas.shape[0],self.num_manual_controls), dtype=np.int)\n            for ns in range(state_meas.shape[0]):\n                # always pistol\n                #if not state_meas[ns,self.meas_for_manual[12]] == 2:\n                    #curr_act[ns, 0] = 1\n                # best weapon\n                curr_ammo = state_meas[ns,self.meas_for_manual[:6]]\n                curr_weapons = state_meas[ns,self.meas_for_manual[6:12]]\n                #print(curr_ammo,curr_weapons)\n                available_weapons = np.logical_and(curr_ammo >= np.array([1,2,1,1,1,40]), curr_weapons)\n                if any(available_weapons):\n                    best_weapon = np.nonzero(available_weapons)[0][-1]\n                    if not state_meas[ns,self.meas_for_manual[12]] == best_weapon+2:\n                        curr_act[ns, best_weapon] = 1\n            return curr_act\n\n    def save(self, checkpoint_dir, iter):\n        self.save_path = self.saver.save(self.sess, checkpoint_dir, global_step=iter)\n        print (\"saving model file: \", self.save_path)\n\n\n\n    def load(self, checkpoint_dir):\n        self.saver.restore(self.sess, tf.train.latest_checkpoint(checkpoint_dir))\n        return True\n", "framework": "tensorflow"}
{"repo_name": "chaitjo/personalized-dialog", "file_path": "MemN2N-split-memory/single_dialog.py", "content": "from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom data_utils import load_dialog_task, vectorize_data, load_candidates, vectorize_candidates, vectorize_candidates_sparse, tokenize\nfrom sklearn import metrics\nfrom memn2n import MemN2NDialog\nfrom itertools import chain\nfrom six.moves import range, reduce\nimport sys\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport pickle\n\ntf.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate for Adam Optimizer.\")\ntf.flags.DEFINE_float(\"epsilon\", 1e-8, \"Epsilon value for Adam Optimizer.\")\ntf.flags.DEFINE_float(\"max_grad_norm\", 40.0, \"Clip gradients to this norm.\")\ntf.flags.DEFINE_integer(\"evaluation_interval\", 10, \"Evaluate and print results every x epochs\")\ntf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\ntf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\ntf.flags.DEFINE_integer(\"epochs\", 250, \"Number of epochs to train for.\")\ntf.flags.DEFINE_integer(\"embedding_size\", 20, \"Embedding size for embedding matrices.\")\ntf.flags.DEFINE_integer(\"memory_size\", 250, \"Maximum size of memory.\")\ntf.flags.DEFINE_integer(\"task_id\", 1, \"task id, 1 <= id <= 5\")\ntf.flags.DEFINE_integer(\"random_state\", None, \"Random state.\")\ntf.flags.DEFINE_string(\"data_dir\", \"../data/personalized-dialog-dataset/full\", \"Directory containing personalized dialog tasks\")\ntf.flags.DEFINE_string(\"model_dir\", \"model/\", \"Directory containing memn2n model checkpoints\")\ntf.flags.DEFINE_boolean('train', True, 'if True, begin to train')\ntf.flags.DEFINE_boolean('OOV', False, 'if True, use OOV test set')\ntf.flags.DEFINE_boolean('save_vocab', False, 'if True, saves vocabulary')\ntf.flags.DEFINE_boolean('load_vocab', False, 'if True, loads vocabulary instead of building it')\nFLAGS = tf.flags.FLAGS\nprint(\"Started Task:\", FLAGS.task_id)\n\n\nclass chatBot(object):\n    def __init__(self, data_dir, model_dir, task_id, \n                 OOV=False, \n                 memory_size=250,\n                 random_state=None, \n                 batch_size=32, \n                 learning_rate=0.001, \n                 epsilon=1e-8, \n                 max_grad_norm=40.0, \n                 evaluation_interval=10, \n                 hops=3, \n                 epochs=200, \n                 embedding_size=20, \n                 save_vocab=False, \n                 load_vocab=False):\n        \"\"\"Creates wrapper for training and testing a chatbot model.\n\n        Args:\n            data_dir: Directory containing personalized dialog tasks.\n            \n            model_dir: Directory containing memn2n model checkpoints.\n\n            task_id: Personalized dialog task id, 1 <= id <= 5. Defaults to `1`.\n\n            OOV: If `True`, use OOV test set. Defaults to `False`\n\n            memory_size: The max size of the memory. Defaults to `250`.\n\n            random_state: Random state to set graph-level random seed. Defaults to `None`.\n\n            batch_size: Size of the batch for training. Defaults to `32`.\n\n            learning_rate: Learning rate for Adam Optimizer. Defaults to `0.001`.\n\n            epsilon: Epsilon value for Adam Optimizer. Defaults to `1e-8`.\n\n            max_gradient_norm: Maximum L2 norm clipping value. Defaults to `40.0`.\n\n            evaluation_interval: Evaluate and print results every x epochs. \n            Defaults to `10`.\n\n            hops: The number of hops over memory for responding. A hop consists \n            of reading and addressing a memory slot. Defaults to `3`.\n\n            epochs: Number of training epochs. Defualts to `200`.\n\n            embedding_size: The size of the word embedding. Defaults to `20`.\n\n            save_vocab: If `True`, save vocabulary file. Defaults to `False`.\n\n            load_vocab: If `True`, load vocabulary from file. Defaults to `False`.\n        \"\"\"\n        \n        self.data_dir = data_dir\n        self.task_id = task_id\n        self.model_dir = model_dir\n        self.OOV = OOV\n        self.memory_size = memory_size\n        self.random_state = random_state\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon\n        self.max_grad_norm = max_grad_norm\n        self.evaluation_interval = evaluation_interval\n        self.hops = hops\n        self.epochs = epochs\n        self.embedding_size = embedding_size\n        self.save_vocab = save_vocab\n        self.load_vocab = load_vocab\n\n        candidates,self.candid2indx = load_candidates(self.data_dir, self.task_id)\n        self.n_cand = len(candidates)\n        print(\"Candidate Size\", self.n_cand)\n        self.indx2candid = dict((self.candid2indx[key],key) \n                                for key in self.candid2indx)\n        \n        # Task data\n        self.trainData, self.testData, self.valData = load_dialog_task(\n            self.data_dir, self.task_id, self.candid2indx, self.OOV)\n        data = self.trainData + self.testData + self.valData\n        \n        self.build_vocab(data,candidates, self.save_vocab, self.load_vocab)\n        \n        self.candidates_vec = vectorize_candidates(\n            candidates,self.word_idx, self.candidate_sentence_size)\n        \n        optimizer = tf.train.AdamOptimizer(\n            learning_rate=self.learning_rate, epsilon=self.epsilon)\n        \n        self.sess = tf.Session()\n        \n        self.model = MemN2NDialog(self.batch_size, self.vocab_size, self.n_cand, \n                                  self.sentence_size, self.embedding_size, \n                                  self.candidates_vec, session=self.sess, \n                                  hops=self.hops, max_grad_norm=self.max_grad_norm, \n                                  optimizer=optimizer, task_id=task_id)\n        \n        self.saver = tf.train.Saver(max_to_keep=50)\n        \n        self.summary_writer = tf.summary.FileWriter(\n            self.model.root_dir, self.model.graph_output.graph)\n        \n\n    def build_vocab(self,data,candidates,save=False,load=False):\n        \"\"\"Build vocabulary of words from all dialog data and candidates.\"\"\"\n        if load:  \n            # Load from vocabulary file\n            vocab_file = open('vocab'+str(self.task_id)+'.obj', 'rb')\n            vocab = pickle.load(vocab_file)\n        else:\n            vocab = reduce(lambda x, y: x | y,\n                           (set(list(chain.from_iterable(s)) + q) \n                             for p, s, q, a in data))\n            vocab |= reduce(lambda x, y: x | y, \n                            (set(list(chain.from_iterable(p)) + q) \n                             for p, s, q, a in data))\n            vocab |= reduce(lambda x, y: x | y, \n                            (set(candidate) for candidate in candidates) )\n            vocab = sorted(vocab)\n        \n        self.word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n        max_story_size = max(map(len, (s for _, s, _, _ in data)))\n        mean_story_size = int(np.mean([ len(s) for _, s, _, _ in data ]))\n        self.sentence_size = max(map(len, chain.from_iterable(s for _, s, _, _ in data)))\n        self.candidate_sentence_size=max(map(len,candidates))\n        query_size = max(map(len, (q for _, _, q, _ in data)))\n        self.memory_size = min(self.memory_size, max_story_size)\n        self.vocab_size = len(self.word_idx) + 1  # +1 for nil word\n        self.sentence_size = max(query_size, self.sentence_size)  # for the position\n        \n        # Print parameters\n        print(\"vocab size:\", self.vocab_size)\n        print(\"Longest sentence length\", self.sentence_size)\n        print(\"Longest candidate sentence length\", self.candidate_sentence_size)\n        print(\"Longest story length\", max_story_size)\n        print(\"Average story length\", mean_story_size)\n\n        # Save to vocabulary file\n        if save:\n            vocab_file = open('vocab'+str(self.task_id)+'.obj', 'wb')\n            pickle.dump(vocab, vocab_file)\n\n    def train(self):\n        \"\"\"Runs the training algorithm over training set data.\n\n        Performs validation at given evaluation intervals.\n        \"\"\"\n        trainP, trainS, trainQ, trainA = vectorize_data(\n            self.trainData, self.word_idx, self.sentence_size, \n            self.batch_size, self.n_cand, self.memory_size)\n        valP, valS, valQ, valA = vectorize_data(\n            self.valData, self.word_idx, self.sentence_size, \n            self.batch_size, self.n_cand, self.memory_size)\n        n_train = len(trainS)\n        n_val = len(valS)\n        print(\"Training Size\", n_train)\n        print(\"Validation Size\", n_val)\n        tf.set_random_seed(self.random_state)\n        batches = zip(range(0, n_train-self.batch_size, self.batch_size), \n                      range(self.batch_size, n_train, self.batch_size))\n        batches = [(start, end) for start, end in batches]\n        best_validation_accuracy=0\n        \n        # Training loop\n        for t in range(1, self.epochs+1):\n            print('Epoch', t)\n            np.random.shuffle(batches)\n            total_cost = 0.0\n            for start, end in batches:\n                p = trainP[start:end]\n                s = trainS[start:end]\n                q = trainQ[start:end]\n                a = trainA[start:end]\n                cost_t = self.model.batch_fit(p, s, q, a)\n                total_cost += cost_t\n            if t % self.evaluation_interval == 0:\n                # Perform validation\n                train_preds = self.batch_predict(trainP,trainS,trainQ,n_train)\n                val_preds = self.batch_predict(valP,valS,valQ,n_val)\n                train_acc = metrics.accuracy_score(np.array(train_preds), trainA)\n                val_acc = metrics.accuracy_score(val_preds, valA)\n                print('-----------------------')\n                print('Epoch', t)\n                print('Total Cost:', total_cost)\n                print('Training Accuracy:', train_acc)\n                print('Validation Accuracy:', val_acc)\n                print('-----------------------')\n\n                # Write summary\n                train_acc_summary = tf.summary.scalar(\n                    'task_' + str(self.task_id) + '/' + 'train_acc', \n                    tf.constant((train_acc), dtype=tf.float32))\n                val_acc_summary = tf.summary.scalar(\n                    'task_' + str(self.task_id) + '/' + 'val_acc', \n                    tf.constant((val_acc), dtype=tf.float32))\n                merged_summary = tf.summary.merge([train_acc_summary, val_acc_summary])\n                summary_str = self.sess.run(merged_summary)\n                self.summary_writer.add_summary(summary_str, t)\n                self.summary_writer.flush()\n                \n                if val_acc > best_validation_accuracy:\n                    best_validation_accuracy = val_acc\n                    self.saver.save(self.sess, self.model_dir+'model.ckpt',\n                                    global_step=t)\n                    \n    def test(self):\n        \"\"\"Runs testing on testing set data.\n\n        Loads best performing model weights based on validation accuracy.\n        \"\"\"\n        ckpt = tf.train.get_checkpoint_state(self.model_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n        else:\n            print(\"...no checkpoint found...\")\n        \n        testP, testS, testQ, testA = vectorize_data(\n            self.testData, self.word_idx, self.sentence_size, \n            self.batch_size, self.n_cand, self.memory_size)\n        n_test = len(testS)\n        print(\"Testing Size\", n_test)\n        \n        test_preds = self.batch_predict(testP, testS, testQ, n_test)\n        test_acc = metrics.accuracy_score(test_preds, testA)\n        print(\"Testing Accuracy:\", test_acc)\n        \n        # # Un-comment below to view correct responses and predictions \n        # print(testA)\n        # for pred in test_preds:\n        #    print(pred, self.indx2candid[pred])\n\n    def batch_predict(self,P,S,Q,n):\n        \"\"\"Predict answers over the passed data in batches.\n\n        Args:\n            P: Tensor (None, sentence_size)\n            S: Tensor (None, memory_size, sentence_size)\n            Q: Tensor (None, sentence_size)\n            n: int\n\n        Returns:\n            preds: Tensor (None, vocab_size)\n        \"\"\"\n        preds = []\n        for start in range(0, n, self.batch_size):\n            end = start + self.batch_size\n            p = P[start:end]\n            s = S[start:end]\n            q = Q[start:end]\n            pred = self.model.predict(p, s, q)\n            preds += list(pred)\n        return preds\n\n    def close_session(self):\n        self.sess.close()\n\n\nif __name__ == '__main__':\n    model_dir = \"task\" + str(FLAGS.task_id) + \"_\" + FLAGS.model_dir\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    chatbot = chatBot(FLAGS.data_dir, model_dir, FLAGS.task_id, OOV=FLAGS.OOV,\n                      batch_size=FLAGS.batch_size, memory_size=FLAGS.memory_size,\n                      epochs=FLAGS.epochs, hops=FLAGS.hops, save_vocab=FLAGS.save_vocab,\n                      load_vocab=FLAGS.load_vocab, learning_rate=FLAGS.learning_rate,\n                      embedding_size=FLAGS.embedding_size)\n    \n    if FLAGS.train:\n        chatbot.train()\n    else:\n        chatbot.test()\n    \n    chatbot.close_session()\n", "framework": "tensorflow"}
{"repo_name": "chaitjo/personalized-dialog", "file_path": "MemN2N/memn2n/memn2n_dialog.py", "content": "from __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\nfrom six.moves import range\nfrom datetime import datetime\n\n\nclass MemN2NDialog(object):\n    \"\"\"End-To-End Memory Network.\"\"\"\n    def __init__(self, batch_size, vocab_size, candidates_size, \n                 sentence_size, embedding_size, candidates_vec,\n                 hops=3,\n                 max_grad_norm=40.0,\n                 nonlin=None,\n                 initializer=tf.random_normal_initializer(stddev=0.1),\n                 optimizer=tf.train.AdamOptimizer(learning_rate=1e-2),\n                 session=tf.Session(),\n                 name='MemN2N',\n                 task_id=1):\n        \"\"\"Creates an End-To-End Memory Network\n\n        Args:\n            batch_size: The size of the batch.\n\n            vocab_size: The size of the vocabulary (should include the nil word). \n            The nil word one-hot encoding should be 0.\n\n            sentence_size: The max size of a sentence in the data. All sentences \n            should be padded to this length. If padding is required it should be \n            done with nil one-hot encoding (0).\n\n            candidates_size: The size of candidates\n\n            memory_size: The max size of the memory. Since Tensorflow currently \n            does not support jagged arrays all memories must be padded to this \n            length. If padding is required, the extra memories should be empty \n            memories; memories filled with the nil word ([0, 0, 0, ......, 0]).\n\n            embedding_size: The size of the word embedding.\n\n            candidates_vec: The numpy array of candidates encoding.\n\n            hops: The number of hops. A hop consists of reading and addressing \n            a memory slot. Defaults to `3`.\n\n            max_grad_norm: Maximum L2 norm clipping value. Defaults to `40.0`.\n\n            nonlin: Non-linearity. Defaults to `None`.\n\n            initializer: Weight initializer. Defaults to `tf.random_normal_initializer(stddev=0.1)`.\n\n            optimizer: Optimizer algorithm used for SGD. Defaults to `tf.train.AdamOptimizer(learning_rate=1e-2)`.\n\n            encoding: A function returning a 2D Tensor (sentence_size, embedding_size). \n            Defaults to `position_encoding`.\n\n            session: Tensorflow Session the model is run with. Defaults to `tf.Session()`.\n\n            name: Name of the End-To-End Memory Network. Defaults to `MemN2N`.\n        \"\"\"\n\n        self._batch_size = batch_size\n        self._vocab_size = vocab_size\n        self._candidates_size = candidates_size\n        self._sentence_size = sentence_size\n        self._embedding_size = embedding_size\n        self._hops = hops\n        self._max_grad_norm = max_grad_norm\n        self._nonlin = nonlin\n        self._init = initializer\n        self._opt = optimizer\n        self._name = name\n        self._candidates=candidates_vec\n\n        self._build_inputs()\n        self._build_vars()\n        \n        # Define summary directory\n        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n        self.root_dir = \"%s_%s_%s_%s/\" % ('task', str(task_id),'summary_output', timestamp)\n        \n        # Calculate cross entropy\n        # dimensions: (batch_size, candidates_size)\n        logits = self._inference(self._stories, self._queries)\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits, self._answers, name=\"cross_entropy\")\n        cross_entropy_sum = tf.reduce_sum(cross_entropy, name=\"cross_entropy_sum\")\n\n        # Define loss op\n        loss_op = cross_entropy_sum\n\n        # Gradient pipeline\n        grads_and_vars = self._opt.compute_gradients(loss_op)\n        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v) \n                            for g,v in grads_and_vars]\n        # grads_and_vars = [(add_gradient_noise(g), v) for g,v in grads_and_vars]\n        nil_grads_and_vars = []\n        for g, v in grads_and_vars:\n            if v.name in self._nil_vars:\n                nil_grads_and_vars.append((zero_nil_slot(g), v))\n            else:\n                nil_grads_and_vars.append((g, v))\n        train_op = self._opt.apply_gradients(nil_grads_and_vars, name=\"train_op\")\n\n        # Define predict ops\n        predict_op = tf.argmax(logits, 1, name=\"predict_op\")\n        predict_proba_op = tf.nn.softmax(logits, name=\"predict_proba_op\")\n        predict_log_proba_op = tf.log(predict_proba_op, name=\"predict_log_proba_op\")\n\n        # Assign ops\n        self.loss_op = loss_op\n        self.predict_op = predict_op\n        self.predict_proba_op = predict_proba_op\n        self.predict_log_proba_op = predict_log_proba_op\n        self.train_op = train_op\n        self.graph_output = self.loss_op\n\n        init_op = tf.global_variables_initializer()\n        self._sess = session\n        self._sess.run(init_op)\n\n    def _build_inputs(self):\n        \"\"\"Define input placeholders\"\"\"\n        self._stories = tf.placeholder(\n            tf.int32, [None, None, self._sentence_size], name=\"stories\")\n        self._queries = tf.placeholder(\n            tf.int32, [None, self._sentence_size], name=\"queries\")\n        self._answers = tf.placeholder(tf.int32, [None], name=\"answers\")\n\n    def _build_vars(self):\n        \"\"\"Define trainable variables\"\"\"\n        with tf.variable_scope(self._name):\n            nil_word_slot = tf.zeros([1, self._embedding_size])\n            A = tf.concat(0, [ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n            self.A = tf.Variable(A, name=\"A\")\n            self.H = tf.Variable(self._init([self._embedding_size, self._embedding_size]), name=\"H\")\n            W = tf.concat(0, [ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n            self.W = tf.Variable(W, name=\"W\")\n        self._nil_vars = set([self.A.name,self.W.name])\n\n    def _inference(self, stories, queries):\n        \"\"\"Forward pass through the model\"\"\"\n        with tf.variable_scope(self._name):\n            q_emb = tf.nn.embedding_lookup(self.A, queries)  # Queries vector\n            \n            # Initial state of memory controller for conversation history\n            u_0 = tf.reduce_sum(q_emb, 1)\n            u = [u_0]\n\n            # Iterate over memory for number of hops\n            for count in range(self._hops):\n                m_emb = tf.nn.embedding_lookup(self.A, stories)  # Stories vector \n                m = tf.reduce_sum(m_emb, 2)  # Conversation history memory \n                \n                # Hack to get around no reduce_dot\n                u_temp = tf.transpose(tf.expand_dims(u[-1], -1), [0, 2, 1])\n                dotted = tf.reduce_sum(m * u_temp, 2)\n\n                # Calculate probabilities\n                probs = tf.nn.softmax(dotted)\n                \n                # # Uncomment below to view attention values over memories during inference:\n                # probs = tf.Print(\n                #     probs, ['memory', count, tf.shape(probs), probs], summarize=200)\n\n                probs_temp = tf.transpose(tf.expand_dims(probs, -1), [0, 2, 1])\n                c_temp = tf.transpose(m, [0, 2, 1])\n                \n                # Compute returned vector\n                o_k = tf.reduce_sum(c_temp * probs_temp, 2)\n\n                # Update controller state\n                u_k = tf.matmul(u[-1], self.H) + o_k\n                \n                # Apply nonlinearity\n                if self._nonlin:\n                    u_k = self._nonlin(u_k)\n\n                u.append(u_k)\n\n            candidates_emb=tf.nn.embedding_lookup(self.W, self._candidates)\n            candidates_emb_sum=tf.reduce_sum(candidates_emb,1)\n            \n            return tf.matmul(u_k,tf.transpose(candidates_emb_sum))\n\n    def batch_fit(self, stories, queries, answers):\n        \"\"\"Runs the training algorithm over the passed batch\n\n        Args:\n            stories: Tensor (None, memory_size, sentence_size)\n            queries: Tensor (None, sentence_size)\n            answers: Tensor (None, vocab_size)\n\n        Returns:\n            loss: floating-point number, the loss computed for the batch\n        \"\"\"\n        feed_dict = {self._stories: stories, self._queries: queries, \n                     self._answers: answers}\n        loss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)\n        return loss\n\n    def predict(self, stories, queries):\n        \"\"\"Predicts answers as one-hot encoding.\n\n        Args:\n            stories: Tensor (None, memory_size, sentence_size)\n            queries: Tensor (None, sentence_size)\n\n        Returns:\n            answers: Tensor (None, vocab_size)\n        \"\"\"\n        feed_dict = {self._stories: stories, self._queries: queries}\n        return self._sess.run(self.predict_op, feed_dict=feed_dict)\n\n\ndef zero_nil_slot(t, name=None):\n    \"\"\"Overwrites the nil_slot (first row) of the input Tensor with zeros.\n\n    The nil_slot is a dummy slot and should not be trained and influence\n    the training algorithm.\n    \"\"\"\n    \n    with tf.name_scope(name, \"zero_nil_slot\", [t]) as name:\n        t = tf.convert_to_tensor(t, name=\"t\")\n        s = tf.shape(t)[1]\n        z = tf.zeros(tf.pack([1, s]))\n        return tf.concat(0, [z, tf.slice(t, [1, 0], [-1, -1])], name=name)\n\n\ndef add_gradient_noise(t, stddev=1e-3, name=None):\n    \"\"\"Adds gradient noise as described in http://arxiv.org/abs/1511.06807 [2].\n\n    The input Tensor `t` should be a gradient.\n\n    The output will be `t` + gaussian noise.\n\n    0.001 was said to be a good fixed value for memory networks [2].\n    \"\"\"\n    with tf.name_scope(name, \"add_gradient_noise\", [t, stddev]) as name:\n        t = tf.convert_to_tensor(t, name=\"t\")\n        gn = tf.random_normal(tf.shape(t), stddev=stddev)\n        return tf.add(t, gn, name=name)\n\n", "framework": "tensorflow"}
{"repo_name": "ram1993/neuralnetwork", "file_path": "Tensorflow/CNN.py", "content": "import tensorflow as tf\nimport random\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(105)\n\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\n\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\nkeep_prob = tf.placeholder(tf.float32)\n\n\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\nX_img = tf.reshape(X, [-1, 28, 28, 1])\n\n\n\n# L1 ImgIn shape=(?, 28, 28, 1)\nW1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n#    Conv     -> (?, 28, 28, 32)\n#    Pool     -> (?, 14, 14, 32)\nL1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\nL1 = tf.nn.relu(L1)\nL1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\nL1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n'''\nTensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\nTensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\nTensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\nTensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n'''\n\n# L2 ImgIn shape=(?, 14, 14, 32)\nW2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n#    Conv      ->(?, 14, 14, 64)\n#    Pool      ->(?, 7, 7, 64)\nL2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\nL2 = tf.nn.relu(L2)\nL2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\nL2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n'''\nTensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\nTensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\nTensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\nTensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n'''\n\n# L3 ImgIn shape=(?, 7, 7, 64)\nW3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n#    Conv      ->(?, 7, 7, 128)\n#    Pool      ->(?, 4, 4, 128)\n#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\nL3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\nL3 = tf.nn.relu(L3)\nL3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\nL3 = tf.nn.dropout(L3, keep_prob=keep_prob)\nL3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n'''\nTensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\nTensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\nTensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\nTensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\nTensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n'''\n\n# L4 FC 4x4x128 inputs -> 625 outputs\nW4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625], initializer=tf.contrib.layers.xavier_initializer())\nb4 = tf.Variable(tf.random_normal([625]))\nL4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\nL4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n'''\nTensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\nTensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n'''\n\n# L5 Final FC 625 inputs -> 10 outputs\nW5 = tf.get_variable(\"W5\", shape=[625, 10], initializer=tf.contrib.layers.xavier_initializer())\nb5 = tf.Variable(tf.random_normal([10]))\nhypothesis = tf.matmul(L4, W5) + b5\n'''\nTensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n'''\n\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n\n\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n        c, _, = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n\nprint('Learning Finished!')\n", "framework": "tensorflow"}
{"repo_name": "romanchom/GestureRecognitionVR", "file_path": "WiiRemoteGestures/Recognizer.py", "content": "import numpy as np\nimport os\nimport gc\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR)\n\n\nclass Recognizer:\n    def __init__(self):\n        self.sess = tf.Session()\n\n    def initialize(self, feature_count, class_count, max_length):\n        tf.reset_default_graph()\n        self.sess.close()\n        gc.collect()\n\n        self.step = 0\n        batch_size = None\n        num_mem_cells = 256\n        conv_width = 4\n        conv_height = 1\n        conv_in_channels = feature_count\n        conv_out_channels = 256\n        # MODEL VARIABLES   \n        with tf.variable_scope('variables'):     \n            # projection matrix\n            self.weight = tf.Variable(tf.truncated_normal([num_mem_cells, class_count], stddev = 0.1))\n            self.bias = tf.Variable(tf.constant(0.1, shape=[class_count]))\n            self.conv_filter = tf.Variable(tf.truncated_normal([conv_height, conv_width, conv_in_channels, conv_out_channels], stddev=0.1))\n            self.conv_biases = tf.Variable(tf.constant(0.1, shape=[conv_out_channels]))\n            \n        \n        # TRAINING GRAPH\n        with tf.variable_scope('training'):\n        # INPUT DATA\n            with tf.variable_scope('input'):\n                self.examples = tf.placeholder(tf.float32, [batch_size, max_length, feature_count], name=\"examples\")\n                self.labels = tf.placeholder(tf.int32, [batch_size], name=\"labels\")\n                self.lengths = tf.placeholder(tf.int32, [batch_size], name=\"lengths\")\n                self.keep_prob = tf.placeholder(tf.float32, (), name='keep_prob')\n\n            batch_size = tf.shape(self.examples)[0]\n            \n            # recurent cells\n\n            with tf.variable_scope('operations'):\n                conv_input = tf.reshape(self.examples, (-1, 1, max_length, conv_in_channels))\n                conv = tf.nn.conv2d(conv_input, self.conv_filter, [1, 1, 1, 1], 'VALID', True)\n                conv = tf.nn.bias_add(conv, self.conv_biases)\n                conv = tf.nn.relu(conv)\n                #conv = tf.nn.max_pool(conv, [1, 1, conv_width, 1], [1, 1, 1, 1], 'SAME')\n                \n                new_max_length = max_length - conv_width + 1\n                new_length = self.lengths - conv_width + 1\n                \n                cells = [\n                    tf.contrib.rnn.DropoutWrapper(\n                        tf.contrib.rnn.LSTMCell(num_mem_cells),\n                        input_keep_prob = self.keep_prob,\n                        output_keep_prob = 1.0,\n                        variational_recurrent = True,\n                        input_size = conv_out_channels,\n                        dtype = tf.float32),\n                    tf.contrib.rnn.DropoutWrapper(\n                        tf.contrib.rnn.LSTMCell(num_mem_cells),\n                        input_keep_prob = self.keep_prob,\n                        output_keep_prob = 1.0,\n                        variational_recurrent = True,\n                        input_size = num_mem_cells,\n                        dtype = tf.float32),\n\n                ]\n                cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n\n                cell_in = tf.reshape(conv, (-1, new_max_length, conv_out_channels))\n                # TRAINING AND VALIDATION OPERATIONS\n                cell_out, cell_state = tf.nn.dynamic_rnn(\n                    cell, cell_in, dtype=tf.float32, sequence_length=new_length)\n                \n                sequence = tf.range(tf.shape(new_length)[0], dtype=tf.int32) #for each batch\n                indices = tf.stack([sequence, new_length - 1], 1, name=\"Last_from_each_row_indices\") # take a slice at the index of length-1 (last)\n                last_output = tf.gather_nd(cell_out, indices)\n\n                self.prediction = tf.matmul(last_output, self.weight) + self.bias\n                                \n                # this op performs internally softmax\n                self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.prediction, labels=self.labels, name=\"softmax_cross_entropy\")\n\n                self.cross_entropy = tf.reduce_mean(self.cross_entropy, name=\"cross_entropy\")\n                \n                optimizer = tf.train.AdamOptimizer(0.001)\n                self.optimize = optimizer.minimize(self.cross_entropy, name=\"optimize\")\n        \n                # EXAMINATION OPERATION\n                self.prediction = tf.nn.softmax(self.prediction)\n                self.correct_percentage = tf.to_int32(tf.argmax(self.prediction, axis=1))\n                self.correct_percentage = tf.equal(self.correct_percentage, self.labels)\n                self.correct_percentage = tf.reduce_mean(tf.to_float(self.correct_percentage))\n\n        self.sess = tf.Session()\n        self.reset()\n        #writer = tf.summary.FileWriter(\"./log\", self.sess.graph)\n        #writer.close()\n\n    def reset(self):\n        init_op = tf.global_variables_initializer()\n        self.sess.run(init_op)\n\n\n    def train(self, examples, labels, lengths):\n        feed = {\n            self.examples : examples,\n            self.labels : labels,\n            self.lengths : lengths,\n            self.keep_prob : 0.25,\n        }\n        _, ret, perc = self.sess.run([self.optimize, self.cross_entropy, self.correct_percentage], feed)\n        return ret, perc\n\n    def test(self, examples, labels, lengths):\n        feed = {\n            self.examples : examples,\n            self.labels : labels,\n            self.lengths : lengths,\n            self.keep_prob : 1.0,\n        }\n        return self.sess.run([self.cross_entropy, self.correct_percentage, self.prediction], feed)\n\n    def save(self):\n        save_dir = \"./save/\"\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        saver = tf.train.Saver()\n        saver.save(self.sess, save_dir + \"model\", global_step=self.step)\n        self.step += 1", "framework": "tensorflow"}
{"repo_name": "SwordYork/sequencing", "file_path": "ava_nmt/nmt_infer.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n#   Author: Sword York\n#   GitHub: https://github.com/SwordYork/sequencing\n#   No rights reserved.\n#\nimport argparse\nimport os\nfrom shutil import copyfile\n\nimport numpy\nimport tensorflow as tf\n\nimport config\nfrom build_inputs import build_source_char_inputs\nfrom build_model import build_attention_model\nfrom sequencing import TIME_MAJOR, MODE, optimistic_restore\n\ndef infer(src_vocab, src_data_file, trg_vocab,\n          params, beam_size=1, batch_size=1, max_step=100,\n          output_file='test.out', model_dir='models/'):\n\n    save_output_dir = 'dev_outputs/'\n    if not os.path.exists(save_output_dir):\n        os.makedirs(save_output_dir)\n\n    # ------------------------------------\n    # prepare data\n    # trg_data_file may be empty.\n    # ------------------------------------\n\n    # load parallel data\n    parallel_data_generator = \\\n        build_source_char_inputs(src_vocab, src_data_file,\n                                batch_size=batch_size, buffer_size=96,\n                                mode=MODE.INFER)\n\n    # ------------------------------------\n    # build model\n    # ------------------------------------\n\n    # placeholder\n    source_ids = tf.placeholder(tf.int32, shape=(None, None),\n                                name='source_ids')\n    source_seq_length = tf.placeholder(tf.int32, shape=(None,),\n                                       name='source_seq_length')\n    source_sample_matrix = tf.placeholder(tf.float32, shape=(None, None, None),\n                                       name='source_sample_matrix')\n    source_word_seq_length = tf.placeholder(tf.int32, shape=(None,),\n                                       name='source_word_seq_length')\n\n    target_ids = None\n    target_seq_length = None\n\n    source_placeholders = {'src': source_ids,\n                          'src_len': source_seq_length,\n                          'src_sample_matrix':source_sample_matrix,\n                          'src_word_len': source_word_seq_length}\n    target_placeholders = {'trg': target_ids,\n                          'trg_len': target_seq_length}\n\n\n    decoder_output_eval, decoder_final_state = \\\n        build_attention_model(params, src_vocab, trg_vocab,\n                              source_placeholders,\n                              target_placeholders,\n                              beam_size=beam_size, mode=MODE.INFER,\n                              max_step=max_step)\n\n    # GPU config\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        last_ckpt = tf.train.latest_checkpoint(model_dir)\n        if last_ckpt:\n            optimistic_restore(sess, last_ckpt)\n        else:\n            raise Exception('No checkpoint found ...')\n\n        output_file_name = os.path.join(save_output_dir,\n                                output_file + last_ckpt.split('-')[-1])\n        output_ = open(output_file_name, 'w')\n\n        for step, current_input in enumerate(parallel_data_generator):\n            current_input_dict = current_input._asdict()\n            feed_dict = {}\n            for key in source_placeholders.keys():\n                feed_dict[source_placeholders[key]] = current_input_dict[key]\n\n            # beam_ids_np: [seq_len, beam_size]\n            # predicted_ids_np: [seq_len, beam_size]\n            predicted_ids_np, beam_ids_np, log_probs_np = sess.run(\n                [decoder_output_eval.predicted_ids,\n                 decoder_output_eval.beam_ids,\n                 decoder_final_state.log_probs],\n                feed_dict=feed_dict)\n            \n            src_len_np = current_input_dict['src_len']\n            data_batch_size = len(src_len_np)\n\n            gathered_pred_ids = numpy.zeros_like(beam_ids_np)\n            for idx in range(beam_ids_np.shape[0]):\n                gathered_pred_ids = gathered_pred_ids[:, beam_ids_np[idx] %\n                                                         beam_ids_np.shape[1]]\n                gathered_pred_ids[idx, :] = predicted_ids_np[idx]\n\n            seq_lens = []\n            for idx in range(beam_ids_np.shape[1]):\n                pred_ids_list = gathered_pred_ids[:, idx].tolist()\n                seq_lens.append(pred_ids_list.index(trg_vocab.eos_id) + 1 \\\n                                    if trg_vocab.eos_id in pred_ids_list \\\n                                    else len(pred_ids_list))\n\n            log_probs_np = log_probs_np / numpy.array(seq_lens)\n            log_probs_np_list = numpy.split(log_probs_np, data_batch_size,\n                                            axis=0)\n            each_max_idx = [numpy.argmax(log_prob) + b * beam_size for\n                            b, log_prob in enumerate(log_probs_np_list)]\n\n            pids = gathered_pred_ids[:, each_max_idx]\n\n            for b in range(data_batch_size):\n                p = trg_vocab.id_to_token(pids[:, b].tolist())\n                if TIME_MAJOR:\n                    s = src_vocab.id_to_token(current_input_dict['src'][:, b].tolist())\n                else:\n                    s = src_vocab.id_to_token(current_input_dict['src'][b, :].tolist())\n                print('src:', s)\n                print('prd:', p)\n                print('---------------------------')\n                print('\\n')\n                output_.write(p + '\\n')\n            output_.flush()\n        output_.close()\n        copyfile(output_file_name, output_file)\n\n\nif __name__ == '__main__':\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    all_configs = [i for i in dir(config) if i.startswith('config_')]\n\n    parser = argparse.ArgumentParser(description='Sequencing Training ...')\n    parser.add_argument('--config', choices=all_configs,\n                        help='specific config name, like {}, '\n                             'see config.py'.format(all_configs),\n                        required=True)\n    parser.add_argument('--test-src', type=str,\n                        help='test src file')\n    parser.add_argument('--output-file', type=str,\n                        help='test output file',\n                        default='test.out')\n\n    args = parser.parse_args()\n    training_configs = getattr(config, args.config)()\n\n    test_src_file = args.test_src if args.test_src else training_configs.test_src_file\n\n    output_file = args.output_file\n\n    infer(training_configs.src_vocab, test_src_file,\n          training_configs.trg_vocab,\n          training_configs.params,\n          beam_size=training_configs.beam_size,\n          batch_size=training_configs.batch_size,\n          max_step=training_configs.max_step,\n          model_dir=training_configs.model_dir,\n          output_file=output_file)\n", "framework": "tensorflow"}
{"repo_name": "pcchenxi/baseline", "file_path": "baselines/acktr/acktr_cont.py", "content": "import numpy as np\nimport tensorflow as tf\nfrom baselines import logger\nfrom baselines import common\nfrom baselines.common import tf_util as U\nfrom baselines.acktr import kfac\nfrom baselines.acktr.filters import ZFilter\n\ndef pathlength(path):\n    return path[\"reward\"].shape[0]# Loss function that we'll differentiate to get the policy gradient\n\ndef rollout(env, policy, max_pathlength, animate=False, obfilter=None):\n    \"\"\"\n    Simulate the env and policy for max_pathlength steps\n    \"\"\"\n    ob = env.reset()\n    prev_ob = np.float32(np.zeros(ob.shape))\n    if obfilter: ob = obfilter(ob)\n    terminated = False\n\n    obs = []\n    acs = []\n    ac_dists = []\n    logps = []\n    rewards = []\n    for _ in range(max_pathlength):\n        if animate:\n            env.render()\n        state = np.concatenate([ob, prev_ob], -1)\n        obs.append(state)\n        ac, ac_dist, logp = policy.act(state)\n        acs.append(ac)\n        ac_dists.append(ac_dist)\n        logps.append(logp)\n        prev_ob = np.copy(ob)\n        scaled_ac = env.action_space.low + (ac + 1.) * 0.5 * (env.action_space.high - env.action_space.low)\n        scaled_ac = np.clip(scaled_ac, env.action_space.low, env.action_space.high)\n        ob, rew, done, _ = env.step(scaled_ac)\n        if obfilter: ob = obfilter(ob)\n        rewards.append(rew)\n        if done:\n            terminated = True\n            break\n    return {\"observation\" : np.array(obs), \"terminated\" : terminated,\n            \"reward\" : np.array(rewards), \"action\" : np.array(acs),\n            \"action_dist\": np.array(ac_dists), \"logp\" : np.array(logps)}\n\ndef learn(env, policy, vf, gamma, lam, timesteps_per_batch, num_timesteps,\n    animate=False, callback=None, desired_kl=0.002):\n\n    obfilter = ZFilter(env.observation_space.shape)\n\n    max_pathlength = env.spec.timestep_limit\n    stepsize = tf.Variable(initial_value=np.float32(np.array(0.03)), name='stepsize')\n    inputs, loss, loss_sampled = policy.update_info\n    optim = kfac.KfacOptimizer(learning_rate=stepsize, cold_lr=stepsize*(1-0.9), momentum=0.9, kfac_update=2,\\\n                                epsilon=1e-2, stats_decay=0.99, async=1, cold_iter=1,\n                                weight_decay_dict=policy.wd_dict, max_grad_norm=None)\n    pi_var_list = []\n    for var in tf.trainable_variables():\n        if \"pi\" in var.name:\n            pi_var_list.append(var)\n\n    update_op, q_runner = optim.minimize(loss, loss_sampled, var_list=pi_var_list)\n    do_update = U.function(inputs, update_op)\n    U.initialize()\n\n    # start queue runners\n    enqueue_threads = []\n    coord = tf.train.Coordinator()\n    for qr in [q_runner, vf.q_runner]:\n        assert (qr != None)\n        enqueue_threads.extend(qr.create_threads(U.get_session(), coord=coord, start=True))\n\n    i = 0\n    timesteps_so_far = 0\n    while True:\n        if timesteps_so_far > num_timesteps:\n            break\n        logger.log(\"********** Iteration %i ************\"%i)\n\n        # Collect paths until we have enough timesteps\n        timesteps_this_batch = 0\n        paths = []\n        while True:\n            path = rollout(env, policy, max_pathlength, animate=(len(paths)==0 and (i % 10 == 0) and animate), obfilter=obfilter)\n            paths.append(path)\n            n = pathlength(path)\n            timesteps_this_batch += n\n            timesteps_so_far += n\n            if timesteps_this_batch > timesteps_per_batch:\n                break\n\n        # Estimate advantage function\n        vtargs = []\n        advs = []\n        for path in paths:\n            rew_t = path[\"reward\"]\n            return_t = common.discount(rew_t, gamma)\n            vtargs.append(return_t)\n            vpred_t = vf.predict(path)\n            vpred_t = np.append(vpred_t, 0.0 if path[\"terminated\"] else vpred_t[-1])\n            delta_t = rew_t + gamma*vpred_t[1:] - vpred_t[:-1]\n            adv_t = common.discount(delta_t, gamma * lam)\n            advs.append(adv_t)\n        # Update value function\n        vf.fit(paths, vtargs)\n\n        # Build arrays for policy update\n        ob_no = np.concatenate([path[\"observation\"] for path in paths])\n        action_na = np.concatenate([path[\"action\"] for path in paths])\n        oldac_dist = np.concatenate([path[\"action_dist\"] for path in paths])\n        logp_n = np.concatenate([path[\"logp\"] for path in paths])\n        adv_n = np.concatenate(advs)\n        standardized_adv_n = (adv_n - adv_n.mean()) / (adv_n.std() + 1e-8)\n\n        # Policy update\n        do_update(ob_no, action_na, standardized_adv_n)\n\n        min_stepsize = np.float32(1e-8)\n        max_stepsize = np.float32(1e0)\n        # Adjust stepsize\n        kl = policy.compute_kl(ob_no, oldac_dist)\n        if kl > desired_kl * 2:\n            logger.log(\"kl too high\")\n            U.eval(tf.assign(stepsize, tf.maximum(min_stepsize, stepsize / 1.5)))\n        elif kl < desired_kl / 2:\n            logger.log(\"kl too low\")\n            U.eval(tf.assign(stepsize, tf.minimum(max_stepsize, stepsize * 1.5)))\n        else:\n            logger.log(\"kl just right!\")\n\n        logger.record_tabular(\"EpRewMean\", np.mean([path[\"reward\"].sum() for path in paths]))\n        logger.record_tabular(\"EpRewSEM\", np.std([path[\"reward\"].sum()/np.sqrt(len(paths)) for path in paths]))\n        logger.record_tabular(\"EpLenMean\", np.mean([pathlength(path) for path in paths]))\n        logger.record_tabular(\"KL\", kl)\n        if callback:\n            callback()\n        logger.dump_tabular()\n        i += 1\n", "framework": "tensorflow"}
{"repo_name": "shygiants/ChangeGAN", "file_path": "change-gan/change-gan/models/change_gan.py", "content": "\"\"\" Contains the definition of the ChangeGAN architecture. \"\"\"\nimport multiprocessing\n\nimport tensorflow as tf\n\nfrom gan_utils import encoder, decoder, transformer, discriminator, preprocess_image\n\nslim = tf.contrib.slim\n\ndefault_image_size = 256\n\n\ndef model_fn(inputs_a, inputs_b, learning_rate, num_blocks=9, is_training=True, scope=None, weight_decay=0.0001):\n    encoder_dims = [32, 64, 128]\n    deep_encoder_dims = [64, 128, 256]\n    decoder_dims = [64, 32, 3]\n    deep_decoder_dims = [128, 64, 3]\n    with tf.variable_scope(scope, 'ChangeGAN', [inputs_a, inputs_b]):\n        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n                            weights_regularizer=slim.l2_regularizer(weight_decay)):\n            with slim.arg_scope([slim.batch_norm],\n                                is_training=True):\n                def converter_ab(inputs_a, reuse=None):\n                    ################\n                    # Encoder part #\n                    ################\n                    z_a = encoder(inputs_a, deep_encoder_dims, scope='Encoder_A', reuse=reuse)\n\n                    # z_a is split into c_b, z_a-b\n                    c_b, z_a_b = tf.split(z_a, num_or_size_splits=2, axis=3)\n\n                    ####################\n                    # Transformer part #\n                    ####################\n                    c_b = transformer(c_b, encoder_dims[-1], num_blocks=num_blocks,\n                                      scope='Transformer_B', reuse=reuse)\n\n                    ################\n                    # Decoder part #\n                    ################\n                    outputs_b = decoder(c_b, decoder_dims, scope='Decoder_B', reuse=reuse)\n\n                    return outputs_b, z_a_b\n\n                def converter_ba(inputs_b, z_a_b, reuse=None):\n                    ################\n                    # Encoder part #\n                    ################\n                    z_b = encoder(inputs_b, encoder_dims, scope='Encoder_B', reuse=reuse)\n\n                    # Concat z_b and z_a-b\n                    c_a = tf.concat([z_b, z_a_b], 3)\n\n                    ####################\n                    # Transformer part #\n                    ####################\n                    c_a = transformer(c_a, deep_encoder_dims[-1], num_blocks=num_blocks,\n                                      scope='Transformer_A', reuse=reuse)\n\n                    ################\n                    # Decoder part #\n                    ################\n                    outputs_a = decoder(c_a, deep_decoder_dims, scope='Decoder_A', reuse=reuse)\n\n                    return outputs_a\n\n                bbox_channel_a = _get_bbox(inputs_a)\n\n                outputs_ab, z_a_b = converter_ab(inputs_a)\n                outputs_bbox_ab = tf.concat([outputs_ab, bbox_channel_a], 3)\n                outputs_ba = converter_ba(inputs_b, z_a_b)\n                outputs_bbox_ba = tf.concat([outputs_ba, bbox_channel_a], 3)\n\n                outputs_bab, _ = converter_ab(outputs_bbox_ba, reuse=True)\n                outputs_aba = converter_ba(outputs_bbox_ab, z_a_b, reuse=True)\n\n                logits_a_real, probs_a_real = discriminator(inputs_a, deep_encoder_dims, scope='Discriminator_A')\n                logits_a_fake, probs_a_fake = discriminator(outputs_bbox_ba, deep_encoder_dims, scope='Discriminator_A', reuse=True)\n                logits_b_real, probs_b_real = discriminator(inputs_b, deep_encoder_dims, scope='Discriminator_B')\n                logits_b_fake, probs_b_fake = discriminator(outputs_bbox_ab, deep_encoder_dims, scope='Discriminator_B', reuse=True)\n\n                outputs = [inputs_a, inputs_b, outputs_ba, outputs_ab, outputs_aba, outputs_bab]\n                outputs = map(lambda image: tf.image.convert_image_dtype(image, dtype=tf.uint8), outputs)\n\n    with tf.name_scope('images'):\n        tf.summary.image('X_A', _remove_bbox(inputs_a))\n        tf.summary.image('X_B', _remove_bbox(inputs_b))\n        tf.summary.image('X_BA', outputs_ba)\n        tf.summary.image('X_AB', outputs_ab)\n        tf.summary.image('X_ABA', outputs_aba)\n        tf.summary.image('X_BAB', outputs_bab)\n\n    global_step = tf.train.get_or_create_global_step()\n\n    if not is_training:\n        return outputs\n\n    t_vars = tf.trainable_variables()\n\n    d_a_vars = [var for var in t_vars if 'Discriminator_A' in var.name]\n    d_b_vars = [var for var in t_vars if 'Discriminator_B' in var.name]\n    g_vars = [var for var in t_vars if 'coder' in var.name or 'Transformer' in var.name]\n\n    ##########\n    # Losses #\n    ##########\n    # Losses for discriminator\n    l_d_a_fake = tf.reduce_mean(tf.square(logits_a_fake))\n    l_d_a_real = tf.reduce_mean(tf.squared_difference(logits_a_real, 1.))\n    l_d_a = 0.5 * (l_d_a_fake + l_d_a_real)\n    train_op_d_a = tf.train.AdamOptimizer(\n        learning_rate=learning_rate,\n        beta1=0.5,\n        beta2=0.999\n    ).minimize(l_d_a, global_step=global_step, var_list=d_a_vars)\n\n    l_d_b_fake = tf.reduce_mean(tf.square(logits_b_fake))\n    l_d_b_real = tf.reduce_mean(tf.squared_difference(logits_b_real, 1.))\n    l_d_b = 0.5 * (l_d_b_fake + l_d_b_real)\n    train_op_d_b = tf.train.AdamOptimizer(\n        learning_rate=learning_rate,\n        beta1=0.5,\n        beta2=0.999\n    ).minimize(l_d_b, global_step=global_step, var_list=d_b_vars)\n\n    l_d = l_d_a + l_d_b\n\n    # Losses for generators\n    l_g_a = tf.reduce_mean(tf.squared_difference(logits_a_fake, 1.))\n    l_g_b = tf.reduce_mean(tf.squared_difference(logits_b_fake, 1.))\n    l_const_a = tf.reduce_mean(tf.losses.absolute_difference(_remove_bbox(inputs_a), outputs_aba))\n    l_const_b = tf.reduce_mean(tf.losses.absolute_difference(_remove_bbox(inputs_b), outputs_bab))\n\n    l_g = l_g_a + l_g_b + 10. * (l_const_a + l_const_b)\n    train_op_g = tf.train.AdamOptimizer(\n        learning_rate=learning_rate,\n        beta1=0.5,\n        beta2=0.999\n    ).minimize(l_g, global_step=global_step, var_list=g_vars)\n\n    with tf.name_scope('losses'):\n        tf.summary.scalar('L_D_A_Fake', l_d_a_fake)\n        tf.summary.scalar('L_D_A_Real', l_d_a_real)\n        tf.summary.scalar('L_D_A', l_d_a)\n        tf.summary.scalar('L_D_B_Fake', l_d_b_fake)\n        tf.summary.scalar('L_D_B_Real', l_d_b_real)\n        tf.summary.scalar('L_D_B', l_d_b)\n        tf.summary.scalar('L_D', l_d)\n\n        tf.summary.scalar('L_G_A', l_g_a)\n        tf.summary.scalar('L_G_B', l_g_b)\n        tf.summary.scalar('L_Const_A', l_const_a)\n        tf.summary.scalar('L_Const_B', l_const_b)\n        tf.summary.scalar('L_G', l_g)\n\n    train_op = tf.group(*[train_op_d_a, train_op_d_b, train_op_g])\n\n    return train_op, global_step, outputs\n\n\ndef input_fn(dataset_a, dataset_b, batch_size=1, num_readers=4, is_training=True):\n    provider_a = slim.dataset_data_provider.DatasetDataProvider(\n        dataset_a,\n        num_readers=num_readers,\n        common_queue_capacity=20 * batch_size,\n        common_queue_min=10 * batch_size)\n    provider_b = slim.dataset_data_provider.DatasetDataProvider(\n        dataset_b,\n        num_readers=num_readers,\n        common_queue_capacity=20 * batch_size,\n        common_queue_min=10 * batch_size)\n    [image_a, bbox_a] = provider_a.get(['image', 'object/bbox'])\n    [image_b, bbox_b] = provider_b.get(['image', 'object/bbox'])\n\n    train_image_size = default_image_size\n\n    def add_channel(image, bbox, padding='ZERO'):\n        ymin = bbox[0]\n        xmin = bbox[1]\n        ymax = bbox[2]\n        xmax = bbox[3]\n\n        image_shape = tf.to_float(tf.shape(image))\n        height = image_shape[0]\n        width = image_shape[1]\n\n        bbox_height = (ymax - ymin) * height\n        bbox_width = (xmax - xmin) * width\n        channel = tf.ones(tf.to_int32(tf.stack([bbox_height, bbox_width])))\n        channel = tf.expand_dims(channel, axis=2)\n\n        pad_top = tf.to_int32(ymin * height)\n        pad_left = tf.to_int32(xmin * width)\n        height = tf.to_int32(height)\n        width = tf.to_int32(width)\n        channel = tf.image.pad_to_bounding_box(channel, pad_top, pad_left, height, width)\n        # TODO: Decide pad one or zero\n        if padding == 'ONE':\n            channel = tf.ones_like(channel) - channel\n\n        image = tf.concat([image, channel], axis=2)\n\n        return image\n\n    image_a = tf.image.convert_image_dtype(image_a, dtype=tf.float32)\n    image_b = tf.image.convert_image_dtype(image_b, dtype=tf.float32)\n    # [Num of boxes, 4] => [4]\n    bbox_a = tf.squeeze(bbox_a, axis=0)\n    bbox_b = tf.squeeze(bbox_b, axis=0)\n    # Add bound box as 4th channel\n    image_a = add_channel(image_a, bbox_a)\n    image_b = add_channel(image_b, bbox_b)\n\n    image_space_a = Image(image_a, bbox_a)\n    image_space_b = Image(image_b, bbox_b)\n\n    # Resize image B\n    ratio = image_space_a.bbox_height / image_space_b.bbox_height\n    image_space_b.resize(ratio)\n\n    # Shift image B to fit bboxes of two images\n    pixel_shift = image_space_a.translate2pxl(image_space_a.bbox_center) - \\\n                  image_space_b.translate2pxl(image_space_b.bbox_center)\n\n    # Calculate ymin and xmin\n    crop_top = tf.less(pixel_shift[0], 0)\n    pad_y = tf.cond(crop_top, true_fn=lambda: 0, false_fn=lambda: pixel_shift[0])\n    crop_ymin = tf.cond(crop_top,\n                        true_fn=lambda: image_space_b.translate2coor(pixel_y=tf.negative(pixel_shift[0])),\n                        false_fn=lambda: 0.)\n    crop_left = tf.less(pixel_shift[1], 0)\n    pad_x = tf.cond(crop_left, true_fn=lambda: 0, false_fn=lambda: pixel_shift[1])\n    crop_xmin = tf.cond(crop_left,\n                        true_fn=lambda: image_space_b.translate2coor(pixel_x=tf.negative(pixel_shift[1])),\n                        false_fn=lambda: 0.)\n\n    # Calculate ymax and xmax\n    over_y = pixel_shift[0] + image_space_b.height - image_space_a.height\n    crop_bottom = tf.greater(over_y, 0)\n    crop_ymax = tf.cond(crop_bottom,\n                        true_fn=lambda: 1. - image_space_b.translate2coor(pixel_y=over_y),\n                        false_fn=lambda: 1.)\n    over_x = pixel_shift[1] + image_space_b.width - image_space_a.width\n    crop_right = tf.greater(over_x, 0)\n    crop_xmax = tf.cond(crop_right,\n                        true_fn=lambda: 1. - image_space_b.translate2coor(pixel_x=over_x),\n                        false_fn=lambda: 1.)\n\n    # Resize, Crop, Pad\n    image_b_cropped = image_space_b.crop(crop_ymin, crop_xmin, crop_ymax, crop_xmax)\n\n    def pad_to_bounding_box(image):\n        return tf.image.pad_to_bounding_box(image, pad_y, pad_x, image_space_a.height, image_space_a.width)\n\n    # Pad differently depending on type of channel\n    image_b_cropped, bbox_channel = _split_image_bbox(image_b_cropped)\n\n    # One padding for RGB\n    rgb_padding = pad_to_bounding_box(tf.ones_like(image_b_cropped))\n    rgb_padding = tf.ones_like(rgb_padding) - rgb_padding\n    # Sample background color and pad\n    rgb_padding *= image_b_cropped[0, 0]\n\n    # Pad for RGB\n    image_b = pad_to_bounding_box(image_b_cropped) + rgb_padding\n\n    # Zero padding for bbox channel\n    bbox_channel = pad_to_bounding_box(bbox_channel)\n\n    # Concat RGB and bbox channel\n    image_b = tf.concat([image_b, bbox_channel], axis=2)\n\n    # Preprocess images\n    image_a = _preprocess_image(image_a, train_image_size, train_image_size, is_training=is_training)\n    image_b = _preprocess_image(image_b, train_image_size, train_image_size, is_training=is_training)\n\n    images_a, images_b, bboxes_a, bboxes_b = tf.train.batch(\n        [image_a, image_b, bbox_a, bbox_b],\n        batch_size=batch_size,\n        num_threads=multiprocessing.cpu_count(),\n        capacity=5 * batch_size)\n\n    batch_queue = slim.prefetch_queue.prefetch_queue(\n        [images_a, images_b, bboxes_a, bboxes_b], capacity=2)\n    images_a, images_b, bboxes_a, bboxes_b = batch_queue.dequeue()\n\n    with tf.name_scope('inputs'):\n        tf.summary.image('X_A', _remove_bbox(images_a))\n        tf.summary.image('X_A_BBox', images_a)\n        tf.summary.image('X_B', _remove_bbox(images_b))\n        tf.summary.image('X_B_BBox', images_b)\n\n    return images_a, images_b, bboxes_a, bboxes_b\n\n\ndef _preprocess_image(image, height, width, is_training=True):\n    if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Central square crop and resize\n    shape = tf.to_float(tf.shape(image))\n    original_height = shape[0]\n    original_width = shape[1]\n    rest = (1. - original_width / original_height) / 2.\n    image = tf.expand_dims(image, 0)\n    images = tf.image.crop_and_resize(image,\n                                      [[rest, 0., 1. - rest, 1.]], [0],\n                                      [height, width])\n    image = tf.squeeze(images, [0])\n    # image = tf.image.resize_images(image, [height, width])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n\n    return image\n\n\ndef _split_image_bbox(image_bbox):\n    image, bbox = tf.split(image_bbox, [3, 1], axis=image_bbox.shape.ndims - 1)\n    return image, bbox\n\n\ndef _remove_bbox(image_bbox):\n    image, bbox = _split_image_bbox(image_bbox)\n    return image\n\n\ndef _get_bbox(image_bbox):\n    image, bbox = _split_image_bbox(image_bbox)\n    return bbox\n\n\nclass Image:\n    def __init__(self, image, bbox):\n        self._image = image\n        self._image_shape = tf.to_float(tf.shape(image))\n        self._height = self._image_shape[0]\n        self._width = self._image_shape[1]\n\n        self._ratio = None\n\n        self._bbox = bbox\n        self._ymin = bbox[0]\n        self._xmin = bbox[1]\n        self._ymax = bbox[2]\n        self._xmax = bbox[3]\n        self._bbox_height = (self._ymax - self._ymin) * self._height\n        self._bbox_width = (self._xmax - self._xmin) * self._width\n\n        self._center_y = (self._ymin + self._ymax) / 2.\n        self._center_x = (self._xmin + self._xmax) / 2.\n\n\n    @property\n    def image(self):\n        return self._image\n\n    @property\n    def height(self):\n        height = self._height\n        if self._ratio is not None:\n            height *= self._ratio\n        return tf.to_int32(height)\n\n    @property\n    def width(self):\n        width = self._width\n        if self._ratio is not None:\n            width *= self._ratio\n        return tf.to_int32(width)\n\n    @property\n    def bbox_height(self):\n        return self._bbox_height\n\n    @property\n    def bbox_center(self):\n        return tf.stack([self._center_y, self._center_x])\n\n    def resize(self, ratio):\n        self._ratio = ratio\n\n    def translate2pxl(self, coor):\n        if coor.dtype != tf.float32:\n            coor = tf.to_float(coor)\n        pixel = coor * self._image_shape[:2]\n        if self._ratio is not None:\n            pixel *= self._ratio\n        return tf.to_int32(pixel)\n\n    def translate2coor(self, pixel_y=None, pixel_x=None):\n        if pixel_y is None and pixel_x is None:\n            raise ValueError\n        if pixel_y is not None and pixel_x is not None:\n            raise ValueError\n\n        divisor = self._image_shape[0 if pixel_y is not None else 1]\n        pixel = pixel_y if pixel_y is not None else pixel_x\n\n        if pixel.dtype != tf.float32:\n            pixel = tf.to_float(pixel)\n\n        if self._ratio is not None:\n            divisor *= self._ratio\n        coor = pixel / divisor\n        return coor\n\n    def crop(self, ymin, xmin, ymax, xmax):\n        image = self._image\n        if self._ratio is not None:\n            target_shape = tf.to_int32(self._image_shape[:2] * self._ratio)\n            image = tf.image.resize_images(image, target_shape)\n\n        shape = tf.to_float(tf.shape(image))\n        height = shape[0]\n        width = shape[1]\n\n        offset_height = tf.to_int32(ymin * height)\n        offset_width = tf.to_int32(xmin * width)\n        target_height = tf.to_int32((ymax - ymin) * height)\n        target_width = tf.to_int32((xmax - xmin) * width)\n        image = tf.image.crop_to_bounding_box(image,\n                                              offset_height,\n                                              offset_width,\n                                              target_height,\n                                              target_width)\n\n        return image\n", "framework": "tensorflow"}
{"repo_name": "wujinjun/TFbook", "file_path": "chapter5/models-master/learning_to_remember_rare_events/memory.py", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\"\"\"Memory module for storing \"nearest neighbors\".\n\nImplements a key-value memory for generalized one-shot learning\nas described in the paper\n\"Learning to Remember Rare Events\"\nby Lukasz Kaiser, Ofir Nachum, Aurko Roy, Samy Bengio,\npublished as a conference paper at ICLR 2017.\n\"\"\"\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Memory(object):\n  \"\"\"Memory module.\"\"\"\n\n  def __init__(self, key_dim, memory_size, vocab_size,\n               choose_k=256, alpha=0.1, correct_in_top=1, age_noise=8.0,\n               var_cache_device='', nn_device=''):\n    self.key_dim = key_dim\n    self.memory_size = memory_size\n    self.vocab_size = vocab_size\n    self.choose_k = min(choose_k, memory_size)\n    self.alpha = alpha\n    self.correct_in_top = correct_in_top\n    self.age_noise = age_noise\n    self.var_cache_device = var_cache_device  # Variables are cached here.\n    self.nn_device = nn_device  # Device to perform nearest neighbour matmul.\n\n    caching_device = var_cache_device if var_cache_device else None\n    self.update_memory = tf.constant(True)  # Can be fed \"false\" if needed.\n    self.mem_keys = tf.get_variable(\n        'memkeys', [self.memory_size, self.key_dim], trainable=False,\n        initializer=tf.random_uniform_initializer(-0.0, 0.0),\n        caching_device=caching_device)\n    self.mem_vals = tf.get_variable(\n        'memvals', [self.memory_size], dtype=tf.int32, trainable=False,\n        initializer=tf.constant_initializer(0, tf.int32),\n        caching_device=caching_device)\n    self.mem_age = tf.get_variable(\n        'memage', [self.memory_size], dtype=tf.float32, trainable=False,\n        initializer=tf.constant_initializer(0.0), caching_device=caching_device)\n    self.recent_idx = tf.get_variable(\n        'recent_idx', [self.vocab_size], dtype=tf.int32, trainable=False,\n        initializer=tf.constant_initializer(0, tf.int32))\n\n    # variable for projecting query vector into memory key\n    self.query_proj = tf.get_variable(\n        'memory_query_proj', [self.key_dim, self.key_dim], dtype=tf.float32,\n        initializer=tf.truncated_normal_initializer(0, 0.01),\n        caching_device=caching_device)\n\n  def get(self):\n    return self.mem_keys, self.mem_vals, self.mem_age, self.recent_idx\n\n  def set(self, k, v, a, r=None):\n    return tf.group(\n        self.mem_keys.assign(k),\n        self.mem_vals.assign(v),\n        self.mem_age.assign(a),\n        (self.recent_idx.assign(r) if r is not None else tf.group()))\n\n  def clear(self):\n    return tf.variables_initializer([self.mem_keys, self.mem_vals, self.mem_age,\n                                     self.recent_idx])\n\n  def get_hint_pool_idxs(self, normalized_query):\n    \"\"\"Get small set of idxs to compute nearest neighbor queries on.\n\n    This is an expensive look-up on the whole memory that is used to\n    avoid more expensive operations later on.\n\n    Args:\n      normalized_query: A Tensor of shape [None, key_dim].\n\n    Returns:\n      A Tensor of shape [None, choose_k] of indices in memory\n      that are closest to the queries.\n\n    \"\"\"\n    # look up in large memory, no gradients\n    with tf.device(self.nn_device):\n      similarities = tf.matmul(tf.stop_gradient(normalized_query),\n                               self.mem_keys, transpose_b=True, name='nn_mmul')\n    _, hint_pool_idxs = tf.nn.top_k(\n        tf.stop_gradient(similarities), k=self.choose_k, name='nn_topk')\n    return hint_pool_idxs\n\n  def make_update_op(self, upd_idxs, upd_keys, upd_vals,\n                     batch_size, use_recent_idx, intended_output):\n    \"\"\"Function that creates all the update ops.\"\"\"\n    mem_age_incr = self.mem_age.assign_add(tf.ones([self.memory_size],\n                                                   dtype=tf.float32))\n    with tf.control_dependencies([mem_age_incr]):\n      mem_age_upd = tf.scatter_update(\n          self.mem_age, upd_idxs, tf.zeros([batch_size], dtype=tf.float32))\n\n    mem_key_upd = tf.scatter_update(\n        self.mem_keys, upd_idxs, upd_keys)\n    mem_val_upd = tf.scatter_update(\n        self.mem_vals, upd_idxs, upd_vals)\n\n    if use_recent_idx:\n      recent_idx_upd = tf.scatter_update(\n          self.recent_idx, intended_output, upd_idxs)\n    else:\n      recent_idx_upd = tf.group()\n\n    return tf.group(mem_age_upd, mem_key_upd, mem_val_upd, recent_idx_upd)\n\n  def query(self, query_vec, intended_output, use_recent_idx=True):\n    \"\"\"Queries memory for nearest neighbor.\n\n    Args:\n      query_vec: A batch of vectors to query (embedding of input to model).\n      intended_output: The values that would be the correct output of the\n        memory.\n      use_recent_idx: Whether to always insert at least one instance of a\n        correct memory fetch.\n\n    Returns:\n      A tuple (result, mask, teacher_loss).\n      result: The result of the memory look up.\n      mask: The affinity of the query to the result.\n      teacher_loss: The loss for training the memory module.\n    \"\"\"\n\n    batch_size = tf.shape(query_vec)[0]\n    output_given = intended_output is not None\n\n    # prepare query for memory lookup\n    query_vec = tf.matmul(query_vec, self.query_proj)\n    normalized_query = tf.nn.l2_normalize(query_vec, dim=1)\n\n    hint_pool_idxs = self.get_hint_pool_idxs(normalized_query)\n\n    if output_given and use_recent_idx:  # add at least one correct memory\n      most_recent_hint_idx = tf.gather(self.recent_idx, intended_output)\n      hint_pool_idxs = tf.concat(\n          axis=1,\n          values=[hint_pool_idxs, tf.expand_dims(most_recent_hint_idx, 1)])\n    choose_k = tf.shape(hint_pool_idxs)[1]\n\n    with tf.device(self.var_cache_device):\n      # create small memory and look up with gradients\n      my_mem_keys = tf.stop_gradient(tf.gather(self.mem_keys, hint_pool_idxs,\n                                               name='my_mem_keys_gather'))\n      similarities = tf.matmul(tf.expand_dims(normalized_query, 1),\n                               my_mem_keys, adjoint_b=True, name='batch_mmul')\n      hint_pool_sims = tf.squeeze(similarities, [1], name='hint_pool_sims')\n      hint_pool_mem_vals = tf.gather(self.mem_vals, hint_pool_idxs,\n                                     name='hint_pool_mem_vals')\n    # Calculate softmax mask on the top-k if requested.\n    # Softmax temperature. Say we have K elements at dist x and one at (x+a).\n    # Softmax of the last is e^tm(x+a)/Ke^tm*x + e^tm(x+a) = e^tm*a/K+e^tm*a.\n    # To make that 20% we'd need to have e^tm*a ~= 0.2K, so tm = log(0.2K)/a.\n    softmax_temp = max(1.0, np.log(0.2 * self.choose_k) / self.alpha)\n    mask = tf.nn.softmax(hint_pool_sims[:, :choose_k - 1] * softmax_temp)\n\n    # prepare hints from the teacher on hint pool\n    teacher_hints = tf.to_float(\n        tf.abs(tf.expand_dims(intended_output, 1) - hint_pool_mem_vals))\n    teacher_hints = 1.0 - tf.minimum(1.0, teacher_hints)\n\n    teacher_vals, teacher_hint_idxs = tf.nn.top_k(\n        hint_pool_sims * teacher_hints, k=1)\n    neg_teacher_vals, _ = tf.nn.top_k(\n        hint_pool_sims * (1 - teacher_hints), k=1)\n\n    # bring back idxs to full memory\n    teacher_idxs = tf.gather(\n        tf.reshape(hint_pool_idxs, [-1]),\n        teacher_hint_idxs[:, 0] + choose_k * tf.range(batch_size))\n\n    # zero-out teacher_vals if there are no hints\n    teacher_vals *= (\n        1 - tf.to_float(tf.equal(0.0, tf.reduce_sum(teacher_hints, 1))))\n\n    # prepare returned values\n    nearest_neighbor = tf.to_int32(\n        tf.argmax(hint_pool_sims[:, :choose_k - 1], 1))\n    no_teacher_idxs = tf.gather(\n        tf.reshape(hint_pool_idxs, [-1]),\n        nearest_neighbor + choose_k * tf.range(batch_size))\n\n    # we'll determine whether to do an update to memory based on whether\n    # memory was queried correctly\n    sliced_hints = tf.slice(teacher_hints, [0, 0], [-1, self.correct_in_top])\n    incorrect_memory_lookup = tf.equal(0.0, tf.reduce_sum(sliced_hints, 1))\n\n    # loss based on triplet loss\n    teacher_loss = (tf.nn.relu(neg_teacher_vals - teacher_vals + self.alpha)\n                    - self.alpha)\n\n    with tf.device(self.var_cache_device):\n      result = tf.gather(self.mem_vals, tf.reshape(no_teacher_idxs, [-1]))\n\n    # prepare memory updates\n    update_keys = normalized_query\n    update_vals = intended_output\n\n    fetched_idxs = teacher_idxs  # correctly fetched from memory\n    with tf.device(self.var_cache_device):\n      fetched_keys = tf.gather(self.mem_keys, fetched_idxs, name='fetched_keys')\n      fetched_vals = tf.gather(self.mem_vals, fetched_idxs, name='fetched_vals')\n\n    # do memory updates here\n    fetched_keys_upd = update_keys + fetched_keys  # Momentum-like update\n    fetched_keys_upd = tf.nn.l2_normalize(fetched_keys_upd, dim=1)\n    # Randomize age a bit, e.g., to select different ones in parallel workers.\n    mem_age_with_noise = self.mem_age + tf.random_uniform(\n        [self.memory_size], - self.age_noise, self.age_noise)\n\n    _, oldest_idxs = tf.nn.top_k(mem_age_with_noise, k=batch_size, sorted=False)\n\n    with tf.control_dependencies([result]):\n      upd_idxs = tf.where(incorrect_memory_lookup,\n                          oldest_idxs,\n                          fetched_idxs)\n      # upd_idxs = tf.Print(upd_idxs, [upd_idxs], \"UPD IDX\", summarize=8)\n      upd_keys = tf.where(incorrect_memory_lookup,\n                          update_keys,\n                          fetched_keys_upd)\n      upd_vals = tf.where(incorrect_memory_lookup,\n                          update_vals,\n                          fetched_vals)\n\n    def make_update_op():\n      return self.make_update_op(upd_idxs, upd_keys, upd_vals,\n                                 batch_size, use_recent_idx, intended_output)\n\n    update_op = tf.cond(self.update_memory, make_update_op, tf.no_op)\n\n    with tf.control_dependencies([update_op]):\n      result = tf.identity(result)\n      mask = tf.identity(mask)\n      teacher_loss = tf.identity(teacher_loss)\n\n    return result, mask, tf.reduce_mean(teacher_loss)\n\n\nclass LSHMemory(Memory):\n  \"\"\"Memory employing locality sensitive hashing.\n\n  Note: Not fully tested.\n  \"\"\"\n\n  def __init__(self, key_dim, memory_size, vocab_size,\n               choose_k=256, alpha=0.1, correct_in_top=1, age_noise=8.0,\n               var_cache_device='', nn_device='',\n               num_hashes=None, num_libraries=None):\n    super(LSHMemory, self).__init__(\n        key_dim, memory_size, vocab_size,\n        choose_k=choose_k, alpha=alpha, correct_in_top=1, age_noise=age_noise,\n        var_cache_device=var_cache_device, nn_device=nn_device)\n\n    self.num_libraries = num_libraries or int(self.choose_k ** 0.5)\n    self.num_per_hash_slot = max(1, self.choose_k // self.num_libraries)\n    self.num_hashes = (num_hashes or\n                       int(np.log2(self.memory_size / self.num_per_hash_slot)))\n    self.num_hashes = min(max(self.num_hashes, 1), 20)\n    self.num_hash_slots = 2 ** self.num_hashes\n\n    # hashing vectors\n    self.hash_vecs = [\n        tf.get_variable(\n            'hash_vecs%d' % i, [self.num_hashes, self.key_dim],\n            dtype=tf.float32, trainable=False,\n            initializer=tf.truncated_normal_initializer(0, 1))\n        for i in xrange(self.num_libraries)]\n\n    # map representing which hash slots map to which mem keys\n    self.hash_slots = [\n        tf.get_variable(\n            'hash_slots%d' % i, [self.num_hash_slots, self.num_per_hash_slot],\n            dtype=tf.int32, trainable=False,\n            initializer=tf.random_uniform_initializer(maxval=self.memory_size,\n                                                      dtype=tf.int32))\n        for i in xrange(self.num_libraries)]\n\n  def get(self):  # not implemented\n    return self.mem_keys, self.mem_vals, self.mem_age, self.recent_idx\n\n  def set(self, k, v, a, r=None):  # not implemented\n    return tf.group(\n        self.mem_keys.assign(k),\n        self.mem_vals.assign(v),\n        self.mem_age.assign(a),\n        (self.recent_idx.assign(r) if r is not None else tf.group()))\n\n  def clear(self):\n    return tf.variables_initializer([self.mem_keys, self.mem_vals, self.mem_age,\n                                     self.recent_idx] + self.hash_slots)\n\n  def get_hash_slots(self, query):\n    \"\"\"Gets hashed-to buckets for batch of queries.\n\n    Args:\n      query: 2-d Tensor of query vectors.\n\n    Returns:\n      A list of hashed-to buckets for each hash function.\n    \"\"\"\n\n    binary_hash = [\n        tf.less(tf.matmul(query, self.hash_vecs[i], transpose_b=True), 0)\n        for i in xrange(self.num_libraries)]\n    hash_slot_idxs = [\n        tf.reduce_sum(\n            tf.to_int32(binary_hash[i]) *\n            tf.constant([[2 ** i for i in xrange(self.num_hashes)]],\n                        dtype=tf.int32), 1)\n        for i in xrange(self.num_libraries)]\n    return hash_slot_idxs\n\n  def get_hint_pool_idxs(self, normalized_query):\n    \"\"\"Get small set of idxs to compute nearest neighbor queries on.\n\n    This is an expensive look-up on the whole memory that is used to\n    avoid more expensive operations later on.\n\n    Args:\n      normalized_query: A Tensor of shape [None, key_dim].\n\n    Returns:\n      A Tensor of shape [None, choose_k] of indices in memory\n      that are closest to the queries.\n\n    \"\"\"\n    # get hash of query vecs\n    hash_slot_idxs = self.get_hash_slots(normalized_query)\n\n    # grab mem idxs in the hash slots\n    hint_pool_idxs = [\n        tf.maximum(tf.minimum(\n            tf.gather(self.hash_slots[i], idxs),\n            self.memory_size - 1), 0)\n        for i, idxs in enumerate(hash_slot_idxs)]\n\n    return tf.concat(axis=1, values=hint_pool_idxs)\n\n  def make_update_op(self, upd_idxs, upd_keys, upd_vals,\n                     batch_size, use_recent_idx, intended_output):\n    \"\"\"Function that creates all the update ops.\"\"\"\n    base_update_op = super(LSHMemory, self).make_update_op(\n        upd_idxs, upd_keys, upd_vals,\n        batch_size, use_recent_idx, intended_output)\n\n    # compute hash slots to be updated\n    hash_slot_idxs = self.get_hash_slots(upd_keys)\n\n    # make updates\n    update_ops = []\n    with tf.control_dependencies([base_update_op]):\n      for i, slot_idxs in enumerate(hash_slot_idxs):\n        # for each slot, choose which entry to replace\n        entry_idx = tf.random_uniform([batch_size],\n                                      maxval=self.num_per_hash_slot,\n                                      dtype=tf.int32)\n        entry_mul = 1 - tf.one_hot(entry_idx, self.num_per_hash_slot,\n                                   dtype=tf.int32)\n        entry_add = (tf.expand_dims(upd_idxs, 1) *\n                     tf.one_hot(entry_idx, self.num_per_hash_slot,\n                                dtype=tf.int32))\n\n        mul_op = tf.scatter_mul(self.hash_slots[i], slot_idxs, entry_mul)\n        with tf.control_dependencies([mul_op]):\n          add_op = tf.scatter_add(self.hash_slots[i], slot_idxs, entry_add)\n          update_ops.append(add_op)\n\n    return tf.group(*update_ops)\n", "framework": "tensorflow"}
{"repo_name": "Microsoft/gated-graph-neural-network-samples", "file_path": "chem_tensorflow.py", "content": "#!/usr/bin/env/python\n\nimport json\nimport os\nimport pickle\nimport random\nimport time\nfrom typing import List, Any, Sequence\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom utils import MLP, ThreadedIterator, SMALL_NUMBER\n\n\nclass ChemModel(object):\n    @classmethod\n    def default_params(cls):\n        return {\n            'num_epochs': 3000,\n            'patience': 25,\n            'learning_rate': 0.001,\n            'clamp_gradient_norm': 1.0,\n            'out_layer_dropout_keep_prob': 1.0,\n\n            'hidden_size': 100,\n            'num_timesteps': 4,\n            'use_graph': True,\n\n            'tie_fwd_bkwd': True,\n            'task_ids': [0],\n\n            'random_seed': 0,\n\n            'train_file': 'molecules_train.json',\n            'valid_file': 'molecules_valid.json'\n        }\n\n    def __init__(self, args):\n        self.args = args\n\n        # Collect argument things:\n        data_dir = ''\n        if '--data_dir' in args and args['--data_dir'] is not None:\n            data_dir = args['--data_dir']\n        self.data_dir = data_dir\n\n        self.run_id = \"_\".join([time.strftime(\"%Y-%m-%d-%H-%M-%S\"), str(os.getpid())])\n        log_dir = args.get('--log_dir') or '.'\n        os.makedirs(log_dir, exist_ok=True)\n        self.log_file = os.path.join(log_dir, \"%s_log.json\" % self.run_id)\n        self.best_model_file = os.path.join(log_dir, \"%s_model_best.pickle\" % self.run_id)\n        tb_log_dir = os.path.join(log_dir, \"tb\", self.run_id)\n        os.makedirs(tb_log_dir, exist_ok=True)\n\n        # Collect parameters:\n        params = self.default_params()\n        config_file = args.get('--config-file')\n        if config_file is not None:\n            with open(config_file, 'r') as f:\n                params.update(json.load(f))\n        config = args.get('--config')\n        if config is not None:\n            params.update(json.loads(config))\n        self.params = params\n        with open(os.path.join(log_dir, \"%s_params.json\" % self.run_id), \"w\") as f:\n            json.dump(params, f)\n        print(\"Run %s starting with following parameters:\\n%s\" % (self.run_id, json.dumps(self.params)))\n        random.seed(params['random_seed'])\n        np.random.seed(params['random_seed'])\n\n        # Load data:\n        self.max_num_vertices = 0\n        self.num_edge_types = 0\n        self.annotation_size = 0\n        self.train_data = self.load_data(params['train_file'], is_training_data=True)\n        self.valid_data = self.load_data(params['valid_file'], is_training_data=False)\n\n        # Build the actual model\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        self.graph = tf.Graph()\n        self.sess = tf.Session(graph=self.graph, config=config)\n        with self.graph.as_default():\n            tf.set_random_seed(params['random_seed'])\n            self.placeholders = {}\n            self.weights = {}\n            self.ops = {}\n            self.make_model()\n            self.make_train_step()\n            self.make_summaries()\n\n            # Restore/initialize variables:\n            restore_file = args.get('--restore')\n            if restore_file is not None:\n                self.train_step_id, self.valid_step_id = self.restore_progress(restore_file)\n            else:\n                self.initialize_model()\n                self.train_step_id = 0\n                self.valid_step_id = 0\n            self.train_writer = tf.summary.FileWriter(os.path.join(tb_log_dir, 'train'), graph=self.graph)\n            self.valid_writer = tf.summary.FileWriter(os.path.join(tb_log_dir, 'validation'), graph=self.graph)\n\n    def load_data(self, file_name, is_training_data: bool):\n        full_path = os.path.join(self.data_dir, file_name)\n\n        print(\"Loading data from %s\" % full_path)\n        with open(full_path, 'r') as f:\n            data = json.load(f)\n\n        restrict = self.args.get(\"--restrict_data\")\n        if restrict is not None and restrict > 0:\n            data = data[:restrict]\n\n        # Get some common data out:\n        num_fwd_edge_types = 0\n        for g in data:\n            self.max_num_vertices = max(self.max_num_vertices, max([v for e in g['graph'] for v in [e[0], e[2]]]))\n            num_fwd_edge_types = max(num_fwd_edge_types, max([e[1] for e in g['graph']]))\n        self.num_edge_types = max(self.num_edge_types, num_fwd_edge_types * (1 if self.params['tie_fwd_bkwd'] else 2))\n        self.annotation_size = max(self.annotation_size, len(data[0][\"node_features\"][0]))\n\n        return self.process_raw_graphs(data, is_training_data)\n\n    @staticmethod\n    def graph_string_to_array(graph_string: str) -> List[List[int]]:\n        return [[int(v) for v in s.split(' ')]\n                for s in graph_string.split('\\n')]\n\n    def process_raw_graphs(self, raw_data: Sequence[Any], is_training_data: bool) -> Any:\n        raise Exception(\"Models have to implement process_raw_graphs!\")\n\n    def make_model(self):\n        self.placeholders['target_values'] = tf.placeholder(tf.float32, [len(self.params['task_ids']), None],\n                                                            name='target_values')\n        self.placeholders['target_mask'] = tf.placeholder(tf.float32, [len(self.params['task_ids']), None],\n                                                          name='target_mask')\n        self.placeholders['num_graphs'] = tf.placeholder(tf.int32, [], name='num_graphs')\n        self.placeholders['out_layer_dropout_keep_prob'] = tf.placeholder(tf.float32, [], name='out_layer_dropout_keep_prob')\n\n        with tf.variable_scope(\"graph_model\"):\n            self.prepare_specific_graph_model()\n            # This does the actual graph work:\n            if self.params['use_graph']:\n                self.ops['final_node_representations'] = self.compute_final_node_representations()\n            else:\n                self.ops['final_node_representations'] = tf.zeros_like(self.placeholders['initial_node_representation'])\n\n        self.ops['losses'] = []\n        for (internal_id, task_id) in enumerate(self.params['task_ids']):\n            with tf.variable_scope(\"out_layer_task%i\" % task_id):\n                with tf.variable_scope(\"regression_gate\"):\n                    self.weights['regression_gate_task%i' % task_id] = MLP(2 * self.params['hidden_size'], 1, [],\n                                                                           self.placeholders['out_layer_dropout_keep_prob'])\n                with tf.variable_scope(\"regression\"):\n                    self.weights['regression_transform_task%i' % task_id] = MLP(self.params['hidden_size'], 1, [],\n                                                                                self.placeholders['out_layer_dropout_keep_prob'])\n                computed_values = self.gated_regression(self.ops['final_node_representations'],\n                                                        self.weights['regression_gate_task%i' % task_id],\n                                                        self.weights['regression_transform_task%i' % task_id])\n                diff = computed_values - self.placeholders['target_values'][internal_id, :]\n                task_target_mask = self.placeholders['target_mask'][internal_id, :]\n                task_target_num = tf.reduce_sum(task_target_mask) + SMALL_NUMBER\n                diff = diff * task_target_mask  # Mask out unused values\n                self.ops['accuracy_task%i' % task_id] = tf.reduce_sum(tf.abs(diff)) / task_target_num\n                task_loss = tf.reduce_sum(0.5 * tf.square(diff)) / task_target_num\n                # Normalise loss to account for fewer task-specific examples in batch:\n                task_loss = task_loss * (1.0 / (self.params['task_sample_ratios'].get(task_id) or 1.0))\n                self.ops['losses'].append(task_loss)\n        self.ops['loss'] = tf.reduce_sum(self.ops['losses'])\n\n    def make_train_step(self):\n        trainable_vars = self.sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n        if self.args.get('--freeze-graph-model'):\n            graph_vars = set(self.sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"graph_model\"))\n            filtered_vars = []\n            for var in trainable_vars:\n                if var not in graph_vars:\n                    filtered_vars.append(var)\n                else:\n                    print(\"Freezing weights of variable %s.\" % var.name)\n            trainable_vars = filtered_vars\n        optimizer = tf.train.AdamOptimizer(self.params['learning_rate'])\n        grads_and_vars = optimizer.compute_gradients(self.ops['loss'], var_list=trainable_vars)\n        clipped_grads = []\n        for grad, var in grads_and_vars:\n            if grad is not None:\n                clipped_grads.append((tf.clip_by_norm(grad, self.params['clamp_gradient_norm']), var))\n            else:\n                clipped_grads.append((grad, var))\n        self.ops['train_step'] = optimizer.apply_gradients(clipped_grads)\n        # Initialize newly-introduced variables:\n        self.sess.run(tf.local_variables_initializer())\n\n    def make_summaries(self):\n        with tf.name_scope('summary'):\n            tf.summary.scalar('loss', self.ops['loss'])\n            for task_id in self.params['task_ids']:\n                tf.summary.scalar('accuracy%i' % task_id, self.ops['accuracy_task%i' % task_id])\n        self.ops['summary'] = tf.summary.merge_all()\n\n    def gated_regression(self, last_h, regression_gate, regression_transform):\n        raise Exception(\"Models have to implement gated_regression!\")\n\n    def prepare_specific_graph_model(self) -> None:\n        raise Exception(\"Models have to implement prepare_specific_graph_model!\")\n\n    def compute_final_node_representations(self) -> tf.Tensor:\n        raise Exception(\"Models have to implement compute_final_node_representations!\")\n\n    def make_minibatch_iterator(self, data: Any, is_training: bool):\n        raise Exception(\"Models have to implement make_minibatch_iterator!\")\n\n    def run_epoch(self, epoch_name: str, data, is_training: bool, start_step: int = 0):\n        chemical_accuracies = np.array([0.066513725, 0.012235489, 0.071939046, 0.033730778, 0.033486113, 0.004278493,\n                                        0.001330901, 0.004165489, 0.004128926, 0.00409976, 0.004527465, 0.012292586,\n                                        0.037467458])\n\n        loss = 0\n        accuracies = []\n        accuracy_ops = [self.ops['accuracy_task%i' % task_id] for task_id in self.params['task_ids']]\n        start_time = time.time()\n        processed_graphs = 0\n        steps = 0\n        batch_iterator = ThreadedIterator(self.make_minibatch_iterator(data, is_training), max_queue_size=5)\n        for step, batch_data in enumerate(batch_iterator):\n            num_graphs = batch_data[self.placeholders['num_graphs']]\n            processed_graphs += num_graphs\n            if is_training:\n                batch_data[self.placeholders['out_layer_dropout_keep_prob']] = self.params['out_layer_dropout_keep_prob']\n                fetch_list = [self.ops['loss'], accuracy_ops, self.ops['summary'], self.ops['train_step']]\n            else:\n                batch_data[self.placeholders['out_layer_dropout_keep_prob']] = 1.0\n                fetch_list = [self.ops['loss'], accuracy_ops, self.ops['summary']]\n            result = self.sess.run(fetch_list, feed_dict=batch_data)\n            (batch_loss, batch_accuracies, batch_summary) = (result[0], result[1], result[2])\n            writer = self.train_writer if is_training else self.valid_writer\n            writer.add_summary(batch_summary, start_step + step)\n            loss += batch_loss * num_graphs\n            accuracies.append(np.array(batch_accuracies) * num_graphs)\n\n            print(\"Running %s, batch %i (has %i graphs). Loss so far: %.4f\" % (epoch_name,\n                                                                               step,\n                                                                               num_graphs,\n                                                                               loss / processed_graphs),\n                  end='\\r')\n            steps += 1\n\n        accuracies = np.sum(accuracies, axis=0) / processed_graphs\n        loss = loss / processed_graphs\n        error_ratios = accuracies / chemical_accuracies[self.params[\"task_ids\"]]\n        instance_per_sec = processed_graphs / (time.time() - start_time)\n        return loss, accuracies, error_ratios, instance_per_sec, steps\n\n    def train(self):\n        log_to_save = []\n        total_time_start = time.time()\n        with self.graph.as_default():\n            if self.args.get('--restore') is not None:\n                _, valid_accs, _, _, steps = self.run_epoch(\"Resumed (validation)\", self.valid_data, False)\n                best_val_acc = np.sum(valid_accs)\n                best_val_acc_epoch = 0\n                print(\"\\r\\x1b[KResumed operation, initial cum. val. acc: %.5f\" % best_val_acc)\n            else:\n                (best_val_acc, best_val_acc_epoch) = (float(\"+inf\"), 0)\n            for epoch in range(1, self.params['num_epochs'] + 1):\n                print(\"== Epoch %i\" % epoch)\n                train_loss, train_accs, train_errs, train_speed, train_steps = self.run_epoch(\"epoch %i (training)\" % epoch,\n                                                                                              self.train_data, True, self.train_step_id)\n                self.train_step_id += train_steps\n                accs_str = \" \".join([\"%i:%.5f\" % (id, acc) for (id, acc) in zip(self.params['task_ids'], train_accs)])\n                errs_str = \" \".join([\"%i:%.5f\" % (id, err) for (id, err) in zip(self.params['task_ids'], train_errs)])\n                print(\"\\r\\x1b[K Train: loss: %.5f | acc: %s | error_ratio: %s | instances/sec: %.2f\" % (train_loss,\n                                                                                                        accs_str,\n                                                                                                        errs_str,\n                                                                                                        train_speed))\n                valid_loss, valid_accs, valid_errs, valid_speed, valid_steps = self.run_epoch(\"epoch %i (validation)\" % epoch,\n                                                                                              self.valid_data, False, self.valid_step_id)\n                self.valid_step_id += valid_steps\n                accs_str = \" \".join([\"%i:%.5f\" % (id, acc) for (id, acc) in zip(self.params['task_ids'], valid_accs)])\n                errs_str = \" \".join([\"%i:%.5f\" % (id, err) for (id, err) in zip(self.params['task_ids'], valid_errs)])\n                print(\"\\r\\x1b[K Valid: loss: %.5f | acc: %s | error_ratio: %s | instances/sec: %.2f\" % (valid_loss,\n                                                                                                        accs_str,\n                                                                                                        errs_str,\n                                                                                                        valid_speed))\n\n                epoch_time = time.time() - total_time_start\n                log_entry = {\n                    'epoch': epoch,\n                    'time': epoch_time,\n                    'train_results': (train_loss, train_accs.tolist(), train_errs.tolist(), train_speed),\n                    'valid_results': (valid_loss, valid_accs.tolist(), valid_errs.tolist(), valid_speed),\n                }\n                log_to_save.append(log_entry)\n                with open(self.log_file, 'w') as f:\n                    json.dump(log_to_save, f, indent=4)\n\n                val_acc = np.sum(valid_accs)  # type: float\n                if val_acc < best_val_acc:\n                    self.save_progress(self.best_model_file, self.train_step_id, self.valid_step_id)\n                    print(\"  (Best epoch so far, cum. val. acc decreased to %.5f from %.5f. Saving to '%s')\" % (\n                        val_acc, best_val_acc, self.best_model_file))\n                    best_val_acc = val_acc\n                    best_val_acc_epoch = epoch\n                elif epoch - best_val_acc_epoch >= self.params['patience']:\n                    print(\"Stopping training after %i epochs without improvement on validation accuracy.\" % self.params['patience'])\n                    break\n\n    def save_progress(self, model_path: str, train_step: int, valid_step: int) -> None:\n        weights_to_save = {}\n        for variable in self.sess.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n            assert variable.name not in weights_to_save\n            weights_to_save[variable.name] = self.sess.run(variable)\n\n        data_to_save = {\n            \"params\": self.params,\n            \"weights\": weights_to_save,\n            \"train_step\": train_step,\n            \"valid_step\": valid_step,\n        }\n\n        with open(model_path, 'wb') as out_file:\n            pickle.dump(data_to_save, out_file, pickle.HIGHEST_PROTOCOL)\n\n    def initialize_model(self) -> None:\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        self.sess.run(init_op)\n\n    def restore_progress(self, model_path: str) -> (int, int):\n        print(\"Restoring weights from file %s.\" % model_path)\n        with open(model_path, 'rb') as in_file:\n            data_to_load = pickle.load(in_file)\n\n        # Assert that we got the same model configuration\n        assert len(self.params) == len(data_to_load['params'])\n        for (par, par_value) in self.params.items():\n            # Fine to have different task_ids:\n            if par not in ['task_ids', 'num_epochs']:\n                assert par_value == data_to_load['params'][par]\n\n        variables_to_initialize = []\n        with tf.name_scope(\"restore\"):\n            restore_ops = []\n            used_vars = set()\n            for variable in self.sess.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n                used_vars.add(variable.name)\n                if variable.name in data_to_load['weights']:\n                    restore_ops.append(variable.assign(data_to_load['weights'][variable.name]))\n                else:\n                    print('Freshly initializing %s since no saved value was found.' % variable.name)\n                    variables_to_initialize.append(variable)\n            for var_name in data_to_load['weights']:\n                if var_name not in used_vars:\n                    print('Saved weights for %s not used by model.' % var_name)\n            restore_ops.append(tf.variables_initializer(variables_to_initialize))\n            self.sess.run(restore_ops)\n\n        return data_to_load['train_step'], data_to_load['valid_step']\n", "framework": "tensorflow"}
{"repo_name": "Aipakazuma/study_clustering", "file_path": "clustering.py", "content": "# -*- coding: utf-8 -*-\n\nimport sys\nimport os\nimport codecs\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster                 import KMeans, MiniBatchKMeans\nfrom sklearn.decomposition           import TruncatedSVD\nfrom sklearn.preprocessing           import Normalizer\n\nclass Analyzer:\n  def __init__(self, args):\n    self.infile       = args[1]\n    self.outfile      = args[2]\n    self.num_clusters = 5       # \u30af\u30e9\u30b9\u6570\n    self.max_df       = 0.8     # \u30d9\u30af\u30c8\u30eb\u751f\u6210\u306b\u5fc5\u8981\u306a\u5024\n    self.max_features = 10000   # \u30d9\u30af\u30c8\u30eb\u751f\u6210\u306b\u5fc5\u8981\u306a\u5024\n    self.minibatch    = True    # \u6761\u4ef6\u5224\u5b9a\n\n  # \u30c6\u30ad\u30b9\u30c8\u3092\u8aad\u8fbc\n  def _read_from_file(self):\n    list = []\n    file = open(self.infile, 'r')\n    for line in file:\n      list.append( line.rstrip() )\n    file.close\n    return list\n\n  # \u30af\u30e9\u30b9\u30bf\u3092\u4f5c\u6210\n  def make_cluster(self):\n    texts = self._read_from_file()\n    # print \"texts are %(texts)s\" %locals()\n\n    # \u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210\n    vectorizer = TfidfVectorizer(\n      max_df       = self.max_df,\n      max_features = self.max_features,\n      stop_words   = 'english'\n      )\n    X = vectorizer.fit_transform(texts)\n    # \u3053\u3053\u3067\u306e\u5024\u306f\u4f55\u5ea6\u3084\u3063\u3066\u3082\u540c\u3058\u3067\u3057\u305f\n    # print \"X values are %(X)s\" %locals()\n\n    # KMeans \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210\u3057\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3059\u308b\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u30c7\u30fc\u30bf\u306e\u91cf\u3084\u7279\u6027\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u3082\u306e\u3092\u4e0e\u3048\u308b\u3088\u3046\u306b\u3059\u308b\n    if self.minibatch:\n      km = MiniBatchKMeans(\n        n_clusters         = self.num_clusters,\n        init               = 'k-means++',\n        batch_size         = 1000,\n        n_init             = 10,\n        max_no_improvement = 10,\n        verbose            = True\n        )\n    else:\n      km = KMeans(\n        n_clusters = self.num_clusters,\n        init       = 'k-means++',\n        n_init     = 1,\n        verbose    = True\n        )\n    km.fit(X)\n    labels = km.labels_\n\n    transformed = km.transform(X)\n    dists       = np.zeros(labels.shape)\n    for i in range(len(labels)):\n      dists[i] = transformed[i, labels[i]]\n\n    clusters = []\n    for i in range(self.num_clusters):\n      cluster = []\n      ii      = np.where(labels==i)[0]\n      dd      = dists[ii]\n      di      = np.vstack([dd,ii]).transpose().tolist()\n      di.sort()\n      for d, j in di:\n        cluster.append(texts[int(j)])\n      clusters.append(cluster)\n\n    return clusters\n\n  def write_cluster(self, clusters):\n    f = codecs.open('%s' % self.outfile, 'w', 'utf-8')\n    for i, texts in enumerate(clusters):\n      for text in texts:\n        f.write('%d: %s\\n' % (i, text.replace('/n', '').decode('utf-8')))\n\nif __name__ == '__main__':\n  if len(sys.argv) > 2:\n    analyzer = Analyzer(sys.argv)\n    clusters = analyzer.make_cluster()\n    # print \"Result clusters are %(clusters)s\" %locals()\n    analyzer.write_cluster(clusters)\n  else:\n    print \"Invalid arguments\"\n", "framework": "tensorflow"}
{"repo_name": "googleforgames/clean-chat", "file_path": "components/model/bert/vertex_pipeline/model.py", "content": "##########################################################################\n#\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n##########################################################################\n\"\"\"Train the Full BERT Model with Keras\"\"\"\n\nfrom typing import Any, Callable\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_transform as tft\nfrom keras.models import Model\nfrom official.nlp import optimization\nfrom tfx import v1 as tfx\n\nLABEL = 'label'\n\n\n# https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_utils_native_keras.py\n# Apache License, Version 2.0. | Copyright 2019 Google LLC.\ndef _get_tf_examples_serving_signature(\n        model: Model, tf_transform_output: tft.TFTransformOutput) -> Callable:\n    \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n\n    # We need to track the layers in the model in order to save it.\n    # TODO(b/162357359): Revise once the bug is resolved.\n    model.tft_layer_inference = tf_transform_output.transform_features_layer()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n    ])\n    def serve_tf_examples_fn(serialized_tf_example):\n        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n        raw_feature_spec = tf_transform_output.raw_feature_spec()\n        # Remove label feature since these will not be present at serving time.\n        raw_feature_spec.pop('target')\n        raw_features = tf.io.parse_example(serialized_tf_example,\n                                           raw_feature_spec)\n        transformed_features = model.tft_layer_inference(raw_features)\n\n        outputs = model(transformed_features)\n        # TODO(b/154085620): Convert the predicted labels from the model using a\n        # reverse-lookup (opposite of transform.py).\n        return {'outputs': outputs}\n\n    return serve_tf_examples_fn\n\n\n# https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_utils_native_keras.py\n# Apache License, Version 2.0. | Copyright 2019 Google LLC.\ndef _get_transform_features_signature(\n        model: Model, tf_transform_output: tft.TFTransformOutput) -> Callable:\n    \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n\n    # We need to track the layers in the model in order to save it.\n    # TODO(b/162357359): Revise once the bug is resolved.\n    model.tft_layer_eval = tf_transform_output.transform_features_layer()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n    ])\n    def transform_features_fn(serialized_tf_example):\n        \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n        raw_feature_spec = tf_transform_output.raw_feature_spec()\n        raw_features = tf.io.parse_example(serialized_tf_example,\n                                           raw_feature_spec)\n        transformed_features = model.tft_layer_eval(raw_features)\n        return transformed_features\n\n    return transform_features_fn\n\n\n# https://github.com/kubeflow/pipelines/blob/master/samples/core/tfx-oss/utils/taxi_utils.py\n# Apache License, Version 2.0. | Copyright 2019 The Kubeflow Authors.\ndef _gzip_reader_fn(filenames: tf.string) -> tf.data.TFRecordDataset:\n    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n\n\ndef _input_fn(\n    file_pattern: Any,\n    tf_transform_output: tft.TFTransformOutput,\n    batch_size: int = 32,\n) -> tf.data.Dataset:\n    \"\"\"Generates features and label for tuning/training.\n\n    Args:\n        file_pattern (Any): file patterns for training/evalluating fn_args\n        tf_transform_output (tft.TFTransformOutput): the output of the tf.Transform\n        batch_size (int, optional): Batch size for train/eval. Defaults to 32.\n\n    Returns:\n        tf.data.Dataset: Represents data.\n    \"\"\"\n\n    transformed_feature_spec = (\n        tf_transform_output.transformed_feature_spec().copy())\n\n    dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern=file_pattern,\n        batch_size=batch_size,\n        features=transformed_feature_spec,\n        reader=_gzip_reader_fn,\n        label_key=LABEL)\n\n    return dataset\n\n\ndef toxicity_model() -> Model:\n    \"\"\"Toxicity BERT Model\n\n    Returns:\n        keras.models.Model: BERT Model\n    \"\"\"\n    bert_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n    preprocess_path = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(preprocess_path, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(bert_path, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n\n    model = tf.keras.Model(text_input, net)\n\n    optimizer = optimization.create_optimizer(init_lr=3e-5,\n                                              num_train_steps=5,\n                                              num_warmup_steps=int(0.1 * 5),\n                                              optimizer_type='adamw')\n\n    # add optimizer, loss, etc in appropriate place\n    model.compile(optimizer=optimizer,\n                  loss=tf.keras.losses.MeanAbsoluteError(),\n                  metrics=tf.metrics.MeanSquaredError())\n\n    return model\n\n\ndef run_fn(fn_args: tfx.components.FnArgs) -> None:\n    \"\"\"Train the model based on given args.\n    Args:\n      fn_args: Holds args used to train the model as name/value pairs.\n    \"\"\"\n    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n    train_dataset = _input_fn(file_pattern=fn_args.train_files,\n                              tf_transform_output=tf_transform_output,\n                              batch_size=32)\n\n    eval_dataset = _input_fn(file_pattern=fn_args.eval_files,\n                             tf_transform_output=tf_transform_output,\n                             batch_size=32)\n\n    model = toxicity_model()\n\n    # Write logs to path\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=fn_args.model_run_dir, update_freq='batch')\n\n    model.fit(train_dataset,\n              steps_per_epoch=fn_args.train_steps,\n              validation_data=eval_dataset,\n              validation_steps=fn_args.eval_steps,\n              callbacks=[tensorboard_callback])\n\n    signatures = {\n        'serving_default':\n            _get_tf_examples_serving_signature(model, tf_transform_output),\n        'transform_features':\n            _get_transform_features_signature(model, tf_transform_output),\n    }\n\n    model.save(fn_args.serving_model_dir,\n               save_format='tf',\n               signatures=signatures)\n", "framework": "tensorflow"}
{"repo_name": "google-research/deadunits", "file_path": "deadunits/generic_convnet.py", "content": "# coding=utf-8\n# Copyright 2021 The Deadunits Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n\"\"\"Implements common architectures in a generic way using tf.keras.Model.\n\nEach generic model inherits from `tf.keras.Model`.\nYou can use following generic_models for now:\n\n- GenericConvnet: sequential models include Conv2D's + Dense's.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nfrom deadunits.layers import MaskedLayer\nfrom deadunits.layers import MeanReplacer\nfrom deadunits.layers import TaylorScorer\nimport gin\nfrom six.moves import range\nimport tensorflow.compat.v2 as tf\n\n_default_generic_convnet_arch = [['C', 16, 5, {}], ['MP', 2, 2],\n                                 ['C', 32, 5, {}], ['MP', 2, 2], ['F'],\n                                 ['D', 256], ['O', 10]]\n\n\n@gin.configurable\nclass GenericConvnet(tf.keras.Model):\n  \"\"\"Creates a tf.keras.Model from according to the flags and arch provided.\n\n  \"\"\"\n  _allowed_layers = {\n      'C': 'conv',\n      'D': 'dense',\n      'MP': 'maxpool',\n      'DO': 'dropout',\n      'O': 'output',\n      'F': 'flatten',\n      'GA': 'gap',\n  }\n\n  # Each layer should have the following form in the arch definition.\n  # 'C': Conv2D layer in the form ['C', n_units, filter_shape, **kwargs]\n  # 'MP': MaxPool2D layer in the form ['MP', pool_size, strides, **kwargs]\n  # 'D': Dense layer in the form ['D', n_units]\n  # 'DO': Dropout layer in the form ['DO', rate]\n  # 'F': Flatten layer in the form ['F']\n  # 'GA': Global average pooling 2D in the form ['GA']\n  # 'O': Dense layer with no activation in the form ['O', n_units]\n  def __init__(self,\n               model_arch=None,\n               name='GenericCifarConvnet',\n               f_activation=tf.keras.activations.relu,\n               use_batchnorm=False,\n               bn_is_affine=False,\n               use_dropout=False,\n               dropout_rate=0.5,\n               use_mean_replacer=False,\n               use_taylor_scorer=False,\n               use_masked_layers=False):\n    \"\"\"Initializes GenericConvnet instance with correct layers.\n\n    Args:\n      model_arch: list, consists of lists defining the cascaded network. refer\n        to `GenericConvnet._allowed_layers`.\n      name: str, name of the model.\n      f_activation: function, from tf.keras.activations\n      use_batchnorm: bool, if True BatchNormalization layer is used.\n      bn_is_affine: bool, if True BatchNormalization performs affine\n        transformation after the normalization.\n      use_dropout: bool, if True Dropout layer is used.\n      dropout_rate: float, dropout fraction for the Dropout layer.\n      use_mean_replacer: bool, if True MeanReplacer layer is used after each\n        layer.\n      use_taylor_scorer: bool, if True TaylorScorer layer is used after each\n        layer.\n      use_masked_layers: bool, if True each layer is wrapped with MaskedLayer.\n\n    Raises:\n      AssertionError: when the provided `model_arch` is not valid.\n    \"\"\"\n    if model_arch is None:\n      model_arch = _default_generic_convnet_arch\n    self._check_arch(model_arch)\n    super(GenericConvnet, self).__init__(name=name)\n    # Initial configration is saved to be able to clone the model.\n    self.init_config = dict([('model_arch', model_arch), ('name', name),\n                             ('f_activation', f_activation),\n                             ('use_batchnorm', use_batchnorm),\n                             ('bn_is_affine', bn_is_affine),\n                             ('use_dropout', use_dropout),\n                             ('dropout_rate', dropout_rate),\n                             ('use_mean_replacer', use_mean_replacer),\n                             ('use_taylor_scorer', use_taylor_scorer),\n                             ('use_masked_layers', use_masked_layers)])\n    # Wrap the layers if asked.\n    wrapper = lambda l: MaskedLayer(l) if use_masked_layers else l\n    # Forward chain has the attribute names in order and used to orchestrate\n    # the forward pass.\n    forward_chain = []\n    for t in model_arch:\n      # The order is:\n      # Layer + bn + Activation + taylorScorer + meanReplacer + Dropout\n      l_type = t[0]\n      l_name = self._get_layer_name(l_type)\n      forward_chain.append(l_name)\n      # If F(flatten) or O(output), we don't have extra layers(dropout,bn,etc..)\n      if l_type == 'F':\n        setattr(self, l_name, tf.keras.layers.Flatten())\n      elif l_type == 'GA':\n        setattr(self, l_name, tf.keras.layers.GlobalAvgPool2D())\n      elif l_type == 'MP':\n        setattr(self, l_name, tf.keras.layers.MaxPool2D(t[1], t[2]))\n      elif l_type == 'O':\n        setattr(self, l_name, tf.keras.layers.Dense(t[1], activation=None))\n      elif l_type == 'DO':\n        setattr(self, l_name, tf.keras.layers.Dropout(t[1]))\n      else:\n        if l_type == 'C':\n          setattr(\n              self, l_name,\n              wrapper(\n                  tf.keras.layers.Conv2D(t[1], t[2], activation=None, **t[3])))\n        elif l_type == 'D':\n          setattr(self, l_name,\n                  wrapper(tf.keras.layers.Dense(t[1], activation=None)))\n\n        if use_batchnorm:\n          c_name = l_name + '_bn'\n          setattr(\n              self, c_name,\n              tf.keras.layers.BatchNormalization(\n                  center=bn_is_affine, scale=bn_is_affine))\n          forward_chain.append(c_name)\n        # Add activation\n        c_name = l_name + '_a'\n        setattr(self, c_name, f_activation)\n        forward_chain.append(c_name)\n        if use_taylor_scorer:\n          c_name = l_name + '_ts'\n          setattr(self, c_name, TaylorScorer())\n          forward_chain.append(c_name)\n        if use_mean_replacer:\n          c_name = l_name + '_mr'\n          setattr(self, c_name, MeanReplacer())\n          forward_chain.append(c_name)\n        if use_dropout:\n          c_name = l_name + '_dr'\n          setattr(self, c_name, tf.keras.layers.Dropout(dropout_rate))\n          forward_chain.append(c_name)\n    self.forward_chain = forward_chain\n\n  def call(self,\n           inputs,\n           training=False,\n           compute_mean_replacement_saliency=False,\n           compute_removal_saliency=False,\n           is_abs=True,\n           aggregate_values=False,\n           is_replacing=False,\n           return_nodes=None):\n    # We need to save the first_input for initiliazing our clone (see .clone()).\n    if not hasattr(self, 'first_input'):\n      self.first_input = inputs\n    x = inputs\n    return_dict = {}\n    for l_name in self.forward_chain:\n      node = getattr(self, l_name)\n      if isinstance(node, MeanReplacer):\n        x = node(x, is_replacing=is_replacing)\n      elif isinstance(node, TaylorScorer):\n        x = node(\n            x,\n            compute_mean_replacement_saliency=compute_mean_replacement_saliency,\n            compute_removal_saliency=compute_removal_saliency,\n            is_abs=is_abs,\n            aggregate_values=aggregate_values)\n      elif isinstance(\n          node, (tf.keras.layers.BatchNormalization, tf.keras.layers.Dropout)):\n        x = node(x, training=training)\n      else:\n        x = node(x)\n      if return_nodes and l_name in return_nodes:\n        return_dict[l_name] = x\n    if return_nodes:\n      return x, return_dict\n    else:\n      return x\n\n  def propagate_bias(self, l_name, input_tensor):\n    \"\"\"Propagates the given input to the bias of the next unit.\n\n    We expect `input_tensor` having constant values at `input_tensor[...,i]` for\n      every unit `i`. However this is not checked and if it is not constant,\n      mean of the all values are used to update the bias.\n\n    If input_tensor casted into same type as the parameters of the `l_name`.\n\n    Args:\n      l_name: str, name of a MaskedLayer such that `hasattr(self, l_name)` is\n        True.\n      input_tensor: Tensor, same shape as the output shape of the l_name. It\n        should also be a float type. i.e. tf.float16/32/64.\n\n    Raises:\n      ValueError: when the l_name is not in the `self.forward_chain` or if\n        there is no parameterized layer exists after `l_name`.\n      AssertionError: when the input_tensor is not float type.\n    \"\"\"\n    assert (input_tensor.dtype in [tf.float16, tf.float32, tf.float64])\n    current_i = self.forward_chain.index(l_name) + 1\n    if current_i == len(self.forward_chain):\n      raise ValueError('Output layer cannot propagate bias')\n    next_layer = getattr(self, self.forward_chain[current_i])\n    forward_tensor = input_tensor\n    # Including `tf.keras.layers.Dense`, too; since the output layer(Dense)\n    # is not wrapped with `MaskedLayer`.\n    parametered_layers = (MaskedLayer, tf.keras.layers.Dense,\n                          tf.keras.layers.Conv2D)\n    while not isinstance(next_layer, parametered_layers):\n      forward_tensor = next_layer(forward_tensor)\n      current_i += 1\n      if current_i == len(self.forward_chain):\n        raise ValueError('No appropriate layer exists after'\n                         '%s to propagate bias.' % l_name)\n      next_layer = getattr(self, self.forward_chain[current_i])\n    # So now we have propageted bias + currrent_bias. This should be our new\n    # bias.\n    forward_tensor = next_layer(forward_tensor)\n    # During Mean Replacement, forward_tensor[...,i] should be a constant\n    # tensor, but it is not verified.\n    bias2add = tf.reduce_mean(\n        forward_tensor, axis=list(range(forward_tensor.shape.ndims - 1)))\n    if isinstance(next_layer, MaskedLayer):\n      next_layer.layer.weights[1].assign(bias2add)\n    else:\n      next_layer.weights[1].assign(bias2add)\n\n  def get_allowed_layer_keys(self):\n    return list(self._allowed_layers.keys())\n\n  def get_layer_keys(self, layer_type, name_filter=lambda _: True):\n    \"\"\"Returns a list of layer_names matching the type and passing the filter.\n\n    `self.forward_chain` is filtered by type and layer_name.\n    Args:\n      layer_type: layer class to be matched.\n      name_filter: function, returning bool given a layer_name.\n    \"\"\"\n    res = []\n    for l_name in self.forward_chain:\n      if name_filter(l_name) and isinstance(getattr(self, l_name), layer_type):\n        res.append(l_name)\n    return res\n\n  def _get_layer_name(self, l_type):\n    \"\"\"Returns names for different layers by incrementing the counter.\n\n    Args:\n      l_type: str from self._allowed_layers.keys()\n\n    Returns:\n      attr_name: str unique attr name for the layer\n    \"\"\"\n    if not hasattr(self, 'layer_name_counter'):\n      self.layer_name_counter = {k: 1 for k in self._allowed_layers.keys()}\n    i = self.layer_name_counter[l_type]\n    self.layer_name_counter[l_type] += 1\n    return '%s_%d' % (self._allowed_layers[l_type], i)\n\n  def clone(self):\n    new_model = GenericConvnet(**self.init_config)\n    # Initilize the new_model params.\n    new_model(self.first_input)\n    new_model.set_weights(self.get_weights())\n    return new_model\n\n  def _check_arch(self, arch):\n    \"\"\"Checks the arch provided has the right form.\n\n    For some reason tensorflow wraps every list/dict to make it checkpointable.\n    For that reason we are using the super classes from collections module.\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpointable/data_structures.py\n    Args:\n      arch: architecture list.\n\n    Raises:\n      AssertionError: If the architecture list is not in the right format.\n    \"\"\"\n    assert arch is not None\n    assert len(arch) >= 1\n    for t in arch:\n      assert isinstance(t, collections.MutableSequence)\n      assert len(t) >= 1\n      assert t[0] in self.get_allowed_layer_keys()\n      if t[0] == 'C':\n        assert len(t) == 4\n        assert isinstance(t[1], int)\n        # t[2] can be an int or list of two integers.\n        assert (isinstance(t[2], int) or\n                (isinstance(t[2], collections.MutableSequence) and\n                 len(t[2]) == 2) and all(isinstance(x, int) for x in t[2]))\n        assert isinstance(t[3], collections.MutableMapping)\n      if t[0] == 'MP':\n        assert len(t) == 3\n        assert (isinstance(t[1], int) or\n                (isinstance(t[1], collections.MutableSequence) and\n                 len(t[1]) == 2) and all(isinstance(x, int) for x in t[1]))\n        assert (isinstance(t[2], int) or\n                (isinstance(t[2], collections.MutableSequence) and\n                 len(t[2]) == 2) and all(isinstance(x, int) for x in t[2]))\n      if t[0] in ('F', 'GA'):\n        assert len(t) == 1\n      if t[0] in ('D', 'O'):\n        assert len(t) == 2\n        assert isinstance(t[1], int)\n      if t[0] == 'DO':\n        assert len(t) == 2\n        assert isinstance(t[1], float) and 0 < t[1] and t[1] < 1\n", "framework": "tensorflow"}
{"repo_name": "HimariO/VideoSum", "file_path": "dnc/dnc.py", "content": "import tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMStateTuple\nfrom .memory import *\nfrom .utility import *\nimport os\n\n\nclass DNC:\n\n    def __init__(self, controller_class, input_size, output_size, max_sequence_length,\n                 memory_words_num=256, memory_word_size=64, memory_read_heads=4, batch_size=1, testing=False, output_feedback=False):\n        \"\"\"\n        constructs a complete DNC architecture as described in the DNC paper\n        http://www.nature.com/nature/journal/vaop/ncurrent/full/nature20101.html\n\n        Parameters:\n        -----------\n        controller_class: BaseController\n            a concrete implementation of the BaseController class\n        input_size: int\n            the size of the input vector\n        output_size: int\n            the size of the output vector\n        max_sequence_length: int\n            the maximum length of an input sequence\n        memory_words_num: int\n            the number of words that can be stored in memory\n        memory_word_size: int\n            the size of an individual word in memory\n        memory_read_heads: int\n            the number of read heads in the memory\n        batch_size: int\n            the size of the data batch\n        \"\"\"\n        self.testing = testing\n        self.feedback = output_feedback\n        self.output_t = tf.zeros([batch_size, output_size])\n\n        self.input_size = input_size + output_size if output_feedback else input_size\n        self.output_size = output_size\n        self.max_sequence_length = max_sequence_length\n        self.words_num = memory_words_num\n        self.word_size = memory_word_size\n        self.read_heads = memory_read_heads\n        self.batch_size = batch_size\n\n        self.memory = SharpMemory(self.words_num, self.word_size, self.read_heads, self.batch_size)\n        self.packed_memory_matrixs = {}\n        self.controller = controller_class(self.input_size, self.output_size, self.read_heads, self.word_size, self.batch_size)\n\n        # input data placeholders\n        self.input_data = tf.placeholder(tf.float32, [batch_size, None, self.input_size], name='input')\n        self.target_output = tf.placeholder(tf.float32, [batch_size, None, self.output_size], name='targets')\n        self.target_output_id = tf.placeholder(tf.int32, [batch_size, None], name='targets_id')\n        # self.target_output_mul_id = tf.placeholder(tf.int32, [batch_size, None, ], name='targets_id')\n        self.sequence_length = tf.placeholder(tf.int32, name='sequence_length')\n\n        self.penalty_term = None\n        self.build_graph()\n\n    def _step_op(self, step, memory_state, controller_state=None):\n        \"\"\"\n        performs a step operation on the input step data\n\n        Parameters:\n        ----------\n        step: Tensor (batch_size, input_size)\n        memory_state: Tuple\n            a tuple of current memory parameters\n        controller_state: Tuple\n            the state of the controller if it's recurrent\n\n        Returns: Tuple\n            output: Tensor (batch_size, output_size)\n            memory_view: dict\n        \"\"\"\n\n        last_read_vectors = memory_state[6]\n        pre_output, interface, nn_state = None, None, None\n\n        if self.controller.has_recurrent_nn:\n            pre_output, interface, nn_state = self.controller.process_input(step, last_read_vectors, controller_state)\n        else:\n            pre_output, interface = self.controller.process_input(step, last_read_vectors)\n\n        usage_vector, write_weighting, memory_matrix, link_matrix, precedence_vector = self.memory.write(\n            memory_state[0], memory_state[1], memory_state[5],\n            memory_state[4], memory_state[2], memory_state[3],\n            interface['write_key'],\n            interface['write_strength'],\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n            interface['write_vector'],\n            interface['erase_vector']\n        )\n\n        if isinstance(self.memory, SharpMemory) or isinstance(self.memory, KMemory):\n            read_weightings, read_vectors = self.memory.read(\n                memory_matrix,\n                memory_state[5],\n                interface['read_keys'],\n                interface['read_strengths'],\n                link_matrix,\n                interface['read_modes'],\n                memory_state[1],\n            )\n        else:\n            read_weightings, read_vectors = self.memory.read(\n                memory_matrix,\n                memory_state[5],\n                interface['read_keys'],\n                interface['read_strengths'],\n                link_matrix,\n                interface['read_modes'],\n            )\n\n        return [\n\n            # report new memory state to be updated outside the condition branch\n            memory_matrix,\n            usage_vector,\n            precedence_vector,\n            link_matrix,\n            write_weighting,\n            read_weightings,\n            read_vectors,\n\n            self.controller.final_output(pre_output, read_vectors),\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n\n            # report new state of RNN if exists\n            nn_state if nn_state is not None else tf.zeros(1),\n        ]\n\n\n    def _loop_body(self, time, memory_state, outputs, free_gates, allocation_gates, write_gates,\n                   read_weightings, write_weightings, usage_vectors, controller_state, *memory_state_record):\n        \"\"\"\n        the body of the DNC sequence processing loop\n\n        Parameters:\n        ----------\n        time: Tensor\n        outputs: TensorArray\n        memory_state: Tuple\n        free_gates: TensorArray\n        allocation_gates: TensorArray\n        write_gates: TensorArray\n        read_weightings: TensorArray,\n        write_weightings: TensorArray,\n        usage_vectors: TensorArray,\n        controller_state: Tuple\n\n        Returns: Tuple containing all updated arguments\n        \"\"\"\n\n        step_input = self.unpacked_input_data.read(time)\n\n        if self.feedback:\n            # redirect output if DNC is at test time(trainging time will use target as feedback so no need to change TF graph)\n            if time == 0 and self.testing:\n                #  get input data with out feedback part.\n                step_input = tf.slice(step_input, [0, 0], [self.batch_size, self.input_size - self.output_size])\n                step_input = tf.concat([step_input, tf.zeros([self.batch_size, self.output_size])], 1)\n            elif self.testing:\n                step_input = tf.slice(step_input, [0, 0], [self.batch_size, self.input_size - self.output_size])\n                step_input = tf.concat([step_input, self.output_t], 1)\n\n        output_list = self._step_op(step_input, memory_state, controller_state)\n\n        # update memory parameters\n\n        new_controller_state = tf.zeros(1)\n        new_memory_state = tuple(output_list[0:7])\n\n        new_controller_state = output_list[11]\n\n        outputs = outputs.write(time, output_list[7])\n        self.output_t = output_list[7]\n\n        # collecting memory view for the current step\n        free_gates = free_gates.write(time, output_list[8])\n        allocation_gates = allocation_gates.write(time, output_list[9])\n        write_gates = write_gates.write(time, output_list[10])\n        read_weightings = read_weightings.write(time, output_list[5])\n        write_weightings = write_weightings.write(time, output_list[4])\n        usage_vectors = usage_vectors.write(time, output_list[1])\n\n        memory_state_list = list(memory_state_record)\n        if self.testing:\n            memory_state_list[0] = memory_state_record[0].write(time, new_memory_state[0])\n            memory_state_list[1] = memory_state_record[1].write(time, new_memory_state[1])\n            memory_state_list[2] = memory_state_record[2].write(time, new_memory_state[2])\n            memory_state_list[3] = memory_state_record[3].write(time, new_memory_state[3])\n            memory_state_list[4] = memory_state_record[4].write(time, new_memory_state[4])\n            memory_state_list[5] = memory_state_record[5].write(time, new_memory_state[5])\n            memory_state_list[6] = memory_state_record[6].write(time, new_memory_state[6])\n\n        return (\n            time + 1, new_memory_state, outputs,\n            free_gates, allocation_gates, write_gates,\n            read_weightings, write_weightings,\n            usage_vectors, new_controller_state,\n            *memory_state_list\n        )\n\n    def build_graph(self):\n        \"\"\"\n        builds the computational graph that performs a step-by-step evaluation\n        of the input data batches\n        \"\"\"\n\n        self.unpacked_input_data = unpack_into_tensorarray(self.input_data, 1, self.sequence_length)\n\n        outputs = tf.TensorArray(tf.float32, self.sequence_length)\n        usage_vectors = tf.TensorArray(tf.float32, self.sequence_length)\n\n        free_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        allocation_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        write_gates = tf.TensorArray(tf.float32, self.sequence_length)\n\n        read_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n        write_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n\n        memory_state = self.memory.init_memory()\n        memory_state_record = [\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n        ]\n\n        controller_state = self.controller.get_state() if self.controller.has_recurrent_nn else (tf.zeros(1), tf.zeros(1))\n        # This 2 line of code will cause problem if controller have more than 1 layer.\n        # if not isinstance(controller_state, LSTMStateTuple):\n        #     controller_state = LSTMStateTuple(controller_state[0], controller_state[1])\n\n        final_results = None\n\n        time = tf.constant(0, dtype=tf.int32)\n\n        final_results = tf.while_loop(\n            cond=lambda time, *_: time < self.sequence_length,\n            body=self._loop_body,\n            loop_vars=(\n                time, memory_state, outputs,\n                free_gates, allocation_gates, write_gates,\n                read_weightings, write_weightings,\n                usage_vectors, controller_state, *memory_state_record,\n            ),\n            parallel_iterations=32,\n            swap_memory=True\n        )\n\n        dependencies = []\n        if self.controller.has_recurrent_nn:\n            dependencies.append(self.controller.update_state(final_results[9]))  # result[9] is new_controller_state\n\n        with tf.control_dependencies(dependencies):\n            self.packed_output = pack_into_tensor(final_results[2], axis=1)\n\n            self.packed_memory_view = {\n                'free_gates': pack_into_tensor(final_results[3], axis=1),\n                'allocation_gates': pack_into_tensor(final_results[4], axis=1),\n                'write_gates': pack_into_tensor(final_results[5], axis=1),\n                'read_weightings': pack_into_tensor(final_results[6], axis=1),\n                'write_weightings': pack_into_tensor(final_results[7], axis=1),\n                'usage_vectors': pack_into_tensor(final_results[8], axis=1)\n            }\n            if self.testing:\n                self.packed_memory_matrixs = {\n                    'memory_matrix': pack_into_tensor(final_results[10], axis=1),\n                    'usage_vector': pack_into_tensor(final_results[11], axis=1),\n                    'precedence_vector': pack_into_tensor(final_results[12], axis=1),\n                    'link_matrix': pack_into_tensor(final_results[13], axis=1),\n                    'write_weighting': pack_into_tensor(final_results[14], axis=1),\n                    'read_weightings': pack_into_tensor(final_results[15], axis=1),\n                    'read_vectors': pack_into_tensor(final_results[16], axis=1),\n                }\n\n    def get_outputs(self):\n        \"\"\"\n        returns the graph nodes for the output and memory view\n\n        Returns: Tuple\n            outputs: Tensor (batch_size, time_steps, output_size)\n            memory_view: dict\n        \"\"\"\n        return self.packed_output, self.packed_memory_view\n\n    def get_memoory_states(self):\n        \"\"\"\n        returns 'memory_matrix','usage_vector','precedence_vector','link_matrix',\n        'write_weighting','read_weightings','read_vectors'\n\n        Returns: Tuple\n            packed_memory_matrixs: dict\n        \"\"\"\n        return self.packed_memory_matrixs\n\n    def save(self, session, ckpts_dir, name):\n        \"\"\"\n        saves the current values of the model's parameters to a checkpoint\n\n        Parameters:\n        ----------\n        session: tf.Session\n            the tensorflow session to save\n        ckpts_dir: string\n            the path to the checkpoints directories\n        name: string\n            the name of the checkpoint subdirectory\n        \"\"\"\n        checkpoint_dir = os.path.join(ckpts_dir, name)\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n        tf.train.Saver(tf.trainable_variables()).save(session, os.path.join(checkpoint_dir, 'model.ckpt'))\n\n    def restore(self, session, ckpts_dir, name):\n        \"\"\"\n        session: tf.Session\n            the tensorflow session to restore into\n        ckpts_dir: string\n            the path to the checkpoints directories\n        name: string\n            the name of the checkpoint subdirectory\n        \"\"\"\n        tf.train.Saver(tf.trainable_variables()).restore(session, os.path.join(ckpts_dir, name, 'model.ckpt'))\n\n\nclass DNCAuto(DNC):\n    def _step_op(self, step, memory_state, controller_state=None):\n        \"\"\"\n        performs a step operation on the input step data\n\n        Parameters:\n        ----------\n        step: Tensor (batch_size, input_size)\n        memory_state: Tuple\n            a tuple of current memory parameters\n        controller_state: Tuple\n            the state of the controller if it's recurrent\n\n        Returns: Tuple\n            output: Tensor (batch_size, output_size)\n            memory_view: dict\n        \"\"\"\n\n        last_read_vectors = memory_state[6]\n        pre_output, interface, nn_state = None, None, None\n\n        if self.controller.has_recurrent_nn:\n            pre_output, interface, nn_state, decoder_out = self.controller.process_input(step, last_read_vectors, controller_state)\n        else:\n            pre_output, interface, decoder_out = self.controller.process_input(step, last_read_vectors)\n\n        usage_vector, write_weighting, memory_matrix, link_matrix, precedence_vector = self.memory.write(\n            memory_state[0], memory_state[1], memory_state[5],\n            memory_state[4], memory_state[2], memory_state[3],\n            interface['write_key'],\n            interface['write_strength'],\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n            interface['write_vector'],\n            interface['erase_vector']\n        )\n\n        if isinstance(self.memory, SharpMemory) or isinstance(self.memory, KMemory):\n            read_weightings, read_vectors = self.memory.read(\n                memory_matrix,\n                memory_state[5],\n                interface['read_keys'],\n                interface['read_strengths'],\n                link_matrix,\n                interface['read_modes'],\n                memory_state[1],\n            )\n        else:\n            read_weightings, read_vectors = self.memory.read(\n                memory_matrix,\n                memory_state[5],\n                interface['read_keys'],\n                interface['read_strengths'],\n                link_matrix,\n                interface['read_modes'],\n            )\n\n        return [\n\n            # report new memory state to be updated outside the condition branch\n            memory_matrix,\n            usage_vector,\n            precedence_vector,\n            link_matrix,\n            write_weighting,\n            read_weightings,\n            read_vectors,\n\n            self.controller.final_output(pre_output, read_vectors),\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n\n            # report new state of RNN if exists\n            nn_state if nn_state is not None else tf.zeros(1),\n            decoder_out\n        ]\n\n    def _loop_body(self, time, memory_state, outputs, decoder_outputs, free_gates, allocation_gates, write_gates,\n                   read_weightings, write_weightings, usage_vectors, controller_state, *memory_state_record):\n        \"\"\"\n        the body of the DNC sequence processing loop\n\n        Parameters:\n        ----------\n        time: Tensor\n        outputs: TensorArray\n        memory_state: Tuple\n        free_gates: TensorArray\n        allocation_gates: TensorArray\n        write_gates: TensorArray\n        read_weightings: TensorArray,\n        write_weightings: TensorArray,\n        usage_vectors: TensorArray,\n        controller_state: Tuple\n\n        Returns: Tuple containing all updated arguments\n        \"\"\"\n\n        step_input = self.unpacked_input_data.read(time)\n\n        if self.feedback:\n            if time == 0 and self.testing:\n                step_input = tf.slice(step_input, [0, 0], [self.batch_size, self.input_size - self.output_size])\n                step_input = tf.concat([step_input, tf.zeros([self.batch_size, self.output_size])], 1)\n            elif self.testing:\n                step_input = tf.slice(step_input, [0, 0], [self.batch_size, self.input_size - self.output_size])\n                step_input = tf.concat([step_input, self.output_t], 1)\n\n        output_list = self._step_op(step_input, memory_state, controller_state)\n\n        # update memory parameters\n\n        new_controller_state = tf.zeros(1)\n        new_memory_state = tuple(output_list[0:7])\n\n        new_controller_state = output_list[11]\n\n        outputs = outputs.write(time, output_list[7])\n        self.output_t = output_list[7]\n        decoder_outputs = decoder_outputs.write(time, output_list[12])\n\n        # collecting memory view for the current step\n        free_gates = free_gates.write(time, output_list[8])\n        allocation_gates = allocation_gates.write(time, output_list[9])\n        write_gates = write_gates.write(time, output_list[10])\n        read_weightings = read_weightings.write(time, output_list[5])\n        write_weightings = write_weightings.write(time, output_list[4])\n        usage_vectors = usage_vectors.write(time, output_list[1])\n\n        memory_state_list = list(memory_state_record)\n        if self.testing:\n            memory_state_list[0] = memory_state_record[0].write(time, new_memory_state[0])\n            memory_state_list[1] = memory_state_record[1].write(time, new_memory_state[1])\n            memory_state_list[2] = memory_state_record[2].write(time, new_memory_state[2])\n            memory_state_list[3] = memory_state_record[3].write(time, new_memory_state[3])\n            memory_state_list[4] = memory_state_record[4].write(time, new_memory_state[4])\n            memory_state_list[5] = memory_state_record[5].write(time, new_memory_state[5])\n            memory_state_list[6] = memory_state_record[6].write(time, new_memory_state[6])\n\n        return (\n            time + 1, new_memory_state, outputs, decoder_outputs,\n            free_gates, allocation_gates, write_gates,\n            read_weightings, write_weightings,\n            usage_vectors, new_controller_state,\n            *memory_state_list\n        )\n\n    def build_graph(self):\n        \"\"\"\n        builds the computational graph that performs a step-by-step evaluation\n        of the input data batches\n        \"\"\"\n\n        self.unpacked_input_data = unpack_into_tensorarray(self.input_data, 1, self.sequence_length)\n\n        outputs = tf.TensorArray(tf.float32, self.sequence_length)\n        decoder_outputs = tf.TensorArray(tf.float32, self.sequence_length)\n        usage_vectors = tf.TensorArray(tf.float32, self.sequence_length)\n\n        free_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        allocation_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        write_gates = tf.TensorArray(tf.float32, self.sequence_length)\n\n        read_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n        write_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n\n        controller_state = self.controller.get_state() if self.controller.has_recurrent_nn else (tf.zeros(1), tf.zeros(1))\n        memory_state = self.memory.init_memory()\n        memory_state_record = [\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n        ]\n\n        # This 2 line of code will cause problem if controller have more than 1 layer.\n        # if not isinstance(controller_state, LSTMStateTuple):\n        #     controller_state = LSTMStateTuple(controller_state[0], controller_state[1])\n\n        final_results = None\n\n        time = tf.constant(0, dtype=tf.int32)\n\n        final_results = tf.while_loop(\n            cond=lambda time, *_: time < self.sequence_length,\n            body=self._loop_body,\n            loop_vars=(\n                time, memory_state, outputs, decoder_outputs,\n                free_gates, allocation_gates, write_gates,\n                read_weightings, write_weightings,\n                usage_vectors, controller_state, *memory_state_record,\n            ),\n            parallel_iterations=64,\n            swap_memory=True\n        )\n\n        dependencies = []\n        if self.controller.has_recurrent_nn:\n            dependencies.append(self.controller.update_state(final_results[9]))  # result[9] is new_controller_state\n\n        with tf.control_dependencies(dependencies):\n            self.packed_output = pack_into_tensor(final_results[2], axis=1)\n            self.packed_decoder_output = pack_into_tensor(final_results[3], axis=1)\n\n            self.packed_memory_view = {\n                'free_gates': pack_into_tensor(final_results[4], axis=1),\n                'allocation_gates': pack_into_tensor(final_results[5], axis=1),\n                'write_gates': pack_into_tensor(final_results[6], axis=1),\n                'read_weightings': pack_into_tensor(final_results[7], axis=1),\n                'write_weightings': pack_into_tensor(final_results[8], axis=1),\n                'usage_vectors': pack_into_tensor(final_results[9], axis=1)\n            }\n            if self.testing:\n                self.packed_memory_matrixs = {\n                    'memory_matrix': pack_into_tensor(final_results[11], axis=1),\n                    'usage_vector': pack_into_tensor(final_results[12], axis=1),\n                    'precedence_vector': pack_into_tensor(final_results[13], axis=1),\n                    'link_matrix': pack_into_tensor(final_results[14], axis=1),\n                    'write_weighting': pack_into_tensor(final_results[15], axis=1),\n                    'read_weightings': pack_into_tensor(final_results[16], axis=1),\n                    'read_vectors': pack_into_tensor(final_results[17], axis=1),\n                }\n\n    def get_outputs(self):\n        \"\"\"\n        Returns: Tuple\n            outputs: Tensor (batch_size, time_steps, output_size)\n            memory_view: dict\n        \"\"\"\n        return self.packed_output, self.packed_memory_view\n\n    def get_decoder_output(self):\n        return self.packed_decoder_output\n\n\nclass DNCPostControl(DNC):\n\n    def __init__(self, controller_class, post_controller_class, input_size, output_size, max_sequence_length,\n                 memory_words_num=256, memory_word_size=64, memory_read_heads=4, batch_size=1, testing=False):\n        self.post_control = post_controller_class(\n            memory_word_size * memory_read_heads + batch_size * output_size,\n            output_size, batch_size\n        )\n\n        self.testing = testing\n        self.feedback = output_feedback\n        self.output_t = tf.zeros([batch_size, output_size])\n\n        self.input_size = input_size + output_size if output_feedback else input_size\n        self.output_size = output_size\n        self.max_sequence_length = max_sequence_length\n        self.words_num = memory_words_num\n        self.word_size = memory_word_size\n        self.read_heads = memory_read_heads\n        self.batch_size = batch_size\n\n        self.memory = Memory(self.words_num, self.word_size, self.read_heads, self.batch_size)\n        self.controller = controller_class(self.input_size, self.input_size, self.read_heads, self.word_size, self.batch_size)\n\n        # input data placeholders\n        self.input_data = tf.placeholder(tf.float32, [batch_size, None, input_size], name='input')\n        self.target_output = tf.placeholder(tf.float32, [batch_size, None, output_size], name='targets')\n        self.target_output_id = tf.placeholder(tf.int32, [batch_size, None], name='targets_id')\n        self.sequence_length = tf.placeholder(tf.int32, name='sequence_length')\n\n        self.build_graph()\n\n    def _step_op(self, step, memory_state, controller_state=None, post_controller_state=None):\n\n        last_read_vectors = memory_state[6]\n        pre_output, interface, nn_state, post_nn_state = None, None, None, None\n\n        if self.controller.has_recurrent_nn:\n            pre_output, interface, nn_state = self.controller.process_input(step, last_read_vectors, controller_state)\n        else:\n            pre_output, interface = self.controller.process_input(step, last_read_vectors)\n\n        usage_vector, write_weighting, memory_matrix, link_matrix, precedence_vector = self.memory.write(\n            memory_state[0], memory_state[1], memory_state[5],\n            memory_state[4], memory_state[2], memory_state[3],\n            interface['write_key'],\n            interface['write_strength'],\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n            interface['write_vector'],\n            interface['erase_vector']\n        )\n\n        read_weightings, read_vectors = self.memory.read(\n            memory_matrix,\n            memory_state[5],\n            interface['read_keys'],\n            interface['read_strengths'],\n            link_matrix,\n            interface['read_modes'],\n        )\n\n        final_out, post_nn_state = self.post_control.network_op(\n            self.controller.final_output(pre_output, read_vectors),\n            post_controller_state\n        )\n\n        return [\n\n            # report new memory state to be updated outside the condition branch\n            memory_matrix,\n            usage_vector,\n            precedence_vector,\n            link_matrix,\n            write_weighting,\n            read_weightings,\n            read_vectors,\n\n            final_out,\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n\n            # report new state of RNN if exists\n            nn_state if nn_state is not None else tf.zeros(1),\n            post_nn_state if nn_state is not None else tf.zeros(1),\n        ]\n\n    def _loop_body(self, time, memory_state, outputs, free_gates, allocation_gates, write_gates, read_weightings,\n                   write_weightings, usage_vectors, controller_state, post_controller_state, *memory_state_record):\n\n        step_input = self.unpacked_input_data.read(time)\n\n        if self.feedback:\n            if time == 0 and self.testing:\n                step_input = tf.slice(step_input, [0, 0], [self.batch_size, self.input_size - self.output_size])\n                step_input = tf.concat([step_input, tf.zeros([self.batch_size, self.output_size])], 1)\n            elif self.testing:\n                step_input = tf.slice(step_input, [0, 0], [self.batch_size, self.input_size - self.output_size])\n                step_input = tf.concat([step_input, self.output_t], 1)\n\n        output_list = self._step_op(step_input, memory_state, controller_state, post_controller_state)\n\n        # update memory parameters\n\n        new_controller_state = tf.zeros(1)\n        new_post_controller_state = tf.zeros(1)\n        new_memory_state = tuple(output_list[0:7])\n\n        new_controller_state = output_list[11]\n        new_post_controller_state = output_list[12]\n\n        outputs = outputs.write(time, output_list[7])\n        self.output_t = output_list[7]\n\n        # collecting memory view for the current step\n        free_gates = free_gates.write(time, output_list[8])\n        allocation_gates = allocation_gates.write(time, output_list[9])\n        write_gates = write_gates.write(time, output_list[10])\n        read_weightings = read_weightings.write(time, output_list[5])\n        write_weightings = write_weightings.write(time, output_list[4])\n        usage_vectors = usage_vectors.write(time, output_list[1])\n\n        memory_state_list = list(memory_state_record)\n        if self.testing:\n            memory_state_list[0] = memory_state_record[0].write(time, new_memory_state[0])\n            memory_state_list[1] = memory_state_record[1].write(time, new_memory_state[1])\n            memory_state_list[2] = memory_state_record[2].write(time, new_memory_state[2])\n            memory_state_list[3] = memory_state_record[3].write(time, new_memory_state[3])\n            memory_state_list[4] = memory_state_record[4].write(time, new_memory_state[4])\n            memory_state_list[5] = memory_state_record[5].write(time, new_memory_state[5])\n            memory_state_list[6] = memory_state_record[6].write(time, new_memory_state[6])\n\n        return (\n            time + 1, new_memory_state, outputs,\n            free_gates, allocation_gates, write_gates,\n            read_weightings, write_weightings,\n            usage_vectors, new_controller_state, new_post_controller_state,\n            *memory_state_list\n        )\n\n    def build_graph(self):\n        \"\"\"\n        builds the computational graph that performs a step-by-step evaluation\n        of the input data batches\n        \"\"\"\n\n        self.unpacked_input_data = unpack_into_tensorarray(self.input_data, 1, self.sequence_length)\n\n        outputs = tf.TensorArray(tf.float32, self.sequence_length)\n        usage_vectors = tf.TensorArray(tf.float32, self.sequence_length)\n\n        free_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        allocation_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        write_gates = tf.TensorArray(tf.float32, self.sequence_length)\n\n        read_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n        write_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n\n        controller_state = self.controller.get_state() if self.controller.has_recurrent_nn else (tf.zeros(1), tf.zeros(1))\n        post_controller_state = self.post_control.get_state()\n\n        memory_state = self.memory.init_memory()\n        memory_state_record = [\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n        ]\n\n        # if not isinstance(controller_state, LSTMStateTuple):\n        #     controller_state = LSTMStateTuple(controller_state[0], controller_state[1])\n        # if not isinstance(post_controller_state, LSTMStateTuple):\n        #     post_controller_state = LSTMStateTuple(post_controller_state[0], post_controller_state[1])\n\n        final_results = None\n\n        time = tf.constant(0, dtype=tf.int32)\n\n        final_results = tf.while_loop(\n            cond=lambda time, *_: time < self.sequence_length,\n            body=self._loop_body,\n            loop_vars=(\n                time, memory_state, outputs,\n                free_gates, allocation_gates, write_gates,\n                read_weightings, write_weightings,\n                usage_vectors, controller_state, post_controller_state,\n                *memory_state_record,\n            ),\n            parallel_iterations=32,\n            swap_memory=True\n        )\n\n        dependencies = []\n        if self.controller.has_recurrent_nn:\n            dependencies.append(self.controller.update_state(final_results[9]))  # result[9] is new_controller_state\n            dependencies.append(self.post_control.update_state(final_results[10]))  # result[9] is new_controller_state\n\n        with tf.control_dependencies(dependencies):\n            self.packed_output = pack_into_tensor(final_results[2], axis=1)\n\n            self.packed_memory_view = {\n                'free_gates': pack_into_tensor(final_results[3], axis=1),\n                'allocation_gates': pack_into_tensor(final_results[4], axis=1),\n                'write_gates': pack_into_tensor(final_results[5], axis=1),\n                'read_weightings': pack_into_tensor(final_results[6], axis=1),\n                'write_weightings': pack_into_tensor(final_results[7], axis=1),\n                'usage_vectors': pack_into_tensor(final_results[8], axis=1)\n            }\n            if self.testing:\n                self.packed_memory_matrixs = {\n                    'memory_matrix': pack_into_tensor(final_results[11], axis=1),\n                    'usage_vector': pack_into_tensor(final_results[12], axis=1),\n                    'precedence_vector': pack_into_tensor(final_results[13], axis=1),\n                    'link_matrix': pack_into_tensor(final_results[14], axis=1),\n                    'write_weighting': pack_into_tensor(final_results[15], axis=1),\n                    'read_weightings': pack_into_tensor(final_results[16], axis=1),\n                    'read_vectors': pack_into_tensor(final_results[17], axis=1),\n                }\n\n\nclass DNCDirectPostControl(DNCPostControl):\n\n    def __init__(self, controller_class, post_controller_class, input_size, output_size, max_sequence_length,\n                 memory_words_num=256, memory_word_size=64, memory_read_heads=4, batch_size=1, testing=False):\n        self.post_control = post_controller_class(\n            input_size + batch_size * input_size,\n            output_size, cell_num=512\n        )\n\n        self.testing = testing\n\n        self.input_size = input_size\n        self.output_size = output_size\n        self.max_sequence_length = max_sequence_length\n        self.words_num = memory_words_num\n        self.word_size = memory_word_size\n        self.read_heads = memory_read_heads\n        self.batch_size = batch_size\n\n        self.memory = Memory(self.words_num, self.word_size, self.read_heads, self.batch_size)\n        self.controller = controller_class(self.input_size, self.post_control.cell_num, self.read_heads, self.word_size, self.batch_size)\n\n        # input data placeholders\n        self.input_data = tf.placeholder(tf.float32, [batch_size, None, input_size], name='input')\n        self.target_output = tf.placeholder(tf.float32, [batch_size, None, output_size], name='targets')\n        self.target_output_id = tf.placeholder(tf.int32, [batch_size, None], name='targets_id')\n        self.sequence_length = tf.placeholder(tf.int32, name='sequence_length')\n\n        self.build_graph()\n\n    def _step_op(self, step_input, memory_state, controller_state=None, post_controller_state=None):\n\n        last_read_vectors = memory_state[6]\n        pre_output, interface, nn_state, post_nn_state = None, None, None, None\n\n        if self.controller.has_recurrent_nn:\n            pre_output, interface, nn_state = self.controller.process_input(step_input, last_read_vectors, controller_state)\n        else:\n            pre_output, interface = self.controller.process_input(step_input, last_read_vectors)\n\n        usage_vector, write_weighting, memory_matrix, link_matrix, precedence_vector = self.memory.write(\n            memory_state[0], memory_state[1], memory_state[5],\n            memory_state[4], memory_state[2], memory_state[3],\n            interface['write_key'],\n            interface['write_strength'],\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n            interface['write_vector'],\n            interface['erase_vector']\n        )\n\n        read_weightings, read_vectors = self.memory.read(\n            memory_matrix,\n            memory_state[5],\n            interface['read_keys'],\n            interface['read_strengths'],\n            link_matrix,\n            interface['read_modes'],\n        )\n\n        final_out, post_nn_state = self.post_control.network_op(\n            self.controller.final_output(pre_output, read_vectors),\n            step_input,\n            post_controller_state\n        )\n\n        return [\n\n            # report new memory state to be updated outside the condition branch\n            memory_matrix,\n            usage_vector,\n            precedence_vector,\n            link_matrix,\n            write_weighting,\n            read_weightings,\n            read_vectors,\n\n            final_out,\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n\n            # report new state of RNN if exists\n            nn_state if nn_state is not None else tf.zeros(1),\n            post_nn_state if nn_state is not None else tf.zeros(1),\n        ]\n\n\nclass DNCDuo(DNCPostControl):\n\n    def __init__(self, controller_class, input_size, output_size, max_sequence_length,\n                 memory_words_num=256, memory_word_size=64, memory_read_heads=4, batch_size=1, testing=False, output_feedback=False):\n\n        self.testing = testing\n        self.feedback = output_feedback\n        self.output_t = tf.zeros([batch_size, output_size])\n\n        self.input_size = input_size + output_size if output_feedback else input_size\n        self.output_size = output_size\n        self.max_sequence_length = max_sequence_length\n        self.words_num = memory_words_num\n        self.word_size = memory_word_size\n        self.read_heads = memory_read_heads\n        self.batch_size = batch_size\n\n        self.memory = KMemory(self.words_num, self.word_size, self.read_heads, self.batch_size)\n        self.packed_memory_matrixs = {}\n        self.controller = controller_class(self.input_size, self.output_size, self.read_heads, self.word_size, self.batch_size)\n\n        # input data placeholders\n        self.input_data = tf.placeholder(tf.float32, [batch_size, None, self.input_size], name='input')\n        self.target_output = tf.placeholder(tf.float32, [batch_size, None, self.output_size], name='targets')\n        self.target_output_id = tf.placeholder(tf.int32, [batch_size, None], name='targets_id')\n        self.sequence_length = tf.placeholder(tf.int32, name='sequence_length')\n\n        self.build_graph()\n\n    def _step_op(self, step, memory_state, controller_state_mem=None, controller_state_pred=None):\n\n        last_read_vectors = memory_state[6]\n        pre_output, interface, nn_state_mem, nn_state_pred = None, None, None, None\n\n        if self.controller.has_recurrent_nn:\n            pre_output, interface, nn_state_mem, nn_state_pred = self.controller.process_input(step, last_read_vectors, controller_state_mem, controller_state_pred)\n        else:\n            pre_output, interface = self.controller.process_input(step, last_read_vectors)\n\n        usage_vector, write_weighting, memory_matrix, link_matrix, precedence_vector = self.memory.write(\n            memory_state[0], memory_state[1], memory_state[5],\n            memory_state[4], memory_state[2], memory_state[3],\n            interface['write_key'],\n            interface['write_strength'],\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n            interface['write_vector'],\n            interface['erase_vector']\n        )\n\n        if type(self.memory) is SharpMemory or isinstance(self.memory, KMemory):\n            read_weightings, read_vectors = self.memory.read(\n                memory_matrix,\n                memory_state[5],\n                interface['read_keys'],\n                interface['read_strengths'],\n                link_matrix,\n                interface['read_modes'],\n                memory_state[1],\n            )\n        else:\n            read_weightings, read_vectors = self.memory.read(\n                memory_matrix,\n                memory_state[5],\n                interface['read_keys'],\n                interface['read_strengths'],\n                link_matrix,\n                interface['read_modes'],\n            )\n\n        final_out = self.controller.final_output(pre_output, read_vectors)\n\n        return [\n\n            # report new memory state to be updated outside the condition branch\n            memory_matrix,\n            usage_vector,\n            precedence_vector,\n            link_matrix,\n            write_weighting,\n            read_weightings,\n            read_vectors,\n\n            final_out,\n            interface['free_gates'],\n            interface['allocation_gate'],\n            interface['write_gate'],\n\n            # report new state of RNN if exists\n            nn_state_mem if nn_state_mem is not None else tf.zeros(1),\n            nn_state_pred if nn_state_pred is not None else tf.zeros(1),\n        ]\n\n    def build_graph(self):\n        \"\"\"\n        builds the computational graph that performs a step-by-step evaluation\n        of the input data batches\n        \"\"\"\n\n        self.unpacked_input_data = unpack_into_tensorarray(self.input_data, 1, self.sequence_length)\n\n        outputs = tf.TensorArray(tf.float32, self.sequence_length)\n        usage_vectors = tf.TensorArray(tf.float32, self.sequence_length)\n\n        free_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        allocation_gates = tf.TensorArray(tf.float32, self.sequence_length)\n        write_gates = tf.TensorArray(tf.float32, self.sequence_length)\n\n        read_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n        write_weightings = tf.TensorArray(tf.float32, self.sequence_length)\n\n        controller_state_mem, controller_state_pred = self.controller.get_state() if self.controller.has_recurrent_nn else ((tf.zeros(1), tf.zeros(1)), (tf.zeros(1), tf.zeros(1)))\n        # controller_state_pred = self.post_control.get_state()\n\n        memory_state = self.memory.init_memory()\n        memory_state_record = [\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n            tf.TensorArray(tf.float32, self.sequence_length),\n        ]\n\n        final_results = None\n\n        time = tf.constant(0, dtype=tf.int32)\n\n        final_results = tf.while_loop(\n            cond=lambda time, *_: time < self.sequence_length,\n            body=self._loop_body,\n            loop_vars=(\n                time, memory_state, outputs,\n                free_gates, allocation_gates, write_gates,\n                read_weightings, write_weightings,\n                usage_vectors, controller_state_mem, controller_state_pred,\n                *memory_state_record,\n            ),\n            parallel_iterations=32,\n            swap_memory=True\n        )\n\n        dependencies = []\n        if self.controller.has_recurrent_nn:\n            dependencies.append(self.controller.update_state(final_results[9]))  # result[9] is new_controller_state\n            # dependencies.append(self.post_control.update_state(final_results[10]))  # result[9] is new_controller_state\n\n        with tf.control_dependencies(dependencies):\n            self.packed_output = pack_into_tensor(final_results[2], axis=1)\n\n            self.packed_memory_view = {\n                'free_gates': pack_into_tensor(final_results[3], axis=1),\n                'allocation_gates': pack_into_tensor(final_results[4], axis=1),\n                'write_gates': pack_into_tensor(final_results[5], axis=1),\n                'read_weightings': pack_into_tensor(final_results[6], axis=1),\n                'write_weightings': pack_into_tensor(final_results[7], axis=1),\n                'usage_vectors': pack_into_tensor(final_results[8], axis=1)\n            }\n            if self.testing:\n                self.packed_memory_matrixs = {\n                    'memory_matrix': pack_into_tensor(final_results[11], axis=1),\n                    'usage_vector': pack_into_tensor(final_results[12], axis=1),\n                    'precedence_vector': pack_into_tensor(final_results[13], axis=1),\n                    'link_matrix': pack_into_tensor(final_results[14], axis=1),\n                    'write_weighting': pack_into_tensor(final_results[15], axis=1),\n                    'read_weightings': pack_into_tensor(final_results[16], axis=1),\n                    'read_vectors': pack_into_tensor(final_results[17], axis=1),\n                }\n", "framework": "tensorflow"}
{"repo_name": "srom/chessbot", "file_path": "estimator/train/model.py", "content": "from __future__ import unicode_literals\n\nimport tensorflow as tf\n\n\nINPUT_DIMENSION = 768  # 8 x 8 squares x 12 piece types\nHIDDEN_UNITS = 2048\nKAPPA = 10.0  # Emphasizes f(p) = -f(q)\n\n\nclass ChessDNNEstimator(object):\n\n    def __init__(self, learning_rate, adam_epsilon):\n        with tf.variable_scope(\"input\"):\n            self.X = tf.placeholder(tf.float32, shape=(None, INPUT_DIMENSION), name='X')\n            self.X_parent = tf.placeholder(tf.float32, shape=(None, INPUT_DIMENSION), name='X_parent')\n            self.X_observed = tf.placeholder(tf.float32, shape=(None, INPUT_DIMENSION), name='X_observed')\n            self.X_random = tf.placeholder(tf.float32, shape=(None, INPUT_DIMENSION), name='X_random')\n\n        with tf.variable_scope(\"f_p\"):\n            self.training = tf.placeholder_with_default(False, shape=(), name='training')\n            self.f = self._get_evaluation_function(self.X)\n            tf.get_variable_scope().reuse_variables()\n            self.f_parent = self._get_evaluation_function(self.X_parent)\n            self.f_observed = self._get_evaluation_function(self.X_observed)\n            self.f_random = self._get_evaluation_function(self.X_random)\n\n        with tf.name_scope('loss'):\n            self.loss = self._get_loss()\n\n        with tf.name_scope('train'):\n            self.training_op = self._get_training_op(learning_rate, adam_epsilon)\n\n    def train(self, session, X_parent, X_observed, X_random):\n        session.run(self.training_op, feed_dict={\n            self.X_parent: X_parent,\n            self.X_observed: X_observed,\n            self.X_random: X_random,\n            self.training: True,\n        })\n\n    def compute_loss(self, session, X_parent, X_observed, X_random):\n        return session.run(self.loss, feed_dict={\n            self.X_parent: X_parent,\n            self.X_observed: X_observed,\n            self.X_random: X_random,\n        })\n\n    def evaluate(self, session, X):\n        return session.run(self.f, feed_dict={\n            self.X: X\n        })\n\n    def _get_evaluation_function(self, X):\n        hidden_1 = tf.layers.dense(X, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_1')\n        hidden_2 = tf.layers.dense(hidden_1, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_2')\n        output = tf.layers.dense(hidden_2, 1, activation=None, name='output')\n        return output\n\n    def _get_loss(self):\n        x_observed_random = self.f_random - self.f_observed\n        x_parent_observed = self.f_parent + self.f_observed\n\n        epsilon_log = 1e-3\n        loss_a = -tf.log(epsilon_log + tf.sigmoid(x_observed_random))\n        loss_b = -tf.log(epsilon_log + tf.sigmoid(KAPPA * x_parent_observed))\n        loss_c = -tf.log(epsilon_log + tf.sigmoid(-KAPPA * x_parent_observed))\n\n        return tf.reduce_mean(loss_a + loss_b + loss_c, name='loss')\n\n    def _get_training_op(self, learning_rate, epsilon):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)\n        return optimizer.minimize(self.loss)\n", "framework": "tensorflow"}
{"repo_name": "nayutaya/tensorflow-rnn-sin", "file_path": "ex1/basic/rnn.py", "content": "\nimport tensorflow as tf\nfrom tensorflow.models.rnn import rnn, rnn_cell\nimport numpy as np\nimport random\n\ndef make_mini_batch(train_data, size_of_mini_batch, length_of_sequences):\n    inputs  = np.empty(0)\n    outputs = np.empty(0)\n    for _ in range(size_of_mini_batch):\n        index   = random.randint(0, len(train_data) - length_of_sequences)\n        part    = train_data[index:index + length_of_sequences]\n        inputs  = np.append(inputs, part[:, 0])\n        outputs = np.append(outputs, part[-1, 1])\n    inputs  = inputs.reshape(-1, length_of_sequences, 1)\n    outputs = outputs.reshape(-1, 1)\n    return (inputs, outputs)\n\ndef make_prediction_initial(train_data, index, length_of_sequences):\n    return train_data[index:index + length_of_sequences, 0]\n\ntrain_data_path             = \"../train_data/normal.npy\"\nnum_of_input_nodes          = 1\nnum_of_hidden_nodes         = 2\nnum_of_output_nodes         = 1\nlength_of_sequences         = 50\nnum_of_training_epochs      = 2000\nlength_of_initial_sequences = 50\nnum_of_prediction_epochs    = 100\nsize_of_mini_batch          = 100\nlearning_rate               = 0.1\nforget_bias                 = 1.0\nprint(\"train_data_path             = %s\" % train_data_path)\nprint(\"num_of_input_nodes          = %d\" % num_of_input_nodes)\nprint(\"num_of_hidden_nodes         = %d\" % num_of_hidden_nodes)\nprint(\"num_of_output_nodes         = %d\" % num_of_output_nodes)\nprint(\"length_of_sequences         = %d\" % length_of_sequences)\nprint(\"num_of_training_epochs      = %d\" % num_of_training_epochs)\nprint(\"length_of_initial_sequences = %d\" % length_of_initial_sequences)\nprint(\"num_of_prediction_epochs    = %d\" % num_of_prediction_epochs)\nprint(\"size_of_mini_batch          = %d\" % size_of_mini_batch)\nprint(\"learning_rate               = %f\" % learning_rate)\nprint(\"forget_bias                 = %f\" % forget_bias)\n\ntrain_data = np.load(train_data_path)\nprint(\"train_data:\", train_data)\n\n# \u4e71\u6570\u30b7\u30fc\u30c9\u3092\u56fa\u5b9a\u3059\u308b\u3002\nrandom.seed(0)\nnp.random.seed(0)\ntf.set_random_seed(0)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n\nwith tf.Graph().as_default():\n    input_ph      = tf.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n    supervisor_ph = tf.placeholder(tf.float32, [None, num_of_output_nodes], name=\"supervisor\")\n    istate_ph     = tf.placeholder(tf.float32, [None, num_of_hidden_nodes * 2], name=\"istate\") # 1\u30bb\u30eb\u3042\u305f\u308a2\u3064\u306e\u5024\u3092\u5fc5\u8981\u3068\u3059\u308b\u3002\n\n    with tf.name_scope(\"inference\") as scope:\n        weight1_var = tf.Variable(tf.truncated_normal([num_of_input_nodes, num_of_hidden_nodes], stddev=0.1), name=\"weight1\")\n        weight2_var = tf.Variable(tf.truncated_normal([num_of_hidden_nodes, num_of_output_nodes], stddev=0.1), name=\"weight2\")\n        bias1_var   = tf.Variable(tf.truncated_normal([num_of_hidden_nodes], stddev=0.1), name=\"bias1\")\n        bias2_var   = tf.Variable(tf.truncated_normal([num_of_output_nodes], stddev=0.1), name=\"bias2\")\n\n        in1 = tf.transpose(input_ph, [1, 0, 2])         # (batch, sequence, data) -> (sequence, batch, data)\n        in2 = tf.reshape(in1, [-1, num_of_input_nodes]) # (sequence, batch, data) -> (sequence * batch, data)\n        in3 = tf.matmul(in2, weight1_var) + bias1_var\n        in4 = tf.split(0, length_of_sequences, in3)     # sequence * (batch, data)\n\n        cell = rnn_cell.BasicLSTMCell(num_of_hidden_nodes, forget_bias=forget_bias)\n        rnn_output, states_op = rnn.rnn(cell, in4, initial_state=istate_ph)\n        output_op = tf.matmul(rnn_output[-1], weight2_var) + bias2_var\n\n    with tf.name_scope(\"loss\") as scope:\n        square_error = tf.reduce_mean(tf.square(output_op - supervisor_ph))\n        loss_op      = square_error\n        tf.scalar_summary(\"loss\", loss_op)\n\n    with tf.name_scope(\"training\") as scope:\n        training_op = optimizer.minimize(loss_op)\n\n    summary_op = tf.merge_all_summaries()\n    init = tf.initialize_all_variables()\n\n    with tf.Session() as sess:\n        saver = tf.train.Saver()\n        summary_writer = tf.train.SummaryWriter(\"data\", graph=sess.graph)\n        sess.run(init)\n\n        for epoch in range(num_of_training_epochs):\n            inputs, supervisors = make_mini_batch(train_data, size_of_mini_batch, length_of_sequences)\n\n            train_dict = {\n                input_ph:      inputs,\n                supervisor_ph: supervisors,\n                istate_ph:     np.zeros((size_of_mini_batch, num_of_hidden_nodes * 2)),\n            }\n            sess.run(training_op, feed_dict=train_dict)\n\n            if (epoch + 1) % 10 == 0:\n                summary_str, train_loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n                summary_writer.add_summary(summary_str, epoch)\n                print(\"train#%d, train loss: %e\" % (epoch + 1, train_loss))\n\n        inputs  = make_prediction_initial(train_data, 0, length_of_initial_sequences)\n        outputs = np.empty(0)\n        states  = np.zeros((num_of_hidden_nodes * 2)),\n\n        print(\"initial:\", inputs)\n        np.save(\"initial.npy\", inputs)\n\n        for epoch in range(num_of_prediction_epochs):\n            pred_dict = {\n                input_ph:  inputs.reshape((1, length_of_sequences, 1)),\n                istate_ph: states,\n            }\n            output, states = sess.run([output_op, states_op], feed_dict=pred_dict)\n            print(\"prediction#%d, output: %f\" % (epoch + 1, output))\n\n            inputs  = np.delete(inputs, 0)\n            inputs  = np.append(inputs, output)\n            outputs = np.append(outputs, output)\n\n        print(\"outputs:\", outputs)\n        np.save(\"output.npy\", outputs)\n\n        saver.save(sess, \"data/model\")\n", "framework": "tensorflow"}
{"repo_name": "ddddwee1/SULsT", "file_path": "example/OctConv/train.py", "content": "import tensorflow as tf \nimport model3 as M \nimport numpy as np \nimport octresnet as resnet\nimport losspart\nimport datareader\nimport time \n\nclass FaceResNet(M.Model):\n\tdef initialize(self, num_classes):\n\t\tself.resnet = resnet.ResNet([64,64,128,256,512], [3, 4, 14, 3], 512, 0.75)\n\t\tself.classifier = losspart.MarginalCosineLayer(num_classes)\n\n\tdef forward(self, x, label):\n\t\t# x = tf.image.resize(x, [128,128])\n\t\tfeat = self.resnet(x)\n\t\t# feat = tf.nn.dropout(feat, 0.4)\n\t\tlogits = self.classifier(feat, label, 1.0, 0.2, 0.0)\n\t\tlogits = logits * 64\n\t\treturn logits\n\nBSIZE = 260 - 4\nEPOCH = 30\ndata_reader = datareader.DataReader('img_list.txt', BSIZE)\ntf.keras.backend.set_learning_phase(True)\n\ndef grad_loss(x, model):\n\tdata, label = x\n\twith tf.GradientTape() as tape:\n\t\tout = model(data, label)\n\t\twd = 0.0001\n\t\tw_reg = wd * sum([tf.reduce_sum(tf.square(w)) for w in model.trainable_variables]) \n\t\tloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=label)) + w_reg\n\tacc = M.accuracy(out, label, one_hot=False) \n\tgrads = tape.gradient(loss, model.trainable_variables)\n\treturn grads, [loss, acc]\n\n# monitoring time\nt0 = time.time()\n# batch = data_reader.get_next()\nLRV = 0.001\nwith tf.device('/cpu:0'):\n\tmodel = FaceResNet(data_reader.max_label + 1)\n\tLR = tf.Variable(LRV, trainable=False)\n\toptimizer = tf.optimizers.SGD(LR,0.9)\n\tsaver = M.Saver(model, optimizer)\n\tsaver.restore('./model/')\n\t\n\n\t_ = model(np.float32(np.ones([1,112,112,3])), np.float32(np.eye(data_reader.max_label+1)[0]))\n\t\n\tpt = M.ParallelTraining(model, optimizer, [0,1,2,3], grad_loss_fn=grad_loss) \n\n\tfor ep in range(EPOCH):\n\t\tif ep==1:\n\t\t\tLRV = 0.1\n\t\t\tLR.assign(LRV)\n\t\tif ep in [8,12,16]:\n\t\t\tLRV *= 0.1 \n\t\t\tLR.assign(LRV)\n\t\tfor it in range(data_reader.iter_per_epoch):\n\t\t\tbatch = data_reader.get_next()\n\t\t\tbatch_distribute = pt.split_data(batch)\n\t\t\tlsacc = pt.train_step(batch_distribute)\n\n\t\t\tif it%10==0:\n\t\t\t\tt1 = time.time()\n\t\t\t\timg_sec = 10 * BSIZE / (t1-t0)\n\t\t\t\tls = tf.reduce_mean([_[0] for _ in lsacc])\n\t\t\t\tacc = tf.reduce_mean([_[1] for _ in lsacc])\n\t\t\t\tt0 = t1 \n\t\t\t\tprint('Epoch:%d\\tIter:%d\\tLoss:%.6f\\tAcc:%.6f\\tSpeed:%.2f\\tLR:%f'%(ep, it, ls, acc, img_sec, LRV))\n\n\t\t\tif it%5000==0 and it>0:\n\t\t\t\tsaver.save('./model/%d_%d.ckpt'%(ep,it))\n\t\t\t\tt0 = time.time()\n\n\t\tsaver.save('./model/%d_%d.ckpt'%(ep,it))\n", "framework": "tensorflow"}
{"repo_name": "ucloud/uai-sdk", "file_path": "examples/tensorflow/train/imagenet/code/imagenet_main.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"ResNet model for classifying images from Imagenet dataset.\n\nSupport single-host training with one or multiple devices.\n\nResNet as proposed in:\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\nDeep Residual Learning for Image Recognition. arXiv:1512.03385\n\n\"\"\"\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport functools\nimport itertools\nimport os\n\nimport imagenet\nimport imagenet_utils\nimport resnet_model\nimport numpy as np\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n_NUM_IMAGES = {\n    'train': 1281167,\n    'validation': 50000,\n}\n\ndef get_model_fn(num_gpus, variable_strategy, num_workers):\n  \"\"\"Returns a function that will build the resnet model.\"\"\"\n\n  def _resnet_model_fn(features, labels, mode, params):\n    \"\"\"Resnet model body.\n\n    Support single host, one or more GPU training. Parameter distribution can\n    be either one of the following scheme.\n    1. CPU is the parameter server and manages gradient updates.\n    2. Parameters are distributed evenly across all GPUs, and the first GPU\n       manages gradient updates.\n\n    Args:\n      features: a list of tensors, one for each tower\n      labels: a list of tensors, one for each tower\n      mode: ModeKeys.TRAIN or EVAL\n      params: Hyperparameters suitable for tuning\n    Returns:\n      A EstimatorSpec object.\n    \"\"\"\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    weight_decay = params.weight_decay\n    momentum = params.momentum\n\n    tower_features = features\n    tower_labels = labels\n    tower_losses = []\n    tower_gradvars = []\n    tower_preds = []\n\n    # channels first (NCHW) is normally optimal on GPU and channels last (NHWC)\n    # on CPU. The exception is Intel MKL on CPU which is optimal with\n    # channels_last.\n    data_format = params.data_format\n    if not data_format:\n      if num_gpus == 0:\n        data_format = 'channels_last'\n      else:\n        data_format = 'channels_first'\n\n    if num_gpus == 0:\n      num_devices = 1\n      device_type = 'cpu'\n    else:\n      num_devices = num_gpus\n      device_type = 'gpu'\n\n    for i in range(num_devices):\n      worker_device = '/{}:{}'.format(device_type, i)\n      if variable_strategy == 'CPU':\n        device_setter = imagenet_utils.local_device_setter(\n            worker_device=worker_device)\n      elif variable_strategy == 'GPU':\n        device_setter = imagenet_utils.local_device_setter(\n            ps_device_type='gpu',\n            worker_device=worker_device,\n            ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(\n                num_gpus, tf.contrib.training.byte_size_load_fn))\n      with tf.variable_scope('resnet', reuse=bool(i != 0)):\n        with tf.name_scope('tower_%d' % i) as name_scope:\n          with tf.device(device_setter):\n            loss, gradvars, preds = _tower_fn(\n                is_training, weight_decay, tower_features[i], tower_labels[i],\n                data_format, params.resnet_size, params.batch_norm_decay,\n                params.batch_norm_epsilon)\n            tower_losses.append(loss)\n            tower_gradvars.append(gradvars)\n            tower_preds.append(preds)\n            if i == 0:\n              # Only trigger batch_norm moving mean and variance update from\n              # the 1st tower. Ideally, we should grab the updates from all\n              # towers but these stats accumulate extremely fast so we can\n              # ignore the other stats from the other towers without\n              # significant detriment.\n              update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,\n                                             name_scope)\n\n    # Now compute global loss and gradients.\n    gradvars = []\n    with tf.name_scope('gradient_averaging'):\n      all_grads = {}\n      for grad, var in itertools.chain(*tower_gradvars):\n        if grad is not None:\n          all_grads.setdefault(var, []).append(grad)\n      for var, grads in six.iteritems(all_grads):\n        # Average gradients on the same device as the variables\n        # to which they apply.\n        with tf.device(var.device):\n          if len(grads) == 1:\n            avg_grad = grads[0]\n          else:\n            avg_grad = tf.multiply(tf.add_n(grads), 1. / len(grads))\n        gradvars.append((avg_grad, var))\n\n    # Device that runs the ops to apply global gradient updates.\n    consolidation_device = '/gpu:0' if variable_strategy == 'GPU' else '/cpu:0'\n    with tf.device(consolidation_device):\n      # Suggested learning rate scheduling from\n      batches_per_epoch = _NUM_IMAGES['train'] / (params.train_batch_size * num_workers)\n\n      boundaries = [\n          int(batches_per_epoch * epoch) for epoch in [30, 60, 80, 90]]\n      staged_lr = [params.learning_rate * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\n\n      learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),\n                                                  boundaries, staged_lr)\n      tf.identity(learning_rate, name='learning_rate')\n      tf.summary.scalar('learning_rate', learning_rate)\n\n      loss = tf.reduce_mean(tower_losses, name='loss')\n\n      examples_sec_hook = imagenet_utils.ExamplesPerSecondHook(\n          params.train_batch_size, every_n_steps=10)\n\n      tensors_to_log = {'learning_rate': learning_rate, 'loss': loss}\n\n      logging_hook = tf.train.LoggingTensorHook(\n          tensors=tensors_to_log, every_n_iter=100)\n\n      train_hooks = [logging_hook, examples_sec_hook]\n\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate, momentum=momentum)\n\n      if params.sync:\n        optimizer = tf.train.SyncReplicasOptimizer(\n            optimizer, replicas_to_aggregate=num_workers)\n        sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)\n        train_hooks.append(sync_replicas_hook)\n\n      # Create single grouped train op\n      train_op = [\n          optimizer.apply_gradients(\n              gradvars, global_step=tf.train.get_global_step())\n      ]\n      train_op.extend(update_ops)\n      train_op = tf.group(*train_op)\n\n      predictions = {\n          'classes':\n              tf.concat([p['classes'] for p in tower_preds], axis=0),\n          'probabilities':\n              tf.concat([p['probabilities'] for p in tower_preds], axis=0)\n      }\n      stacked_labels = tf.concat(labels, axis=0)\n      accuracy = tf.metrics.accuracy(stacked_labels, predictions['classes'])\n      metrics = {\n          'accuracy': accuracy\n      }\n      tf.identity(accuracy[1], name='train_accuracy')\n      tf.summary.scalar('train_accuracy', accuracy[1])\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op,\n        training_hooks=train_hooks,\n        eval_metric_ops=metrics)\n\n  return _resnet_model_fn\n\n\ndef _tower_fn(is_training, weight_decay, feature, label, data_format,\n              resnet_size, batch_norm_decay, batch_norm_epsilon):\n  \"\"\"Build computation tower (Resnet).\n\n  Args:\n    is_training: true if is training graph.\n    weight_decay: weight regularization strength, a float.\n    feature: a Tensor.\n    label: a Tensor.\n    data_format: channels_last (NHWC) or channels_first (NCHW).\n    num_layers: number of layers, an int.\n    batch_norm_decay: decay for batch normalization, a float.\n    batch_norm_epsilon: epsilon for batch normalization, a float.\n\n  Returns:\n    A tuple with the loss for the tower, the gradients and parameters, and\n    predictions.\n\n  \"\"\"\n  network = resnet_model.imagenet_resnet_v2(resnet_size, 1000 + 1, data_format)\n  logits = network(feature, is_training=is_training)\n  tower_pred = {\n      'classes': tf.argmax(input=logits, axis=1),\n      'probabilities': tf.nn.softmax(logits)\n  }\n\n  tower_loss = tf.losses.sparse_softmax_cross_entropy(\n      logits=logits, labels=label)\n  tower_loss = tf.reduce_mean(tower_loss)\n\n  model_params = tf.trainable_variables()\n  tower_loss += weight_decay * tf.add_n(\n      [tf.nn.l2_loss(v) for v in model_params])\n\n  tower_grad = tf.gradients(tower_loss, model_params)\n\n  return tower_loss, zip(tower_grad, model_params), tower_pred\n\n\ndef input_fn(data_dir,\n             subset,\n             num_shards,\n             batch_size,\n             use_distortion_for_training=True):\n  \"\"\"Create input graph for model.\n\n  Args:\n    data_dir: Directory where TFRecords representing the dataset are located.\n    subset: one of 'train', 'validate' and 'eval'.\n    num_shards: num of towers participating in data-parallel training.\n    batch_size: total batch size for training to be divided by the number of\n    shards.\n    use_distortion_for_training: True to use distortions.\n  Returns:\n    two lists of tensors for features and labels, each of num_shards length.\n  \"\"\"\n  with tf.device('/cpu:0'):\n    use_distortion = subset == 'train' and use_distortion_for_training\n    dataset = imagenet.ImagenetDataSet(data_dir, subset, use_distortion)\n\n    feature_shards, label_shards = dataset.make_batch(batch_size, is_training=(subset == 'train'), num_shards=num_shards)\n    return feature_shards, label_shards\n\ndef get_experiment_fn(data_dir,\n                      num_gpus,\n                      variable_strategy,\n                      use_distortion_for_training=True):\n  \"\"\"Returns an Experiment function.\n\n  Experiments perform training on several workers in parallel,\n  in other words experiments know how to invoke train and eval in a sensible\n  fashion for distributed training. Arguments passed directly to this\n  function are not tunable, all other arguments should be passed within\n  tf.HParams, passed to the enclosed function.\n\n  Args:\n      data_dir: str. Location of the data for input_fns.\n      num_gpus: int. Number of GPUs on each worker.\n      variable_strategy: String. CPU to use CPU as the parameter server\n      and GPU to use the GPUs as the parameter server.\n      use_distortion_for_training: bool. See imagenet.ImagenetDataSet.\n  Returns:\n      A function (tf.estimator.RunConfig, tf.contrib.training.HParams) ->\n      tf.contrib.learn.Experiment.\n\n      Suitable for use by tf.contrib.learn.learn_runner, which will run various\n      methods on Experiment (train, evaluate) based on information\n      about the current runner in `run_config`.\n  \"\"\"\n\n  def _experiment_fn(run_config, hparams):\n    \"\"\"Returns an Experiment.\"\"\"\n    # Create estimator.\n    train_input_fn = functools.partial(\n        input_fn,\n        data_dir,\n        subset='train',\n        num_shards=num_gpus,\n        batch_size=hparams.train_batch_size,\n        use_distortion_for_training=use_distortion_for_training)\n\n    eval_input_fn = functools.partial(\n        input_fn,\n        data_dir,\n        subset='validation',\n        batch_size=hparams.eval_batch_size,\n        num_shards=num_gpus)\n\n    num_eval_examples = _NUM_IMAGES['validation']\n    if num_eval_examples % hparams.eval_batch_size != 0:\n      raise ValueError(\n          'validation set size must be multiple of eval_batch_size')\n\n    train_steps = hparams.train_steps\n    eval_steps = num_eval_examples // hparams.eval_batch_size\n \n    classifier = tf.estimator.Estimator(\n        model_fn=get_model_fn(num_gpus, variable_strategy,\n                              run_config.num_worker_replicas or 1),\n        config=run_config,\n        params=hparams)\n\n    return classifier\n  return _experiment_fn\n\n\ndef main(output_dir, data_dir, num_gpus, train_epochs, epochs_per_eval, variable_strategy,\n         use_distortion_for_training, log_device_placement, num_intra_threads,\n         **hparams):\n  # The env variable is on deprecation path, default is set to off.\n  os.environ['TF_SYNC_ON_FINISH'] = '0'\n  os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n\n  # Session configuration.\n  sess_config = tf.ConfigProto(\n      allow_soft_placement=True,\n      log_device_placement=log_device_placement,\n      intra_op_parallelism_threads=num_intra_threads,\n      gpu_options=tf.GPUOptions(force_gpu_compatible=True))\n\n  # In UAI Platform We use output_dir to store model_dir\n  #                 We use data_dir to store input data\n  config = imagenet_utils.RunConfig(\n      session_config=sess_config, model_dir=output_dir).replace(save_checkpoints_secs=1e4)\n\n  hparams=tf.contrib.training.HParams(\n           is_chief=config.is_chief,\n           **hparams)\n\n  resnet_classifier = tf.estimator.Estimator(\n      model_fn=get_model_fn(num_gpus, variable_strategy, config.num_worker_replicas or 1),\n      config=config,\n      params=hparams)\n\n  for _ in range(train_epochs // epochs_per_eval):\n    tensors_to_log = {\n        'learning_rate': 'learning_rate',\n        'train_accuracy': 'train_accuracy'\n    }\n\n    logging_hook = tf.train.LoggingTensorHook(\n        tensors=tensors_to_log, every_n_iter=100)\n\n    print('Starting a training cycle.')\n    resnet_classifier.train(\n        input_fn=lambda: input_fn(\n            data_dir,\n            subset='train',\n            num_shards=num_gpus,\n            batch_size=hparams.train_batch_size,\n            use_distortion_for_training=use_distortion_for_training),\n        hooks=[logging_hook])\n\n    print('Starting to evaluate.')\n    eval_results = resnet_classifier.evaluate(\n        input_fn=lambda: input_fn(\n          data_dir,\n          subset='validation',\n          batch_size=hparams.eval_batch_size,\n          num_shards=num_gpus))\n    print(eval_results)\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  \n  \"\"\"UAI SDK use data_dir output_dir and num_gpus to transfer system specific data\n  \"\"\"\n  parser.add_argument(\n      '--data_dir',\n      type=str,\n      required=True,\n      help='UAI SDK related. The directory where the imagenet input data is stored.')\n  parser.add_argument(\n      '--output_dir',\n      type=str,\n      required=True,\n      help='UAI SDK related. The directory where the model will be stored.')\n  parser.add_argument(\n      '--variable-strategy',\n      choices=['CPU', 'GPU'],\n      type=str,\n      default='CPU',\n      help='Where to locate variable operations')\n  parser.add_argument(\n      '--num_gpus',\n      type=int,\n      default=1,\n      help='UAI SDK related. The number of gpus used.')\n  parser.add_argument(\n    '--resnet_size', type=int, default=50, choices=[18, 34, 50, 101, 152, 200],\n    help='The size of the ResNet model to use.')\n  parser.add_argument(\n      '--num-layers',\n      type=int,\n      default=44,\n      help='The number of layers of the model.')\n  parser.add_argument(\n      '--train-epochs',\n      type=int,\n      default=100,\n      help='The number of epochs to use for training.')\n  parser.add_argument(\n      '--epochs_per_eval', \n      type=int, \n      default=1,\n      help='The number of training epochs to run between evaluations.')\n  parser.add_argument(\n      '--train-batch-size',\n      type=int,\n      default=128,\n      help='Batch size for training.')\n  parser.add_argument(\n      '--eval-batch-size',\n      type=int,\n      default=100,\n      help='Batch size for validation.')\n  parser.add_argument(\n      '--momentum',\n      type=float,\n      default=0.9,\n      help='Momentum for MomentumOptimizer.')\n  parser.add_argument(\n      '--weight-decay',\n      type=float,\n      default=1e-4,\n      help='Weight decay for convolutions.')\n  parser.add_argument(\n      '--learning-rate',\n      type=float,\n      default=0.1,\n      help=\"\"\"\\\n      This is the inital learning rate value. The learning rate will decrease\n      during training. For more details check the model_fn implementation in\n      this file.\\\n      \"\"\")\n  parser.add_argument(\n      '--use-distortion-for-training',\n      type=bool,\n      default=True,\n      help='If doing image distortion for training.')\n  parser.add_argument(\n      '--sync',\n      action='store_true',\n      default=False,\n      help=\"\"\"\\\n      If present when running in a distributed environment will run on sync mode.\\\n      \"\"\")\n  parser.add_argument(\n      '--num-intra-threads',\n      type=int,\n      default=0,\n      help=\"\"\"\\\n      Number of threads to use for intra-op parallelism. When training on CPU\n      set to 0 to have the system pick the appropriate number or alternatively\n      set it to the number of physical CPU cores.\\\n      \"\"\")\n  parser.add_argument(\n      '--num-inter-threads',\n      type=int,\n      default=0,\n      help=\"\"\"\\\n      Number of threads to use for inter-op parallelism. If set to 0, the\n      system will pick an appropriate number.\\\n      \"\"\")\n  parser.add_argument(\n      '--data-format',\n      type=str,\n      default=None,\n      help=\"\"\"\\\n      If not set, the data format best for the training device is used. \n      Allowed values: channels_first (NCHW) channels_last (NHWC).\\\n      \"\"\")\n  parser.add_argument(\n      '--log-device-placement',\n      action='store_true',\n      default=False,\n      help='Whether to log device placement.')\n  parser.add_argument(\n      '--batch-norm-decay',\n      type=float,\n      default=0.997,\n      help='Decay for batch norm.')\n  parser.add_argument(\n      '--batch-norm-epsilon',\n      type=float,\n      default=1e-5,\n      help='Epsilon for batch norm.')\n  parser.add_argument(\n      '--work_dir',\n      type=str,\n      default='/data/',\n      help='UAI SDK related.')\n  parser.add_argument(\n      '--log_dir',\n      type=str,\n      default='/data/data/',\n      help='UAI SDK related.'\n  )\n  args = parser.parse_args()\n\n  if args.num_gpus < 0:\n    raise ValueError(\n        'Invalid GPU count: \\\"--num-gpus\\\" must be 0 or a positive integer.')\n  if args.num_gpus == 0 and args.variable_strategy == 'GPU':\n    raise ValueError('num-gpus=0, CPU must be used as parameter server. Set'\n                     '--variable-strategy=CPU.')\n  if (args.num_layers - 2) % 6 != 0:\n    raise ValueError('Invalid --num-layers parameter.')\n  if args.num_gpus != 0 and args.train_batch_size % args.num_gpus != 0:\n    raise ValueError('--train-batch-size must be multiple of --num-gpus.')\n  if args.num_gpus != 0 and args.eval_batch_size % args.num_gpus != 0:\n    raise ValueError('--eval-batch-size must be multiple of --num-gpus.')\n\n  main(**vars(args))\n", "framework": "tensorflow"}
{"repo_name": "IshankGulati/scikit-learn", "file_path": "examples/text/document_clustering.py", "content": "\"\"\"\n=======================================\nClustering text documents using k-means\n=======================================\n\nThis is an example showing how the scikit-learn can be used to cluster\ndocuments by topics using a bag-of-words approach. This example uses\na scipy.sparse matrix to store the features instead of standard numpy arrays.\n\nTwo feature extraction methods can be used in this example:\n\n  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most\n    frequent words to features indices and hence compute a word occurrence\n    frequency (sparse) matrix. The word frequencies are then reweighted using\n    the Inverse Document Frequency (IDF) vector collected feature-wise over\n    the corpus.\n\n  - HashingVectorizer hashes word occurrences to a fixed dimensional space,\n    possibly with collisions. The word count vectors are then normalized to\n    each have l2-norm equal to one (projected to the euclidean unit-ball) which\n    seems to be important for k-means to work in high dimensional space.\n\n    HashingVectorizer does not provide IDF weighting as this is a stateless\n    model (the fit method does nothing). When IDF weighting is needed it can\n    be added by pipelining its output to a TfidfTransformer instance.\n\nTwo algorithms are demoed: ordinary k-means and its more scalable cousin\nminibatch k-means.\n\nAdditionally, latent semantic analysis can also be used to reduce dimensionality\nand discover latent patterns in the data.\n\nIt can be noted that k-means (and minibatch k-means) are very sensitive to\nfeature scaling and that in this case the IDF weighting helps improve the\nquality of the clustering by quite a lot as measured against the \"ground truth\"\nprovided by the class label assignments of the 20 newsgroups dataset.\n\nThis improvement is not visible in the Silhouette Coefficient which is small\nfor both as this measure seem to suffer from the phenomenon called\n\"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional\ndatasets such as text data. Other measures such as V-measure and Adjusted Rand\nIndex are information theoretic based evaluation scores: as they are only based\non cluster assignments rather than distances, hence not affected by the curse\nof dimensionality.\n\nNote: as k-means is optimizing a non-convex objective function, it will likely\nend up in a local optimum. Several runs with independent random init might be\nnecessary to get a good convergence.\n\n\"\"\"\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Lars Buitinck\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nimport numpy as np\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option(\"--lsa\",\n              dest=\"n_components\", type=\"int\",\n              help=\"Preprocess documents with latent semantic analysis.\")\nop.add_option(\"--no-minibatch\",\n              action=\"store_false\", dest=\"minibatch\", default=True,\n              help=\"Use ordinary k-means algorithm (in batch mode).\")\nop.add_option(\"--no-idf\",\n              action=\"store_false\", dest=\"use_idf\", default=True,\n              help=\"Disable Inverse Document Frequency feature weighting.\")\nop.add_option(\"--use-hashing\",\n              action=\"store_true\", default=False,\n              help=\"Use a hashing feature vectorizer\")\nop.add_option(\"--n-features\", type=int, default=10000,\n              help=\"Maximum number of features (dimensions)\"\n                   \" to extract from text.\")\nop.add_option(\"--verbose\",\n              action=\"store_true\", dest=\"verbose\", default=False,\n              help=\"Print progress reports inside k-means algorithm.\")\n\nprint(__doc__)\nop.print_help()\n\n\ndef is_interactive():\n    return not hasattr(sys.modules['__main__'], '__file__')\n\n# work-around for Jupyter notebook and IPython console\nargv = [] if is_interactive() else sys.argv[1:]\n(opts, args) = op.parse_args(argv)\nif len(args) > 0:\n    op.error(\"this script takes no arguments.\")\n    sys.exit(1)\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space',\n]\n# Uncomment the following to do the analysis on all the categories\n# categories = None\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\n\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint(\"%d documents\" % len(dataset.data))\nprint(\"%d categories\" % len(dataset.target_names))\nprint()\n\nlabels = dataset.target\ntrue_k = np.unique(labels).shape[0]\n\nprint(\"Extracting features from the training dataset using a sparse vectorizer\")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        # Perform an IDF normalization on the output of HashingVectorizer\n        hasher = HashingVectorizer(n_features=opts.n_features,\n                                   stop_words='english', non_negative=True,\n                                   norm=None, binary=False)\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words='english',\n                                       non_negative=False, norm='l2',\n                                       binary=False)\nelse:\n    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                 min_df=2, stop_words='english',\n                                 use_idf=opts.use_idf)\nX = vectorizer.fit_transform(dataset.data)\n\nprint(\"done in %fs\" % (time() - t0))\nprint(\"n_samples: %d, n_features: %d\" % X.shape)\nprint()\n\nif opts.n_components:\n    print(\"Performing dimensionality reduction using LSA\")\n    t0 = time()\n    # Vectorizer results are normalized, which makes KMeans behave as\n    # spherical k-means for better results. Since LSA/SVD results are\n    # not normalized, we have to redo the normalization.\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print(\"done in %fs\" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print(\"Explained variance of the SVD step: {}%\".format(\n        int(explained_variance * 100)))\n\n    print()\n\n\n###############################################################################\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n                         init_size=1000, batch_size=1000, verbose=opts.verbose)\nelse:\n    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n                verbose=opts.verbose)\n\nprint(\"Clustering sparse data with %s\" % km)\nt0 = time()\nkm.fit(X)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %.3f\"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n\nprint()\n\n\nif not opts.use_hashing:\n    print(\"Top terms per cluster:\")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print(\"Cluster %d:\" % i, end='')\n        for ind in order_centroids[i, :10]:\n            print(' %s' % terms[ind], end='')\n        print()\n", "framework": "tensorflow"}
{"repo_name": "zuku1985/scikit-learn", "file_path": "examples/text/document_clustering.py", "content": "\"\"\"\n=======================================\nClustering text documents using k-means\n=======================================\n\nThis is an example showing how the scikit-learn can be used to cluster\ndocuments by topics using a bag-of-words approach. This example uses\na scipy.sparse matrix to store the features instead of standard numpy arrays.\n\nTwo feature extraction methods can be used in this example:\n\n  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most\n    frequent words to features indices and hence compute a word occurrence\n    frequency (sparse) matrix. The word frequencies are then reweighted using\n    the Inverse Document Frequency (IDF) vector collected feature-wise over\n    the corpus.\n\n  - HashingVectorizer hashes word occurrences to a fixed dimensional space,\n    possibly with collisions. The word count vectors are then normalized to\n    each have l2-norm equal to one (projected to the euclidean unit-ball) which\n    seems to be important for k-means to work in high dimensional space.\n\n    HashingVectorizer does not provide IDF weighting as this is a stateless\n    model (the fit method does nothing). When IDF weighting is needed it can\n    be added by pipelining its output to a TfidfTransformer instance.\n\nTwo algorithms are demoed: ordinary k-means and its more scalable cousin\nminibatch k-means.\n\nAdditionally, latent semantic analysis can also be used to reduce dimensionality\nand discover latent patterns in the data.\n\nIt can be noted that k-means (and minibatch k-means) are very sensitive to\nfeature scaling and that in this case the IDF weighting helps improve the\nquality of the clustering by quite a lot as measured against the \"ground truth\"\nprovided by the class label assignments of the 20 newsgroups dataset.\n\nThis improvement is not visible in the Silhouette Coefficient which is small\nfor both as this measure seem to suffer from the phenomenon called\n\"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional\ndatasets such as text data. Other measures such as V-measure and Adjusted Rand\nIndex are information theoretic based evaluation scores: as they are only based\non cluster assignments rather than distances, hence not affected by the curse\nof dimensionality.\n\nNote: as k-means is optimizing a non-convex objective function, it will likely\nend up in a local optimum. Several runs with independent random init might be\nnecessary to get a good convergence.\n\n\"\"\"\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Lars Buitinck\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nimport numpy as np\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option(\"--lsa\",\n              dest=\"n_components\", type=\"int\",\n              help=\"Preprocess documents with latent semantic analysis.\")\nop.add_option(\"--no-minibatch\",\n              action=\"store_false\", dest=\"minibatch\", default=True,\n              help=\"Use ordinary k-means algorithm (in batch mode).\")\nop.add_option(\"--no-idf\",\n              action=\"store_false\", dest=\"use_idf\", default=True,\n              help=\"Disable Inverse Document Frequency feature weighting.\")\nop.add_option(\"--use-hashing\",\n              action=\"store_true\", default=False,\n              help=\"Use a hashing feature vectorizer\")\nop.add_option(\"--n-features\", type=int, default=10000,\n              help=\"Maximum number of features (dimensions)\"\n                   \" to extract from text.\")\nop.add_option(\"--verbose\",\n              action=\"store_true\", dest=\"verbose\", default=False,\n              help=\"Print progress reports inside k-means algorithm.\")\n\nprint(__doc__)\nop.print_help()\n\n\ndef is_interactive():\n    return not hasattr(sys.modules['__main__'], '__file__')\n\n# work-around for Jupyter notebook and IPython console\nargv = [] if is_interactive() else sys.argv[1:]\n(opts, args) = op.parse_args(argv)\nif len(args) > 0:\n    op.error(\"this script takes no arguments.\")\n    sys.exit(1)\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space',\n]\n# Uncomment the following to do the analysis on all the categories\n# categories = None\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\n\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint(\"%d documents\" % len(dataset.data))\nprint(\"%d categories\" % len(dataset.target_names))\nprint()\n\nlabels = dataset.target\ntrue_k = np.unique(labels).shape[0]\n\nprint(\"Extracting features from the training dataset using a sparse vectorizer\")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        # Perform an IDF normalization on the output of HashingVectorizer\n        hasher = HashingVectorizer(n_features=opts.n_features,\n                                   stop_words='english', non_negative=True,\n                                   norm=None, binary=False)\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words='english',\n                                       non_negative=False, norm='l2',\n                                       binary=False)\nelse:\n    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                 min_df=2, stop_words='english',\n                                 use_idf=opts.use_idf)\nX = vectorizer.fit_transform(dataset.data)\n\nprint(\"done in %fs\" % (time() - t0))\nprint(\"n_samples: %d, n_features: %d\" % X.shape)\nprint()\n\nif opts.n_components:\n    print(\"Performing dimensionality reduction using LSA\")\n    t0 = time()\n    # Vectorizer results are normalized, which makes KMeans behave as\n    # spherical k-means for better results. Since LSA/SVD results are\n    # not normalized, we have to redo the normalization.\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print(\"done in %fs\" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print(\"Explained variance of the SVD step: {}%\".format(\n        int(explained_variance * 100)))\n\n    print()\n\n\n###############################################################################\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n                         init_size=1000, batch_size=1000, verbose=opts.verbose)\nelse:\n    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n                verbose=opts.verbose)\n\nprint(\"Clustering sparse data with %s\" % km)\nt0 = time()\nkm.fit(X)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %.3f\"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n\nprint()\n\n\nif not opts.use_hashing:\n    print(\"Top terms per cluster:\")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print(\"Cluster %d:\" % i, end='')\n        for ind in order_centroids[i, :10]:\n            print(' %s' % terms[ind], end='')\n        print()\n", "framework": "tensorflow"}
{"repo_name": "jaidevd/scikit-learn", "file_path": "examples/text/document_clustering.py", "content": "\"\"\"\n=======================================\nClustering text documents using k-means\n=======================================\n\nThis is an example showing how the scikit-learn can be used to cluster\ndocuments by topics using a bag-of-words approach. This example uses\na scipy.sparse matrix to store the features instead of standard numpy arrays.\n\nTwo feature extraction methods can be used in this example:\n\n  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most\n    frequent words to features indices and hence compute a word occurrence\n    frequency (sparse) matrix. The word frequencies are then reweighted using\n    the Inverse Document Frequency (IDF) vector collected feature-wise over\n    the corpus.\n\n  - HashingVectorizer hashes word occurrences to a fixed dimensional space,\n    possibly with collisions. The word count vectors are then normalized to\n    each have l2-norm equal to one (projected to the euclidean unit-ball) which\n    seems to be important for k-means to work in high dimensional space.\n\n    HashingVectorizer does not provide IDF weighting as this is a stateless\n    model (the fit method does nothing). When IDF weighting is needed it can\n    be added by pipelining its output to a TfidfTransformer instance.\n\nTwo algorithms are demoed: ordinary k-means and its more scalable cousin\nminibatch k-means.\n\nAdditionally, latent semantic analysis can also be used to reduce dimensionality\nand discover latent patterns in the data.\n\nIt can be noted that k-means (and minibatch k-means) are very sensitive to\nfeature scaling and that in this case the IDF weighting helps improve the\nquality of the clustering by quite a lot as measured against the \"ground truth\"\nprovided by the class label assignments of the 20 newsgroups dataset.\n\nThis improvement is not visible in the Silhouette Coefficient which is small\nfor both as this measure seem to suffer from the phenomenon called\n\"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional\ndatasets such as text data. Other measures such as V-measure and Adjusted Rand\nIndex are information theoretic based evaluation scores: as they are only based\non cluster assignments rather than distances, hence not affected by the curse\nof dimensionality.\n\nNote: as k-means is optimizing a non-convex objective function, it will likely\nend up in a local optimum. Several runs with independent random init might be\nnecessary to get a good convergence.\n\n\"\"\"\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Lars Buitinck\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nimport numpy as np\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option(\"--lsa\",\n              dest=\"n_components\", type=\"int\",\n              help=\"Preprocess documents with latent semantic analysis.\")\nop.add_option(\"--no-minibatch\",\n              action=\"store_false\", dest=\"minibatch\", default=True,\n              help=\"Use ordinary k-means algorithm (in batch mode).\")\nop.add_option(\"--no-idf\",\n              action=\"store_false\", dest=\"use_idf\", default=True,\n              help=\"Disable Inverse Document Frequency feature weighting.\")\nop.add_option(\"--use-hashing\",\n              action=\"store_true\", default=False,\n              help=\"Use a hashing feature vectorizer\")\nop.add_option(\"--n-features\", type=int, default=10000,\n              help=\"Maximum number of features (dimensions)\"\n                   \" to extract from text.\")\nop.add_option(\"--verbose\",\n              action=\"store_true\", dest=\"verbose\", default=False,\n              help=\"Print progress reports inside k-means algorithm.\")\n\nprint(__doc__)\nop.print_help()\n\n\ndef is_interactive():\n    return not hasattr(sys.modules['__main__'], '__file__')\n\n# work-around for Jupyter notebook and IPython console\nargv = [] if is_interactive() else sys.argv[1:]\n(opts, args) = op.parse_args(argv)\nif len(args) > 0:\n    op.error(\"this script takes no arguments.\")\n    sys.exit(1)\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space',\n]\n# Uncomment the following to do the analysis on all the categories\n# categories = None\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\n\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint(\"%d documents\" % len(dataset.data))\nprint(\"%d categories\" % len(dataset.target_names))\nprint()\n\nlabels = dataset.target\ntrue_k = np.unique(labels).shape[0]\n\nprint(\"Extracting features from the training dataset using a sparse vectorizer\")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        # Perform an IDF normalization on the output of HashingVectorizer\n        hasher = HashingVectorizer(n_features=opts.n_features,\n                                   stop_words='english', non_negative=True,\n                                   norm=None, binary=False)\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words='english',\n                                       non_negative=False, norm='l2',\n                                       binary=False)\nelse:\n    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                 min_df=2, stop_words='english',\n                                 use_idf=opts.use_idf)\nX = vectorizer.fit_transform(dataset.data)\n\nprint(\"done in %fs\" % (time() - t0))\nprint(\"n_samples: %d, n_features: %d\" % X.shape)\nprint()\n\nif opts.n_components:\n    print(\"Performing dimensionality reduction using LSA\")\n    t0 = time()\n    # Vectorizer results are normalized, which makes KMeans behave as\n    # spherical k-means for better results. Since LSA/SVD results are\n    # not normalized, we have to redo the normalization.\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print(\"done in %fs\" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print(\"Explained variance of the SVD step: {}%\".format(\n        int(explained_variance * 100)))\n\n    print()\n\n\n###############################################################################\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n                         init_size=1000, batch_size=1000, verbose=opts.verbose)\nelse:\n    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n                verbose=opts.verbose)\n\nprint(\"Clustering sparse data with %s\" % km)\nt0 = time()\nkm.fit(X)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %.3f\"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n\nprint()\n\n\nif not opts.use_hashing:\n    print(\"Top terms per cluster:\")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print(\"Cluster %d:\" % i, end='')\n        for ind in order_centroids[i, :10]:\n            print(' %s' % terms[ind], end='')\n        print()\n", "framework": "tensorflow"}
{"repo_name": "wlamond/scikit-learn", "file_path": "examples/text/document_clustering.py", "content": "\"\"\"\n=======================================\nClustering text documents using k-means\n=======================================\n\nThis is an example showing how the scikit-learn can be used to cluster\ndocuments by topics using a bag-of-words approach. This example uses\na scipy.sparse matrix to store the features instead of standard numpy arrays.\n\nTwo feature extraction methods can be used in this example:\n\n  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most\n    frequent words to features indices and hence compute a word occurrence\n    frequency (sparse) matrix. The word frequencies are then reweighted using\n    the Inverse Document Frequency (IDF) vector collected feature-wise over\n    the corpus.\n\n  - HashingVectorizer hashes word occurrences to a fixed dimensional space,\n    possibly with collisions. The word count vectors are then normalized to\n    each have l2-norm equal to one (projected to the euclidean unit-ball) which\n    seems to be important for k-means to work in high dimensional space.\n\n    HashingVectorizer does not provide IDF weighting as this is a stateless\n    model (the fit method does nothing). When IDF weighting is needed it can\n    be added by pipelining its output to a TfidfTransformer instance.\n\nTwo algorithms are demoed: ordinary k-means and its more scalable cousin\nminibatch k-means.\n\nAdditionally, latent semantic analysis can also be used to reduce dimensionality\nand discover latent patterns in the data.\n\nIt can be noted that k-means (and minibatch k-means) are very sensitive to\nfeature scaling and that in this case the IDF weighting helps improve the\nquality of the clustering by quite a lot as measured against the \"ground truth\"\nprovided by the class label assignments of the 20 newsgroups dataset.\n\nThis improvement is not visible in the Silhouette Coefficient which is small\nfor both as this measure seem to suffer from the phenomenon called\n\"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional\ndatasets such as text data. Other measures such as V-measure and Adjusted Rand\nIndex are information theoretic based evaluation scores: as they are only based\non cluster assignments rather than distances, hence not affected by the curse\nof dimensionality.\n\nNote: as k-means is optimizing a non-convex objective function, it will likely\nend up in a local optimum. Several runs with independent random init might be\nnecessary to get a good convergence.\n\n\"\"\"\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Lars Buitinck\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nimport numpy as np\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option(\"--lsa\",\n              dest=\"n_components\", type=\"int\",\n              help=\"Preprocess documents with latent semantic analysis.\")\nop.add_option(\"--no-minibatch\",\n              action=\"store_false\", dest=\"minibatch\", default=True,\n              help=\"Use ordinary k-means algorithm (in batch mode).\")\nop.add_option(\"--no-idf\",\n              action=\"store_false\", dest=\"use_idf\", default=True,\n              help=\"Disable Inverse Document Frequency feature weighting.\")\nop.add_option(\"--use-hashing\",\n              action=\"store_true\", default=False,\n              help=\"Use a hashing feature vectorizer\")\nop.add_option(\"--n-features\", type=int, default=10000,\n              help=\"Maximum number of features (dimensions)\"\n                   \" to extract from text.\")\nop.add_option(\"--verbose\",\n              action=\"store_true\", dest=\"verbose\", default=False,\n              help=\"Print progress reports inside k-means algorithm.\")\n\nprint(__doc__)\nop.print_help()\n\n\ndef is_interactive():\n    return not hasattr(sys.modules['__main__'], '__file__')\n\n# work-around for Jupyter notebook and IPython console\nargv = [] if is_interactive() else sys.argv[1:]\n(opts, args) = op.parse_args(argv)\nif len(args) > 0:\n    op.error(\"this script takes no arguments.\")\n    sys.exit(1)\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space',\n]\n# Uncomment the following to do the analysis on all the categories\n# categories = None\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\n\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint(\"%d documents\" % len(dataset.data))\nprint(\"%d categories\" % len(dataset.target_names))\nprint()\n\nlabels = dataset.target\ntrue_k = np.unique(labels).shape[0]\n\nprint(\"Extracting features from the training dataset using a sparse vectorizer\")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        # Perform an IDF normalization on the output of HashingVectorizer\n        hasher = HashingVectorizer(n_features=opts.n_features,\n                                   stop_words='english', non_negative=True,\n                                   norm=None, binary=False)\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words='english',\n                                       non_negative=False, norm='l2',\n                                       binary=False)\nelse:\n    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                 min_df=2, stop_words='english',\n                                 use_idf=opts.use_idf)\nX = vectorizer.fit_transform(dataset.data)\n\nprint(\"done in %fs\" % (time() - t0))\nprint(\"n_samples: %d, n_features: %d\" % X.shape)\nprint()\n\nif opts.n_components:\n    print(\"Performing dimensionality reduction using LSA\")\n    t0 = time()\n    # Vectorizer results are normalized, which makes KMeans behave as\n    # spherical k-means for better results. Since LSA/SVD results are\n    # not normalized, we have to redo the normalization.\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print(\"done in %fs\" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print(\"Explained variance of the SVD step: {}%\".format(\n        int(explained_variance * 100)))\n\n    print()\n\n\n###############################################################################\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n                         init_size=1000, batch_size=1000, verbose=opts.verbose)\nelse:\n    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n                verbose=opts.verbose)\n\nprint(\"Clustering sparse data with %s\" % km)\nt0 = time()\nkm.fit(X)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %.3f\"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n\nprint()\n\n\nif not opts.use_hashing:\n    print(\"Top terms per cluster:\")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print(\"Cluster %d:\" % i, end='')\n        for ind in order_centroids[i, :10]:\n            print(' %s' % terms[ind], end='')\n        print()\n", "framework": "tensorflow"}
{"repo_name": "rishikksh20/scikit-learn", "file_path": "examples/text/document_clustering.py", "content": "\"\"\"\n=======================================\nClustering text documents using k-means\n=======================================\n\nThis is an example showing how the scikit-learn can be used to cluster\ndocuments by topics using a bag-of-words approach. This example uses\na scipy.sparse matrix to store the features instead of standard numpy arrays.\n\nTwo feature extraction methods can be used in this example:\n\n  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most\n    frequent words to features indices and hence compute a word occurrence\n    frequency (sparse) matrix. The word frequencies are then reweighted using\n    the Inverse Document Frequency (IDF) vector collected feature-wise over\n    the corpus.\n\n  - HashingVectorizer hashes word occurrences to a fixed dimensional space,\n    possibly with collisions. The word count vectors are then normalized to\n    each have l2-norm equal to one (projected to the euclidean unit-ball) which\n    seems to be important for k-means to work in high dimensional space.\n\n    HashingVectorizer does not provide IDF weighting as this is a stateless\n    model (the fit method does nothing). When IDF weighting is needed it can\n    be added by pipelining its output to a TfidfTransformer instance.\n\nTwo algorithms are demoed: ordinary k-means and its more scalable cousin\nminibatch k-means.\n\nAdditionally, latent semantic analysis can also be used to reduce dimensionality\nand discover latent patterns in the data.\n\nIt can be noted that k-means (and minibatch k-means) are very sensitive to\nfeature scaling and that in this case the IDF weighting helps improve the\nquality of the clustering by quite a lot as measured against the \"ground truth\"\nprovided by the class label assignments of the 20 newsgroups dataset.\n\nThis improvement is not visible in the Silhouette Coefficient which is small\nfor both as this measure seem to suffer from the phenomenon called\n\"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional\ndatasets such as text data. Other measures such as V-measure and Adjusted Rand\nIndex are information theoretic based evaluation scores: as they are only based\non cluster assignments rather than distances, hence not affected by the curse\nof dimensionality.\n\nNote: as k-means is optimizing a non-convex objective function, it will likely\nend up in a local optimum. Several runs with independent random init might be\nnecessary to get a good convergence.\n\n\"\"\"\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Lars Buitinck\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nimport numpy as np\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option(\"--lsa\",\n              dest=\"n_components\", type=\"int\",\n              help=\"Preprocess documents with latent semantic analysis.\")\nop.add_option(\"--no-minibatch\",\n              action=\"store_false\", dest=\"minibatch\", default=True,\n              help=\"Use ordinary k-means algorithm (in batch mode).\")\nop.add_option(\"--no-idf\",\n              action=\"store_false\", dest=\"use_idf\", default=True,\n              help=\"Disable Inverse Document Frequency feature weighting.\")\nop.add_option(\"--use-hashing\",\n              action=\"store_true\", default=False,\n              help=\"Use a hashing feature vectorizer\")\nop.add_option(\"--n-features\", type=int, default=10000,\n              help=\"Maximum number of features (dimensions)\"\n                   \" to extract from text.\")\nop.add_option(\"--verbose\",\n              action=\"store_true\", dest=\"verbose\", default=False,\n              help=\"Print progress reports inside k-means algorithm.\")\n\nprint(__doc__)\nop.print_help()\n\n\ndef is_interactive():\n    return not hasattr(sys.modules['__main__'], '__file__')\n\n# work-around for Jupyter notebook and IPython console\nargv = [] if is_interactive() else sys.argv[1:]\n(opts, args) = op.parse_args(argv)\nif len(args) > 0:\n    op.error(\"this script takes no arguments.\")\n    sys.exit(1)\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space',\n]\n# Uncomment the following to do the analysis on all the categories\n# categories = None\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\n\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint(\"%d documents\" % len(dataset.data))\nprint(\"%d categories\" % len(dataset.target_names))\nprint()\n\nlabels = dataset.target\ntrue_k = np.unique(labels).shape[0]\n\nprint(\"Extracting features from the training dataset using a sparse vectorizer\")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        # Perform an IDF normalization on the output of HashingVectorizer\n        hasher = HashingVectorizer(n_features=opts.n_features,\n                                   stop_words='english', non_negative=True,\n                                   norm=None, binary=False)\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words='english',\n                                       non_negative=False, norm='l2',\n                                       binary=False)\nelse:\n    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                 min_df=2, stop_words='english',\n                                 use_idf=opts.use_idf)\nX = vectorizer.fit_transform(dataset.data)\n\nprint(\"done in %fs\" % (time() - t0))\nprint(\"n_samples: %d, n_features: %d\" % X.shape)\nprint()\n\nif opts.n_components:\n    print(\"Performing dimensionality reduction using LSA\")\n    t0 = time()\n    # Vectorizer results are normalized, which makes KMeans behave as\n    # spherical k-means for better results. Since LSA/SVD results are\n    # not normalized, we have to redo the normalization.\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print(\"done in %fs\" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print(\"Explained variance of the SVD step: {}%\".format(\n        int(explained_variance * 100)))\n\n    print()\n\n\n###############################################################################\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n                         init_size=1000, batch_size=1000, verbose=opts.verbose)\nelse:\n    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n                verbose=opts.verbose)\n\nprint(\"Clustering sparse data with %s\" % km)\nt0 = time()\nkm.fit(X)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %.3f\"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n\nprint()\n\n\nif not opts.use_hashing:\n    print(\"Top terms per cluster:\")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print(\"Cluster %d:\" % i, end='')\n        for ind in order_centroids[i, :10]:\n            print(' %s' % terms[ind], end='')\n        print()\n", "framework": "tensorflow"}
{"repo_name": "meduz/scikit-learn", "file_path": "examples/text/document_clustering.py", "content": "\"\"\"\n=======================================\nClustering text documents using k-means\n=======================================\n\nThis is an example showing how the scikit-learn can be used to cluster\ndocuments by topics using a bag-of-words approach. This example uses\na scipy.sparse matrix to store the features instead of standard numpy arrays.\n\nTwo feature extraction methods can be used in this example:\n\n  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most\n    frequent words to features indices and hence compute a word occurrence\n    frequency (sparse) matrix. The word frequencies are then reweighted using\n    the Inverse Document Frequency (IDF) vector collected feature-wise over\n    the corpus.\n\n  - HashingVectorizer hashes word occurrences to a fixed dimensional space,\n    possibly with collisions. The word count vectors are then normalized to\n    each have l2-norm equal to one (projected to the euclidean unit-ball) which\n    seems to be important for k-means to work in high dimensional space.\n\n    HashingVectorizer does not provide IDF weighting as this is a stateless\n    model (the fit method does nothing). When IDF weighting is needed it can\n    be added by pipelining its output to a TfidfTransformer instance.\n\nTwo algorithms are demoed: ordinary k-means and its more scalable cousin\nminibatch k-means.\n\nAdditionally, latent semantic analysis can also be used to reduce dimensionality\nand discover latent patterns in the data.\n\nIt can be noted that k-means (and minibatch k-means) are very sensitive to\nfeature scaling and that in this case the IDF weighting helps improve the\nquality of the clustering by quite a lot as measured against the \"ground truth\"\nprovided by the class label assignments of the 20 newsgroups dataset.\n\nThis improvement is not visible in the Silhouette Coefficient which is small\nfor both as this measure seem to suffer from the phenomenon called\n\"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional\ndatasets such as text data. Other measures such as V-measure and Adjusted Rand\nIndex are information theoretic based evaluation scores: as they are only based\non cluster assignments rather than distances, hence not affected by the curse\nof dimensionality.\n\nNote: as k-means is optimizing a non-convex objective function, it will likely\nend up in a local optimum. Several runs with independent random init might be\nnecessary to get a good convergence.\n\n\"\"\"\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Lars Buitinck\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nimport numpy as np\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option(\"--lsa\",\n              dest=\"n_components\", type=\"int\",\n              help=\"Preprocess documents with latent semantic analysis.\")\nop.add_option(\"--no-minibatch\",\n              action=\"store_false\", dest=\"minibatch\", default=True,\n              help=\"Use ordinary k-means algorithm (in batch mode).\")\nop.add_option(\"--no-idf\",\n              action=\"store_false\", dest=\"use_idf\", default=True,\n              help=\"Disable Inverse Document Frequency feature weighting.\")\nop.add_option(\"--use-hashing\",\n              action=\"store_true\", default=False,\n              help=\"Use a hashing feature vectorizer\")\nop.add_option(\"--n-features\", type=int, default=10000,\n              help=\"Maximum number of features (dimensions)\"\n                   \" to extract from text.\")\nop.add_option(\"--verbose\",\n              action=\"store_true\", dest=\"verbose\", default=False,\n              help=\"Print progress reports inside k-means algorithm.\")\n\nprint(__doc__)\nop.print_help()\n\n\ndef is_interactive():\n    return not hasattr(sys.modules['__main__'], '__file__')\n\n# work-around for Jupyter notebook and IPython console\nargv = [] if is_interactive() else sys.argv[1:]\n(opts, args) = op.parse_args(argv)\nif len(args) > 0:\n    op.error(\"this script takes no arguments.\")\n    sys.exit(1)\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space',\n]\n# Uncomment the following to do the analysis on all the categories\n# categories = None\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\n\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint(\"%d documents\" % len(dataset.data))\nprint(\"%d categories\" % len(dataset.target_names))\nprint()\n\nlabels = dataset.target\ntrue_k = np.unique(labels).shape[0]\n\nprint(\"Extracting features from the training dataset using a sparse vectorizer\")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        # Perform an IDF normalization on the output of HashingVectorizer\n        hasher = HashingVectorizer(n_features=opts.n_features,\n                                   stop_words='english', non_negative=True,\n                                   norm=None, binary=False)\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words='english',\n                                       non_negative=False, norm='l2',\n                                       binary=False)\nelse:\n    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                 min_df=2, stop_words='english',\n                                 use_idf=opts.use_idf)\nX = vectorizer.fit_transform(dataset.data)\n\nprint(\"done in %fs\" % (time() - t0))\nprint(\"n_samples: %d, n_features: %d\" % X.shape)\nprint()\n\nif opts.n_components:\n    print(\"Performing dimensionality reduction using LSA\")\n    t0 = time()\n    # Vectorizer results are normalized, which makes KMeans behave as\n    # spherical k-means for better results. Since LSA/SVD results are\n    # not normalized, we have to redo the normalization.\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print(\"done in %fs\" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print(\"Explained variance of the SVD step: {}%\".format(\n        int(explained_variance * 100)))\n\n    print()\n\n\n###############################################################################\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n                         init_size=1000, batch_size=1000, verbose=opts.verbose)\nelse:\n    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n                verbose=opts.verbose)\n\nprint(\"Clustering sparse data with %s\" % km)\nt0 = time()\nkm.fit(X)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %.3f\"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\"Silhouette Coefficient: %0.3f\"\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n\nprint()\n\n\nif not opts.use_hashing:\n    print(\"Top terms per cluster:\")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print(\"Cluster %d:\" % i, end='')\n        for ind in order_centroids[i, :10]:\n            print(' %s' % terms[ind], end='')\n        print()\n", "framework": "tensorflow"}
{"repo_name": "MingLin-home/Ming_slim", "file_path": "extract_midlayer_feat_tfrec_old.py", "content": "\"\"\"\nExtract midlayer features and save them as tfrec tfd files\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport GPUtil\nfrom joblib import Parallel, delayed\n\nfrom optparse import OptionParser\nfrom itertools import *\n\n\nimport os\nimport numpy as np\nimport distutils.dir_util\nimport sys\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom datasets import dataset_factory\nfrom deployment import model_deploy\nfrom nets import vgg\nfrom preprocessing import cifar10_vgg_preprocessing\n\nslim = tf.contrib.slim\n\nimport project_config\n\nmodel_configure_dict = {}\nmodel_configure_dict['vgg_16_2016_08_28'] = {\n    'model_filename': os.path.join(project_config.model_repo_dir, 'vgg_16_2016_08_28/vgg_16.ckpt'),\n    'layers_to_extract_list': ['fc6', 'fc7', ],\n}\n\n\ndef grouper(iterable, n, fillvalue=None):\n    \"Collect data into fixed-length chunks or blocks\"\n    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n    args = [iter(iterable)] * n\n    return zip_longest(*args, fillvalue=fillvalue)\n\n\ndef load_cifar10():\n    \"\"\"\n    train_images, train_labels, test_images, test_labels = load_cifar10()\n    load cifar-10 dataset as numpy array\n    :return: train_images, train_labels, test_images, test_labels. images are of shape [number_of_images, height, width, colors]. Images are uint8. labels are unit8\n    \"\"\"\n    _IMAGE_SIZE = 32\n    _IMAGE_COLOR_CHANNEL = 3\n    cifar10_data_repo_dir = os.path.join(project_config.data_repo_dir, 'cifar10/cifar-10-batches-bin/')\n    \n    train_images = None\n    test_images = None\n    train_labels = None\n    test_labels = None\n    # load train\n    for train_bin_batch_count in range(5):\n        data_bin_filename = os.path.join(cifar10_data_repo_dir, 'data_batch_%d.bin' % (train_bin_batch_count + 1))\n        with open(data_bin_filename, 'rb') as fid:\n            all_byte = np.fromfile(fid, dtype=np.uint8)\n            one_record_len = _IMAGE_SIZE * _IMAGE_SIZE * _IMAGE_COLOR_CHANNEL + 1\n            all_byte = all_byte.reshape((-1, one_record_len,))\n            labels = all_byte[:, 0]\n            num_images = all_byte.shape[0]\n            images = all_byte[:, 1:].reshape((num_images, 3, 32, 32))\n            print('load from %s, num_images=%d' % (data_bin_filename, num_images))\n            images = np.transpose(images, [0, 2, 3, 1, ])\n            if train_images is None:\n                train_images = images\n                train_labels = labels\n            else:\n                train_images = np.vstack([train_images, images])\n                train_labels = np.concatenate([train_labels, labels])\n        pass  # end with\n    pass  # end for train_bin_batch_count\n    \n    # load test\n    data_bin_filename = os.path.join(cifar10_data_repo_dir, 'test_batch.bin')\n    with open(data_bin_filename, 'rb') as fid:\n        all_byte = np.fromfile(fid, dtype=np.uint8)\n        one_record_len = _IMAGE_SIZE * _IMAGE_SIZE * _IMAGE_COLOR_CHANNEL + 1\n        all_byte = all_byte.reshape((-1, one_record_len,))\n        labels = all_byte[:, 0]\n        num_images = all_byte.shape[0]\n        images = all_byte[:, 1:].reshape((num_images, 3, 32, 32))\n        print('load from %s, num_images=%d' % (data_bin_filename, num_images))\n        images = np.transpose(images, [0, 2, 3, 1, ])\n        if test_images is None:\n            test_images = images\n            test_labels = labels\n        else:\n            test_images = np.vstack([test_images, images])\n            test_labels = np.concatenate([test_labels, labels])\n    pass  # end with\n    \n    return train_images, train_labels, test_images, test_labels\n\n\npass  # end def\n\n\ndef extract_vgg_16_features(train_images, gpu_device_config, real_gpu_device_config, cpu_device_config, checkpoint_file, perturb_count=-1, is_training=False,\n                            ):\n    \"\"\"\n    pool5_feature_matrix, fc6_feature_matrix, fc7_feature_matrix =  extract_vgg_16_features(...)\n    :return:\n    \"\"\"\n    image_size = vgg.vgg_16.default_image_size\n    n = train_images.shape[0]\n    \n    with tf.Graph().as_default(), tf.device(cpu_device_config):\n        # image_input is a uint8 image, shape=[height, width, color]\n        image_input = tf.placeholder(tf.uint8, shape=[32, 32, 3], name='image_input')\n        processed_image = cifar10_vgg_preprocessing.preprocess_image(image_input, image_size, image_size, is_training=is_training,\n                                                                     )\n        processed_images = tf.expand_dims(processed_image, 0)\n    \n        with slim.arg_scope(vgg.vgg_arg_scope()):\n            with tf.device(gpu_device_config):  # since we mask GPU via $CUDA_VISIBLE_DEVICES, tf can only see '0' gpu now\n                logits, end_points = vgg.vgg_16(processed_images, num_classes=1000, is_training=is_training, dropout_keep_prob=0.5, )\n            pass  # end with tf.device\n        pass  # end with slim.arg_scope\n    \n        init_fn = slim.assign_from_checkpoint_fn(checkpoint_file, slim.get_model_variables('vgg_16'))\n\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True, )) as sess:\n            #   Load weights\n            init_fn(sess)\n\n            # vgg_16_pool4_layer = end_points['vgg_16/pool4']\n            vgg_16_pool5_layer = end_points['vgg_16/pool5']\n            vgg_16_fc6_layer = end_points['vgg_16/fc6']\n            vgg_16_fc7_layer = end_points['vgg_16/fc7']\n\n            # trainset_pool4_feature_matrix = None\n            trainset_pool5_feature_matrix = None\n            trainset_fc6_feature_matrix = None\n            trainset_fc7_feature_matrix = None\n\n            for image_count in range(train_images.shape[0]):\n                vgg_16_pool5_output, vgg_16_fc6_output, vgg_16_fc7_output = \\\n                    sess.run([vgg_16_pool5_layer, vgg_16_fc6_layer, vgg_16_fc7_layer], feed_dict={\n                        image_input: np.squeeze(train_images[image_count, :, :, :]), })\n    \n                # if trainset_pool4_feature_matrix is None:\n                #     trainset_pool4_feature_matrix = np.zeros((n, np.prod(vgg_16_pool4_output.shape[1:])))\n                    \n                if trainset_pool5_feature_matrix is None:\n                    trainset_pool5_feature_matrix = np.zeros((n, np.prod(vgg_16_pool5_output.shape[1:])))\n    \n                if trainset_fc6_feature_matrix is None:\n                    trainset_fc6_feature_matrix = np.zeros((n, vgg_16_fc6_output.shape[3]))\n    \n                if trainset_fc7_feature_matrix is None:\n                    trainset_fc7_feature_matrix = np.zeros((n, vgg_16_fc7_output.shape[3]))\n\n                # trainset_pool4_feature_matrix[image_count, :] = np.ravel(vgg_16_pool4_output)\n                trainset_pool5_feature_matrix[image_count, :] = np.ravel(vgg_16_pool5_output)\n                trainset_fc6_feature_matrix[image_count, :] = np.ravel(vgg_16_fc6_output)\n                trainset_fc7_feature_matrix[image_count, :] = np.ravel(vgg_16_fc7_output)\n    \n                if image_count % (train_images.shape[0] / 100) == 0:\n                    print('[%s] image_count=%d, n=%d, perturb_count=%d' % (real_gpu_device_config, image_count, n, perturb_count))\n            pass  # end for\n        pass # end with tf.Session\n    pass # end with tf.Graph\n    return trainset_pool5_feature_matrix, trainset_fc6_feature_matrix, trainset_fc7_feature_matrix\n\npass # end def\n\n\ndef extract_vgg_16_2016_08_28(options, parameters):\n    num_trainset_blocks = options.num_trainset_blocks\n    num_testset_blocks = options.num_testset_blocks\n    \n    gpu_id, num_gpus = parameters\n    model_name = 'vgg_16_2016_08_28'\n    checkpoint_file = model_configure_dict[model_name]['model_filename']\n    \n    \n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n    if options.debug=='True':\n        gpu_device_config = '/cpu:0'\n    else:\n        gpu_device_config = '/gpu:0'\n    real_gpu_device_config = str(gpu_id)\n    cpu_device_config = '/cpu:%d' % (gpu_id + 1)\n    \n\n    train_images, train_labels, test_images, test_labels = load_cifar10()\n    \n    \n\n    if options.debug == 'True':\n        train_images = train_images[0:500, :]\n        train_labels = train_labels[0:500]\n        test_images = test_images[0:500, :]\n        test_labels = test_labels[0:500]\n    pass  # end if\n\n    train_index_list = np.array_split(range(train_images.shape[0]), num_gpus)\n    train_subsplit_index = train_index_list[gpu_id]\n    train_images = train_images[train_subsplit_index, :]\n    train_labels = train_labels[train_subsplit_index]\n\n    test_index_list = np.array_split(range(test_images.shape[0]), num_gpus)\n    test_subsplit_index = test_index_list[gpu_id]\n    test_images = test_images[test_subsplit_index, :]\n    test_labels = test_labels[test_subsplit_index]\n\n    print('split=%d/%d, trainset size=%d, testset size=%d' % (gpu_id, num_gpus, train_images.shape[0], test_images.shape[0]))\n\n\n    trainset_block_index_list = np.array_split( range(train_images.shape[0]), num_trainset_blocks )\n    \n    for trainset_block_count, trainset_block_index in enumerate(trainset_block_index_list):\n        for perturb_count in range(options.num_perturb):\n            # trainset_output_pool4_filename = os.path.join(\n            #     project_config.output_dir, 'midlayer_feat_tfrec/cifar10/vgg_16/trainset_feat_pert%d_sp%d_bl%d_pool4.tfd' % (perturb_count, gpu_id, trainset_block_count))\n            trainset_output_pool5_filename = os.path.join(\n                project_config.output_dir, 'midlayer_feat_tfrec/cifar10/vgg_16/trainset_feat_pert%d_sp%d_bl%d_pool5.tfd' % (perturb_count, gpu_id, trainset_block_count))\n            trainset_output_fc6_filename = os.path.join(\n                project_config.output_dir, 'midlayer_feat_tfrec/cifar10/vgg_16/trainset_feat_pert%d_sp%d_bl%d_fc6.tfd' % (perturb_count, gpu_id, trainset_block_count))\n            trainset_output_fc7_filename = os.path.join(\n                project_config.output_dir, 'midlayer_feat_tfrec/cifar10/vgg_16/trainset_feat_pert%d_sp%d_bl%d_fc7.tfd' % (perturb_count, gpu_id, trainset_block_count))\n        \n            bool_should_run_trainset = True\n            if os.path.isfile(trainset_output_pool5_filename) and os.path.isfile(trainset_output_fc6_filename) and os.path.isfile(trainset_output_fc7_filename):\n                bool_should_run_trainset = False\n            pass  # end if\n        \n            is_training = False if perturb_count == 0 else True\n        \n            if bool_should_run_trainset:\n                pool5_feature_matrix, fc6_feature_matrix, fc7_feature_matrix = \\\n                    extract_vgg_16_features(train_images[trainset_block_index,:], gpu_device_config, real_gpu_device_config, cpu_device_config,\n                                            checkpoint_file, perturb_count=perturb_count, is_training=is_training,\n                                            )\n                \n                if not os.path.isfile(trainset_output_pool5_filename):\n                    distutils.dir_util.mkpath(os.path.dirname(trainset_output_pool5_filename))\n                    save_as_tfrecord(trainset_output_pool5_filename, features=pool5_feature_matrix, labels=train_labels[trainset_block_index], unique_image_id=trainset_block_index)\n            \n                if not os.path.isfile(trainset_output_fc6_filename):\n                    distutils.dir_util.mkpath(os.path.dirname(trainset_output_fc6_filename))\n                    save_as_tfrecord(trainset_output_fc6_filename, features=fc6_feature_matrix, labels=train_labels[trainset_block_index], unique_image_id=trainset_block_index)\n            \n                if not os.path.isfile(trainset_output_fc7_filename):\n                    distutils.dir_util.mkpath(os.path.dirname(trainset_output_fc7_filename))\n                    save_as_tfrecord(trainset_output_fc7_filename, features=fc7_feature_matrix, labels=train_labels[trainset_block_index], unique_image_id=trainset_block_index)\n            pass  # end if bool_should_run_trainset\n        pass  # end for perturb_count\n        pass\n    pass # end for trainset_block_count\n\n    testset_block_index_list = np.array_split(range(test_images.shape[0]), num_testset_blocks)\n    \n    for testset_block_count, testset_block_index in enumerate(testset_block_index_list):\n        # testset_output_pool4_filename = os.path.join(project_config.output_dir, 'midlayer_feat_tfrec/cifar10/vgg_16/testset_feat_sp%d_bl%d_pool4.tfd' % (gpu_id, testset_block_count))\n        testset_output_pool5_filename = os.path.join(project_config.output_dir, 'midlayer_feat_tfrec/cifar10/vgg_16/testset_feat_sp%d_bl%d_pool5.tfd' % (gpu_id, testset_block_count))\n        testset_output_fc6_filename = os.path.join(project_config.output_dir, 'midlayer_feat_tfrec/cifar10/vgg_16/testset_feat_sp%d_bl%d_fc6.tfd' % (gpu_id, testset_block_count))\n        testset_output_fc7_filename = os.path.join(project_config.output_dir, 'midlayer_feat_tfrec/cifar10/vgg_16/testset_feat_sp%d_bl%d_fc7.tfd' % (gpu_id, testset_block_count))\n    \n        bool_should_run_testset = True\n        if os.path.isfile(testset_output_pool5_filename) and os.path.isfile(testset_output_fc6_filename) and os.path.isfile(testset_output_fc7_filename):\n            bool_should_run_testset = False\n    \n        if bool_should_run_testset:\n            pool5_feature_matrix, fc6_feature_matrix, fc7_feature_matrix = \\\n                extract_vgg_16_features(test_images[testset_block_index,:], gpu_device_config, real_gpu_device_config, cpu_device_config, checkpoint_file, is_training=False,\n                                        )\n        \n            # export to numpy files\n            # if not os.path.isfile(testset_output_pool4_filename):\n            #     distutils.dir_util.mkpath(os.path.dirname(testset_output_pool4_filename))\n            #     save_as_tfrecord(testset_output_pool4_filename, features=pool4_feature_matrix, labels=test_labels)\n        \n            if not os.path.isfile(testset_output_pool5_filename):\n                distutils.dir_util.mkpath(os.path.dirname(testset_output_pool5_filename))\n                save_as_tfrecord(testset_output_pool5_filename, features=pool5_feature_matrix, labels=test_labels[testset_block_index], unique_image_id=testset_block_index)\n        \n            if not os.path.isfile(testset_output_fc6_filename):\n                distutils.dir_util.mkpath(os.path.dirname(testset_output_fc6_filename))\n                save_as_tfrecord(testset_output_fc6_filename, features=fc6_feature_matrix, labels=test_labels[testset_block_index], unique_image_id=testset_block_index)\n        \n            if not os.path.isfile(testset_output_fc7_filename):\n                distutils.dir_util.mkpath(os.path.dirname(testset_output_fc7_filename))\n                save_as_tfrecord(testset_output_fc7_filename, features=fc7_feature_matrix, labels=test_labels[testset_block_index], unique_image_id=testset_block_index)\n    \n        pass  # end if bool_should_run_testset\n    pass # end for testset_block_count\n    \n\n\n\npass # end def\n\n\ndef save_as_tfrecord(save_filename, features, labels, unique_image_id):\n    \"\"\"\n    save features and labels to tfrecord file\n    :param save_filename:\n    :param features:\n    :param labels:\n    :return:\n    \"\"\"\n    writer = tf.python_io.TFRecordWriter(save_filename)\n    for i in range(len(labels)):\n        # print('feature_shape=(%d,%d), len_labels=%d' % (features.shape[0], features.shape[1], len(labels)))\n        example = tf.train.Example(features=tf.train.Features(feature={  # SequenceExample for seuqnce example\n            \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[labels[i]])),\n            'feature': tf.train.Feature(float_list=tf.train.FloatList(value=features[i,:].tolist() ),),\n            'unique_image_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[unique_image_id[i]])),\n        }))\n        writer.write(example.SerializeToString())  # Serialize To String\n    pass\n    writer.close()\npass # end def\n\nif __name__ == '__main__':\n    \"\"\"\n    python extract_midlayer_features.py --num_gpus=4 --num_trainset_blocks=250 --num_testset_blocks=25 --num_perturb=2 --debug=True\n    \"\"\"\n    parser = OptionParser()\n    parser.add_option('--num_gpus', type='int', dest='num_gpus', default=4, help='number of gpu.')\n    parser.add_option('--num_trainset_blocks', type='int', dest='num_trainset_blocks', default=128, help='number of data blocks to split the training dataset.')\n    parser.add_option('--num_testset_blocks', type='int', dest='num_testset_blocks', default=1, help='number of data blocks to split the testing dataset.')\n    parser.add_option('--num_perturb', type='int', dest='num_perturb', default=10, help='number of random perturbation for each image.')\n    parser.add_option('--debug', type='string', dest='debug', default=False, help='run debug code.')\n    (options, args) = parser.parse_args()\n    \n    # generate task list\n    task_list = []\n    for gpu_id in range(options.num_gpus):\n        task_list.append([gpu_id, options.num_gpus])\n    pass # end for\n\n    par_results = Parallel(n_jobs=options.num_gpus, verbose=50, batch_size=1)(delayed(extract_vgg_16_2016_08_28)(options, par_for_parameters) for par_for_parameters in task_list)", "framework": "tensorflow"}
{"repo_name": "moonbury/notebooks", "file_path": "github/MasteringMLWithScikit-learn/8365OS_08_Codes/ch-perceptron.py", "content": "################# Figure 1: Scatter plot of data #################\n\"\"\"\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.array([\n    [0.2, 0.1],\n    [0.4, 0.6],\n    [0.5, 0.2],\n    [0.7, 0.9]\n])\n\ny = [0, 0, 0, 1]\n\nmarkers = ['.', 'x']\nplt.scatter(X[:3, 0], X[:3, 1], marker='.', s=400)\nplt.scatter(X[3, 0], X[3, 1], marker='x', s=400)\nplt.xlabel('Proportion of the day spent sleeping')\nplt.ylabel('Proportion of the day spent being grumpy')\nplt.title('Kittens and Adult Cats')\nplt.show()\n\n\n################# Sample 1 #################\n\"\"\"\n>>> from sklearn.datasets import fetch_20newsgroups\n>>> from sklearn.metrics.metrics import f1_score, classification_report\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> from sklearn.linear_model import Perceptron\n\n>>> categories = ['rec.sport.hockey', 'rec.sport.baseball', 'rec.autos']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n>>> newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n\n>>> vectorizer = TfidfVectorizer()\n>>> X_train = vectorizer.fit_transform(newsgroups_train.data)\n>>> X_test = vectorizer.transform(newsgroups_test.data)\n\n>>> classifier = Perceptron(n_iter=100, eta0=0.1)\n>>> classifier.fit_transform(X_train, newsgroups_train.target)\n>>> predictions = classifier.predict(X_test)\n>>> print classification_report(newsgroups_test.target, predictions)\n             precision    recall  f1-score   support\n\n          0       0.89      0.87      0.88       396\n          1       0.87      0.78      0.82       397\n          2       0.79      0.88      0.83       399\n\navg / total       0.85      0.85      0.85      1192\n\"\"\"\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics.metrics import f1_score, classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Perceptron\n\ncategories = ['rec.sport.hockey', 'rec.sport.baseball', 'rec.autos']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(newsgroups_train.data)\nX_test = vectorizer.transform(newsgroups_test.data)\n\nclassifier = Perceptron(n_iter=100, eta0=0.1)\nclassifier.fit_transform(X_train, newsgroups_train.target)\npredictions = classifier.predict(X_test)\nprint classification_report(newsgroups_test.target, predictions)\n\n\n################# Example #################\n\"\"\"\n\n\"\"\"\n\"\"\"\nsudo apt-get remove libopenblas-base\nopenblas (required for video contextualization)\nis incompatible with scipy.\n\"\"\"\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Qt4Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Perceptron\n\nX = np.array([\n    [1, 0.2, 0.1],\n    [1, 0.4, 0.6],\n    [1, 0.5, 0.2],\n    [1, 0.7, 0.9]\n])\n# X = np.array([\n#     [1, 0, 0],\n#     [1, 0, 1],\n#     [1, 1, 0],\n#     [1, 1, 1]\n# ])\nY = np.array([1, 1, 1, 0])\nh = .01  # step size in the mesh\nthreshold = 0.5\nlearning_rate = 0.1\nweights = [0, 0, 0]\nn_epochs = 10\n\n\ndef predict(x, weights):\n    weighted_sum = np.dot(x, weights)\n    if weighted_sum > threshold:\n        return 1\n    return 0\n\n\ndef get_activation_string(x, weights):\n    l = []\n    for i, v in enumerate(x):\n        l.append('%s*%s' % (v, weights[i]))\n    s = ' + '.join(l) + ' = ' + str(np.dot(x, weights))\n    return s\n\n\ndef predict_with_bias(x, weights):\n    Z = []\n    for i in x:\n        biased = np.hstack(([1], i))\n        weighted_sum = np.dot(biased, weights)\n        if weighted_sum > threshold:\n            Z.append(1)\n        else:\n            Z.append(0)\n    return np.array(Z)\n\n\nx_min, x_max = -0.4, 1.4\ny_min, y_max = -0.4, 1.4\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nplot_num = 1\nfor epoch in range(1, n_epochs):\n    print 'Epoch', epoch\n    print 'Instance;Initial Weights;x;Activation;Prediction;Target;Correct;Updated Weights'\n    converged = True\n    for i in range(len(X)):\n        previous_weights = np.array(weights)\n        prediction = predict(X[i], weights)\n        correct = True\n        if prediction != Y[i]:\n            correct = False\n            converged = False\n            correction = -1\n            if Y[i] == 1:\n                correction = 1\n            weights += X[i] * learning_rate * correction\n        print '%s;%s;%s;%s;%s;%s;%s;%s' % (\n            i,\n            ', '.join([str(w) for w in previous_weights]),\n            ', '.join([str(x) for x in X[i]]),\n            get_activation_string(X[i], previous_weights),\n            prediction, Y[i], correct,\n            ', '.join([str(w) for w in weights]))\n                # Plot the decision boundary and the instances\n        plt.subplot(9, 4, plot_num)\n        Z = predict_with_bias(np.c_[xx.ravel(), yy.ravel()], weights)\n        Z = Z.reshape(xx.shape)\n        plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n        plt.scatter(X[:, 1], X[:, 2], c=Y, cmap=plt.cm.Paired)\n        # plt.axis('off')\n        plt.xlim((-0.2, 1.2))\n        plt.ylim((-0.2, 1.2))\n        plot_num += 1\n    if converged:\n        print 'Converged during epoch %s!' % epoch\n        break\n\n# Plot the decision boundary and the instances\nplt.figure()\nplt.title('Perceptron Decision Boundary')\nZ = predict_with_bias(np.c_[xx.ravel(), yy.ravel()], weights)\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.scatter(X[:, 1], X[:, 2], c=Y, cmap=plt.cm.Paired)\n# plt.axis('off')\nplt.xlim((-0.2, 1.2))\nplt.ylim((-0.2, 1.2))\nplot_num += 1\nplt.show()\n\n\n################# Figure 9 #################\n\"\"\"\n\n\"\"\"\nimport matplotlib\nmatplotlib.use('Qt4Agg')\nimport numpy as np\nimport matplotlib.pyplot as plt\n\npositive = [\n    [0, 0],\n    [1, 1]\n]\n\nnegative = [\n    [1, 0],\n    [0, 1]\n]\n\nplt.axes().set_aspect('equal')\nframe1 = plt.gca()\nframe1.axes.get_xaxis().set_ticks([0, 1])\nframe1.axes.get_yaxis().set_ticks([0, 1])\nplt.ylim((-.2, 1.2))\nplt.xlim((-.2, 1.2))\n\nplt.scatter([0, 1], [0, 1], marker='D', c='k', s=200)\nplt.scatter([1, 0], [0, 1], marker='.', c='r', s=800)\nplt.show()", "framework": "tensorflow"}
{"repo_name": "hang-qi/models", "file_path": "learning_to_remember_rare_events/memory.py", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# ==============================================================================\n\"\"\"Memory module for storing \"nearest neighbors\".\n\nImplements a key-value memory for generalized one-shot learning\nas described in the paper\n\"Learning to Remember Rare Events\"\nby Lukasz Kaiser, Ofir Nachum, Aurko Roy, Samy Bengio,\npublished as a conference paper at ICLR 2017.\n\"\"\"\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Memory(object):\n  \"\"\"Memory module.\"\"\"\n\n  def __init__(self, key_dim, memory_size, vocab_size,\n               choose_k=256, alpha=0.1, correct_in_top=1, age_noise=8.0,\n               var_cache_device='', nn_device=''):\n    self.key_dim = key_dim\n    self.memory_size = memory_size\n    self.vocab_size = vocab_size\n    self.choose_k = min(choose_k, memory_size)\n    self.alpha = alpha\n    self.correct_in_top = correct_in_top\n    self.age_noise = age_noise\n    self.var_cache_device = var_cache_device  # Variables are cached here.\n    self.nn_device = nn_device  # Device to perform nearest neighbour matmul.\n\n    caching_device = var_cache_device if var_cache_device else None\n    self.update_memory = tf.constant(True)  # Can be fed \"false\" if needed.\n    self.mem_keys = tf.get_variable(\n        'memkeys', [self.memory_size, self.key_dim], trainable=False,\n        initializer=tf.random_uniform_initializer(-0.0, 0.0),\n        caching_device=caching_device)\n    self.mem_vals = tf.get_variable(\n        'memvals', [self.memory_size], dtype=tf.int32, trainable=False,\n        initializer=tf.constant_initializer(0, tf.int32),\n        caching_device=caching_device)\n    self.mem_age = tf.get_variable(\n        'memage', [self.memory_size], dtype=tf.float32, trainable=False,\n        initializer=tf.constant_initializer(0.0), caching_device=caching_device)\n    self.recent_idx = tf.get_variable(\n        'recent_idx', [self.vocab_size], dtype=tf.int32, trainable=False,\n        initializer=tf.constant_initializer(0, tf.int32))\n\n    # variable for projecting query vector into memory key\n    self.query_proj = tf.get_variable(\n        'memory_query_proj', [self.key_dim, self.key_dim], dtype=tf.float32,\n        initializer=tf.truncated_normal_initializer(0, 0.01),\n        caching_device=caching_device)\n\n  def get(self):\n    return self.mem_keys, self.mem_vals, self.mem_age, self.recent_idx\n\n  def set(self, k, v, a, r=None):\n    return tf.group(\n        self.mem_keys.assign(k),\n        self.mem_vals.assign(v),\n        self.mem_age.assign(a),\n        (self.recent_idx.assign(r) if r is not None else tf.group()))\n\n  def clear(self):\n    return tf.variables_initializer([self.mem_keys, self.mem_vals, self.mem_age,\n                                     self.recent_idx])\n\n  def get_hint_pool_idxs(self, normalized_query):\n    \"\"\"Get small set of idxs to compute nearest neighbor queries on.\n\n    This is an expensive look-up on the whole memory that is used to\n    avoid more expensive operations later on.\n\n    Args:\n      normalized_query: A Tensor of shape [None, key_dim].\n\n    Returns:\n      A Tensor of shape [None, choose_k] of indices in memory\n      that are closest to the queries.\n\n    \"\"\"\n    # look up in large memory, no gradients\n    with tf.device(self.nn_device):\n      similarities = tf.matmul(tf.stop_gradient(normalized_query),\n                               self.mem_keys, transpose_b=True, name='nn_mmul')\n    _, hint_pool_idxs = tf.nn.top_k(\n        tf.stop_gradient(similarities), k=self.choose_k, name='nn_topk')\n    return hint_pool_idxs\n\n  def make_update_op(self, upd_idxs, upd_keys, upd_vals,\n                     batch_size, use_recent_idx, intended_output):\n    \"\"\"Function that creates all the update ops.\"\"\"\n    mem_age_incr = self.mem_age.assign_add(tf.ones([self.memory_size],\n                                                   dtype=tf.float32))\n    with tf.control_dependencies([mem_age_incr]):\n      mem_age_upd = tf.scatter_update(\n          self.mem_age, upd_idxs, tf.zeros([batch_size], dtype=tf.float32))\n\n    mem_key_upd = tf.scatter_update(\n        self.mem_keys, upd_idxs, upd_keys)\n    mem_val_upd = tf.scatter_update(\n        self.mem_vals, upd_idxs, upd_vals)\n\n    if use_recent_idx:\n      recent_idx_upd = tf.scatter_update(\n          self.recent_idx, intended_output, upd_idxs)\n    else:\n      recent_idx_upd = tf.group()\n\n    return tf.group(mem_age_upd, mem_key_upd, mem_val_upd, recent_idx_upd)\n\n  def query(self, query_vec, intended_output, use_recent_idx=True):\n    \"\"\"Queries memory for nearest neighbor.\n\n    Args:\n      query_vec: A batch of vectors to query (embedding of input to model).\n      intended_output: The values that would be the correct output of the\n        memory.\n      use_recent_idx: Whether to always insert at least one instance of a\n        correct memory fetch.\n\n    Returns:\n      A tuple (result, mask, teacher_loss).\n      result: The result of the memory look up.\n      mask: The affinity of the query to the result.\n      teacher_loss: The loss for training the memory module.\n    \"\"\"\n\n    batch_size = tf.shape(query_vec)[0]\n    output_given = intended_output is not None\n\n    # prepare query for memory lookup\n    query_vec = tf.matmul(query_vec, self.query_proj)\n    normalized_query = tf.nn.l2_normalize(query_vec, dim=1)\n\n    hint_pool_idxs = self.get_hint_pool_idxs(normalized_query)\n\n    if output_given and use_recent_idx:  # add at least one correct memory\n      most_recent_hint_idx = tf.gather(self.recent_idx, intended_output)\n      hint_pool_idxs = tf.concat(\n          axis=1,\n          values=[hint_pool_idxs, tf.expand_dims(most_recent_hint_idx, 1)])\n    choose_k = tf.shape(hint_pool_idxs)[1]\n\n    with tf.device(self.var_cache_device):\n      # create small memory and look up with gradients\n      my_mem_keys = tf.stop_gradient(tf.gather(self.mem_keys, hint_pool_idxs,\n                                               name='my_mem_keys_gather'))\n      similarities = tf.matmul(tf.expand_dims(normalized_query, 1),\n                               my_mem_keys, adjoint_b=True, name='batch_mmul')\n      hint_pool_sims = tf.squeeze(similarities, [1], name='hint_pool_sims')\n      hint_pool_mem_vals = tf.gather(self.mem_vals, hint_pool_idxs,\n                                     name='hint_pool_mem_vals')\n    # Calculate softmax mask on the top-k if requested.\n    # Softmax temperature. Say we have K elements at dist x and one at (x+a).\n    # Softmax of the last is e^tm(x+a)/Ke^tm*x + e^tm(x+a) = e^tm*a/K+e^tm*a.\n    # To make that 20% we'd need to have e^tm*a ~= 0.2K, so tm = log(0.2K)/a.\n    softmax_temp = max(1.0, np.log(0.2 * self.choose_k) / self.alpha)\n    mask = tf.nn.softmax(hint_pool_sims[:, :choose_k - 1] * softmax_temp)\n\n    # prepare hints from the teacher on hint pool\n    teacher_hints = tf.to_float(\n        tf.abs(tf.expand_dims(intended_output, 1) - hint_pool_mem_vals))\n    teacher_hints = 1.0 - tf.minimum(1.0, teacher_hints)\n\n    teacher_vals, teacher_hint_idxs = tf.nn.top_k(\n        hint_pool_sims * teacher_hints, k=1)\n    neg_teacher_vals, _ = tf.nn.top_k(\n        hint_pool_sims * (1 - teacher_hints), k=1)\n\n    # bring back idxs to full memory\n    teacher_idxs = tf.gather(\n        tf.reshape(hint_pool_idxs, [-1]),\n        teacher_hint_idxs[:, 0] + choose_k * tf.range(batch_size))\n\n    # zero-out teacher_vals if there are no hints\n    teacher_vals *= (\n        1 - tf.to_float(tf.equal(0.0, tf.reduce_sum(teacher_hints, 1))))\n\n    # prepare returned values\n    nearest_neighbor = tf.to_int32(\n        tf.argmax(hint_pool_sims[:, :choose_k - 1], 1))\n    no_teacher_idxs = tf.gather(\n        tf.reshape(hint_pool_idxs, [-1]),\n        nearest_neighbor + choose_k * tf.range(batch_size))\n\n    # we'll determine whether to do an update to memory based on whether\n    # memory was queried correctly\n    sliced_hints = tf.slice(teacher_hints, [0, 0], [-1, self.correct_in_top])\n    incorrect_memory_lookup = tf.equal(0.0, tf.reduce_sum(sliced_hints, 1))\n\n    # loss based on triplet loss\n    teacher_loss = (tf.nn.relu(neg_teacher_vals - teacher_vals + self.alpha)\n                    - self.alpha)\n\n    with tf.device(self.var_cache_device):\n      result = tf.gather(self.mem_vals, tf.reshape(no_teacher_idxs, [-1]))\n\n    # prepare memory updates\n    update_keys = normalized_query\n    update_vals = intended_output\n\n    fetched_idxs = teacher_idxs  # correctly fetched from memory\n    with tf.device(self.var_cache_device):\n      fetched_keys = tf.gather(self.mem_keys, fetched_idxs, name='fetched_keys')\n      fetched_vals = tf.gather(self.mem_vals, fetched_idxs, name='fetched_vals')\n\n    # do memory updates here\n    fetched_keys_upd = update_keys + fetched_keys  # Momentum-like update\n    fetched_keys_upd = tf.nn.l2_normalize(fetched_keys_upd, dim=1)\n    # Randomize age a bit, e.g., to select different ones in parallel workers.\n    mem_age_with_noise = self.mem_age + tf.random_uniform(\n        [self.memory_size], - self.age_noise, self.age_noise)\n\n    _, oldest_idxs = tf.nn.top_k(mem_age_with_noise, k=batch_size, sorted=False)\n\n    with tf.control_dependencies([result]):\n      upd_idxs = tf.where(incorrect_memory_lookup,\n                          oldest_idxs,\n                          fetched_idxs)\n      # upd_idxs = tf.Print(upd_idxs, [upd_idxs], \"UPD IDX\", summarize=8)\n      upd_keys = tf.where(incorrect_memory_lookup,\n                          update_keys,\n                          fetched_keys_upd)\n      upd_vals = tf.where(incorrect_memory_lookup,\n                          update_vals,\n                          fetched_vals)\n\n    def make_update_op():\n      return self.make_update_op(upd_idxs, upd_keys, upd_vals,\n                                 batch_size, use_recent_idx, intended_output)\n\n    update_op = tf.cond(self.update_memory, make_update_op, tf.no_op)\n\n    with tf.control_dependencies([update_op]):\n      result = tf.identity(result)\n      mask = tf.identity(mask)\n      teacher_loss = tf.identity(teacher_loss)\n\n    return result, mask, tf.reduce_mean(teacher_loss)\n\n\nclass LSHMemory(Memory):\n  \"\"\"Memory employing locality sensitive hashing.\n\n  Note: Not fully tested.\n  \"\"\"\n\n  def __init__(self, key_dim, memory_size, vocab_size,\n               choose_k=256, alpha=0.1, correct_in_top=1, age_noise=8.0,\n               var_cache_device='', nn_device='',\n               num_hashes=None, num_libraries=None):\n    super(LSHMemory, self).__init__(\n        key_dim, memory_size, vocab_size,\n        choose_k=choose_k, alpha=alpha, correct_in_top=1, age_noise=age_noise,\n        var_cache_device=var_cache_device, nn_device=nn_device)\n\n    self.num_libraries = num_libraries or int(self.choose_k ** 0.5)\n    self.num_per_hash_slot = max(1, self.choose_k // self.num_libraries)\n    self.num_hashes = (num_hashes or\n                       int(np.log2(self.memory_size / self.num_per_hash_slot)))\n    self.num_hashes = min(max(self.num_hashes, 1), 20)\n    self.num_hash_slots = 2 ** self.num_hashes\n\n    # hashing vectors\n    self.hash_vecs = [\n        tf.get_variable(\n            'hash_vecs%d' % i, [self.num_hashes, self.key_dim],\n            dtype=tf.float32, trainable=False,\n            initializer=tf.truncated_normal_initializer(0, 1))\n        for i in xrange(self.num_libraries)]\n\n    # map representing which hash slots map to which mem keys\n    self.hash_slots = [\n        tf.get_variable(\n            'hash_slots%d' % i, [self.num_hash_slots, self.num_per_hash_slot],\n            dtype=tf.int32, trainable=False,\n            initializer=tf.random_uniform_initializer(maxval=self.memory_size,\n                                                      dtype=tf.int32))\n        for i in xrange(self.num_libraries)]\n\n  def get(self):  # not implemented\n    return self.mem_keys, self.mem_vals, self.mem_age, self.recent_idx\n\n  def set(self, k, v, a, r=None):  # not implemented\n    return tf.group(\n        self.mem_keys.assign(k),\n        self.mem_vals.assign(v),\n        self.mem_age.assign(a),\n        (self.recent_idx.assign(r) if r is not None else tf.group()))\n\n  def clear(self):\n    return tf.variables_initializer([self.mem_keys, self.mem_vals, self.mem_age,\n                                     self.recent_idx] + self.hash_slots)\n\n  def get_hash_slots(self, query):\n    \"\"\"Gets hashed-to buckets for batch of queries.\n\n    Args:\n      query: 2-d Tensor of query vectors.\n\n    Returns:\n      A list of hashed-to buckets for each hash function.\n    \"\"\"\n\n    binary_hash = [\n        tf.less(tf.matmul(query, self.hash_vecs[i], transpose_b=True), 0)\n        for i in xrange(self.num_libraries)]\n    hash_slot_idxs = [\n        tf.reduce_sum(\n            tf.to_int32(binary_hash[i]) *\n            tf.constant([[2 ** i for i in xrange(self.num_hashes)]],\n                        dtype=tf.int32), 1)\n        for i in xrange(self.num_libraries)]\n    return hash_slot_idxs\n\n  def get_hint_pool_idxs(self, normalized_query):\n    \"\"\"Get small set of idxs to compute nearest neighbor queries on.\n\n    This is an expensive look-up on the whole memory that is used to\n    avoid more expensive operations later on.\n\n    Args:\n      normalized_query: A Tensor of shape [None, key_dim].\n\n    Returns:\n      A Tensor of shape [None, choose_k] of indices in memory\n      that are closest to the queries.\n\n    \"\"\"\n    # get hash of query vecs\n    hash_slot_idxs = self.get_hash_slots(normalized_query)\n\n    # grab mem idxs in the hash slots\n    hint_pool_idxs = [\n        tf.maximum(tf.minimum(\n            tf.gather(self.hash_slots[i], idxs),\n            self.memory_size - 1), 0)\n        for i, idxs in enumerate(hash_slot_idxs)]\n\n    return tf.concat(axis=1, values=hint_pool_idxs)\n\n  def make_update_op(self, upd_idxs, upd_keys, upd_vals,\n                     batch_size, use_recent_idx, intended_output):\n    \"\"\"Function that creates all the update ops.\"\"\"\n    base_update_op = super(LSHMemory, self).make_update_op(\n        upd_idxs, upd_keys, upd_vals,\n        batch_size, use_recent_idx, intended_output)\n\n    # compute hash slots to be updated\n    hash_slot_idxs = self.get_hash_slots(upd_keys)\n\n    # make updates\n    update_ops = []\n    with tf.control_dependencies([base_update_op]):\n      for i, slot_idxs in enumerate(hash_slot_idxs):\n        # for each slot, choose which entry to replace\n        entry_idx = tf.random_uniform([batch_size],\n                                      maxval=self.num_per_hash_slot,\n                                      dtype=tf.int32)\n        entry_mul = 1 - tf.one_hot(entry_idx, self.num_per_hash_slot,\n                                   dtype=tf.int32)\n        entry_add = (tf.expand_dims(upd_idxs, 1) *\n                     tf.one_hot(entry_idx, self.num_per_hash_slot,\n                                dtype=tf.int32))\n\n        mul_op = tf.scatter_mul(self.hash_slots[i], slot_idxs, entry_mul)\n        with tf.control_dependencies([mul_op]):\n          add_op = tf.scatter_add(self.hash_slots[i], slot_idxs, entry_add)\n          update_ops.append(add_op)\n\n    return tf.group(*update_ops)\n", "framework": "tensorflow"}
{"repo_name": "Denisolt/Tensorflow_Chat_Bot", "file_path": "local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/classifier_test.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Tests for Classifier.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport tempfile\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.contrib import learn\nfrom tensorflow.contrib.learn.python.learn.estimators import _sklearn\nfrom tensorflow.contrib.session_bundle import manifest_pb2\n\n\ndef iris_input_fn(num_epochs=None):\n  iris = tf.contrib.learn.datasets.load_iris()\n  features = tf.train.limit_epochs(\n      tf.reshape(tf.constant(iris.data), [-1, 4]), num_epochs=num_epochs)\n  labels = tf.reshape(tf.constant(iris.target), [-1])\n  return features, labels\n\n\ndef logistic_model_fn(features, labels, unused_mode):\n  labels = tf.one_hot(labels, 3, 1, 0)\n  prediction, loss = tf.contrib.learn.models.logistic_regression_zero_init(\n      features, labels)\n  train_op = tf.contrib.layers.optimize_loss(\n      loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n      learning_rate=0.1)\n  return prediction, loss, train_op\n\n\ndef logistic_model_params_fn(features, labels, unused_mode, params):\n  labels = tf.one_hot(labels, 3, 1, 0)\n  prediction, loss = tf.contrib.learn.models.logistic_regression_zero_init(\n      features, labels)\n  train_op = tf.contrib.layers.optimize_loss(\n      loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n      learning_rate=params['learning_rate'])\n  return prediction, loss, train_op\n\n\nclass ClassifierTest(tf.test.TestCase):\n\n  def testIrisAll(self):\n    est = tf.contrib.learn.Classifier(model_fn=logistic_model_fn, n_classes=3)\n    self._runIrisAll(est)\n\n  def testIrisAllWithParams(self):\n    est = tf.contrib.learn.Classifier(model_fn=logistic_model_params_fn,\n                                      n_classes=3,\n                                      params={'learning_rate': 0.01})\n    self._runIrisAll(est)\n\n  def testIrisInputFn(self):\n    iris = tf.contrib.learn.datasets.load_iris()\n    est = tf.contrib.learn.Classifier(model_fn=logistic_model_fn, n_classes=3)\n    est.fit(input_fn=iris_input_fn, steps=100)\n    est.evaluate(input_fn=iris_input_fn, steps=1, name='eval')\n    predict_input_fn = functools.partial(iris_input_fn, num_epochs=1)\n    predictions = list(est.predict(input_fn=predict_input_fn))\n    self.assertEqual(len(predictions), iris.target.shape[0])\n\n  def _runIrisAll(self, est):\n    iris = tf.contrib.learn.datasets.load_iris()\n    est.fit(iris.data, iris.target, steps=100)\n    scores = est.evaluate(x=iris.data, y=iris.target, name='eval')\n    predictions = list(est.predict(x=iris.data))\n    predictions_proba = list(est.predict_proba(x=iris.data))\n    self.assertEqual(len(predictions), iris.target.shape[0])\n    self.assertAllEqual(predictions, np.argmax(predictions_proba, axis=1))\n    other_score = _sklearn.accuracy_score(iris.target, predictions)\n    self.assertAllClose(other_score, scores['accuracy'])\n\n  def _get_default_signature(self, export_meta_filename):\n    \"\"\"Gets the default signature from the export.meta file.\"\"\"\n    with tf.Session():\n      save = tf.train.import_meta_graph(export_meta_filename)\n      meta_graph_def = save.export_meta_graph()\n      collection_def = meta_graph_def.collection_def\n\n      signatures_any = collection_def['serving_signatures'].any_list.value\n      self.assertEquals(len(signatures_any), 1)\n      signatures = manifest_pb2.Signatures()\n      signatures_any[0].Unpack(signatures)\n      default_signature = signatures.default_signature\n      return default_signature\n\n  # Disable this test case until b/31032996 is fixed.\n  def _testExportMonitorRegressionSignature(self):\n    iris = tf.contrib.learn.datasets.load_iris()\n    est = tf.contrib.learn.Classifier(model_fn=logistic_model_fn, n_classes=3)\n    export_dir = tempfile.mkdtemp() + 'export/'\n    export_monitor = learn.monitors.ExportMonitor(\n        every_n_steps=1,\n        export_dir=export_dir,\n        exports_to_keep=1,\n        signature_fn=tf.contrib.learn.classifier.classification_signature_fn)\n    est.fit(iris.data, iris.target, steps=2, monitors=[export_monitor])\n\n    self.assertTrue(tf.gfile.Exists(export_dir))\n    self.assertFalse(tf.gfile.Exists(export_dir + '00000000/export'))\n    self.assertTrue(tf.gfile.Exists(export_dir + '00000002/export'))\n    # Validate the signature\n    signature = self._get_default_signature(export_dir + '00000002/export.meta')\n    self.assertTrue(signature.HasField('classification_signature'))\n\n\nif __name__ == '__main__':\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "Amber819/chatbot_AM", "file_path": "memn2n/attentionn2n_dialogNew.py", "content": "from __future__ import division\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport numpy as np\nfrom six.moves import range\nfrom datetime import datetime\nfrom memn2n.modules import *\n\ndef zero_nil_slot(t, name=None):\n    \"\"\"\n    Overwrites the nil_slot (first row) of the input Tensor with zeros.\n\n    The nil_slot is a dummy slot and should not be trained and influence\n    the training algorithm.\n    \"\"\"\n    with tf.op_scope([t], name, \"zero_nil_slot\") as name:\n        t = tf.convert_to_tensor(t, name=\"t\")\n        s = tf.shape(t)[1]\n        z = tf.zeros(tf.stack([1, s]))\n        # z = tf.zeros([1, s])\n        return tf.concat([z, tf.slice(t, [1, 0], [-1, -1])], 0, name=name)\n\n\ndef add_gradient_noise(t, stddev=1e-3, name=None):\n    \"\"\"\n    Adds gradient noise as described in http://arxiv.org/abs/1511.06807 [2].\n\n    The input Tensor `t` should be a gradient.\n\n    The output will be `t` + gaussian noise.\n\n    0.001 was said to be a good fixed value for memory networks [2].\n    \"\"\"\n    with tf.op_scope([t, stddev], name, \"add_gradient_noise\") as name:\n        t = tf.convert_to_tensor(t, name=\"t\")\n        gn = tf.random_normal(tf.shape(t), stddev=stddev)\n        return tf.add(t, gn, name=name)\n\n\nclass AttentionN2NDialogNew(object):\n    \"\"\"End-To-End Memory Network.\"\"\"\n\n    @staticmethod\n    def default_params():\n        return {\n            \"batch_size\": 10,\n            \"vocab_size\": 40,\n            \"sentence_size\": 10,\n            \"embedding_size\": 32,\n            \"blocks\": 2,\n            \"num_heads\": 2,\n            \"dropout_rate\": 0.1,\n            \"max_grad_norm\": 40.0,\n            \"nonlin\": None,\n            \"initializer\": tf.random_normal_initializer(stddev=0.1),\n            \"optimizer\": tf.train.AdamOptimizer(learning_rate=1e-2),\n            \"session\": tf.Session(),\n            \"name\": 'Attention',\n            \"task_id\": 6\n        }\n\n    def __init__(self, batch_size, vocab_size, sentence_size, embedding_size,\n                 blocks=6, num_heads=8, dropout_rate=0.1,\n                 max_grad_norm=40.0,\n                 nonlin=None,\n                 initializer=tf.random_normal_initializer(stddev=0.1),\n                 optimizer=tf.train.AdamOptimizer(learning_rate=1e-2),\n                 session=tf.Session(),\n                 name='AttentionN2N',\n                 candidate_size=29,\n                 task_id=6):\n        \"\"\"Creates an End-To-End Full Attention Network\n\n        Args:\n            batch_size: The size of the batch.\n\n            vocab_size: The size of the vocabulary (should include the nil word). The nil word\n            one-hot encoding should be 0.\n\n            sentence_size: The max size of a sentence in the data. All sentences should be padded\n            to this length. If padding is required it should be done with nil one-hot encoding (0).\n\n            candidates_size: The size of candidates\n\n            memory_size: The max size of the memory. Since Tensorflow currently does not support jagged arrays\n            all memories must be padded to this length. If padding is required, the extra memories should be\n            empty memories; memories filled with the nil word ([0, 0, 0, ......, 0]).\n\n            embedding_size: The size of the word embedding.\n\n            max_grad_norm: Maximum L2 norm clipping value. Defaults to `40.0`.\n\n            nonlin: Non-linearity. Defaults to `None`.\n\n            initializer: Weight initializer. Defaults to `tf.random_normal_initializer(stddev=0.1)`.\n\n            optimizer: Optimizer algorithm used for SGD. Defaults to `tf.train.AdamOptimizer(learning_rate=1e-2)`.\n\n            encoding: A function returning a 2D Tensor (sentence_size, embedding_size). Defaults to `position_encoding`.\n\n            session: Tensorflow Session the model is run with. Defaults to `tf.Session()`.\n\n            name: Name of the End-To-End Memory Network. Defaults to `MemN2N`.\n        \"\"\"\n\n        self._batch_size = batch_size\n        self._vocab_size = vocab_size\n        self._blocks = blocks\n        self._dropout_rate = dropout_rate\n        self._num_heads = num_heads\n        self._sentence_size = sentence_size\n        self._candidate_size = candidate_size\n        self._embedding_size = embedding_size\n        self._max_grad_norm = max_grad_norm\n        self._nonlin = nonlin\n        self._init = initializer\n        self._opt = optimizer\n        self._name = name\n\n        self._build_inputs()\n\n        # define summary directory\n        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n        self.root_dir = \"%s_%s_%s_%s/\" % ('task',\n                                          str(task_id), 'summary_output', timestamp)\n\n        # seq loss\n        logits = self._inference(self._stories, self._queries, self._answers, self._is_training)\n        # logits = self._rnn_inference(self._stories, self._answers)\n        self.logits = logits\n        self.preds = tf.to_int32(tf.arg_max(logits, dimension=-1))\n        self.istarget = tf.to_float(tf.not_equal(self._answers, 0))\n        self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self._answers)) * self.istarget) / (\n            tf.reduce_sum(self.istarget))\n        self.y_smoothed = label_smoothing(tf.one_hot(self._answers, depth=self._vocab_size))\n        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.y_smoothed)\n        mean_loss = tf.reduce_sum(loss * self.istarget) / (tf.reduce_sum(self.istarget))\n\n        # loss op\n        loss_op = mean_loss\n\n        # gradient pipeline\n        grads_and_vars = self._opt.compute_gradients(loss_op)\n        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v)\n                          for g, v in grads_and_vars]\n        grads_and_vars = [(add_gradient_noise(g), v) for g,v in grads_and_vars]\n\n        train_op = self._opt.apply_gradients(\n            grads_and_vars, name=\"train_op\")\n\n        # predict ops\n        self.predict_op = self.preds\n\n        # assign ops\n        self.loss_op = loss_op\n\n        # self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.98, epsilon=1e-8)\n        # train_op = self.optimizer.minimize(mean_loss)\n\n        self.train_op = train_op\n\n        self.graph_output = self.loss_op\n\n        init_op = tf.initialize_all_variables()\n        self._sess = session\n        self._sess.run(init_op)\n\n    def _build_inputs(self):\n        self._stories = tf.placeholder(tf.int32, shape=(None, None), name=\"stories\")\n        self._answers = tf.placeholder(tf.int32, shape=(None, self._candidate_size), name=\"answers\")\n        self._is_training = tf.placeholder(tf.bool, shape=None, name='is_training')\n\n    def _inference(self, stories,queries, answers, is_training):\n        #  Encoder Embedding\n        self.enc = embedding(queries,\n                             vocab_size=self._vocab_size,\n                             num_units=self._embedding_size,\n                             scale=True,\n                             scope=\"embed\")\n        ## Positional Encoding\n        self.enc += embedding(\n            tf.tile(tf.expand_dims(tf.range(tf.shape(stories)[1]), 0), [tf.shape(stories)[0], 1]),\n            vocab_size=self._sentence_size,\n            num_units=self._embedding_size,\n            zero_pad=False,\n            scale=False,\n            scope=\"enc_pe\")\n\n        ## Dropout\n        self.enc = tf.layers.dropout(self.enc,\n                                     rate=self._dropout_rate,\n                                     training=is_training)\n        self.history = embedding(stories,\n                                 vocab_size=self._vocab_size,\n                                 num_units=self._embedding_size,\n                                 scale=True,\n                                 scope=\"embed\")\n        #  Decoder Embedding\n        self.decoder_inputs = tf.concat((tf.ones_like(answers[:, :1]) * 2, answers[:, :-1]), -1)  # 2:<S>\n        self.dec = embedding(self.decoder_inputs,\n                             vocab_size=self._vocab_size,\n                             num_units=self._embedding_size,\n                             scale=True,\n                             reuse=True,\n                             scope=\"embed\")\n\n        ## Positional Encoding\n        self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decoder_inputs)[1]), 0),\n                                      [tf.shape(self.decoder_inputs)[0], 1]),\n                              vocab_size=self._candidate_size,\n                              num_units=self._embedding_size,\n                              zero_pad=False,\n                              scale=False,\n                              scope=\"dec_pe\")\n\n        ## Dropout\n        self.dec = tf.layers.dropout(self.dec,\n                                     rate=self._dropout_rate,\n                                     training=is_training)\n\n        with tf.variable_scope(\"encoder\"):\n            ## Blocks\n            for i in range(self._blocks):\n                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n                    ### Multihead Attention\n                    self.enc = multihead_attention(queries=self.enc,\n                                                   keys=self.history,\n                                                   num_units=self._embedding_size,\n                                                   num_heads=self._num_heads,\n                                                   dropout_rate=self._dropout_rate,\n                                                   is_training=is_training,\n                                                   causality=False)\n\n                    ### Feed Forward\n                    self.enc = feedforward(self.enc, num_units=[4 * self._embedding_size, self._embedding_size])\n\n\n        with tf.variable_scope(\"decoder\"):\n            ## Blocks\n            for i in range(self._blocks):\n                with tf.variable_scope(\"num_blocks_{}\".format(i)):\n                    ## Multihead Attention ( self-attention)\n                    self.dec = multihead_attention(queries=self.dec,\n                                                   keys=self.dec,\n                                                   num_units=self._embedding_size,\n                                                   num_heads=self._num_heads,\n                                                   dropout_rate=self._dropout_rate,\n                                                   is_training=is_training,\n                                                   causality=True,\n                                                   scope=\"self_attention\")\n\n                    ## Multihead Attention ( vanilla attention)\n                    self.dec = multihead_attention(queries=self.dec,\n                                                   keys=self.enc,\n                                                   num_units=self._embedding_size,\n                                                   num_heads=self._num_heads,\n                                                   dropout_rate=self._dropout_rate,\n                                                   is_training=is_training,\n                                                   causality=False,\n                                                   scope=\"vanilla_attention\")\n\n                    ## Feed Forward\n                    self.dec = feedforward(self.dec, num_units=[4 * self._embedding_size, self._embedding_size])\n\n        logits = tf.layers.dense(self.dec, self._vocab_size)\n        return logits\n\n    def _rnn_inference(self, stories, answers):\n        decoder_inputs = tf.concat((tf.ones_like(answers[:, :1]) * 2, answers[:, :-1]), -1)\n        embeddings = tf.Variable(tf.random_uniform([self._vocab_size, self._embedding_size], -1.0, 1.0), dtype=tf.float32)\n\n        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, stories)\n        decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n\n        encoder_cell = tf.contrib.rnn.LSTMCell(self._embedding_size)\n\n        self.encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n            encoder_cell, encoder_inputs_embedded,\n            dtype=tf.float32, scope=\"plain_encoder\"\n        )\n        decoder_cell = tf.contrib.rnn.LSTMCell(self._embedding_size)\n\n        decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n            decoder_cell, decoder_inputs_embedded,\n\n            initial_state=encoder_final_state,\n\n            dtype=tf.float32, scope=\"plain_decoder\",\n        )\n\n        decoder_logits = tf.contrib.layers.linear(decoder_outputs, self._vocab_size)\n        return decoder_logits\n\n    def batch_fit(self, stories, queries, answers):\n        \"\"\"Runs the training algorithm over the passed batch\n\n        Args:\n            stories: Tensor (None, sentence_size)\n            queries: Tensor (None, sentence_size)\n            answers: Tensor (None, vocab_size)\n\n        Returns:\n            loss: floating-point number, the loss computed for the batch\n        \"\"\"\n        X = np.matrix(stories, dtype='int32')\n        Y = np.matrix(queries, dtype='int32')\n        Z = np.matrix(answers, dtype='int32')\n\n        feed_dict = {self._stories: X, self._queries: Y, self._answers: Z, self._is_training: True}\n        loss, _ = self._sess.run(\n            [self.loss_op, self.train_op], feed_dict=feed_dict)\n        return loss\n\n    def predict(self, stories, queries):\n        \"\"\"Predicts answers as one-hot encoding.\n\n        Args:\n            stories: Tensor (None, memory_size, sentence_size)\n            queries: Tensor (None, sentence_size)\n\n        Returns:\n            answers: Tensor (None, vocab_size)\n        \"\"\"\n        # TODO:split the \\s symbol get right sentence indexes\n        stories = np.matrix(stories, dtype='int32')\n        queries = np.matrix(queries, dtype='int32')\n        ### Autoregressive inference\n        preds = np.zeros((stories.shape[0], self._candidate_size), np.int32)\n        for j in range(self._candidate_size):\n            feed_dict = {self._stories: stories, self._queries: queries, self._answers: preds, self._is_training: False}\n            _preds = self._sess.run(self.predict_op, feed_dict=feed_dict)\n            preds[:, j] = _preds[:, j]\n\n        return preds\n\n\nclass AttentionModelTest():\n    \"\"\"\n    Tests the UnidirectionalRNNEncoder class.\n    \"\"\"\n\n    def __init__(self):\n        # super(AttentionModelTest, self).setUp()\n        rnn_encoder = AttentionN2NDialogNew\n        self.batch_size = 10\n        self.sequence_length = 15\n        self.mode = tf.contrib.learn.ModeKeys.TRAIN\n        self.params = rnn_encoder.default_params()\n        self.model = AttentionN2NDialog(**self.params)\n        self.encode_fn = self.model._inference\n    def test_encode(self):\n        inputs = np.random.random_integers(0, 30, [self.batch_size, self.sequence_length])\n        inputs = tf.Variable(initial_value=inputs, dtype=tf.int32)\n        encoder_output = self.encode_fn(inputs, inputs, is_training=True)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            encoder_output_ = sess.run(encoder_output)\n        print(encoder_output_.shape)\n        # np.testing.assert_array_equal(encoder_output_.shape,\n        #                               [self.batch_size, self.sequence_length, 32])\n\n    def test_batch_pred(self):\n        inputs = np.random.random_integers(0, 30, [self.batch_size, self.sequence_length])\n        inputs2 = np.random.random_integers(0, 30, [self.batch_size, 29])\n        loss = self.model.batch_fit(inputs, inputs2)\n        # loss = self.model.predict(inputs)\n        print(loss.shape)\n\nif __name__ == '__main__':\n    model = AttentionModelTest()\n    model.test_batch_pred()\n    # tf.test.main()", "framework": "tensorflow"}
{"repo_name": "abhishekg2389/youtube-8m-challenge", "file_path": "missing.py", "content": "import tensorflow as tf\nimport glob as glob\nimport getopt\nimport sys\nimport cPickle as pkl\nimport numpy as np\nimport time\nimport os\n\nopts, _ = getopt.getopt(sys.argv[1:],\"\",[\"chunk_file_path=\", \"comp_file_path=\", \"means_file_path=\", \"output_dir=\", \"input_dir=\"])\nchunk_file_path = \"../video_level_feat_v1/train*.tfrecord\"\ncomp_file_path = \"../video_level_feat_v1/train*.tfrecord\"\nmeans_file_path = \"../video_level_feat_v1/means.pkl\"\noutput_dir = \"../video_level_feat_v1/\"\ninput_dir = \"../video_level_feat_v1/\"\nprint(opts)\nfor opt, arg in opts:\n  if opt in (\"--chunk_file_path\"):\n    chunk_file_path = arg\n  if opt in (\"--comp_file_path\"):\n    comp_file_path = arg\n  if opt in (\"--means_file_path\"):\n    means_file_path = arg\n  if opt in (\"--output_dir\"):\n    output_dir = arg\n  if opt in (\"--input_dir\"):\n    input_dir = arg\n\n# filepaths to do\nf = file(chunk_file_path, 'rb')\nrecords_chunk = pkl.load(f)\nf.close()\n\n# means\nf = open(means_file_path, 'rb')\nmeans = pkl.load(f)\nf.close()\n\nfilepaths = [input_dir+x for x in records_chunk]\nfilepaths_queue = tf.train.string_input_producer(filepaths, num_epochs=1)\nreader = tf.TFRecordReader()\n_, serialized_example = reader.read(filepaths_queue)\n\nfeatures_format = {}\nfeature_names = []\nfor x in ['q0', 'q1', 'q2', 'q3', 'q4', 'mean', 'stddv', 'skew', 'kurt', 'iqr', 'rng', 'coeffvar', 'efficiency']:\n    features_format[x + '_rgb_frame'] = tf.FixedLenFeature([1024], tf.float32)\n    features_format[x + '_audio_frame'] = tf.FixedLenFeature([128], tf.float32)\n    feature_names.append(str(x + '_rgb_frame'))\n    feature_names.append(str(x + '_audio_frame'))\n\nfeatures_format['video_id'] = tf.FixedLenFeature([], tf.string)\nfeatures_format['labels'] = tf.VarLenFeature(tf.int64)\nfeatures_format['video_length'] = tf.FixedLenFeature([], tf.float32)\n\nstart_time = time.time()\n\nfor record in records_chunk:\n  # filepaths done\n  if os.path.isfile(comp_file_path):\n    f = file(comp_file_path, 'rb')\n    records_comp = pkl.load(f)\n    f.close()\n  else:\n    records_comp = {}\n  \n  if record in records_comp:\n    print(record + ' : Skipped')\n    print(len(records_comp)/float(len(records_chunk)))\n    continue\n  \n  filepaths_queue = tf.train.string_input_producer([input_dir+record], num_epochs=1)\n  reader = tf.TFRecordReader()\n  _, serialized_example = reader.read(filepaths_queue)\n  features = tf.parse_single_example(serialized_example,features=features_format)\n  \n  new_filepath = output_dir+record\n  writer = tf.python_io.TFRecordWriter(new_filepath)\n\n  with tf.Session() as sess:\n    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n    sess.run(init_op)\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n    counter = 0\n    try:\n      while True:\n        proc_features, = sess.run([features])\n        counter += 1\n        for feature_name in feature_names:\n          if np.isnan(proc_features[feature_name]).sum() > 0:\n            proc_features[feature_name][np.isnan(proc_features[feature_name])] = means[feature_name][np.isnan(proc_features[feature_name])]\n          elif np.isinf(proc_features[feature_name]).sum() > 0:\n            proc_features[feature_name][np.isinf(proc_features[feature_name])] = means[feature_name][np.isinf(proc_features[feature_name])]\n        \n        # writing tfrecord v1\n        proc_features['video_id'] = [proc_features['video_id']]\n        proc_features['video_length'] = [proc_features['video_length']]\n        proc_features['labels'] = proc_features['labels'].values\n        tf_features_format = {}\n        for key, value in proc_features.items():\n          if key == 'video_id':\n            tf_features_format[key] = tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n          elif key == 'labels':\n            tf_features_format[key] = tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n          else:\n            tf_features_format[key] = tf.train.Feature(float_list=tf.train.FloatList(value=value))\n        example = tf.train.Example(features=tf.train.Features(feature=tf_features_format))\n        writer.write(example.SerializeToString())\n    except tf.errors.OutOfRangeError, e:\n      coord.request_stop(e)\n    finally:\n      coord.request_stop()\n      coord.join(threads)\n      \n      print(record + ' : Done')\n      records_comp[record] = 1\n      print(len(records_comp)/float(len(records_chunk)))\n      f = file(comp_file_path, 'wb')\n      pkl.dump(records_comp, f, protocol=pkl.HIGHEST_PROTOCOL)\n      f.close()\n\n  # writing tfrecord v1\n  writer.close()\n  print(time.time() - start_time)\n", "framework": "tensorflow"}
{"repo_name": "maxkferg/smart-city-model", "file_path": "modules/collision/ddpg/space_network.py", "content": "\n'''\nA Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\nThis neural network is used to compress the state matrix into a representation\ninto a distribution over occupied space :)\n\nThe weights of this network should be trained along with the actor and the critic.\n\nAuthor: Max Ferguson\n'''\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib.distributions import MultivariateNormalDiag\nfrom tensorflow.contrib import rnn\n\n\nLEARNING_RATE = 1e-3\n\n\nclass SpaceNetwork:\n\n    def __init__(self, sess, n_input, n_steps, n_hidden, output_size):\n        \"\"\"\n        Create the network\n\n        @n_input: The size of each input state vector\n        @n_steps: The number of steps in the RNN sequence\n        @n_hidden: The number of hidden units\n        @n_output: The size of the output vector\n\n        Input Size: (batch_size, n_steps, n_input)\n        Output Size: (batch_size, n_mixture, 5)\n\n        The 3rd dimension is in the form:\n        [mu_x, mu_y, sigma_x, sigma_y, rho]\n\n        Global properties\n            - self.x_input: The input placeholder\n            - self.y_input: The input labels placeholder\n            - self.output: The output placeholder\n            - self.trainable_weights: The trainable weight\n        \"\"\"\n        self.sess = sess\n        self.n_steps = n_steps\n        self.n_hidden = n_hidden\n        self.output_size = output_size\n        self.input = tf.placeholder(\"float\",[None, n_steps, n_input], name=\"input\") # RNN feed\n\n        # Build the neural network\n        output, weights = self.create_network(self.input)\n        self.output = output\n        self.trainable_weights = weights\n\n        # Add the optimizer code\n        self.create_training_method(n_input)\n\n\n    def create_training_method(self, n_input):\n        #n_objects = int(n_input/2)\n        #self.train_positions = tf.placeholder(\"float\",[None, n_input], name=\"train_positions\") # Position for training\n        #self.train_labels = tf.placeholder(\"float\",   [None, n_objects], name=\"train_labels\") # Prob for training\n        # Reshape to separate x and y\n        # train_positions = tf.reshape(self.train_positions, [-1,n_objects,2])\n        # Define training optimizer\n        #prediction = tf.add_n([\n        #    MultivariateNormalDiag(loc=mu, scale_diag=sd).prob(train_positions) for mu,sd \\\n        #        in zip(tf.unstack(self.mu, self.n_mixture, 1), tf.unstack(self.sd, self.n_mixture, 1))\n        #])\n        size = [None]+list(self.output_size)\n        self.train_labels = tf.placeholder(\"float\", size, name=\"train_labels\") # Prob for training\n        self.loss = tf.nn.l2_loss(self.output - self.train_labels)\n        self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)\n\n\n    def create_network(self, inputs):\n        \"\"\"\n        Return the weights and output tensor\n        \"\"\"\n        # Prepare data shape to match `rnn` function requirements\n        # Current data input shape: (batch_size, n_steps, n_input)\n        # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n\n        # Output weights and biases\n        w1 = tf.Variable(tf.random_normal([self.n_hidden, 2000]))\n        b1 = tf.Variable(tf.random_normal([2000]))\n\n        w2 = tf.Variable(tf.random_normal([self.n_hidden, 80*80]))\n        b2 = tf.Variable(tf.random_normal([80*80]))\n\n        # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n        x = tf.unstack(inputs, self.n_steps, 1)\n\n        # Define a lstm cell with tensorflow\n        lstm_cell = rnn.BasicLSTMCell(self.n_hidden, forget_bias=1.0)\n\n        # Get lstm cell output\n        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n\n        # Linear activation, using rnn inner loop last output\n        layer1 = tf.sigmoid(tf.matmul(outputs[-1], w1) + b1)\n        layer2 = tf.sigmoid(tf.matmul(outputs[-1], w2) + b2)\n\n        # Reshape to 80*80\n        layer2 = tf.reshape(layer2, [-1,80,80])\n\n        # Gather all the trainable weights\n        weights = lstm_cell.trainable_weights + [w1, w2, b1, b2]\n\n        # Return the weights from inside the LSTM as well as the output\n        return layer2, weights\n\n\n    def train_on_batch(self, x_input, train_labels):\n        \"\"\"\n        Return a tensor that represents the loss function\n        \"\"\"\n        loss, other = self.sess.run((self.loss, self.optimizer), feed_dict={\n            self.input: x_input,\n            self.train_labels: train_labels\n        })\n        print('Train Loss',loss)\n\n\n    def test_on_batch(self, x_input, train_labels):\n        \"\"\"\n        Return the test loss\n        \"\"\"\n        loss = self.sess.run(self.loss, feed_dict={\n            self.input: x_input,\n            self.train_labels: train_labels\n        })\n        return loss\n\n\n", "framework": "tensorflow"}
{"repo_name": "google-research/morph-net", "file_path": "morph_net/network_regularizers/latency_regularizer.py", "content": "\"\"\"A NetworkRegularizer that targets inference latency.\"\"\"\n\nfrom typing import List, Optional, Type\n\nfrom morph_net.framework import batch_norm_source_op_handler\nfrom morph_net.framework import conv2d_transpose_source_op_handler as conv2d_transpose_handler\nfrom morph_net.framework import conv_source_op_handler as conv_handler\nfrom morph_net.framework import generic_regularizers\nfrom morph_net.framework import matmul_source_op_handler as matmul_handler\n\nfrom morph_net.framework import op_handler_decorator\nfrom morph_net.framework import op_handlers\nfrom morph_net.framework import op_regularizer_manager as orm\nfrom morph_net.network_regularizers import cost_calculator\nfrom morph_net.network_regularizers import logistic_sigmoid_regularizer\nfrom morph_net.network_regularizers import resource_function\nimport tensorflow.compat.v1 as tf\n\n\nclass LogisticSigmoidLatencyRegularizer(\n    logistic_sigmoid_regularizer.LogisticSigmoidRegularizer):\n  \"\"\"A LogisticSigmoidRegularizer that targets Latency.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      batch_size: Integer batch size to calculate cost/loss for.\n      regularize_on_mask: Bool. If True uses the binary mask as the\n        regularization vector. Else uses the probability vector.\n      alive_threshold: Float. Threshold below which values are considered dead.\n        This can be used both when mask_as_alive_vector is True and then the\n        threshold is used to binarize the sampled values and\n        when mask_as_alive_vector is False, and then the threshold is on the\n        channel probability.\n      mask_as_alive_vector: Bool. If True use the thresholded sampled mask\n        as the alive vector. Else, use thresholded probabilities from the\n        logits.\n      regularizer_decorator: A string, the name of the regularizer decorators to\n        use. Supported decorators are listed in\n        op_regularizer_decorator.SUPPORTED_DECORATORS.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use '|' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for more\n        detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n  \"\"\"\n\n  def __init__(self,\n               output_boundary: List[tf.Operation],\n               hardware,\n               batch_size=1,\n               regularize_on_mask=True,\n               alive_threshold=0.1,\n               mask_as_alive_vector=True,\n               regularizer_decorator: Optional[Type[\n                   generic_regularizers.OpRegularizer]] = None,\n               decorator_parameters=None,\n               input_boundary: Optional[List[tf.Operation]] = None,\n               force_group=None,\n               regularizer_blacklist=None):\n\n    self._hardware = hardware\n    self._batch_size = batch_size\n\n    super().__init__(\n        output_boundary=output_boundary,\n        regularize_on_mask=regularize_on_mask,\n        alive_threshold=alive_threshold,\n        mask_as_alive_vector=mask_as_alive_vector,\n        regularizer_decorator=regularizer_decorator,\n        decorator_parameters=decorator_parameters,\n        input_boundary=input_boundary,\n        force_group=force_group,\n        regularizer_blacklist=regularizer_blacklist)\n\n  def get_calculator(self):\n    return cost_calculator.CostCalculator(\n        self._manager, resource_function.latency_function_factory(\n            self._hardware, self._batch_size))\n\n  @property\n  def name(self):\n    return 'LogisticSigmoidLatency'\n\n  @property\n  def cost_name(self):\n    return self._hardware + ' Latency'\n\n\nclass GammaLatencyRegularizer(generic_regularizers.NetworkRegularizer):\n  \"\"\"A NetworkRegularizer that targets latency using Gamma L1.\"\"\"\n\n  def __init__(self,\n               output_boundary: List[tf.Operation],\n               gamma_threshold,\n               hardware,\n               batch_size=1,\n               regularizer_decorator: Optional[Type[\n                   generic_regularizers.OpRegularizer]] = None,\n               decorator_parameters=None,\n               input_boundary: Optional[List[tf.Operation]] = None,\n               force_group=None,\n               regularizer_blacklist=None) -> None:\n    \"\"\"Creates a GammaLatencyRegularizer object.\n\n    Latency cost and regularization loss is calculated for a specified hardware\n    platform.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      gamma_threshold: A float scalar, will be used as a 'gamma_threshold' for\n        all instances GammaL1Regularizer created by this class.\n      hardware: String name of hardware platform to target.  Must be a key from\n        resource_function.PEAK_COMPUTE.\n      batch_size: Integer batch size to calculate cost/loss for.\n      regularizer_decorator: A string, the name of the regularizer decorators\n        to use. Supported decorators are listed in\n        op_regularizer_decorator.SUPPORTED_DECORATORS.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use '|' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for\n        more detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    \"\"\"\n    source_op_handler = batch_norm_source_op_handler.BatchNormSourceOpHandler(\n        gamma_threshold)\n    if regularizer_decorator:\n      source_op_handler = op_handler_decorator.OpHandlerDecorator(\n          source_op_handler, regularizer_decorator,\n          decorator_parameters)\n    op_handler_dict = op_handlers.get_gamma_op_handler_dict()\n    op_handler_dict.update({\n        'FusedBatchNorm': source_op_handler,\n        'FusedBatchNormV2': source_op_handler,\n        'FusedBatchNormV3': source_op_handler,\n    })\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary, op_handler_dict, input_boundary=input_boundary,\n        force_group=force_group, regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager,\n        resource_function.latency_function_factory(hardware, batch_size))\n    self._hardware = hardware\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return 'Latency'\n\n  @property\n  def cost_name(self):\n    return self._hardware + ' Latency'\n\n\nclass GroupLassoLatencyRegularizer(generic_regularizers.NetworkRegularizer):\n  \"\"\"A NetworkRegularizer that targets Latency using L1 group lasso.\"\"\"\n\n  def __init__(self,\n               output_boundary,\n               threshold,\n               hardware,\n               batch_size=1,\n               l1_fraction=0,\n               regularizer_decorator=None,\n               decorator_parameters=None,\n               input_boundary=None,\n               force_group=None,\n               regularizer_blacklist=None):\n    \"\"\"Creates a GroupLassoFlopsRegularizer object.\n\n    Args:\n      output_boundary: An OpRegularizer will be created for all these\n        operations, and recursively for all ops they depend on via data\n        dependency that does not involve ops from input_boundary.\n      threshold: A float scalar, will be used as a 'threshold' for all\n        regularizer instances created by this class.\n      hardware: String name of hardware platform to target. Must be a key from\n        resource_function.PEAK_COMPUTE.\n      batch_size: Integer batch size to calculate cost/loss for.\n      l1_fraction: Relative weight of L1 in L1 + L2 regularization.\n      regularizer_decorator: A class of OpRegularizer decorator to use.\n      decorator_parameters: A dictionary of parameters to pass to the decorator\n        factory. To be used only with decorators that requires parameters,\n        otherwise use None.\n      input_boundary: A list of ops that represent the input boundary of the\n        subgraph being regularized (input boundary is not regularized).\n      force_group: List of regex for ops that should be force-grouped.  Each\n        regex corresponds to a separate group.  Use '|' operator to specify\n        multiple patterns in a single regex. See op_regularizer_manager for more\n        detail.\n      regularizer_blacklist: List of regex for ops that should not be\n        regularized. See op_regularizer_manager for more detail.\n    \"\"\"\n    custom_handlers = {\n        'Conv2D':\n            conv_handler.ConvSourceOpHandler(threshold, l1_fraction),\n        'Conv3D':\n            conv_handler.ConvSourceOpHandler(threshold, l1_fraction),\n        'Conv2DBackpropInput':\n            conv2d_transpose_handler.Conv2DTransposeSourceOpHandler(\n                threshold, l1_fraction),\n        'MatMul':\n            matmul_handler.MatMulSourceOpHandler(threshold, l1_fraction)\n    }\n    if regularizer_decorator:\n      for key in custom_handlers:\n        custom_handlers[key] = op_handler_decorator.OpHandlerDecorator(\n            custom_handlers[key], regularizer_decorator, decorator_parameters)\n\n    op_handler_dict = op_handlers.get_group_lasso_op_handler_dict()\n    op_handler_dict.update(custom_handlers)\n\n    self._manager = orm.OpRegularizerManager(\n        output_boundary,\n        op_handler_dict,\n        input_boundary=input_boundary,\n        force_group=force_group,\n        regularizer_blacklist=regularizer_blacklist)\n    self._calculator = cost_calculator.CostCalculator(\n        self._manager,\n        resource_function.latency_function_factory(hardware, batch_size))\n    self._hardware = hardware\n\n  def get_regularization_term(self, ops=None):\n    return self._calculator.get_regularization_term(ops)\n\n  def get_cost(self, ops=None):\n    return self._calculator.get_cost(ops)\n\n  @property\n  def op_regularizer_manager(self):\n    return self._manager\n\n  @property\n  def name(self):\n    return 'Latency'\n\n  @property\n  def cost_name(self):\n    return self._hardware + ' Latency'\n", "framework": "tensorflow"}
{"repo_name": "NifTK/NiftyNet", "file_path": "niftynet/contrib/sampler_pairwise/sampler_pairwise_uniform.py", "content": "from __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n#from tensorflow.contrib.data.python.ops.dataset_ops import Dataset\n\nfrom niftynet.engine.image_window import ImageWindow\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom niftynet.layer.linear_resize import LinearResizeLayer as Resize\n#from niftynet.layer.approximated_smoothing import SmoothingLayer as Smooth\n\n\nclass PairwiseUniformSampler(Layer):\n    def __init__(self,\n                 reader_0,\n                 reader_1,\n                 data_param,\n                 batch_size=1):\n        Layer.__init__(self, name='pairwise_sampler_uniform')\n        # reader for the fixed images\n        self.reader_0 = reader_0\n        # reader for the moving images\n        self.reader_1 = reader_1\n\n        # TODO:\n        # 0) check the readers should have the same length file list\n        # 1) detect window shape mismatches or defaulting\n        #    windows to the fixed image reader properties\n        # 2) reshape images to (supporting multi-modal data)\n        #    [batch, x, y, channel] or [batch, x, y, z, channels]\n        # 3) infer spatial rank\n        # 4) make ``label`` optional\n        self.batch_size = batch_size\n        self.spatial_rank = 3\n        self.window = ImageWindow.from_data_reader_properties(\n            self.reader_0.input_sources,\n            self.reader_0.shapes,\n            self.reader_0.tf_dtypes,\n            data_param)\n        if self.window.has_dynamic_shapes:\n            tf.logging.fatal('Dynamic shapes not supported.\\nPlease specify '\n                             'all spatial dims of the input data, for the '\n                             'spatial_window_size parameter.')\n            raise NotImplementedError\n        # TODO: check spatial dims the same across input modalities\n        self.image_shape = \\\n            self.reader_0.shapes['fixed_image'][:self.spatial_rank]\n        self.moving_image_shape = \\\n            self.reader_1.shapes['moving_image'][:self.spatial_rank]\n        self.window_size = self.window.shapes['fixed_image'][1:]\n\n        # initialise a dataset prefetching pairs of image and label volumes\n        n_subjects = len(self.reader_0.output_list)\n        rand_ints = np.random.randint(n_subjects, size=[n_subjects])\n        image_dataset = tf.data.Dataset.from_tensor_slices(rand_ints)\n        # mapping random integer id to 4 volumes moving/fixed x image/label\n        # tf.py_func wrapper of ``get_pairwise_inputs``\n        image_dataset = image_dataset.map(\n            lambda image_id: tuple(tf.py_func(\n                self.get_pairwise_inputs, [image_id],\n                [tf.int64, tf.float32, tf.float32, tf.int32, tf.int32])),\n            num_parallel_calls=4)  # supported by tf 1.4?\n        image_dataset = image_dataset.repeat()  # num_epochs can be param\n        image_dataset = image_dataset.shuffle(\n            buffer_size=self.batch_size * 20)\n        image_dataset = image_dataset.batch(self.batch_size)\n        self.iterator = image_dataset.make_initializable_iterator()\n\n    def get_pairwise_inputs(self, image_id):\n        # fetch fixed image\n        fixed_inputs = []\n        fixed_inputs.append(self._get_image('fixed_image', image_id)[0])\n        fixed_inputs.append(self._get_image('fixed_label', image_id)[0])\n        fixed_inputs = np.concatenate(fixed_inputs, axis=-1)\n        fixed_shape = np.asarray(fixed_inputs.shape).T.astype(np.int32)\n\n        # fetch moving image\n        moving_inputs = []\n        moving_inputs.append(self._get_image('moving_image', image_id)[0])\n        moving_inputs.append(self._get_image('moving_label', image_id)[0])\n        moving_inputs = np.concatenate(moving_inputs, axis=-1)\n        moving_shape = np.asarray(moving_inputs.shape).T.astype(np.int32)\n\n        return image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape\n\n    def _get_image(self, image_source_type, image_id):\n        # returns a random image from either the list of fixed images\n        # or the list of moving images\n        try:\n            image_source_type = image_source_type.decode()\n        except AttributeError:\n            pass\n        if image_source_type.startswith('fixed'):\n            _, data, _ = self.reader_0(idx=image_id)\n        else:  # image_source_type.startswith('moving'):\n            _, data, _ = self.reader_1(idx=image_id)\n        image = np.asarray(data[image_source_type]).astype(np.float32)\n        image_shape = list(image.shape)\n        image = np.reshape(image, image_shape[:self.spatial_rank] + [-1])\n        image_shape = np.asarray(image.shape).astype(np.int32)\n        return image, image_shape\n\n    def layer_op(self):\n        \"\"\"\n        This function concatenate image and label volumes at the last dim\n        and randomly cropping the volumes (also the cropping margins)\n        \"\"\"\n        image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape = \\\n            self.iterator.get_next()\n        # TODO preprocessing layer modifying\n        #      image shapes will not be supported\n        # assuming the same shape across modalities, using the first\n        image_id.set_shape((self.batch_size,))\n        image_id = tf.to_float(image_id)\n\n        fixed_inputs.set_shape(\n            (self.batch_size,) + (None,) * self.spatial_rank + (2,))\n        # last dim is 1 image + 1 label\n        moving_inputs.set_shape(\n            (self.batch_size,) + self.moving_image_shape + (2,))\n        fixed_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n        moving_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n\n        # resizing the moving_inputs to match the target\n        # assumes the same shape across the batch\n        target_spatial_shape = \\\n            tf.unstack(fixed_shape[0], axis=0)[:self.spatial_rank]\n        moving_inputs = Resize(new_size=target_spatial_shape)(moving_inputs)\n        combined_volume = tf.concat([fixed_inputs, moving_inputs], axis=-1)\n\n        # smoothing_layer = Smoothing(\n        #     sigma=1, truncate=3.0, type_str='gaussian')\n        # combined_volume = tf.unstack(combined_volume, axis=-1)\n        # combined_volume[0] = tf.expand_dims(combined_volume[0], axis=-1)\n        # combined_volume[1] = smoothing_layer(\n        #     tf.expand_dims(combined_volume[1]), axis=-1)\n        # combined_volume[2] = tf.expand_dims(combined_volume[2], axis=-1)\n        # combined_volume[3] = smoothing_layer(\n        #     tf.expand_dims(combined_volume[3]), axis=-1)\n        # combined_volume = tf.stack(combined_volume, axis=-1)\n\n        # TODO affine data augmentation here\n        if self.spatial_rank == 3:\n\n            window_channels = np.prod(self.window_size[self.spatial_rank:]) * 4\n            # TODO if no affine augmentation:\n            img_spatial_shape = target_spatial_shape\n            win_spatial_shape = [tf.constant(dim) for dim in\n                                 self.window_size[:self.spatial_rank]]\n            # when img==win make sure shift => 0.0\n            # otherwise interpolation is out of bound\n            batch_shift = [\n                tf.random_uniform(\n                    shape=(self.batch_size, 1),\n                    minval=0,\n                    maxval=tf.maximum(tf.to_float(img - win - 1), 0.01))\n                for (win, img) in zip(win_spatial_shape, img_spatial_shape)]\n            batch_shift = tf.concat(batch_shift, axis=1)\n            affine_constraints = ((1.0, 0.0, 0.0, None),\n                                  (0.0, 1.0, 0.0, None),\n                                  (0.0, 0.0, 1.0, None))\n            computed_grid = AffineGridWarperLayer(\n                source_shape=(None, None, None),\n                output_shape=self.window_size[:self.spatial_rank],\n                constraints=affine_constraints)(batch_shift)\n            computed_grid.set_shape((self.batch_size,) +\n                                    self.window_size[:self.spatial_rank] +\n                                    (self.spatial_rank,))\n            resampler = ResamplerLayer(\n                interpolation='linear', boundary='replicate')\n            windows = resampler(combined_volume, computed_grid)\n            out_shape = [self.batch_size] + \\\n                        list(self.window_size[:self.spatial_rank]) + \\\n                        [window_channels]\n            windows.set_shape(out_shape)\n\n            image_id = tf.reshape(image_id, (self.batch_size, 1))\n            start_location = tf.zeros((self.batch_size, self.spatial_rank))\n            locations = tf.concat([\n                image_id, start_location, batch_shift], axis=1)\n        return windows, locations\n        # return windows, [tf.reduce_max(computed_grid), batch_shift]\n\n    # overriding input buffers\n    def run_threads(self, session, *args, **argvs):\n        \"\"\"\n        To be called at the beginning of running graph variables\n        \"\"\"\n        session.run(self.iterator.initializer)\n        return\n\n    def close_all(self):\n        # do nothing\n        pass\n", "framework": "tensorflow"}
{"repo_name": "NifTK/NiftyNet", "file_path": "niftynet/contrib/sampler_pairwise/sampler_pairwise_uniform_csv.py", "content": "from __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport tensorflow as tf\n#from tensorflow.contrib.data.python.ops.dataset_ops import Dataset\n\nfrom niftynet.engine.image_window import ImageWindow\nfrom niftynet.layer.base_layer import Layer\nfrom niftynet.layer.grid_warper import AffineGridWarperLayer\nfrom niftynet.layer.resampler import ResamplerLayer\nfrom niftynet.layer.linear_resize import LinearResizeLayer as Resize\n#from niftynet.layer.approximated_smoothing import SmoothingLayer as Smooth\n\n\nclass PairwiseUniformSampler(Layer):\n    def __init__(self,\n                 reader_0,\n                 reader_1,\n                 data_param,\n                 batch_size=1):\n        Layer.__init__(self, name='pairwise_sampler_uniform')\n        # reader for the fixed images\n        self.reader_0 = reader_0\n        # reader for the moving images\n        self.reader_1 = reader_1\n\n        # TODO:\n        # 0) check the readers should have the same length file list\n        # 1) detect window shape mismatches or defaulting\n        #    windows to the fixed image reader properties\n        # 2) reshape images to (supporting multi-modal data)\n        #    [batch, x, y, channel] or [batch, x, y, z, channels]\n        # 3) infer spatial rank\n        # 4) make ``label`` optional\n        self.batch_size = batch_size\n        self.spatial_rank = 3\n        self.window = ImageWindow.from_data_reader_properties(\n            self.reader_0.input_sources,\n            self.reader_0.shapes,\n            self.reader_0.tf_dtypes,\n            data_param)\n        if self.window.has_dynamic_shapes:\n            tf.logging.fatal('Dynamic shapes not supported.\\nPlease specify '\n                             'all spatial dims of the input data, for the '\n                             'spatial_window_size parameter.')\n            raise NotImplementedError\n        # TODO: check spatial dims the same across input modalities\n        self.image_shape = \\\n            self.reader_0.shapes['fixed_image'][:self.spatial_rank]\n        self.moving_image_shape = \\\n            self.reader_1.shapes['moving_image'][:self.spatial_rank]\n        self.window_size = self.window.shapes['fixed_image'][1:]\n\n        # initialise a dataset prefetching pairs of image and label volumes\n        n_subjects = len(self.reader_0.output_list)\n        rand_ints = np.random.randint(n_subjects, size=[n_subjects])\n        image_dataset = tf.data.Dataset.from_tensor_slices(rand_ints)\n        # mapping random integer id to 4 volumes moving/fixed x image/label\n        # tf.py_func wrapper of ``get_pairwise_inputs``\n        image_dataset = image_dataset.map(\n            lambda image_id: tuple(tf.py_func(\n                self.get_pairwise_inputs, [image_id],\n                [tf.int64, tf.float32, tf.float32, tf.int32, tf.int32])),\n            num_parallel_calls=4)  # supported by tf 1.4?\n        image_dataset = image_dataset.repeat()  # num_epochs can be param\n        image_dataset = image_dataset.shuffle(\n            buffer_size=self.batch_size * 20)\n        image_dataset = image_dataset.batch(self.batch_size)\n        self.iterator = image_dataset.make_initializable_iterator()\n\n    def get_pairwise_inputs(self, image_id):\n        # fetch fixed image\n        fixed_inputs = []\n        fixed_inputs.append(self._get_image('fixed_image', image_id)[0])\n        fixed_inputs.append(self._get_image('fixed_label', image_id)[0])\n        fixed_inputs = np.concatenate(fixed_inputs, axis=-1)\n        fixed_shape = np.asarray(fixed_inputs.shape).T.astype(np.int32)\n\n        # fetch moving image\n        moving_inputs = []\n        moving_inputs.append(self._get_image('moving_image', image_id)[0])\n        moving_inputs.append(self._get_image('moving_label', image_id)[0])\n        moving_inputs = np.concatenate(moving_inputs, axis=-1)\n        moving_shape = np.asarray(moving_inputs.shape).T.astype(np.int32)\n\n        return image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape\n\n    def _get_image(self, image_source_type, image_id):\n        # returns a random image from either the list of fixed images\n        # or the list of moving images\n        try:\n            image_source_type = image_source_type.decode()\n        except AttributeError:\n            pass\n        if image_source_type.startswith('fixed'):\n            _, data, _ = self.reader_0(idx=image_id)\n        else:  # image_source_type.startswith('moving'):\n            _, data, _ = self.reader_1(idx=image_id)\n        image = np.asarray(data[image_source_type]).astype(np.float32)\n        image_shape = list(image.shape)\n        image = np.reshape(image, image_shape[:self.spatial_rank] + [-1])\n        image_shape = np.asarray(image.shape).astype(np.int32)\n        return image, image_shape\n\n    def layer_op(self):\n        \"\"\"\n        This function concatenate image and label volumes at the last dim\n        and randomly cropping the volumes (also the cropping margins)\n        \"\"\"\n        image_id, fixed_inputs, moving_inputs, fixed_shape, moving_shape = \\\n            self.iterator.get_next()\n        # TODO preprocessing layer modifying\n        #      image shapes will not be supported\n        # assuming the same shape across modalities, using the first\n        image_id.set_shape((self.batch_size,))\n        image_id = tf.to_float(image_id)\n\n        fixed_inputs.set_shape(\n            (self.batch_size,) + (None,) * self.spatial_rank + (2,))\n        # last dim is 1 image + 1 label\n        moving_inputs.set_shape(\n            (self.batch_size,) + self.moving_image_shape + (2,))\n        fixed_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n        moving_shape.set_shape((self.batch_size, self.spatial_rank + 1))\n\n        # resizing the moving_inputs to match the target\n        # assumes the same shape across the batch\n        target_spatial_shape = \\\n            tf.unstack(fixed_shape[0], axis=0)[:self.spatial_rank]\n        moving_inputs = Resize(new_size=target_spatial_shape)(moving_inputs)\n        combined_volume = tf.concat([fixed_inputs, moving_inputs], axis=-1)\n\n        # smoothing_layer = Smoothing(\n        #     sigma=1, truncate=3.0, type_str='gaussian')\n        # combined_volume = tf.unstack(combined_volume, axis=-1)\n        # combined_volume[0] = tf.expand_dims(combined_volume[0], axis=-1)\n        # combined_volume[1] = smoothing_layer(\n        #     tf.expand_dims(combined_volume[1]), axis=-1)\n        # combined_volume[2] = tf.expand_dims(combined_volume[2], axis=-1)\n        # combined_volume[3] = smoothing_layer(\n        #     tf.expand_dims(combined_volume[3]), axis=-1)\n        # combined_volume = tf.stack(combined_volume, axis=-1)\n\n        # TODO affine data augmentation here\n        if self.spatial_rank == 3:\n\n            window_channels = np.prod(self.window_size[self.spatial_rank:]) * 4\n            # TODO if no affine augmentation:\n            img_spatial_shape = target_spatial_shape\n            win_spatial_shape = [tf.constant(dim) for dim in\n                                 self.window_size[:self.spatial_rank]]\n            # when img==win make sure shift => 0.0\n            # otherwise interpolation is out of bound\n            batch_shift = [\n                tf.random_uniform(\n                    shape=(self.batch_size, 1),\n                    minval=0,\n                    maxval=tf.maximum(tf.to_float(img - win - 1), 0.01))\n                for (win, img) in zip(win_spatial_shape, img_spatial_shape)]\n            batch_shift = tf.concat(batch_shift, axis=1)\n            affine_constraints = ((1.0, 0.0, 0.0, None),\n                                  (0.0, 1.0, 0.0, None),\n                                  (0.0, 0.0, 1.0, None))\n            computed_grid = AffineGridWarperLayer(\n                source_shape=(None, None, None),\n                output_shape=self.window_size[:self.spatial_rank],\n                constraints=affine_constraints)(batch_shift)\n            computed_grid.set_shape((self.batch_size,) +\n                                    self.window_size[:self.spatial_rank] +\n                                    (self.spatial_rank,))\n            resampler = ResamplerLayer(\n                interpolation='linear', boundary='replicate')\n            windows = resampler(combined_volume, computed_grid)\n            out_shape = [self.batch_size] + \\\n                        list(self.window_size[:self.spatial_rank]) + \\\n                        [window_channels]\n            windows.set_shape(out_shape)\n\n            image_id = tf.reshape(image_id, (self.batch_size, 1))\n            start_location = tf.zeros((self.batch_size, self.spatial_rank))\n            locations = tf.concat([\n                image_id, start_location, batch_shift], axis=1)\n        return windows, locations\n        # return windows, [tf.reduce_max(computed_grid), batch_shift]\n\n    # overriding input buffers\n    def run_threads(self, session, *args, **argvs):\n        \"\"\"\n        To be called at the beginning of running graph variables\n        \"\"\"\n        session.run(self.iterator.initializer)\n        return\n\n    def close_all(self):\n        # do nothing\n        pass\n", "framework": "tensorflow"}
{"repo_name": "google-research/pegasus", "file_path": "pegasus/layers/transformer_block.py", "content": "# Copyright 2022 The PEGASUS Authors..\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Transformer block.\n\nFrom \"Attention Is All You Need\", https://arxiv.org/abs/1706.03762.\n\nNotations:\n  B: batch_size, I: max_input_len, M: max_memory_len, D: hidden_size\n\"\"\"\n# \n# pylint: disable=invalid-name\n# pylint: disable=g-long-lambda\n\nfrom pegasus.layers import attention\nimport tensorflow as tf\nfrom tensorflow.contrib import layers as contrib_layers\n\n\nclass TransformerBlock(object):\n  \"\"\"Transformer block.\n\n  Attention block of self-attention, attention over external memory, and\n  feedforward network.\n  Initialize the block with\n    block = TransformerBlock(hidden_size, filter_size, num_heads, dropout)\n  To create an encoder self attention layer, use\n    x = block(x, x_bias, None, None)\n  To create a decoder attention layer, use\n    y = block(y, upper_triangle_bias, x, x_bias)\n  \"\"\"\n\n  def __init__(self, hidden_size, filter_size, num_heads, dropout):\n    self._self_attn_layer = attention.SelfAttention(hidden_size, num_heads,\n                                                    dropout)\n    self._attn_layer = attention.Attention(hidden_size, num_heads, dropout)\n    self._relu_layer = tf.layers.Dense(filter_size, activation=tf.nn.relu)\n    self._output_layer = tf.layers.Dense(hidden_size)\n    self._dropout_fn = lambda x, training: tf.compat.v2.nn.dropout(\n        x, dropout, noise_shape=[x.shape[0], 1, x.shape[2]]) if training else x\n\n  def __call__(self,\n               training,\n               inputs_BxIxD,\n               bias_BxIxI,\n               memory_BxMxD,\n               bias_BxIxM,\n               cache=None,\n               decode_i=None):\n    s_BxIxD = inputs_BxIxD\n    with tf.variable_scope(\"self_attention\"):\n      y_BxIxD = contrib_layers.layer_norm(s_BxIxD, begin_norm_axis=2)\n      y_BxIxD = self._self_attn_layer(\n          y_BxIxD, bias_BxIxI, training, cache=cache, decode_i=decode_i)\n      s_BxIxD += self._dropout_fn(y_BxIxD, training)\n    if memory_BxMxD is not None:\n      with tf.variable_scope(\"memory_attention\"):\n        y_BxIxD = contrib_layers.layer_norm(s_BxIxD, begin_norm_axis=2)\n        y_BxIxD = self._attn_layer(y_BxIxD, memory_BxMxD, bias_BxIxM, training)\n        s_BxIxD += self._dropout_fn(y_BxIxD, training)\n    with tf.variable_scope(\"ffn\"):\n      y_BxIxD = contrib_layers.layer_norm(s_BxIxD, begin_norm_axis=2)\n      y_BxIxD = self._dropout_fn(self._relu_layer(y_BxIxD), training)\n      s_BxIxD += self._dropout_fn(self._output_layer(y_BxIxD), training)\n    return s_BxIxD\n\n\ndef stack(layers,\n          training,\n          inputs_BxIxD,\n          bias_BxIxI,\n          memory_BxMxD,\n          bias_BxIxM,\n          cache=None,\n          decode_i=None):\n  \"\"\"Stack AttentionBlock layers.\"\"\"\n  if (memory_BxMxD is None) != (bias_BxIxM is None):\n    raise ValueError(\"memory and memory_bias need to be provided together.\")\n  s_BxIxD = inputs_BxIxD\n  for i, layer in enumerate(layers):\n    with tf.variable_scope(\"layer_%d\" % i):\n      s_BxIxD = layer(\n          training,\n          s_BxIxD,\n          bias_BxIxI,\n          memory_BxMxD,\n          bias_BxIxM,\n          cache=cache[str(i)] if cache is not None else None,\n          decode_i=decode_i)\n  return s_BxIxD\n", "framework": "tensorflow"}
{"repo_name": "cshallue/models", "file_path": "research/compression/entropy_coder/dataset/gen_synthetic_single.py", "content": "# Copyright 2016 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Generate a single synthetic sample.\"\"\"\n\nimport io\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nimport synthetic_model\n\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    'sample_filename', None,\n    \"\"\"Output file to store the generated binary code.\"\"\")\n\n\ndef GenerateSample(filename, code_shape, layer_depth):\n  # {0, +1} binary codes.\n  # No conversion since the output file is expected to store\n  # codes using {0, +1} codes (and not {-1, +1}).\n  code = synthetic_model.GenerateSingleCode(code_shape)\n  code = np.round(code)\n\n  # Reformat the code so as to be compatible with what is generated\n  # by the image encoder.\n  # The image encoder generates a tensor of size:\n  # iteration_count x batch_size x height x width x iteration_depth.\n  # Here: batch_size = 1\n  if code_shape[-1] % layer_depth != 0:\n    raise ValueError('Number of layers is not an integer')\n  height = code_shape[0]\n  width = code_shape[1]\n  code = code.reshape([1, height, width, -1, layer_depth])\n  code = np.transpose(code, [3, 0, 1, 2, 4])\n\n  int_codes = code.astype(np.int8)\n  exported_codes = np.packbits(int_codes.reshape(-1))\n\n  output = io.BytesIO()\n  np.savez_compressed(output, shape=int_codes.shape, codes=exported_codes)\n  with tf.gfile.FastGFile(filename, 'wb') as code_file:\n    code_file.write(output.getvalue())\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  # Note: the height and the width is different from the training dataset.\n  # The main purpose is to show that the entropy coder model is fully\n  # convolutional and can be used on any image size.\n  layer_depth = 2\n  GenerateSample(FLAGS.sample_filename, [31, 36, 8], layer_depth)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n\n", "framework": "tensorflow"}
{"repo_name": "cshallue/models", "file_path": "research/rebar/rebar_train.py", "content": "# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport random\nimport sys\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nimport rebar\nimport datasets\nimport logger as L\n\ntry:\n  xrange          # Python 2\nexcept NameError:\n  xrange = range  # Python 3\n\ngfile = tf.gfile\n\ntf.app.flags.DEFINE_string(\"working_dir\", \"/tmp/rebar\",\n                           \"\"\"Directory where to save data, write logs, etc.\"\"\")\ntf.app.flags.DEFINE_string('hparams', '',\n                           '''Comma separated list of name=value pairs.''')\ntf.app.flags.DEFINE_integer('eval_freq', 20,\n                           '''How often to run the evaluation step.''')\nFLAGS = tf.flags.FLAGS\n\ndef manual_scalar_summary(name, value):\n  value = tf.Summary.Value(tag=name, simple_value=value)\n  summary_str = tf.Summary(value=[value])\n  return summary_str\n\ndef eval(sbn, eval_xs, n_samples=100, batch_size=5):\n  n = eval_xs.shape[0]\n  i = 0\n  res = []\n  while i < n:\n    batch_xs = eval_xs[i:min(i+batch_size, n)]\n    res.append(sbn.partial_eval(batch_xs, n_samples))\n    i += batch_size\n  res = np.mean(res, axis=0)\n  return res\n\ndef train(sbn, train_xs, valid_xs, test_xs, training_steps, debug=False):\n  hparams = sorted(sbn.hparams.values().items())\n  hparams = (map(str, x) for x in hparams)\n  hparams = ('_'.join(x) for x in hparams)\n  hparams_str = '.'.join(hparams)\n\n  logger = L.Logger()\n\n  # Create the experiment name from the hparams\n  experiment_name = ([str(sbn.hparams.n_hidden) for i in xrange(sbn.hparams.n_layer)] +\n                     [str(sbn.hparams.n_input)])\n  if sbn.hparams.nonlinear:\n    experiment_name = '~'.join(experiment_name)\n  else:\n    experiment_name = '-'.join(experiment_name)\n  experiment_name = 'SBN_%s' % experiment_name\n  rowkey = {'experiment': experiment_name,\n            'model': hparams_str}\n\n  # Create summary writer\n  summ_dir = os.path.join(FLAGS.working_dir, hparams_str)\n  summary_writer = tf.summary.FileWriter(\n      summ_dir, flush_secs=15, max_queue=100)\n\n  sv = tf.train.Supervisor(logdir=os.path.join(\n      FLAGS.working_dir, hparams_str),\n                     save_summaries_secs=0,\n                     save_model_secs=1200,\n                     summary_op=None,\n                     recovery_wait_secs=30,\n                     global_step=sbn.global_step)\n  with sv.managed_session() as sess:\n    # Dump hparams to file\n    with gfile.Open(os.path.join(FLAGS.working_dir,\n                                 hparams_str,\n                                 'hparams.json'),\n                    'w') as out:\n      json.dump(sbn.hparams.values(), out)\n\n    sbn.initialize(sess)\n    batch_size = sbn.hparams.batch_size\n    scores = []\n    n = train_xs.shape[0]\n    index = range(n)\n\n    while not sv.should_stop():\n      lHats = []\n      grad_variances = []\n      temperatures = []\n      random.shuffle(index)\n      i = 0\n      while i < n:\n        batch_index = index[i:min(i+batch_size, n)]\n        batch_xs = train_xs[batch_index, :]\n\n        if sbn.hparams.dynamic_b:\n          # Dynamically binarize the batch data\n          batch_xs = (np.random.rand(*batch_xs.shape) < batch_xs).astype(float)\n\n        lHat, grad_variance, step, temperature = sbn.partial_fit(batch_xs,\n                                                    sbn.hparams.n_samples)\n        if debug:\n          print(i, lHat)\n          if i > 100:\n            return\n        lHats.append(lHat)\n        grad_variances.append(grad_variance)\n        temperatures.append(temperature)\n        i += batch_size\n\n      grad_variances = np.log(np.mean(grad_variances, axis=0)).tolist()\n      summary_strings = []\n      if isinstance(grad_variances, list):\n        grad_variances = dict(zip([k for (k, v) in sbn.losses], map(float, grad_variances)))\n        rowkey['step'] = step\n        logger.log(rowkey, {'step': step,\n                             'train': np.mean(lHats, axis=0)[0],\n                             'grad_variances': grad_variances,\n                             'temperature': np.mean(temperatures), })\n        grad_variances = '\\n'.join(map(str, sorted(grad_variances.iteritems())))\n      else:\n        rowkey['step'] = step\n        logger.log(rowkey, {'step': step,\n                             'train': np.mean(lHats, axis=0)[0],\n                             'grad_variance': grad_variances,\n                             'temperature': np.mean(temperatures), })\n        summary_strings.append(manual_scalar_summary(\"log grad variance\", grad_variances))\n\n      print('Step %d: %s\\n%s' % (step, str(np.mean(lHats, axis=0)), str(grad_variances)))\n\n      # Every few epochs compute test and validation scores\n      epoch = int(step / (train_xs.shape[0] / sbn.hparams.batch_size))\n      if epoch % FLAGS.eval_freq == 0:\n        valid_res = eval(sbn, valid_xs)\n        test_res= eval(sbn, test_xs)\n\n        print('\\nValid %d: %s' % (step, str(valid_res)))\n        print('Test %d: %s\\n' % (step, str(test_res)))\n        logger.log(rowkey, {'step': step,\n                             'valid': valid_res[0],\n                             'test': test_res[0]})\n        logger.flush()  # Flush infrequently\n\n      # Create summaries\n      summary_strings.extend([\n        manual_scalar_summary(\"Train ELBO\", np.mean(lHats, axis=0)[0]),\n        manual_scalar_summary(\"Temperature\", np.mean(temperatures)),\n      ])\n      for summ_str in summary_strings:\n        summary_writer.add_summary(summ_str, global_step=step)\n      summary_writer.flush()\n\n      sys.stdout.flush()\n      scores.append(np.mean(lHats, axis=0))\n\n      if step > training_steps:\n        break\n\n    return scores\n\n\ndef main():\n  # Parse hyperparams\n  hparams = rebar.default_hparams\n  hparams.parse(FLAGS.hparams)\n  print(hparams.values())\n\n  train_xs, valid_xs, test_xs = datasets.load_data(hparams)\n  mean_xs = np.mean(train_xs, axis=0)  # Compute mean centering on training\n\n  training_steps = 2000000\n  model = getattr(rebar, hparams.model)\n  sbn = model(hparams, mean_xs=mean_xs)\n\n  scores = train(sbn, train_xs, valid_xs, test_xs,\n                 training_steps=training_steps, debug=False)\n\nif __name__ == '__main__':\n  main()\n", "framework": "tensorflow"}
{"repo_name": "google-research/federated", "file_path": "shrink_unshrink/models.py", "content": "# Copyright 2020, The TensorFlow Federated Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Creates a pair of recurrant models for the Stack Overflow next word prediction task.\n\nModified version of\ntff.simulation.baselines.stackoverflow.create_word_prediction_task and dependent\nfunctions which allows for different sized recurrant models\n\"\"\"\nimport functools\nimport tensorflow as tf\nimport tensorflow_federated as tff\n\n\nclass TransposableEmbedding(tf.keras.layers.Layer):\n  \"\"\"A Keras layer implementing a transposed projection output layer.\"\"\"\n\n  def __init__(self, embedding_layer: tf.keras.layers.Embedding):\n    super().__init__()\n    self.embeddings = embedding_layer.embeddings\n\n  # Placing `tf.matmul` under the `call` method is important for backpropagating\n  # the gradients of `self.embeddings` in graph mode.\n  def call(self, inputs):\n    return tf.matmul(inputs, self.embeddings, transpose_b=True)\n\n\ndef create_recurrent_model(vocab_size: int,\n                           embedding_size: int = 96,\n                           num_lstm_layers: int = 1,\n                           lstm_size: int = 670,\n                           shared_embedding: bool = False) -> tf.keras.Model:\n  \"\"\"Constructs a recurrent model with an initial embeding layer.\n\n  The resulting model embeds sequences of integer tokens (whose values vary\n  between `0` and `vocab_size-1`) into an `embedding_size`-dimensional space.\n  It then applies `num_lstm_layers` LSTM layers, each of size `lstm_size`.\n  Each LSTM is followed by a dense layer mapping the output to `embedding_size`\n  units. The model then has a final dense layer mapping to `vocab_size` logits\n  units. Note that this model does not compute any kind of softmax on the final\n  logits. This should instead be done in the loss function for the purposes of\n  backpropagation.\n\n  Args:\n    vocab_size: Vocabulary size to use in the initial embedding layer.\n    embedding_size: The size of the embedding layer.\n    num_lstm_layers: The number of LSTM layers in the model.\n    lstm_size: The size of each LSTM layer.\n    shared_embedding: If set to `True`, the final layer of the model is a dense\n      layer given by the transposition of the embedding layer. If `False`, the\n      final dense layer is instead learned separately.\n\n  Returns:\n    An uncompiled `tf.keras.Model`.\n  \"\"\"\n  if vocab_size < 1:\n    raise ValueError('vocab_size must be a positive integer.')\n  if embedding_size < 1:\n    raise ValueError('embedding_size must be a positive integer.')\n  if num_lstm_layers < 1:\n    raise ValueError('num_lstm_layers must be a positive integer.')\n  if lstm_size < 1:\n    raise ValueError('lstm_size must be a positive integer.')\n\n  inputs = tf.keras.layers.Input(shape=(None,))\n  input_embedding = tf.keras.layers.Embedding(\n      input_dim=vocab_size, output_dim=embedding_size, mask_zero=True)\n  embedded = input_embedding(inputs)\n  projected = embedded\n\n  for _ in range(num_lstm_layers):\n    layer = tf.keras.layers.LSTM(lstm_size, return_sequences=True)\n    processed = layer(projected)\n    projected = tf.keras.layers.Dense(embedding_size)(processed)\n\n  if shared_embedding:\n    transposed_embedding = TransposableEmbedding(input_embedding)\n    logits = transposed_embedding(projected)\n  else:\n    logits = tf.keras.layers.Dense(vocab_size, activation=None)(projected)\n\n  return tf.keras.Model(inputs=inputs, outputs=logits)\n\n\ndef make_big_and_small_stackoverflow_model_fn(my_task,\n                                              vocab_size=10000,\n                                              num_out_of_vocab_buckets=1,\n                                              big_embedding_size=96,\n                                              big_lstm_size=670,\n                                              small_embedding_size=72,\n                                              small_lstm_size=503):\n  \"\"\"Generates two model functions for a given task.\n\n  This code is a modified version of\n  tff.simulation.baselines.stackoverflow.create_word_prediction_task\n\n  Args:\n    my_task: a tff.simulation.baselines.BaselineTask object\n    vocab_size: an integer specifying the vocab size\n    num_out_of_vocab_buckets: an integer specifying the number of out of vocab\n      buckets\n    big_embedding_size: an integer specifying the size of the embedding layer of\n      the big model\n    big_lstm_size: an integer specifying the size of the lstm layer of the big\n      model\n    small_embedding_size: an integer specifying the size of the embedding layer\n      of the small model\n    small_lstm_size: an integer specifying the size of the lstm layer of the\n      small model\n\n  Returns:\n    Two model_fn functions\n  \"\"\"\n\n  extended_vocab_size = vocab_size + 3 + num_out_of_vocab_buckets\n\n  def big_stackoverflownwp_rnn_model_fn():\n    return tff.learning.from_keras_model(\n        keras_model=create_recurrent_model(\n            vocab_size=extended_vocab_size,\n            embedding_size=big_embedding_size,\n            lstm_size=big_lstm_size),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        input_spec=my_task.datasets.element_type_structure,\n    )\n\n  # the standard size corresponding the stackoverflow baseline task\n  # has embedding_size=96, lstm_size=670\n  def small_stackoverflownwp_rnn_model_fn():\n    return tff.learning.from_keras_model(\n        keras_model=create_recurrent_model(\n            vocab_size=extended_vocab_size,\n            embedding_size=small_embedding_size,\n            lstm_size=small_lstm_size),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        input_spec=my_task.datasets.element_type_structure,\n    )\n\n  return big_stackoverflownwp_rnn_model_fn, small_stackoverflownwp_rnn_model_fn\n\n\ndef create_conv_dropout_model(conv1_filters=32,\n                              conv2_filters=64,\n                              dense_size=128,\n                              only_digits: bool = True) -> tf.keras.Model:\n  \"\"\"Create a convolutional network with dropout.\n\n  When `only_digits=True`, the summary of returned model is\n  ```\n  Model: \"sequential\"\n  _________________________________________________________________\n  Layer (type)                 Output Shape              Param #\n  =================================================================\n  reshape (Reshape)            (None, 28, 28, 1)         0\n  _________________________________________________________________\n  conv2d (Conv2D)              (None, 26, 26, 32)        320\n  _________________________________________________________________\n  conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496\n  _________________________________________________________________\n  max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0\n  _________________________________________________________________\n  dropout (Dropout)            (None, 12, 12, 64)        0\n  _________________________________________________________________\n  flatten (Flatten)            (None, 9216)              0\n  _________________________________________________________________\n  dense (Dense)                (None, 128)               1179776\n  _________________________________________________________________\n  dropout_1 (Dropout)          (None, 128)               0\n  _________________________________________________________________\n  dense_1 (Dense)              (None, 10)                1290\n  =================================================================\n  Total params: 1,199,882\n  Trainable params: 1,199,882\n  Non-trainable params: 0\n  ```\n  For `only_digits=False`, the last dense layer is slightly larger.\n\n  Args:\n    conv1_filters: The number of convolutional filters in the 1st convolutional\n      layer\n    conv2_filters: The number of convolutional filters in the 2nd convolutional\n      layer\n    dense_size: The number of neurons in the last dense layer\n    only_digits: If `True`, uses a final layer with 10 outputs, for use with the\n      digits only EMNIST dataset. If `False`, uses 62 outputs for the larger\n      dataset.\n\n  Returns:\n    An uncompiled `tf.keras.Model`.\n  \"\"\"\n  data_format = 'channels_last'\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.Conv2D(\n          conv1_filters,\n          kernel_size=(3, 3),\n          activation='relu',\n          data_format=data_format,\n          input_shape=(28, 28, 1)),\n      tf.keras.layers.Conv2D(\n          conv2_filters,\n          kernel_size=(3, 3),\n          activation='relu',\n          data_format=data_format),\n      tf.keras.layers.MaxPool2D(pool_size=(2, 2), data_format=data_format),\n      tf.keras.layers.Dropout(0.25),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(dense_size, activation='relu'),\n      tf.keras.layers.Dropout(0.5),\n      tf.keras.layers.Dense(\n          10 if only_digits else 62, activation=tf.nn.softmax),\n  ])\n\n  return model\n\n\ndef create_conv_dropout_model_mfactor(\n    conv1_filters=32,\n    conv2_filters=64,\n    dense_size=128,\n    mfactor1=1.0,\n    mfactor2=1.0,\n    mfactor_dense=1.0,\n    only_digits: bool = True) -> tf.keras.Model:\n  \"\"\"Create a convolutional network with dropout.\n\n  When `only_digits=True`, the summary of returned model is\n  ```\n  Model: \"sequential\"\n  _________________________________________________________________\n  Layer (type)                 Output Shape              Param #\n  =================================================================\n  reshape (Reshape)            (None, 28, 28, 1)         0\n  _________________________________________________________________\n  conv2d (Conv2D)              (None, 26, 26, 32)        320\n  _________________________________________________________________\n  conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496\n  _________________________________________________________________\n  max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0\n  _________________________________________________________________\n  dropout (Dropout)            (None, 12, 12, 64)        0\n  _________________________________________________________________\n  flatten (Flatten)            (None, 9216)              0\n  _________________________________________________________________\n  dense (Dense)                (None, 128)               1179776\n  _________________________________________________________________\n  dropout_1 (Dropout)          (None, 128)               0\n  _________________________________________________________________\n  dense_1 (Dense)              (None, 10)                1290\n  =================================================================\n  Total params: 1,199,882\n  Trainable params: 1,199,882\n  Non-trainable params: 0\n  ```\n  For `only_digits=False`, the last dense layer is slightly larger.\n  Args:\n    conv1_filters: The number of convolutional filters in the 1st convolutional\n      layer\n    conv2_filters: The number of convolutional filters in the 2nd convolutional\n      layer\n    dense_size: The number of neurons in the last dense layer\n    mfactor1: The multiplicative scaling applied after the first convolutional\n      layer\n    mfactor2: The multiplicative scaling applied after the second convolutional\n      layer\n    mfactor_dense: The multiplicative scaling applied after the dense layer\n    only_digits: If `True`, uses a final layer with 10 outputs, for use with the\n      digits only EMNIST dataset. If `False`, uses 62 outputs for the larger\n      dataset.\n\n  Returns:\n    An uncompiled `tf.keras.Model`.\n  \"\"\"\n  data_format = 'channels_last'\n  model = tf.keras.models.Sequential([\n      tf.keras.layers.Conv2D(\n          conv1_filters,\n          kernel_size=(3, 3),\n          activation='relu',\n          data_format=data_format,\n          input_shape=(28, 28, 1)),\n      tf.keras.layers.Lambda(lambda x: mfactor1 * x),\n      tf.keras.layers.Conv2D(\n          conv2_filters,\n          kernel_size=(3, 3),\n          activation='relu',\n          data_format=data_format),\n      tf.keras.layers.Lambda(lambda x: mfactor2 * x),\n      tf.keras.layers.MaxPool2D(pool_size=(2, 2), data_format=data_format),\n      tf.keras.layers.Dropout(0.25),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(dense_size, activation='relu'),\n      tf.keras.layers.Lambda(lambda x: mfactor_dense * x),\n      tf.keras.layers.Dropout(0.5),\n      tf.keras.layers.Dense(\n          10 if only_digits else 62, activation=tf.nn.softmax),\n  ])\n\n  return model\n\n\ndef create_original_fedavg_cnn_model(\n    conv1_filters=32,\n    conv2_filters=64,\n    dense_size=512,\n    only_digits: bool = True) -> tf.keras.Model:\n  \"\"\"Create a convolutional network without dropout.\n\n  This recreates the CNN model used in the original FedAvg paper,\n  https://arxiv.org/abs/1602.05629. The number of parameters when\n  `only_digits=True` is (1,663,370), which matches what is reported in the\n  paper. When `only_digits=True`, the summary of returned model is\n  ```\n  Model: \"sequential\"\n  _________________________________________________________________\n  Layer (type)                 Output Shape              Param #\n  =================================================================\n  reshape (Reshape)            (None, 28, 28, 1)         0\n  _________________________________________________________________\n  conv2d (Conv2D)              (None, 28, 28, 32)        832\n  _________________________________________________________________\n  max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0\n  _________________________________________________________________\n  conv2d_1 (Conv2D)            (None, 14, 14, 64)        51264\n  _________________________________________________________________\n  max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0\n  _________________________________________________________________\n  flatten (Flatten)            (None, 3136)              0\n  _________________________________________________________________\n  dense (Dense)                (None, 512)               1606144\n  _________________________________________________________________\n  dense_1 (Dense)              (None, 10)                5130\n  =================================================================\n  Total params: 1,663,370\n  Trainable params: 1,663,370\n  Non-trainable params: 0\n  ```\n  For `only_digits=False`, the last dense layer is slightly larger.\n\n  Args:\n    conv1_filters: The number of convolutional filters in the 1st convolutional\n      layer\n    conv2_filters: The number of convolutional filters in the 2nd convolutional\n      layer\n    dense_size: The number of neurons in the last dense layer\n    only_digits: If `True`, uses a final layer with 10 outputs, for use with the\n      digits only EMNIST dataset. If `False`, uses 62 outputs for the larger\n      dataset.\n\n  Returns:\n    An uncompiled `tf.keras.Model`.\n  \"\"\"\n  data_format = 'channels_last'\n  max_pool = functools.partial(\n      tf.keras.layers.MaxPooling2D,\n      pool_size=(2, 2),\n      padding='same',\n      data_format=data_format)\n  conv2d = functools.partial(\n      tf.keras.layers.Conv2D,\n      kernel_size=5,\n      padding='same',\n      data_format=data_format,\n      activation=tf.nn.relu)\n  model = tf.keras.models.Sequential([\n      conv2d(filters=conv1_filters, input_shape=(28, 28, 1)),\n      max_pool(),\n      conv2d(filters=conv2_filters),\n      max_pool(),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(dense_size, activation=tf.nn.relu),\n      tf.keras.layers.Dense(\n          10 if only_digits else 62, activation=tf.nn.softmax),\n  ])\n  return model\n\n\ndef make_big_and_small_emnist_cnn_model_fn(my_task,\n                                           big_conv1_filters=32,\n                                           big_conv2_filters=64,\n                                           big_dense_size=512,\n                                           small_conv1_filters=24,\n                                           small_conv2_filters=48,\n                                           small_dense_size=384):\n  \"\"\"Generates two model functions for a given task.\n\n  Args:\n    my_task: a tff.simulation.baselines.BaselineTask object\n    big_conv1_filters: The number of convolutional filters in the 1st\n      convolutional layer of the big model\n    big_conv2_filters: The number of convolutional filters in the 2nd\n      convolutional layer of the big model\n    big_dense_size: The number of neurons in the last dense layer of the big\n      model\n    small_conv1_filters: The number of convolutional filters in the 1st\n      convolutional layer of the small model\n    small_conv2_filters: The number of convolutional filters in the 2nd\n      convolutional layer of the small model\n    small_dense_size: The number of neurons in the last dense layer of the small\n      model\n\n  Returns:\n    Two model_fn functions\n  \"\"\"\n\n  def big_model_fn():\n    return tff.learning.from_keras_model(\n        keras_model=create_original_fedavg_cnn_model(\n            only_digits=False,\n            conv1_filters=big_conv1_filters,\n            conv2_filters=big_conv2_filters,\n            dense_size=big_dense_size),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        input_spec=my_task.datasets.element_type_structure,\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n\n  def small_model_fn():\n    return tff.learning.from_keras_model(\n        keras_model=create_original_fedavg_cnn_model(\n            only_digits=False,\n            conv1_filters=small_conv1_filters,\n            conv2_filters=small_conv2_filters,\n            dense_size=small_dense_size),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        input_spec=my_task.datasets.element_type_structure,\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n\n  return big_model_fn, small_model_fn\n\n\ndef make_big_and_small_emnist_cnn_dropout_model_fn(my_task,\n                                                   big_conv1_filters=32,\n                                                   big_conv2_filters=64,\n                                                   big_dense_size=128,\n                                                   small_conv1_filters=24,\n                                                   small_conv2_filters=48,\n                                                   small_dense_size=96):\n  \"\"\"Generates two model functions for a given task.\n\n  Args:\n    my_task: a tff.simulation.baselines.BaselineTask object\n    big_conv1_filters: The number of convolutional filters in the 1st\n      convolutional layer of the big model\n    big_conv2_filters: The number of convolutional filters in the 2nd\n      convolutional layer of the big model\n    big_dense_size: The number of neurons in the last dense layer of the big\n      model\n    small_conv1_filters: The number of convolutional filters in the 1st\n      convolutional layer of the small model\n    small_conv2_filters: The number of convolutional filters in the 2nd\n      convolutional layer of the small model\n    small_dense_size: The number of neurons in the last dense layer of the small\n      model\n\n  Returns:\n    Two model_fn functions.\n  \"\"\"\n\n  def big_model_fn():\n    return tff.learning.from_keras_model(\n        keras_model=create_conv_dropout_model(\n            only_digits=False,\n            conv1_filters=big_conv1_filters,\n            conv2_filters=big_conv2_filters,\n            dense_size=big_dense_size),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        input_spec=my_task.datasets.element_type_structure,\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n\n  def small_model_fn():\n    return tff.learning.from_keras_model(\n        keras_model=create_conv_dropout_model(\n            only_digits=False,\n            conv1_filters=small_conv1_filters,\n            conv2_filters=small_conv2_filters,\n            dense_size=small_dense_size),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        input_spec=my_task.datasets.element_type_structure,\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n\n  return big_model_fn, small_model_fn\n\n\ndef make_big_and_small_emnist_cnn_dropout_mfactor_model_fn(\n    my_task,\n    big_conv1_filters=32,\n    big_conv2_filters=64,\n    big_dense_size=128,\n    small_conv1_filters=24,\n    small_conv2_filters=48,\n    small_dense_size=96):\n  \"\"\"Generates two model functions for a given task.\n\n  Args:\n    my_task: a tff.simulation.baselines.BaselineTask object\n    big_conv1_filters: The number of convolutional filters in the 1st\n      convolutional layer of the big model\n    big_conv2_filters: The number of convolutional filters in the 2nd\n      convolutional layer of the big model\n    big_dense_size: The number of neurons in the last dense layer of the big\n      model\n    small_conv1_filters: The number of convolutional filters in the 1st\n      convolutional layer of the small model\n    small_conv2_filters: The number of convolutional filters in the 2nd\n      convolutional layer of the small model\n    small_dense_size: The number of neurons in the last dense layer of the small\n      model\n\n  Returns:\n    Two model_fn functions.\n  \"\"\"\n\n  def big_model_fn():\n    return tff.learning.from_keras_model(\n        keras_model=create_conv_dropout_model_mfactor(\n            only_digits=False,\n            conv1_filters=big_conv1_filters,\n            conv2_filters=big_conv2_filters,\n            dense_size=big_dense_size,\n            mfactor1=1.0,\n            mfactor2=1.0,\n            mfactor_dense=1.0),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        input_spec=my_task.datasets.element_type_structure,\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n\n  def small_model_fn():\n    return tff.learning.from_keras_model(\n        keras_model=create_conv_dropout_model_mfactor(\n            only_digits=False,\n            conv1_filters=small_conv1_filters,\n            conv2_filters=small_conv2_filters,\n            dense_size=small_dense_size,\n            mfactor1=tf.cast(\n                big_conv1_filters / small_conv1_filters, tf.float32\n            ),  # cast this as a float since these could be integers\n            mfactor2=tf.cast(big_conv2_filters / small_conv2_filters,\n                             tf.float32),\n            mfactor_dense=tf.cast(big_dense_size / small_dense_size,\n                                  tf.float32)),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        input_spec=my_task.datasets.element_type_structure,\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n\n  return big_model_fn, small_model_fn\n", "framework": "tensorflow"}
{"repo_name": "maigimenez/jon-siamese", "file_path": "src/siamese.py", "content": "import tensorflow as tf\nfrom utils import contrastive_loss\n\nclass Siamese:\n\n    # Create model\n    def __init__(self, sequence_length, vocab_size, embedding_size,\n                 filter_sizes, num_filters, margin):\n        with tf.name_scope(\"embeddings\") as embeddings_scope:\n            self.filter_sizes = filter_sizes\n            self.embedding_size = embedding_size\n            self.num_filters = num_filters\n            self.W_embedding = tf.Variable(tf.random_uniform([vocab_size, self.embedding_size], -1.0, 1.0),\n                                           trainable=True, name=\"W_embedding\")\n            self.is_training = tf.placeholder(tf.bool, [], name='is_training')\n\n        with tf.variable_scope(\"siamese\") as siam_scope:\n            # 1ST LAYER: Embedding layer\n            with tf.variable_scope(\"embeddings-siamese\") as input_scope:\n                self.left_input = tf.placeholder(tf.int32, [None, sequence_length], name='left')\n                left_embedded_words = tf.nn.embedding_lookup(self.W_embedding, self.left_input)\n                self.left_embedded = tf.expand_dims(left_embedded_words, -1, name='left_embeddings')\n                print('  ---> EMBEDDING LEFT: ', self.left_embedded)\n\n                self.right_input = tf.placeholder(tf.int32, [None, sequence_length], name='right')\n                right_embedded_words = tf.nn.embedding_lookup(self.W_embedding, self.right_input)\n                self.right_embedded = tf.expand_dims(right_embedded_words, -1, name='right_embeddings')\n                print('  ---> EMBEDDING RIGHT: ', self.right_embedded)\n\n            self.left_siamese = self.subnet(self.left_embedded, 'left', False)\n            print(\"---> SIAMESE TENSOR: \", self.left_siamese)\n            siam_scope.reuse_variables()\n            self.right_siamese = self.subnet(self.right_embedded, 'right', True)\n            print(\"---> SIAMESE TENSOR: \", self.right_siamese)\n\n        with tf.name_scope(\"similarity\"):\n            print('\\n ----------------------- JOIN SIAMESE ----------------------------')\n            self.labels = tf.placeholder(tf.int32, [None, 1], name='labels')\n            self.labels = tf.to_float(self.labels)\n            print('---> LABELS: ', self.labels)\n\n            with tf.variable_scope(\"loss\"):\n                self.margin = tf.get_variable('margin', dtype=tf.float32,\n                                              initializer=tf.constant(margin, shape=[1]),\n                                              trainable=False)\n                self.loss, self.attr, \\\n                self.rep, self.distance, self.maxpart = contrastive_loss(self.labels,\n                                                           self.left_siamese,\n                                                           self.right_siamese,\n                                                           self.margin)\n\n        with tf.name_scope(\"prediction\"):\n            # TODO Este es un par\u00e1metro de configuraci\u00f3n\n            self.threshold = tf.get_variable('threshold', dtype=tf.float32,\n                                             initializer=tf.constant(1.0, shape=[1]))\n            self.predictions = tf.less_equal(self.distance, self.threshold)\n            self.predictions = tf.cast(self.predictions, 'float32')\n            self.correct_predictions = tf.equal(self.predictions, self.labels)\n            self.accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, tf.float32))\n\n\n    def subnet(self, input_sentence, sub_name, reuse=False):\n        with tf.name_scope(\"subnet_\"+sub_name):\n            print('\\n ----------------------- SUBNET ----------------------------')\n            # 2ND LAYER: Convolutional + ReLU + Max-pooling\n            pooled_outputs, conv_outputs = [], []\n            for filter_size in self.filter_sizes:\n                with tf.variable_scope(\"conv-{}\".format(filter_size)) as conv_scope:\n                    name = 'conv-' + str(filter_size)\n                    output_conv = self.conv_layer(input_sentence,\n                                                  filter_size, self.embedding_size,\n                                                  1, self.num_filters, name)\n                    output_conv = tf.contrib.layers.batch_norm(output_conv,\n                                                               is_training=self.is_training,\n                                                               center=True,\n                                                               scale=False,\n                                                               trainable=True,\n                                                               updates_collections=None,\n                                                               scope='bn',\n                                                               decay=0.9)\n                    conv_outputs.append(output_conv)\n                    pooled = self.max_pool_layer(output_conv, name)\n                    pooled_outputs.append(pooled)\n\n            # 3RD LAYER: CONCAT ALL THE POOLED FEATURES\n            num_filters_total = self.num_filters * len(self.filter_sizes)\n            h_pool = tf.concat(pooled_outputs, 1)\n            print('  ---> CONCAT:', h_pool)\n\n            h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total], name='re_sim')\n            print('  ---> RESHAPE:', h_pool_flat)\n\n        return h_pool_flat\n\n    def conv_layer(self, input, kernel_height, kernel_width, channels, kernels_num, name):\n        # Define the convolution layer\n        filter_shape = [kernel_height, kernel_width, channels, kernels_num]\n        W_init = tf.truncated_normal_initializer(stddev=0.1)\n        W = tf.get_variable(name + '_W', dtype=tf.float32, initializer=W_init,\n                            shape=filter_shape)\n        b_init = tf.constant(0.1, shape=[kernels_num])\n        b = tf.get_variable(name + '_b', dtype=tf.float32, initializer=b_init)\n        strides = [1, 1, 1, 1]\n        cnn = tf.nn.conv2d(input, W, strides=strides, padding='VALID', name=name+'_cnn')\n        activation = tf.nn.tanh(tf.nn.bias_add(cnn, b), name=name+'_relu')\n        print('    ---> CNN: ', activation)\n        return activation\n\n    def max_pool_layer(self, output_conv, name):\n        conv_heigh = output_conv.get_shape()[1]\n        pool_size = [1, conv_heigh, 1, 1]\n\n        pooled = tf.nn.max_pool(output_conv, ksize=pool_size, strides=[1, 1, 1, 1],\n                                padding='VALID', name=name+'_pool')\n        print('    ---> MAXPOOL: ', pooled)\n        return pooled\n\n\n\n", "framework": "tensorflow"}
{"repo_name": "sangwook236/SWDT", "file_path": "sw_dev/python/rnd/test/machine_learning/tflearn/tflearn_basic.py", "content": "# REF [site] >> http://tflearn.org/tutorials/quickstart.html\n\nfrom __future__ import print_function\n\nimport numpy as np\nimport tflearn\n\n# Download the Titanic dataset.\nfrom tflearn.datasets import titanic\ntitanic.download_dataset('titanic_dataset.csv')\n\n# Load CSV file, indicate that the first column represents labels.\nfrom tflearn.data_utils import load_csv\ndata, labels = load_csv('titanic_dataset.csv', target_column=0,\n\tcategorical_labels=True, n_classes=2)\n\n\n# Preprocessing function.\ndef preprocess(data, columns_to_ignore):\n    # Sort by descending id and delete columns\n    for id in sorted(columns_to_ignore, reverse=True):\n        [r.pop(id) for r in data]\n    for i in range(len(data)):\n\t\t# Converting 'sex' field to float (id is 1 after removing labels column).\n\t\tdata[i][1] = 1. if data[i][1] == 'female' else 0.\n\treturn np.array(data, dtype=np.float32)\n\n# Ignore 'name' and 'ticket' columns (id 1 & 6 of data array).\nto_ignore=[1, 6]\n\n# Preprocess data.\ndata = preprocess(data, to_ignore)\n\n# Build neural network.\nnet = tflearn.input_data(shape=[None, 6])\nnet = tflearn.fully_connected(net, 32)\nnet = tflearn.fully_connected(net, 32)\nnet = tflearn.fully_connected(net, 2, activation='softmax')\nnet = tflearn.regression(net)\n\n# Define model.\nmodel = tflearn.DNN(net)\n# Start training (apply gradient descent algorithm).\nmodel.fit(data, labels, n_epoch=10, batch_size=16, show_metric=True)\n\n# Let's create some data for DiCaprio and Winslet.\ndicaprio = [3, 'Jack Dawson', 'male', 19, 0, 0, 'N/A', 5.0000]\nwinslet = [1, 'Rose DeWitt Bukater', 'female', 17, 1, 2, 'N/A', 100.0000]\n\n# Preprocess data.\ndicaprio, winslet = preprocess([dicaprio, winslet], to_ignore)\n\n# Predict surviving chances (class 1 results).\npred = model.predict([dicaprio, winslet])\nprint(\"DiCaprio Surviving Rate:\", pred[0][1])\nprint(\"Winslet Surviving Rate:\", pred[1][1])\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/tensor2tensor", "file_path": "tensor2tensor/models/research/glow_test.py", "content": "# coding=utf-8\n# Copyright 2022 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for tensor2tensor.models.research.glow_model.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\nimport numpy as np\nfrom six.moves import range\nfrom tensor2tensor import problems\nfrom tensor2tensor.data_generators import cifar  # pylint: disable=unused-import\nfrom tensor2tensor.models.research import glow\nfrom tensor2tensor.utils import registry  # pylint: disable=unused-import\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.compat.v1 import estimator as tf_estimator\n\nMODES = tf_estimator.ModeKeys\n\n\nclass GlowModelTest(tf.test.TestCase):\n\n  def batch(self, one_shot_iterator, batch_size=16):\n    x_batch, y_batch = [], []\n    for _ in range(batch_size):\n      curr = one_shot_iterator.get_next()\n      x_batch.append(curr['inputs'])\n      y_batch.append(curr['targets'])\n    return tf.stack(x_batch), tf.stack(y_batch)\n\n  def test_glow(self):\n    with tf.Graph().as_default():\n      hparams = glow.glow_hparams()\n      hparams.depth = 15\n      hparams.n_levels = 2\n      hparams.init_batch_size = 256\n      hparams.batch_size = 1\n      hparams.data_dir = ''\n      cifar_problem = problems.problem('image_cifar10_plain_random_shift')\n      hparams.problem = cifar_problem\n      model = glow.Glow(hparams, tf_estimator.ModeKeys.TRAIN)\n      train_dataset = cifar_problem.dataset(MODES.TRAIN)\n      one_shot = train_dataset.make_one_shot_iterator()\n      x_batch, y_batch = self.batch(one_shot)\n      features = {'inputs': x_batch, 'targets': y_batch}\n      _, obj_dict = model.body(features)\n      objective = obj_dict['training']\n      with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        # Run initialization.\n        init_op = tf.get_collection('glow_init_op')\n        sess.run(init_op)\n\n        # Run forward pass.\n        obj_np = sess.run(objective)\n        mean_obj = np.mean(obj_np)\n\n        # Check that one forward-propagation does not NaN, i.e\n        # initialization etc works as expected.\n        self.assertTrue(mean_obj > 0 and mean_obj < 10.0)\n\n  def test_glow_inference(self):\n    hparams = glow.glow_hparams()\n    hparams.depth = 15\n    hparams.n_levels = 2\n    hparams.data_dir = ''\n    curr_dir = tempfile.mkdtemp()\n\n    # Training pipeline\n    with tf.Graph().as_default():\n      cifar_problem = problems.problem('image_cifar10_plain_random_shift')\n      hparams.problem = cifar_problem\n      model = glow.Glow(hparams, tf_estimator.ModeKeys.TRAIN)\n      train_dataset = cifar_problem.dataset(MODES.TRAIN)\n      one_shot = train_dataset.make_one_shot_iterator()\n      x_batch, y_batch = self.batch(one_shot)\n      features = {'inputs': x_batch, 'targets': y_batch}\n      model_path = os.path.join(curr_dir, 'model')\n      model(features)\n\n      with tf.Session() as session:\n        saver = tf.train.Saver()\n        session.run(tf.global_variables_initializer())\n\n        init_op = tf.get_collection('glow_init_op')\n        session.run(init_op)\n        z = session.run([model.z])\n        mean_z = np.mean(z)\n        is_undefined = np.isnan(mean_z) or np.isinf(mean_z)\n        self.assertTrue(not is_undefined)\n        saver.save(session, model_path)\n\n    # Inference pipeline\n    with tf.Graph().as_default():\n      cifar_problem = problems.problem('image_cifar10_plain_random_shift')\n      hparams.problem = cifar_problem\n      model = glow.Glow(hparams, tf_estimator.ModeKeys.PREDICT)\n      test_dataset = cifar_problem.dataset(MODES.EVAL)\n      one_shot = test_dataset.make_one_shot_iterator()\n      x_batch, y_batch = self.batch(one_shot)\n      features = {'inputs': x_batch, 'targets': y_batch}\n      model_path = os.path.join(curr_dir, 'model')\n\n      predictions = model.infer(features)\n      with tf.Session() as session:\n        saver = tf.train.Saver()\n        saver.restore(session, model_path)\n        predictions_np = session.run(predictions)\n        self.assertTrue(np.all(predictions_np <= 255))\n        self.assertTrue(np.all(predictions_np >= 0))\n\nif __name__ == '__main__':\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/tensor2tensor", "file_path": "tensor2tensor/models/research/neural_stack.py", "content": "# coding=utf-8\n# Copyright 2022 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Stacks and Queues implemented as encoder-decoder models.\n\nBased off of the following research:\n\nLearning to Transduce with Unbounded Memory\nEdward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, Phil Blunsom\nhttps://arxiv.org/abs/1506.02516, 2015\n\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom tensor2tensor.layers import common_hparams\nfrom tensor2tensor.layers import common_layers\nfrom tensor2tensor.utils import contrib\nfrom tensor2tensor.utils import registry\nfrom tensor2tensor.utils import t2t_model\n\nimport tensorflow.compat.v1 as tf\n\n# This is the interface between the RNN controller and the neural stack.\nNeuralStackControllerInterface = collections.namedtuple(\n    \"NeuralStackControllerInterface\",\n    \"push_strengths, pop_strengths, write_values, outputs, state\")\n\n# This is recurrent state of the neural stack RNN cell.\nNeuralStackState = collections.namedtuple(\n    \"NeuralStackState\",\n    \"controller_state, read_values, memory_values, read_strengths, \" +\n    \"write_strengths\")\n\n\nclass NeuralStackCell(tf.nn.rnn_cell.RNNCell):\n  \"\"\"An RNN cell base class that can implement a stack or queue.\n  \"\"\"\n\n  def __init__(self, num_units, memory_size, embedding_size,\n               num_read_heads=1, num_write_heads=1, reuse=None):\n    \"\"\"Create a new NeuralStackCell.\n\n    Args:\n      num_units: The number of hidden units in the RNN cell.\n      memory_size: The maximum memory size allocated for the stack.\n      embedding_size:  The embedding width of the individual stack values.\n      num_read_heads: This should always be 1 for a regular stack.\n      num_write_heads: This should always be 1 for a regular stack.\n      reuse: Whether to reuse the weights.\n    \"\"\"\n    super(NeuralStackCell, self).__init__(dtype=tf.float32, _reuse=reuse)\n    self._num_units = num_units\n    self._embedding_size = embedding_size\n    self._memory_size = memory_size\n    self._num_read_heads = num_read_heads\n    self._num_write_heads = num_write_heads\n\n  @property\n  def state_size(self):\n    \"\"\"The NeuralStackCell maintains a tuple of state values.\n\n    Returns:\n      (controller_state.shape,\n       read_values.shape,\n       memory_values.shape,\n       read_strengths.shape,\n       write_strengths.shape)\n    \"\"\"\n    return (tf.TensorShape([self._num_units]),\n            tf.TensorShape([self._num_read_heads, self._embedding_size]),\n            tf.TensorShape([self._memory_size, self._embedding_size]),\n            tf.TensorShape([1, self._memory_size, 1]),\n            tf.TensorShape([self._num_write_heads, self._memory_size, 1]))\n\n  @property\n  def output_size(self):\n    return tf.TensorShape([1, self._embedding_size])\n\n  def initialize_write_strengths(self, batch_size):\n    \"\"\"Initialize write strengths to write to the first memory address.\n\n    This is exposed as its own function so that it can be overridden to provide\n    alternate write adressing schemes.\n\n    Args:\n      batch_size: The size of the current batch.\n\n    Returns:\n      A tf.float32 tensor of shape [num_write_heads, memory_size, 1] where the\n      first element in the second dimension is set to 1.0.\n    \"\"\"\n    return tf.expand_dims(\n        tf.one_hot([[0] * self._num_write_heads] * batch_size,\n                   depth=self._memory_size, dtype=tf.float32), axis=3)\n\n  def zero_state(self, batch_size, dtype):\n    \"\"\"Initialize the tuple of state values to zeros except write strengths.\n\n    Args:\n      batch_size: The size of the current batch.\n      dtype: The default datatype to initialize to.\n\n    Returns:\n      A new NeuralStackState tuple.\n    \"\"\"\n    parent_state = NeuralStackState(*super(NeuralStackCell, self).zero_state(\n        batch_size, dtype))\n    return NeuralStackState(\n        controller_state=parent_state.controller_state,\n        read_values=parent_state.read_values,\n        memory_values=parent_state.memory_values,\n        read_strengths=parent_state.read_strengths,\n        write_strengths=self.initialize_write_strengths(batch_size))\n\n  def get_read_mask(self, read_head_index):\n    \"\"\"Creates a mask which allows us to attenuate subsequent read strengths.\n\n    This is exposed as its own function so that it can be overridden to provide\n    alternate read adressing schemes.\n\n    Args:\n      read_head_index: Identifies which read head we're getting the mask for.\n\n    Returns:\n      A tf.float32 tensor of shape [1, 1, memory_size, memory_size]\n    \"\"\"\n    if read_head_index == 0:\n      return tf.expand_dims(\n          common_layers.mask_pos_lt(self._memory_size, self._memory_size),\n          axis=0)\n    else:\n      raise ValueError(\"Read head index must be 0 for stack.\")\n\n  def get_write_head_offset(self, write_head_index):\n    \"\"\"Lookup the offset to shift the write head at each step.\n\n    By default, we move each write head forward by 1.\n\n    This is exposed as its own function so that it can be overridden to provide\n    alternate write adressing schemes.\n\n    Args:\n      write_head_index: Identifies which write head we're getting the index for.\n\n    Returns:\n      An integer offset to move the write head at each step.\n    \"\"\"\n    if write_head_index == 0:\n      return 1\n    else:\n      raise ValueError(\"Write head index must be 0 for stack.\")\n\n  def add_scalar_projection(self, name, size):\n    \"\"\"A helper function for mapping scalar controller outputs.\n\n    Args:\n      name: A prefix for the variable names.\n      size: The desired number of scalar outputs.\n\n    Returns:\n      A tuple of (weights, bias) where weights has shape [num_units, size] and\n      bias has shape [size].\n    \"\"\"\n    weights = self.add_variable(\n        name + \"_projection_weights\",\n        shape=[self._num_units, size],\n        dtype=self.dtype)\n    bias = self.add_variable(\n        name + \"_projection_bias\",\n        shape=[size],\n        initializer=tf.zeros_initializer(dtype=self.dtype))\n    return weights, bias\n\n  def add_vector_projection(self, name, size):\n    \"\"\"A helper function for mapping embedding controller outputs.\n\n    Args:\n      name: A prefix for the variable names.\n      size: The desired number of embedding outputs.\n\n    Returns:\n      A tuple of (weights, bias) where weights has shape\n      [num_units, size * embedding_size] and bias has shape\n      [size * embedding_size].\n    \"\"\"\n    weights = self.add_variable(\n        name + \"_projection_weights\",\n        shape=[self._num_units, size * self._embedding_size],\n        dtype=self.dtype)\n    bias = self.add_variable(\n        name + \"_projection_bias\",\n        shape=[size * self._embedding_size],\n        initializer=tf.zeros_initializer(dtype=self.dtype))\n    return weights, bias\n\n  def build_controller(self):\n    \"\"\"Create the RNN and output projections for controlling the stack.\n    \"\"\"\n    with tf.name_scope(\"controller\"):\n      self.rnn = contrib.rnn().BasicRNNCell(self._num_units)\n      self._input_proj = self.add_variable(\n          \"input_projection_weights\",\n          shape=[self._embedding_size * (self._num_read_heads + 1),\n                 self._num_units],\n          dtype=self.dtype)\n      self._input_bias = self.add_variable(\n          \"input_projection_bias\",\n          shape=[self._num_units],\n          initializer=tf.zeros_initializer(dtype=self.dtype))\n      self._push_proj, self._push_bias = self.add_scalar_projection(\n          \"push\", self._num_write_heads)\n      self._pop_proj, self._pop_bias = self.add_scalar_projection(\n          \"pop\", self._num_write_heads)\n      self._value_proj, self._value_bias = self.add_vector_projection(\n          \"value\", self._num_write_heads)\n      self._output_proj, self._output_bias = self.add_vector_projection(\n          \"output\", 1)\n\n  def build(self, _):\n    \"\"\"Build the controller.\n    \"\"\"\n    self.build_controller()\n    self.built = True\n\n  def get_controller_shape(self, batch_size):\n    \"\"\"Define the output shapes of the neural stack controller.\n\n    Making this a separate functions so that it can be used in unit tests.\n\n    Args:\n      batch_size: The size of the current batch of data.\n\n    Returns:\n      A tuple of shapes for each output returned from the controller.\n    \"\"\"\n    return (\n        # push_strengths,\n        [batch_size, self._num_write_heads, 1, 1],\n        # pop_strengths\n        [batch_size, self._num_write_heads, 1, 1],\n        # write_values\n        [batch_size, self._num_write_heads, self._embedding_size],\n        # outputs\n        [batch_size, 1, self._embedding_size],\n        # state\n        [batch_size, self._num_units])\n\n  def call_controller(self, input_value, read_values, prev_state, batch_size):\n    \"\"\"Make a call to the neural stack controller.\n\n    See Section 3.1 of Grefenstette et al., 2015.\n\n    Args:\n      input_value: The input to the neural stack cell should be a tf.float32\n        tensor with shape [batch_size, 1, embedding_size]\n      read_values: The values of the read heads at the previous timestep.\n      prev_state: The hidden state from the previous time step.\n      batch_size: The size of the current batch of input values.\n\n    Returns:\n      A tuple of outputs and the new NeuralStackControllerInterface.\n    \"\"\"\n    with tf.name_scope(\"controller\"):\n      # Concatenate the current input value with the read values from the\n      # previous timestep before feeding them into the controller.\n      controller_inputs = tf.concat([\n          contrib.layers().flatten(input_value),\n          contrib.layers().flatten(read_values),\n      ],\n                                    axis=1)\n\n      rnn_input = tf.tanh(tf.nn.bias_add(tf.matmul(\n          controller_inputs, self._input_proj), self._input_bias))\n\n      (rnn_output, state) = self.rnn(rnn_input, prev_state)\n\n      push_strengths = tf.sigmoid(tf.nn.bias_add(tf.matmul(\n          rnn_output, self._push_proj), self._push_bias))\n\n      pop_strengths = tf.sigmoid(tf.nn.bias_add(tf.matmul(\n          rnn_output, self._pop_proj), self._pop_bias))\n\n      write_values = tf.tanh(tf.nn.bias_add(tf.matmul(\n          rnn_output, self._value_proj), self._value_bias))\n\n      outputs = tf.tanh(tf.nn.bias_add(tf.matmul(\n          rnn_output, self._output_proj), self._output_bias))\n\n      # Reshape all the outputs according to the shapes specified by\n      # get_controller_shape()\n      projected_outputs = [push_strengths,\n                           pop_strengths,\n                           write_values,\n                           outputs,\n                           state]\n      next_state = [\n          tf.reshape(output, shape=output_shape) for output, output_shape\n          in zip(projected_outputs, self.get_controller_shape(batch_size))]\n      return NeuralStackControllerInterface(*next_state)\n\n  def call(self, inputs, prev_state):\n    \"\"\"Evaluates one timestep of the current neural stack cell.\n\n    See section 3.4 of Grefenstette et al., 2015.\n\n    Args:\n      inputs: The inputs to the neural stack cell should be a tf.float32 tensor\n        with shape [batch_size, embedding_size]\n      prev_state: The NeuralStackState from the previous timestep.\n\n    Returns:\n      A tuple of the output of the stack as well as the new NeuralStackState.\n    \"\"\"\n    batch_size = tf.shape(inputs)[0]\n\n    # Call the controller and get controller interface values.\n    with tf.control_dependencies([prev_state.read_strengths]):\n      controller_output = self.call_controller(\n          inputs, prev_state.read_values, prev_state.controller_state,\n          batch_size)\n\n    # Always write input values to memory regardless of push strength.\n    # See Equation-1 in Grefenstette et al., 2015.\n    new_memory_values = prev_state.memory_values + tf.reduce_sum(\n        tf.expand_dims(controller_output.write_values, axis=2) *\n        prev_state.write_strengths,\n        axis=1)\n\n    # Attenuate the read strengths of existing memory values depending on the\n    # current pop strength.\n    # See Equation-2 in Grefenstette et al., 2015.\n    new_read_strengths = prev_state.read_strengths\n    for h in range(self._num_read_heads - 1, -1, -1):\n      new_read_strengths = tf.nn.relu(new_read_strengths - tf.nn.relu(\n          tf.slice(controller_output.pop_strengths,\n                   [0, h, 0, 0],\n                   [-1, 1, -1, -1]) -\n          tf.expand_dims(\n              tf.reduce_sum(new_read_strengths * self.get_read_mask(h), axis=2),\n              axis=3)))\n\n    # Combine all write heads and their associated push values into a single set\n    # of read weights.\n    new_read_strengths += tf.reduce_sum(\n        controller_output.push_strengths * prev_state.write_strengths,\n        axis=1, keep_dims=True)\n\n    # Calculate the \"top\" value of the stack by looking at read strengths.\n    # See Equation-3 in Grefenstette et al., 2015.\n    new_read_values = tf.reduce_sum(\n        tf.minimum(\n            new_read_strengths,\n            tf.nn.relu(1 - tf.expand_dims(\n                tf.reduce_sum(\n                    new_read_strengths * tf.concat([\n                        self.get_read_mask(h)\n                        for h in range(self._num_read_heads)\n                    ], axis=1),\n                    axis=2),\n                axis=3))\n        ) * tf.expand_dims(new_memory_values, axis=1),\n        axis=2)\n\n    # Temporarily split write strengths apart so they can be shifted in\n    # different directions.\n    write_strengths_by_head = tf.split(prev_state.write_strengths,\n                                       self._num_write_heads,\n                                       axis=1)\n    # Shift the write strengths for each write head in the direction indicated\n    # by get_write_head_offset().\n    new_write_strengths = tf.concat([\n        tf.roll(write_strength, shift=self.get_write_head_offset(h), axis=2)\n        for h, write_strength in enumerate(write_strengths_by_head)\n    ], axis=1)\n\n    return (controller_output.outputs, NeuralStackState(\n        controller_state=controller_output.state,\n        read_values=new_read_values,\n        memory_values=new_memory_values,\n        read_strengths=new_read_strengths,\n        write_strengths=new_write_strengths))\n\n\nclass NeuralQueueCell(NeuralStackCell):\n  \"\"\"An subclass of the NeuralStackCell which reads from the opposite direction.\n\n  See section 3.2 of Grefenstette et al., 2015.\n  \"\"\"\n\n  def get_read_mask(self, read_head_index):\n    \"\"\"Uses mask_pos_lt() instead of mask_pos_gt() to reverse read values.\n\n    Args:\n      read_head_index: Identifies which read head we're getting the mask for.\n\n    Returns:\n      A tf.float32 tensor of shape [1, 1, memory_size, memory_size].\n    \"\"\"\n    if read_head_index == 0:\n      return tf.expand_dims(\n          common_layers.mask_pos_gt(self._memory_size, self._memory_size),\n          axis=0)\n    else:\n      raise ValueError(\"Read head index must be 0 for queue.\")\n\n\nclass NeuralDequeCell(NeuralStackCell):\n  \"\"\"An subclass of the NeuralStackCell which reads/writes in both directions.\n\n  See section 3.3 of Grefenstette et al., 2015.\n  \"\"\"\n\n  def __init__(self, num_units, memory_size, embedding_size, reuse=None):\n    # Override constructor to set 2 read/write heads.\n    super(NeuralDequeCell, self).__init__(num_units,\n                                          memory_size,\n                                          embedding_size,\n                                          num_read_heads=2,\n                                          num_write_heads=2,\n                                          reuse=reuse)\n\n  def get_read_mask(self, read_head_index):\n    if read_head_index == 0:\n      # Use the same read mask as the queue for the bottom of the deque.\n      return tf.expand_dims(\n          common_layers.mask_pos_gt(self._memory_size, self._memory_size),\n          axis=0)\n    elif read_head_index == 1:\n      # Use the same read mask as the stack for the top of the deque.\n      return tf.expand_dims(\n          common_layers.mask_pos_lt(self._memory_size, self._memory_size),\n          axis=0)\n    else:\n      raise ValueError(\"Read head index must be either 0 or 1 for deque.\")\n\n  def get_write_head_offset(self, write_head_index):\n    if write_head_index == 0:\n      # Move the bottom write position back at each timestep.\n      return -1\n    elif write_head_index == 1:\n      # Move the top write position forward at each timestep.\n      return 1\n    else:\n      raise ValueError(\"Write head index must be 0 or 1 for deque.\")\n\n  def initialize_write_strengths(self, batch_size):\n    \"\"\"Initialize write strengths which write in both directions.\n\n    Unlike in Grefenstette et al., It's writing out from the center of the\n    memory so that it doesn't need to shift the entire memory forward at each\n    step.\n\n    Args:\n      batch_size: The size of the current batch.\n\n    Returns:\n      A tf.float32 tensor of shape [num_write_heads, memory_size, 1].\n    \"\"\"\n    memory_center = self._memory_size // 2\n    return tf.expand_dims(\n        tf.concat([\n            # The write strength for the deque bottom.\n            # Should be shifted back at each timestep.\n            tf.one_hot([[memory_center - 1]] * batch_size,\n                       depth=self._memory_size, dtype=tf.float32),\n            # The write strength for the deque top.\n            # Should be shifted forward at each timestep.\n            tf.one_hot([[memory_center]] * batch_size,\n                       depth=self._memory_size, dtype=tf.float32)\n        ], axis=1), axis=3)\n\n\n@registry.register_model\nclass NeuralStackModel(t2t_model.T2TModel):\n  \"\"\"An encoder-decoder T2TModel that uses NeuralStackCells.\n  \"\"\"\n\n  def cell(self, hidden_size):\n    \"\"\"Build an RNN cell.\n\n    This is exposed as its own function so that it can be overridden to provide\n    different types of RNN cells.\n\n    Args:\n      hidden_size: The hidden size of the cell.\n\n    Returns:\n      A new RNNCell with the given hidden size.\n    \"\"\"\n    return NeuralStackCell(hidden_size,\n                           self._hparams.memory_size,\n                           self._hparams.embedding_size)\n\n  def _rnn(self, inputs, name, initial_state=None, sequence_length=None):\n    \"\"\"A helper method to build tf.nn.dynamic_rnn.\n\n    Args:\n      inputs: The inputs to the RNN. A tensor of shape\n              [batch_size, max_seq_length, embedding_size]\n      name: A namespace for the RNN.\n      initial_state: An optional initial state for the RNN.\n      sequence_length: An optional sequence length for the RNN.\n\n    Returns:\n      A tf.nn.dynamic_rnn operator.\n    \"\"\"\n    layers = [self.cell(layer_size)\n              for layer_size in self._hparams.controller_layer_sizes]\n    with tf.variable_scope(name):\n      return tf.nn.dynamic_rnn(\n          contrib.rnn().MultiRNNCell(layers),\n          inputs,\n          initial_state=initial_state,\n          sequence_length=sequence_length,\n          dtype=tf.float32,\n          time_major=False)\n\n  def body(self, features):\n    \"\"\"Build the main body of the model.\n\n    Args:\n      features: A dict of \"inputs\" and \"targets\" which have already been passed\n        through an embedding layer. Inputs should have shape\n        [batch_size, max_seq_length, 1, embedding_size]. Targets should have\n        shape [batch_size, max_seq_length, 1, 1]\n\n    Returns:\n      The logits which get passed to the top of the model for inference.\n      A tensor of shape [batch_size, seq_length, 1, embedding_size]\n    \"\"\"\n    inputs = features.get(\"inputs\")\n    targets = features[\"targets\"]\n\n    if inputs is not None:\n      inputs = common_layers.flatten4d3d(inputs)\n      _, final_encoder_state = self._rnn(tf.reverse(inputs, axis=[1]),\n                                         \"encoder\")\n    else:\n      final_encoder_state = None\n\n    shifted_targets = common_layers.shift_right(targets)\n    decoder_outputs, _ = self._rnn(\n        common_layers.flatten4d3d(shifted_targets),\n        \"decoder\",\n        initial_state=final_encoder_state)\n    return decoder_outputs\n\n\n@registry.register_model\nclass NeuralQueueModel(NeuralStackModel):\n  \"\"\"Subcalss of NeuralStackModel which implements a queue.\n  \"\"\"\n\n  def cell(self, hidden_size):\n    \"\"\"Build a NeuralQueueCell instead of a NeuralStackCell.\n\n    Args:\n      hidden_size: The hidden size of the cell.\n\n    Returns:\n      A new NeuralQueueCell with the given hidden size.\n    \"\"\"\n    return NeuralQueueCell(hidden_size,\n                           self._hparams.memory_size,\n                           self._hparams.embedding_size)\n\n\n@registry.register_model\nclass NeuralDequeModel(NeuralStackModel):\n  \"\"\"Subclass of NeuralStackModel which implements a double-ended queue.\n  \"\"\"\n\n  def cell(self, hidden_size):\n    \"\"\"Build a NeuralDequeCell instead of a NeuralStackCell.\n\n    Args:\n      hidden_size: The hidden size of the cell.\n\n    Returns:\n      A new NeuralDequeCell with the given hidden size.\n    \"\"\"\n    return NeuralDequeCell(hidden_size,\n                           self._hparams.memory_size,\n                           self._hparams.embedding_size)\n\n\n@registry.register_hparams\ndef lstm_transduction():\n  \"\"\"HParams for LSTM base on transduction tasks.\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.daisy_chain_variables = False\n  hparams.batch_size = 10\n  hparams.clip_grad_norm = 1.0\n  hparams.hidden_size = 128\n  hparams.num_hidden_layers = 4\n  hparams.initializer = \"uniform_unit_scaling\"\n  hparams.initializer_gain = 1.0\n  hparams.optimizer = \"RMSProp\"\n  hparams.learning_rate = 0.01\n  hparams.weight_decay = 0.0\n\n  hparams.add_hparam(\"memory_size\", 128)\n  hparams.add_hparam(\"embedding_size\", 32)\n  return hparams\n\n\n@registry.register_hparams\ndef neural_stack():\n  \"\"\"HParams for neural stacks and queues.\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.daisy_chain_variables = False\n  hparams.batch_size = 10\n  hparams.clip_grad_norm = 1.0\n  hparams.initializer = \"uniform_unit_scaling\"\n  hparams.initializer_gain = 1.0\n  hparams.optimizer = \"RMSProp\"\n  hparams.learning_rate = 0.0001\n  hparams.weight_decay = 0.0\n\n  hparams.add_hparam(\"controller_layer_sizes\", [256, 512])\n  hparams.add_hparam(\"memory_size\", 128)\n  hparams.add_hparam(\"embedding_size\", 64)\n  hparams.hidden_size = hparams.embedding_size\n  return hparams\n\n\n@registry.register_hparams\ndef neural_deque():\n  \"\"\"HParams for neural deques.\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.daisy_chain_variables = False\n  hparams.batch_size = 10\n  hparams.clip_grad_norm = 1.0\n  hparams.initializer = \"uniform_unit_scaling\"\n  hparams.initializer_gain = 1.0\n  hparams.optimizer = \"RMSProp\"\n  hparams.learning_rate = 0.0001\n  hparams.weight_decay = 0.0\n\n  hparams.add_hparam(\"controller_layer_sizes\", [256, 512])\n  hparams.add_hparam(\"memory_size\", 256)\n  hparams.add_hparam(\"embedding_size\", 64)\n  hparams.hidden_size = hparams.embedding_size\n  return hparams\n", "framework": "tensorflow"}
{"repo_name": "mortada/tensorflow", "file_path": "tensorflow/contrib/distributions/python/kernel_tests/mixture_test.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Mixture distribution.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\n\nimport numpy as np\nfrom scipy import stats\n\nfrom tensorflow.contrib import distributions\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging as logging\n\ndistributions_py = distributions\n\n\ndef _swap_first_last_axes(array):\n  rank = len(array.shape)\n  transpose = [rank - 1] + list(range(0, rank - 1))\n  return array.transpose(transpose)\n\n\n@contextlib.contextmanager\ndef _test_capture_mvndiag_sample_outputs():\n  \"\"\"Use monkey-patching to capture the output of an MVNDiag _sample_n.\"\"\"\n  data_container = []\n  true_mvndiag_sample_n = distributions_py.MultivariateNormalDiag._sample_n\n\n  def _capturing_mvndiag_sample_n(self, n, seed=None):\n    samples = true_mvndiag_sample_n(self, n=n, seed=seed)\n    data_container.append(samples)\n    return samples\n\n  distributions_py.MultivariateNormalDiag._sample_n = (\n      _capturing_mvndiag_sample_n)\n  yield data_container\n  distributions_py.MultivariateNormalDiag._sample_n = true_mvndiag_sample_n\n\n\n@contextlib.contextmanager\ndef _test_capture_normal_sample_outputs():\n  \"\"\"Use monkey-patching to capture the output of an Normal _sample_n.\"\"\"\n  data_container = []\n  true_normal_sample_n = distributions_py.Normal._sample_n\n\n  def _capturing_normal_sample_n(self, n, seed=None):\n    samples = true_normal_sample_n(self, n=n, seed=seed)\n    data_container.append(samples)\n    return samples\n\n  distributions_py.Normal._sample_n = _capturing_normal_sample_n\n  yield data_container\n  distributions_py.Normal._sample_n = true_normal_sample_n\n\n\ndef make_univariate_mixture(batch_shape, num_components):\n  batch_shape = ops.convert_to_tensor(batch_shape, dtypes.int32)\n  logits = random_ops.random_uniform(\n      array_ops.concat((batch_shape, [num_components]), axis=0),\n      -1, 1, dtype=dtypes.float32) - 50.\n  components = [\n      distributions_py.Normal(\n          loc=random_ops.random_normal(batch_shape),\n          scale=10 * random_ops.random_uniform(batch_shape))\n      for _ in range(num_components)\n  ]\n  cat = distributions_py.Categorical(logits, dtype=dtypes.int32)\n  return distributions_py.Mixture(cat, components)\n\n\ndef make_multivariate_mixture(batch_shape, num_components, event_shape,\n                              batch_shape_tensor=None):\n  if batch_shape_tensor is None:\n    batch_shape_tensor = batch_shape\n  batch_shape_tensor = ops.convert_to_tensor(batch_shape_tensor, dtypes.int32)\n  logits = random_ops.random_uniform(\n      array_ops.concat((batch_shape_tensor, [num_components]), 0),\n      -1, 1, dtype=dtypes.float32) - 50.\n  logits.set_shape(\n      tensor_shape.TensorShape(batch_shape).concatenate(num_components))\n  static_batch_and_event_shape = (\n      tensor_shape.TensorShape(batch_shape).concatenate(event_shape))\n  event_shape = ops.convert_to_tensor(event_shape, dtypes.int32)\n  batch_and_event_shape = array_ops.concat((batch_shape_tensor, event_shape), 0)\n  def create_component():\n    loc = random_ops.random_normal(batch_and_event_shape)\n    scale_diag = 10 * random_ops.random_uniform(batch_and_event_shape)\n    loc.set_shape(static_batch_and_event_shape)\n    scale_diag.set_shape(static_batch_and_event_shape)\n    return distributions_py.MultivariateNormalDiag(\n        loc=loc, scale_diag=scale_diag)\n  components = [create_component() for _ in range(num_components)]\n  cat = distributions_py.Categorical(logits, dtype=dtypes.int32)\n  return distributions_py.Mixture(cat, components)\n\n\nclass MixtureTest(test.TestCase):\n\n  def testShapes(self):\n    with self.test_session():\n      for batch_shape in ([], [1], [2, 3, 4]):\n        dist = make_univariate_mixture(batch_shape, num_components=10)\n        self.assertAllEqual(batch_shape, dist.batch_shape)\n        self.assertAllEqual(batch_shape, dist.batch_shape_tensor().eval())\n        self.assertAllEqual([], dist.event_shape)\n        self.assertAllEqual([], dist.event_shape_tensor().eval())\n\n        for event_shape in ([1], [2]):\n          dist = make_multivariate_mixture(\n              batch_shape, num_components=10, event_shape=event_shape)\n          self.assertAllEqual(batch_shape, dist.batch_shape)\n          self.assertAllEqual(batch_shape, dist.batch_shape_tensor().eval())\n          self.assertAllEqual(event_shape, dist.event_shape)\n          self.assertAllEqual(event_shape, dist.event_shape_tensor().eval())\n\n  def testBrokenShapesStatic(self):\n    with self.assertRaisesWithPredicateMatch(ValueError,\n                                             r\"cat.num_classes != len\"):\n      distributions_py.Mixture(\n          distributions_py.Categorical([0.1, 0.5]),  # 2 classes\n          [distributions_py.Normal(loc=1.0, scale=2.0)])\n    with self.assertRaisesWithPredicateMatch(\n        ValueError, r\"\\(\\) and \\(2,\\) are not compatible\"):\n      # The value error is raised because the batch shapes of the\n      # Normals are not equal.  One is a scalar, the other is a\n      # vector of size (2,).\n      distributions_py.Mixture(\n          distributions_py.Categorical([-0.5, 0.5]),  # scalar batch\n          [\n              distributions_py.Normal(\n                  loc=1.0, scale=2.0),  # scalar dist\n              distributions_py.Normal(\n                  loc=[1.0, 1.0], scale=[2.0, 2.0])\n          ])\n    with self.assertRaisesWithPredicateMatch(ValueError, r\"Could not infer\"):\n      cat_logits = array_ops.placeholder(shape=[1, None], dtype=dtypes.float32)\n      distributions_py.Mixture(\n          distributions_py.Categorical(cat_logits),\n          [distributions_py.Normal(\n              loc=[1.0], scale=[2.0])])\n\n  def testBrokenShapesDynamic(self):\n    with self.test_session():\n      d0_param = array_ops.placeholder(dtype=dtypes.float32)\n      d1_param = array_ops.placeholder(dtype=dtypes.float32)\n      d = distributions_py.Mixture(\n          distributions_py.Categorical([0.1, 0.2]), [\n              distributions_py.Normal(\n                  loc=d0_param, scale=d0_param), distributions_py.Normal(\n                      loc=d1_param, scale=d1_param)\n          ],\n          validate_args=True)\n      with self.assertRaisesOpError(r\"batch shape must match\"):\n        d.sample().eval(feed_dict={d0_param: [2.0, 3.0], d1_param: [1.0]})\n      with self.assertRaisesOpError(r\"batch shape must match\"):\n        d.sample().eval(feed_dict={d0_param: [2.0, 3.0], d1_param: 1.0})\n\n  def testBrokenTypes(self):\n    with self.assertRaisesWithPredicateMatch(TypeError, \"Categorical\"):\n      distributions_py.Mixture(None, [])\n    cat = distributions_py.Categorical([0.3, 0.2])\n    # components must be a list of distributions\n    with self.assertRaisesWithPredicateMatch(\n        TypeError, \"all .* must be Distribution instances\"):\n      distributions_py.Mixture(cat, [None])\n    with self.assertRaisesWithPredicateMatch(TypeError, \"same dtype\"):\n      distributions_py.Mixture(\n          cat, [\n              distributions_py.Normal(loc=[1.0], scale=[2.0]),\n              distributions_py.Normal(loc=[np.float16(1.0)],\n                                      scale=[np.float16(2.0)]),\n          ])\n    with self.assertRaisesWithPredicateMatch(ValueError, \"non-empty list\"):\n      distributions_py.Mixture(distributions_py.Categorical([0.3, 0.2]), None)\n\n    # TODO(ebrevdo): once distribution Domains have been added, add a\n    # test to ensure that the domains of the distributions in a\n    # mixture are checked for equivalence.\n\n  def testMeanUnivariate(self):\n    with self.test_session() as sess:\n      for batch_shape in ((), (2,), (2, 3)):\n        dist = make_univariate_mixture(\n            batch_shape=batch_shape, num_components=2)\n        mean = dist.mean()\n        self.assertEqual(batch_shape, mean.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_means = [d.mean() for d in dist.components]\n\n        mean_value, cat_probs_value, dist_means_value = sess.run(\n            [mean, cat_probs, dist_means])\n        self.assertEqual(batch_shape, mean_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n        true_mean = sum(\n            [c_p * m for (c_p, m) in zip(cat_probs_value, dist_means_value)])\n\n        self.assertAllClose(true_mean, mean_value)\n\n  def testMeanMultivariate(self):\n    with self.test_session() as sess:\n      for batch_shape in ((), (2,), (2, 3)):\n        dist = make_multivariate_mixture(\n            batch_shape=batch_shape, num_components=2, event_shape=(4,))\n        mean = dist.mean()\n        self.assertEqual(batch_shape + (4,), mean.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_means = [d.mean() for d in dist.components]\n\n        mean_value, cat_probs_value, dist_means_value = sess.run(\n            [mean, cat_probs, dist_means])\n        self.assertEqual(batch_shape + (4,), mean_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n\n        # Add a new innermost dimension for broadcasting to mvn vector shape\n        cat_probs_value = [np.expand_dims(c_p, -1) for c_p in cat_probs_value]\n\n        true_mean = sum(\n            [c_p * m for (c_p, m) in zip(cat_probs_value, dist_means_value)])\n\n        self.assertAllClose(true_mean, mean_value)\n\n  def testProbScalarUnivariate(self):\n    with self.test_session() as sess:\n      dist = make_univariate_mixture(batch_shape=[], num_components=2)\n      for x in [\n          np.array(\n              [1.0, 2.0], dtype=np.float32), np.array(\n                  1.0, dtype=np.float32),\n          np.random.randn(3, 4).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n\n        self.assertEqual(x.shape, p_x.get_shape())\n        cat_probs = nn_ops.softmax([dist.cat.logits])[0]\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n        self.assertEqual(x.shape, p_x_value.shape)\n\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testProbScalarMultivariate(self):\n    with self.test_session() as sess:\n      dist = make_multivariate_mixture(\n          batch_shape=[], num_components=2, event_shape=[3])\n      for x in [\n          np.array(\n              [[-1.0, 0.0, 1.0], [0.5, 1.0, -0.3]], dtype=np.float32), np.array(\n                  [-1.0, 0.0, 1.0], dtype=np.float32),\n          np.random.randn(2, 2, 3).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n\n        self.assertEqual(x.shape[:-1], p_x.get_shape())\n\n        cat_probs = nn_ops.softmax([dist.cat.logits])[0]\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n\n        self.assertEqual(x.shape[:-1], p_x_value.shape)\n\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testProbBatchUnivariate(self):\n    with self.test_session() as sess:\n      dist = make_univariate_mixture(batch_shape=[2, 3], num_components=2)\n\n      for x in [\n          np.random.randn(2, 3).astype(np.float32),\n          np.random.randn(4, 2, 3).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n        self.assertEqual(x.shape, p_x.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n        self.assertEqual(x.shape, p_x_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testProbBatchMultivariate(self):\n    with self.test_session() as sess:\n      dist = make_multivariate_mixture(\n          batch_shape=[2, 3], num_components=2, event_shape=[4])\n\n      for x in [\n          np.random.randn(2, 3, 4).astype(np.float32),\n          np.random.randn(4, 2, 3, 4).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n        self.assertEqual(x.shape[:-1], p_x.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n        self.assertEqual(x.shape[:-1], p_x_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testSampleScalarBatchUnivariate(self):\n    with self.test_session() as sess:\n      num_components = 3\n      batch_shape = []\n      dist = make_univariate_mixture(\n          batch_shape=batch_shape, num_components=num_components)\n      n = 4\n      with _test_capture_normal_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      self.assertEqual((4,), samples.get_shape())\n      cat_samples = dist.cat.sample(n, seed=123)\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples])\n      self.assertEqual((4,), sample_values.shape)\n\n      for c in range(num_components):\n        which_c = np.where(cat_sample_values == c)[0]\n        size_c = which_c.size\n        # Scalar Batch univariate case: batch_size == 1, rank 1\n        which_dist_samples = dist_sample_values[c][:size_c]\n        self.assertAllClose(which_dist_samples, sample_values[which_c])\n\n  # Test that sampling with the same seed twice gives the same results.\n  def testSampleMultipleTimes(self):\n    # 5 component mixture.\n    logits = [-10.0, -5.0, 0.0, 5.0, 10.0]\n    mus = [-5.0, 0.0, 5.0, 4.0, 20.0]\n    sigmas = [0.1, 5.0, 3.0, 0.2, 4.0]\n\n    with self.test_session():\n      n = 100\n\n      random_seed.set_random_seed(654321)\n      components = [\n          distributions_py.Normal(\n              loc=mu, scale=sigma) for mu, sigma in zip(mus, sigmas)\n      ]\n      cat = distributions_py.Categorical(\n          logits, dtype=dtypes.int32, name=\"cat1\")\n      dist1 = distributions_py.Mixture(cat, components, name=\"mixture1\")\n      samples1 = dist1.sample(n, seed=123456).eval()\n\n      random_seed.set_random_seed(654321)\n      components2 = [\n          distributions_py.Normal(\n              loc=mu, scale=sigma) for mu, sigma in zip(mus, sigmas)\n      ]\n      cat2 = distributions_py.Categorical(\n          logits, dtype=dtypes.int32, name=\"cat2\")\n      dist2 = distributions_py.Mixture(cat2, components2, name=\"mixture2\")\n      samples2 = dist2.sample(n, seed=123456).eval()\n\n      self.assertAllClose(samples1, samples2)\n\n  def testSampleScalarBatchMultivariate(self):\n    with self.test_session() as sess:\n      num_components = 3\n      dist = make_multivariate_mixture(\n          batch_shape=[], num_components=num_components, event_shape=[2])\n      n = 4\n      with _test_capture_mvndiag_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      self.assertEqual((4, 2), samples.get_shape())\n      cat_samples = dist.cat.sample(n, seed=123)\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples])\n      self.assertEqual((4, 2), sample_values.shape)\n      for c in range(num_components):\n        which_c = np.where(cat_sample_values == c)[0]\n        size_c = which_c.size\n        # Scalar Batch multivariate case: batch_size == 1, rank 2\n        which_dist_samples = dist_sample_values[c][:size_c, :]\n        self.assertAllClose(which_dist_samples, sample_values[which_c, :])\n\n  def testSampleBatchUnivariate(self):\n    with self.test_session() as sess:\n      num_components = 3\n      dist = make_univariate_mixture(\n          batch_shape=[2, 3], num_components=num_components)\n      n = 4\n      with _test_capture_normal_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      self.assertEqual((4, 2, 3), samples.get_shape())\n      cat_samples = dist.cat.sample(n, seed=123)\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples])\n      self.assertEqual((4, 2, 3), sample_values.shape)\n      for c in range(num_components):\n        which_c_s, which_c_b0, which_c_b1 = np.where(cat_sample_values == c)\n        size_c = which_c_s.size\n        # Batch univariate case: batch_size == [2, 3], rank 3\n        which_dist_samples = dist_sample_values[c][range(size_c), which_c_b0,\n                                                   which_c_b1]\n        self.assertAllClose(which_dist_samples,\n                            sample_values[which_c_s, which_c_b0, which_c_b1])\n\n  def _testSampleBatchMultivariate(self, fully_known_batch_shape):\n    with self.test_session() as sess:\n      num_components = 3\n      if fully_known_batch_shape:\n        batch_shape = [2, 3]\n        batch_shape_tensor = [2, 3]\n      else:\n        batch_shape = [None, 3]\n        batch_shape_tensor = array_ops.placeholder(dtype=dtypes.int32)\n\n      dist = make_multivariate_mixture(\n          batch_shape=batch_shape,\n          num_components=num_components, event_shape=[4],\n          batch_shape_tensor=batch_shape_tensor)\n      n = 5\n      with _test_capture_mvndiag_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      if fully_known_batch_shape:\n        self.assertEqual((5, 2, 3, 4), samples.get_shape())\n      else:\n        self.assertEqual([5, None, 3, 4], samples.get_shape().as_list())\n      cat_samples = dist.cat.sample(n, seed=123)\n      if fully_known_batch_shape:\n        feed_dict = {}\n      else:\n        feed_dict = {batch_shape_tensor: [2, 3]}\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples],\n          feed_dict=feed_dict)\n      self.assertEqual((5, 2, 3, 4), sample_values.shape)\n\n      for c in range(num_components):\n        which_c_s, which_c_b0, which_c_b1 = np.where(cat_sample_values == c)\n        size_c = which_c_s.size\n        # Batch univariate case: batch_size == [2, 3], rank 4 (multivariate)\n        which_dist_samples = dist_sample_values[c][range(size_c), which_c_b0,\n                                                   which_c_b1, :]\n        self.assertAllClose(which_dist_samples,\n                            sample_values[which_c_s, which_c_b0, which_c_b1, :])\n\n  def testSampleBatchMultivariateFullyKnownBatchShape(self):\n    self._testSampleBatchMultivariate(fully_known_batch_shape=True)\n\n  def testSampleBatchMultivariateNotFullyKnownBatchShape(self):\n    self._testSampleBatchMultivariate(fully_known_batch_shape=False)\n\n  def testEntropyLowerBoundMultivariate(self):\n    with self.test_session() as sess:\n      for batch_shape in ((), (2,), (2, 3)):\n        dist = make_multivariate_mixture(\n            batch_shape=batch_shape, num_components=2, event_shape=(4,))\n        entropy_lower_bound = dist.entropy_lower_bound()\n        self.assertEqual(batch_shape, entropy_lower_bound.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_entropy = [d.entropy() for d in dist.components]\n\n        entropy_lower_bound_value, cat_probs_value, dist_entropy_value = (\n            sess.run([entropy_lower_bound, cat_probs, dist_entropy]))\n        self.assertEqual(batch_shape, entropy_lower_bound_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n\n        # entropy_lower_bound = sum_i pi_i entropy_i\n        # for i in num_components, batchwise.\n        true_entropy_lower_bound = sum(\n            [c_p * m for (c_p, m) in zip(cat_probs_value, dist_entropy_value)])\n\n        self.assertAllClose(true_entropy_lower_bound, entropy_lower_bound_value)\n\n  def testCdfScalarUnivariate(self):\n    \"\"\"Tests CDF against scipy for a mixture of seven gaussians.\"\"\"\n    # Construct a mixture of gaussians with seven components.\n    n_components = 7\n\n    # pre-softmax mixture probabilities.\n    mixture_weight_logits = np.random.uniform(\n        low=-1, high=1, size=(n_components,)).astype(np.float32)\n\n    def _scalar_univariate_softmax(x):\n      e_x = np.exp(x - np.max(x))\n      return e_x / e_x.sum()\n\n    # Construct the distributions_py.Mixture object.\n    mixture_weights = _scalar_univariate_softmax(mixture_weight_logits)\n    means = [np.random.uniform(low=-10, high=10, size=()).astype(np.float32)\n             for _ in range(n_components)]\n    sigmas = [np.ones(shape=(), dtype=np.float32) for _ in range(n_components)]\n    cat_tf = distributions_py.Categorical(probs=mixture_weights)\n    components_tf = [distributions_py.Normal(loc=mu, scale=sigma)\n                     for (mu, sigma) in zip(means, sigmas)]\n    mixture_tf = distributions_py.Mixture(cat=cat_tf, components=components_tf)\n\n    x_tensor = array_ops.placeholder(shape=(), dtype=dtypes.float32)\n\n    # These are two test cases to verify.\n    xs_to_check = [\n        np.array(1.0, dtype=np.float32),\n        np.array(np.random.randn()).astype(np.float32)\n    ]\n\n    # Carry out the test for both d.cdf and exp(d.log_cdf).\n    x_cdf_tf = mixture_tf.cdf(x_tensor)\n    x_log_cdf_tf = mixture_tf.log_cdf(x_tensor)\n\n    with self.test_session() as sess:\n      for x_feed in xs_to_check:\n        x_cdf_tf_result, x_log_cdf_tf_result = sess.run(\n            [x_cdf_tf, x_log_cdf_tf], feed_dict={x_tensor: x_feed})\n\n        # Compute the cdf with scipy.\n        scipy_component_cdfs = [stats.norm.cdf(x=x_feed, loc=mu, scale=sigma)\n                                for (mu, sigma) in zip(means, sigmas)]\n        scipy_cdf_result = np.dot(mixture_weights,\n                                  np.array(scipy_component_cdfs))\n        self.assertAllClose(x_cdf_tf_result, scipy_cdf_result)\n        self.assertAllClose(np.exp(x_log_cdf_tf_result), scipy_cdf_result)\n\n  def testCdfBatchUnivariate(self):\n    \"\"\"Tests against scipy for a (batch of) mixture(s) of seven gaussians.\"\"\"\n    n_components = 7\n    batch_size = 5\n    mixture_weight_logits = np.random.uniform(\n        low=-1, high=1, size=(batch_size, n_components)).astype(np.float32)\n\n    def _batch_univariate_softmax(x):\n      e_x = np.exp(x)\n      e_x_sum = np.expand_dims(np.sum(e_x, axis=1), axis=1)\n      return e_x / np.tile(e_x_sum, reps=[1, x.shape[1]])\n\n    psize = (batch_size,)\n    mixture_weights = _batch_univariate_softmax(mixture_weight_logits)\n    means = [np.random.uniform(low=-10, high=10, size=psize).astype(np.float32)\n             for _ in range(n_components)]\n    sigmas = [np.ones(shape=psize, dtype=np.float32)\n              for _ in range(n_components)]\n    cat_tf = distributions_py.Categorical(probs=mixture_weights)\n    components_tf = [distributions_py.Normal(loc=mu, scale=sigma)\n                     for (mu, sigma) in zip(means, sigmas)]\n    mixture_tf = distributions_py.Mixture(cat=cat_tf, components=components_tf)\n\n    x_tensor = array_ops.placeholder(shape=psize, dtype=dtypes.float32)\n    xs_to_check = [\n        np.array([1.0, 5.9, -3, 0.0, 0.0], dtype=np.float32),\n        np.random.randn(batch_size).astype(np.float32)\n    ]\n\n    x_cdf_tf = mixture_tf.cdf(x_tensor)\n    x_log_cdf_tf = mixture_tf.log_cdf(x_tensor)\n\n    with self.test_session() as sess:\n      for x_feed in xs_to_check:\n        x_cdf_tf_result, x_log_cdf_tf_result = sess.run(\n            [x_cdf_tf, x_log_cdf_tf],\n            feed_dict={x_tensor: x_feed})\n\n        # Compute the cdf with scipy.\n        scipy_component_cdfs = [stats.norm.cdf(x=x_feed, loc=mu, scale=sigma)\n                                for (mu, sigma) in zip(means, sigmas)]\n        weights_and_cdfs = zip(np.transpose(mixture_weights, axes=[1, 0]),\n                               scipy_component_cdfs)\n        final_cdf_probs_per_component = [\n            np.multiply(c_p_value, d_cdf_value)\n            for (c_p_value, d_cdf_value) in weights_and_cdfs]\n        scipy_cdf_result = np.sum(final_cdf_probs_per_component, axis=0)\n        self.assertAllClose(x_cdf_tf_result, scipy_cdf_result)\n        self.assertAllClose(np.exp(x_log_cdf_tf_result), scipy_cdf_result)\n\n\nclass MixtureBenchmark(test.Benchmark):\n\n  def _runSamplingBenchmark(self, name, create_distribution, use_gpu,\n                            num_components, batch_size, num_features,\n                            sample_size):\n    config = config_pb2.ConfigProto()\n    config.allow_soft_placement = True\n    np.random.seed(127)\n    with session.Session(config=config, graph=ops.Graph()) as sess:\n      random_seed.set_random_seed(0)\n      with ops.device(\"/gpu:0\" if use_gpu else \"/cpu:0\"):\n        mixture = create_distribution(\n            num_components=num_components,\n            batch_size=batch_size,\n            num_features=num_features)\n        sample_op = mixture.sample(sample_size).op\n        sess.run(variables.global_variables_initializer())\n        reported = self.run_op_benchmark(\n            sess,\n            sample_op,\n            min_iters=10,\n            name=(\"%s_%s_components_%d_batch_%d_features_%d_sample_%d\" %\n                  (name, use_gpu, num_components, batch_size, num_features,\n                   sample_size)))\n        logging.vlog(2, \"\\t\".join([\"%s\", \"%d\", \"%d\", \"%d\", \"%d\", \"%g\"]) % (\n            use_gpu, num_components, batch_size, num_features, sample_size,\n            reported[\"wall_time\"]))\n\n  def benchmarkSamplingMVNDiag(self):\n    logging.vlog(\n        2, \"mvn_diag\\tuse_gpu\\tcomponents\\tbatch\\tfeatures\\tsample\\twall_time\")\n\n    def create_distribution(batch_size, num_components, num_features):\n      cat = distributions_py.Categorical(\n          logits=np.random.randn(batch_size, num_components))\n      mus = [\n          variables.Variable(np.random.randn(batch_size, num_features))\n          for _ in range(num_components)\n      ]\n      sigmas = [\n          variables.Variable(np.random.rand(batch_size, num_features))\n          for _ in range(num_components)\n      ]\n      components = list(\n          distributions_py.MultivariateNormalDiag(\n              loc=mu, scale_diag=sigma) for (mu, sigma) in zip(mus, sigmas))\n      return distributions_py.Mixture(cat, components)\n\n    for use_gpu in False, True:\n      if use_gpu and not test.is_gpu_available():\n        continue\n      for num_components in 1, 8, 16:\n        for batch_size in 1, 32:\n          for num_features in 1, 64, 512:\n            for sample_size in 1, 32, 128:\n              self._runSamplingBenchmark(\n                  \"mvn_diag\",\n                  create_distribution=create_distribution,\n                  use_gpu=use_gpu,\n                  num_components=num_components,\n                  batch_size=batch_size,\n                  num_features=num_features,\n                  sample_size=sample_size)\n\n  def benchmarkSamplingMVNFull(self):\n    logging.vlog(\n        2, \"mvn_full\\tuse_gpu\\tcomponents\\tbatch\\tfeatures\\tsample\\twall_time\")\n\n    def psd(x):\n      \"\"\"Construct batch-wise PSD matrices.\"\"\"\n      return np.stack([np.dot(np.transpose(z), z) for z in x])\n\n    def create_distribution(batch_size, num_components, num_features):\n      cat = distributions_py.Categorical(\n          logits=np.random.randn(batch_size, num_components))\n      mus = [\n          variables.Variable(np.random.randn(batch_size, num_features))\n          for _ in range(num_components)\n      ]\n      sigmas = [\n          variables.Variable(\n              psd(np.random.rand(batch_size, num_features, num_features)))\n          for _ in range(num_components)\n      ]\n      components = list(\n          distributions_py.MultivariateNormalTriL(\n              loc=mu, scale_tril=linalg_ops.cholesky(sigma))\n          for (mu, sigma) in zip(mus, sigmas))\n      return distributions_py.Mixture(cat, components)\n\n    for use_gpu in False, True:\n      if use_gpu and not test.is_gpu_available():\n        continue\n      for num_components in 1, 8, 16:\n        for batch_size in 1, 32:\n          for num_features in 1, 64, 512:\n            for sample_size in 1, 32, 128:\n              self._runSamplingBenchmark(\n                  \"mvn_full\",\n                  create_distribution=create_distribution,\n                  use_gpu=use_gpu,\n                  num_components=num_components,\n                  batch_size=batch_size,\n                  num_features=num_features,\n                  sample_size=sample_size)\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "framework": "tensorflow"}
{"repo_name": "Moriadry/tensorflow", "file_path": "tensorflow/contrib/distributions/python/kernel_tests/mixture_test.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Mixture distribution.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\n\nimport numpy as np\nfrom scipy import stats\n\nfrom tensorflow.contrib import distributions\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging as logging\n\ndistributions_py = distributions\n\n\ndef _swap_first_last_axes(array):\n  rank = len(array.shape)\n  transpose = [rank - 1] + list(range(0, rank - 1))\n  return array.transpose(transpose)\n\n\n@contextlib.contextmanager\ndef _test_capture_mvndiag_sample_outputs():\n  \"\"\"Use monkey-patching to capture the output of an MVNDiag _sample_n.\"\"\"\n  data_container = []\n  true_mvndiag_sample_n = distributions_py.MultivariateNormalDiag._sample_n\n\n  def _capturing_mvndiag_sample_n(self, n, seed=None):\n    samples = true_mvndiag_sample_n(self, n=n, seed=seed)\n    data_container.append(samples)\n    return samples\n\n  distributions_py.MultivariateNormalDiag._sample_n = (\n      _capturing_mvndiag_sample_n)\n  yield data_container\n  distributions_py.MultivariateNormalDiag._sample_n = true_mvndiag_sample_n\n\n\n@contextlib.contextmanager\ndef _test_capture_normal_sample_outputs():\n  \"\"\"Use monkey-patching to capture the output of an Normal _sample_n.\"\"\"\n  data_container = []\n  true_normal_sample_n = distributions_py.Normal._sample_n\n\n  def _capturing_normal_sample_n(self, n, seed=None):\n    samples = true_normal_sample_n(self, n=n, seed=seed)\n    data_container.append(samples)\n    return samples\n\n  distributions_py.Normal._sample_n = _capturing_normal_sample_n\n  yield data_container\n  distributions_py.Normal._sample_n = true_normal_sample_n\n\n\ndef make_univariate_mixture(batch_shape, num_components):\n  batch_shape = ops.convert_to_tensor(batch_shape, dtypes.int32)\n  logits = random_ops.random_uniform(\n      array_ops.concat((batch_shape, [num_components]), axis=0),\n      -1, 1, dtype=dtypes.float32) - 50.\n  components = [\n      distributions_py.Normal(\n          loc=random_ops.random_normal(batch_shape),\n          scale=10 * random_ops.random_uniform(batch_shape))\n      for _ in range(num_components)\n  ]\n  cat = distributions_py.Categorical(logits, dtype=dtypes.int32)\n  return distributions_py.Mixture(cat, components)\n\n\ndef make_multivariate_mixture(batch_shape, num_components, event_shape,\n                              batch_shape_tensor=None):\n  if batch_shape_tensor is None:\n    batch_shape_tensor = batch_shape\n  batch_shape_tensor = ops.convert_to_tensor(batch_shape_tensor, dtypes.int32)\n  logits = random_ops.random_uniform(\n      array_ops.concat((batch_shape_tensor, [num_components]), 0),\n      -1, 1, dtype=dtypes.float32) - 50.\n  logits.set_shape(\n      tensor_shape.TensorShape(batch_shape).concatenate(num_components))\n  static_batch_and_event_shape = (\n      tensor_shape.TensorShape(batch_shape).concatenate(event_shape))\n  event_shape = ops.convert_to_tensor(event_shape, dtypes.int32)\n  batch_and_event_shape = array_ops.concat((batch_shape_tensor, event_shape), 0)\n  def create_component():\n    loc = random_ops.random_normal(batch_and_event_shape)\n    scale_diag = 10 * random_ops.random_uniform(batch_and_event_shape)\n    loc.set_shape(static_batch_and_event_shape)\n    scale_diag.set_shape(static_batch_and_event_shape)\n    return distributions_py.MultivariateNormalDiag(\n        loc=loc, scale_diag=scale_diag)\n  components = [create_component() for _ in range(num_components)]\n  cat = distributions_py.Categorical(logits, dtype=dtypes.int32)\n  return distributions_py.Mixture(cat, components)\n\n\nclass MixtureTest(test.TestCase):\n\n  def testShapes(self):\n    with self.test_session():\n      for batch_shape in ([], [1], [2, 3, 4]):\n        dist = make_univariate_mixture(batch_shape, num_components=10)\n        self.assertAllEqual(batch_shape, dist.batch_shape)\n        self.assertAllEqual(batch_shape, dist.batch_shape_tensor().eval())\n        self.assertAllEqual([], dist.event_shape)\n        self.assertAllEqual([], dist.event_shape_tensor().eval())\n\n        for event_shape in ([1], [2]):\n          dist = make_multivariate_mixture(\n              batch_shape, num_components=10, event_shape=event_shape)\n          self.assertAllEqual(batch_shape, dist.batch_shape)\n          self.assertAllEqual(batch_shape, dist.batch_shape_tensor().eval())\n          self.assertAllEqual(event_shape, dist.event_shape)\n          self.assertAllEqual(event_shape, dist.event_shape_tensor().eval())\n\n  def testBrokenShapesStatic(self):\n    with self.assertRaisesWithPredicateMatch(ValueError,\n                                             r\"cat.num_classes != len\"):\n      distributions_py.Mixture(\n          distributions_py.Categorical([0.1, 0.5]),  # 2 classes\n          [distributions_py.Normal(loc=1.0, scale=2.0)])\n    with self.assertRaisesWithPredicateMatch(\n        ValueError, r\"\\(\\) and \\(2,\\) are not compatible\"):\n      # The value error is raised because the batch shapes of the\n      # Normals are not equal.  One is a scalar, the other is a\n      # vector of size (2,).\n      distributions_py.Mixture(\n          distributions_py.Categorical([-0.5, 0.5]),  # scalar batch\n          [\n              distributions_py.Normal(\n                  loc=1.0, scale=2.0),  # scalar dist\n              distributions_py.Normal(\n                  loc=[1.0, 1.0], scale=[2.0, 2.0])\n          ])\n    with self.assertRaisesWithPredicateMatch(ValueError, r\"Could not infer\"):\n      cat_logits = array_ops.placeholder(shape=[1, None], dtype=dtypes.float32)\n      distributions_py.Mixture(\n          distributions_py.Categorical(cat_logits),\n          [distributions_py.Normal(\n              loc=[1.0], scale=[2.0])])\n\n  def testBrokenShapesDynamic(self):\n    with self.test_session():\n      d0_param = array_ops.placeholder(dtype=dtypes.float32)\n      d1_param = array_ops.placeholder(dtype=dtypes.float32)\n      d = distributions_py.Mixture(\n          distributions_py.Categorical([0.1, 0.2]), [\n              distributions_py.Normal(\n                  loc=d0_param, scale=d0_param), distributions_py.Normal(\n                      loc=d1_param, scale=d1_param)\n          ],\n          validate_args=True)\n      with self.assertRaisesOpError(r\"batch shape must match\"):\n        d.sample().eval(feed_dict={d0_param: [2.0, 3.0], d1_param: [1.0]})\n      with self.assertRaisesOpError(r\"batch shape must match\"):\n        d.sample().eval(feed_dict={d0_param: [2.0, 3.0], d1_param: 1.0})\n\n  def testBrokenTypes(self):\n    with self.assertRaisesWithPredicateMatch(TypeError, \"Categorical\"):\n      distributions_py.Mixture(None, [])\n    cat = distributions_py.Categorical([0.3, 0.2])\n    # components must be a list of distributions\n    with self.assertRaisesWithPredicateMatch(\n        TypeError, \"all .* must be Distribution instances\"):\n      distributions_py.Mixture(cat, [None])\n    with self.assertRaisesWithPredicateMatch(TypeError, \"same dtype\"):\n      distributions_py.Mixture(\n          cat, [\n              distributions_py.Normal(loc=[1.0], scale=[2.0]),\n              distributions_py.Normal(loc=[np.float16(1.0)],\n                                      scale=[np.float16(2.0)]),\n          ])\n    with self.assertRaisesWithPredicateMatch(ValueError, \"non-empty list\"):\n      distributions_py.Mixture(distributions_py.Categorical([0.3, 0.2]), None)\n\n    # TODO(ebrevdo): once distribution Domains have been added, add a\n    # test to ensure that the domains of the distributions in a\n    # mixture are checked for equivalence.\n\n  def testMeanUnivariate(self):\n    with self.test_session() as sess:\n      for batch_shape in ((), (2,), (2, 3)):\n        dist = make_univariate_mixture(\n            batch_shape=batch_shape, num_components=2)\n        mean = dist.mean()\n        self.assertEqual(batch_shape, mean.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_means = [d.mean() for d in dist.components]\n\n        mean_value, cat_probs_value, dist_means_value = sess.run(\n            [mean, cat_probs, dist_means])\n        self.assertEqual(batch_shape, mean_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n        true_mean = sum(\n            [c_p * m for (c_p, m) in zip(cat_probs_value, dist_means_value)])\n\n        self.assertAllClose(true_mean, mean_value)\n\n  def testMeanMultivariate(self):\n    with self.test_session() as sess:\n      for batch_shape in ((), (2,), (2, 3)):\n        dist = make_multivariate_mixture(\n            batch_shape=batch_shape, num_components=2, event_shape=(4,))\n        mean = dist.mean()\n        self.assertEqual(batch_shape + (4,), mean.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_means = [d.mean() for d in dist.components]\n\n        mean_value, cat_probs_value, dist_means_value = sess.run(\n            [mean, cat_probs, dist_means])\n        self.assertEqual(batch_shape + (4,), mean_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n\n        # Add a new innermost dimension for broadcasting to mvn vector shape\n        cat_probs_value = [np.expand_dims(c_p, -1) for c_p in cat_probs_value]\n\n        true_mean = sum(\n            [c_p * m for (c_p, m) in zip(cat_probs_value, dist_means_value)])\n\n        self.assertAllClose(true_mean, mean_value)\n\n  def testProbScalarUnivariate(self):\n    with self.test_session() as sess:\n      dist = make_univariate_mixture(batch_shape=[], num_components=2)\n      for x in [\n          np.array(\n              [1.0, 2.0], dtype=np.float32), np.array(\n                  1.0, dtype=np.float32),\n          np.random.randn(3, 4).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n\n        self.assertEqual(x.shape, p_x.get_shape())\n        cat_probs = nn_ops.softmax([dist.cat.logits])[0]\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n        self.assertEqual(x.shape, p_x_value.shape)\n\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testProbScalarMultivariate(self):\n    with self.test_session() as sess:\n      dist = make_multivariate_mixture(\n          batch_shape=[], num_components=2, event_shape=[3])\n      for x in [\n          np.array(\n              [[-1.0, 0.0, 1.0], [0.5, 1.0, -0.3]], dtype=np.float32), np.array(\n                  [-1.0, 0.0, 1.0], dtype=np.float32),\n          np.random.randn(2, 2, 3).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n\n        self.assertEqual(x.shape[:-1], p_x.get_shape())\n\n        cat_probs = nn_ops.softmax([dist.cat.logits])[0]\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n\n        self.assertEqual(x.shape[:-1], p_x_value.shape)\n\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testProbBatchUnivariate(self):\n    with self.test_session() as sess:\n      dist = make_univariate_mixture(batch_shape=[2, 3], num_components=2)\n\n      for x in [\n          np.random.randn(2, 3).astype(np.float32),\n          np.random.randn(4, 2, 3).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n        self.assertEqual(x.shape, p_x.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n        self.assertEqual(x.shape, p_x_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testProbBatchMultivariate(self):\n    with self.test_session() as sess:\n      dist = make_multivariate_mixture(\n          batch_shape=[2, 3], num_components=2, event_shape=[4])\n\n      for x in [\n          np.random.randn(2, 3, 4).astype(np.float32),\n          np.random.randn(4, 2, 3, 4).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n        self.assertEqual(x.shape[:-1], p_x.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n        self.assertEqual(x.shape[:-1], p_x_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testSampleScalarBatchUnivariate(self):\n    with self.test_session() as sess:\n      num_components = 3\n      batch_shape = []\n      dist = make_univariate_mixture(\n          batch_shape=batch_shape, num_components=num_components)\n      n = 4\n      with _test_capture_normal_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      self.assertEqual((4,), samples.get_shape())\n      cat_samples = dist.cat.sample(n, seed=123)\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples])\n      self.assertEqual((4,), sample_values.shape)\n\n      for c in range(num_components):\n        which_c = np.where(cat_sample_values == c)[0]\n        size_c = which_c.size\n        # Scalar Batch univariate case: batch_size == 1, rank 1\n        which_dist_samples = dist_sample_values[c][:size_c]\n        self.assertAllClose(which_dist_samples, sample_values[which_c])\n\n  # Test that sampling with the same seed twice gives the same results.\n  def testSampleMultipleTimes(self):\n    # 5 component mixture.\n    logits = [-10.0, -5.0, 0.0, 5.0, 10.0]\n    mus = [-5.0, 0.0, 5.0, 4.0, 20.0]\n    sigmas = [0.1, 5.0, 3.0, 0.2, 4.0]\n\n    with self.test_session():\n      n = 100\n\n      random_seed.set_random_seed(654321)\n      components = [\n          distributions_py.Normal(\n              loc=mu, scale=sigma) for mu, sigma in zip(mus, sigmas)\n      ]\n      cat = distributions_py.Categorical(\n          logits, dtype=dtypes.int32, name=\"cat1\")\n      dist1 = distributions_py.Mixture(cat, components, name=\"mixture1\")\n      samples1 = dist1.sample(n, seed=123456).eval()\n\n      random_seed.set_random_seed(654321)\n      components2 = [\n          distributions_py.Normal(\n              loc=mu, scale=sigma) for mu, sigma in zip(mus, sigmas)\n      ]\n      cat2 = distributions_py.Categorical(\n          logits, dtype=dtypes.int32, name=\"cat2\")\n      dist2 = distributions_py.Mixture(cat2, components2, name=\"mixture2\")\n      samples2 = dist2.sample(n, seed=123456).eval()\n\n      self.assertAllClose(samples1, samples2)\n\n  def testSampleScalarBatchMultivariate(self):\n    with self.test_session() as sess:\n      num_components = 3\n      dist = make_multivariate_mixture(\n          batch_shape=[], num_components=num_components, event_shape=[2])\n      n = 4\n      with _test_capture_mvndiag_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      self.assertEqual((4, 2), samples.get_shape())\n      cat_samples = dist.cat.sample(n, seed=123)\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples])\n      self.assertEqual((4, 2), sample_values.shape)\n      for c in range(num_components):\n        which_c = np.where(cat_sample_values == c)[0]\n        size_c = which_c.size\n        # Scalar Batch multivariate case: batch_size == 1, rank 2\n        which_dist_samples = dist_sample_values[c][:size_c, :]\n        self.assertAllClose(which_dist_samples, sample_values[which_c, :])\n\n  def testSampleBatchUnivariate(self):\n    with self.test_session() as sess:\n      num_components = 3\n      dist = make_univariate_mixture(\n          batch_shape=[2, 3], num_components=num_components)\n      n = 4\n      with _test_capture_normal_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      self.assertEqual((4, 2, 3), samples.get_shape())\n      cat_samples = dist.cat.sample(n, seed=123)\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples])\n      self.assertEqual((4, 2, 3), sample_values.shape)\n      for c in range(num_components):\n        which_c_s, which_c_b0, which_c_b1 = np.where(cat_sample_values == c)\n        size_c = which_c_s.size\n        # Batch univariate case: batch_size == [2, 3], rank 3\n        which_dist_samples = dist_sample_values[c][range(size_c), which_c_b0,\n                                                   which_c_b1]\n        self.assertAllClose(which_dist_samples,\n                            sample_values[which_c_s, which_c_b0, which_c_b1])\n\n  def _testSampleBatchMultivariate(self, fully_known_batch_shape):\n    with self.test_session() as sess:\n      num_components = 3\n      if fully_known_batch_shape:\n        batch_shape = [2, 3]\n        batch_shape_tensor = [2, 3]\n      else:\n        batch_shape = [None, 3]\n        batch_shape_tensor = array_ops.placeholder(dtype=dtypes.int32)\n\n      dist = make_multivariate_mixture(\n          batch_shape=batch_shape,\n          num_components=num_components, event_shape=[4],\n          batch_shape_tensor=batch_shape_tensor)\n      n = 5\n      with _test_capture_mvndiag_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      if fully_known_batch_shape:\n        self.assertEqual((5, 2, 3, 4), samples.get_shape())\n      else:\n        self.assertEqual([5, None, 3, 4], samples.get_shape().as_list())\n      cat_samples = dist.cat.sample(n, seed=123)\n      if fully_known_batch_shape:\n        feed_dict = {}\n      else:\n        feed_dict = {batch_shape_tensor: [2, 3]}\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples],\n          feed_dict=feed_dict)\n      self.assertEqual((5, 2, 3, 4), sample_values.shape)\n\n      for c in range(num_components):\n        which_c_s, which_c_b0, which_c_b1 = np.where(cat_sample_values == c)\n        size_c = which_c_s.size\n        # Batch univariate case: batch_size == [2, 3], rank 4 (multivariate)\n        which_dist_samples = dist_sample_values[c][range(size_c), which_c_b0,\n                                                   which_c_b1, :]\n        self.assertAllClose(which_dist_samples,\n                            sample_values[which_c_s, which_c_b0, which_c_b1, :])\n\n  def testSampleBatchMultivariateFullyKnownBatchShape(self):\n    self._testSampleBatchMultivariate(fully_known_batch_shape=True)\n\n  def testSampleBatchMultivariateNotFullyKnownBatchShape(self):\n    self._testSampleBatchMultivariate(fully_known_batch_shape=False)\n\n  def testEntropyLowerBoundMultivariate(self):\n    with self.test_session() as sess:\n      for batch_shape in ((), (2,), (2, 3)):\n        dist = make_multivariate_mixture(\n            batch_shape=batch_shape, num_components=2, event_shape=(4,))\n        entropy_lower_bound = dist.entropy_lower_bound()\n        self.assertEqual(batch_shape, entropy_lower_bound.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_entropy = [d.entropy() for d in dist.components]\n\n        entropy_lower_bound_value, cat_probs_value, dist_entropy_value = (\n            sess.run([entropy_lower_bound, cat_probs, dist_entropy]))\n        self.assertEqual(batch_shape, entropy_lower_bound_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n\n        # entropy_lower_bound = sum_i pi_i entropy_i\n        # for i in num_components, batchwise.\n        true_entropy_lower_bound = sum(\n            [c_p * m for (c_p, m) in zip(cat_probs_value, dist_entropy_value)])\n\n        self.assertAllClose(true_entropy_lower_bound, entropy_lower_bound_value)\n\n  def testCdfScalarUnivariate(self):\n    \"\"\"Tests CDF against scipy for a mixture of seven gaussians.\"\"\"\n    # Construct a mixture of gaussians with seven components.\n    n_components = 7\n\n    # pre-softmax mixture probabilities.\n    mixture_weight_logits = np.random.uniform(\n        low=-1, high=1, size=(n_components,)).astype(np.float32)\n\n    def _scalar_univariate_softmax(x):\n      e_x = np.exp(x - np.max(x))\n      return e_x / e_x.sum()\n\n    # Construct the distributions_py.Mixture object.\n    mixture_weights = _scalar_univariate_softmax(mixture_weight_logits)\n    means = [np.random.uniform(low=-10, high=10, size=()).astype(np.float32)\n             for _ in range(n_components)]\n    sigmas = [np.ones(shape=(), dtype=np.float32) for _ in range(n_components)]\n    cat_tf = distributions_py.Categorical(probs=mixture_weights)\n    components_tf = [distributions_py.Normal(loc=mu, scale=sigma)\n                     for (mu, sigma) in zip(means, sigmas)]\n    mixture_tf = distributions_py.Mixture(cat=cat_tf, components=components_tf)\n\n    x_tensor = array_ops.placeholder(shape=(), dtype=dtypes.float32)\n\n    # These are two test cases to verify.\n    xs_to_check = [\n        np.array(1.0, dtype=np.float32),\n        np.array(np.random.randn()).astype(np.float32)\n    ]\n\n    # Carry out the test for both d.cdf and exp(d.log_cdf).\n    x_cdf_tf = mixture_tf.cdf(x_tensor)\n    x_log_cdf_tf = mixture_tf.log_cdf(x_tensor)\n\n    with self.test_session() as sess:\n      for x_feed in xs_to_check:\n        x_cdf_tf_result, x_log_cdf_tf_result = sess.run(\n            [x_cdf_tf, x_log_cdf_tf], feed_dict={x_tensor: x_feed})\n\n        # Compute the cdf with scipy.\n        scipy_component_cdfs = [stats.norm.cdf(x=x_feed, loc=mu, scale=sigma)\n                                for (mu, sigma) in zip(means, sigmas)]\n        scipy_cdf_result = np.dot(mixture_weights,\n                                  np.array(scipy_component_cdfs))\n        self.assertAllClose(x_cdf_tf_result, scipy_cdf_result)\n        self.assertAllClose(np.exp(x_log_cdf_tf_result), scipy_cdf_result)\n\n  def testCdfBatchUnivariate(self):\n    \"\"\"Tests against scipy for a (batch of) mixture(s) of seven gaussians.\"\"\"\n    n_components = 7\n    batch_size = 5\n    mixture_weight_logits = np.random.uniform(\n        low=-1, high=1, size=(batch_size, n_components)).astype(np.float32)\n\n    def _batch_univariate_softmax(x):\n      e_x = np.exp(x)\n      e_x_sum = np.expand_dims(np.sum(e_x, axis=1), axis=1)\n      return e_x / np.tile(e_x_sum, reps=[1, x.shape[1]])\n\n    psize = (batch_size,)\n    mixture_weights = _batch_univariate_softmax(mixture_weight_logits)\n    means = [np.random.uniform(low=-10, high=10, size=psize).astype(np.float32)\n             for _ in range(n_components)]\n    sigmas = [np.ones(shape=psize, dtype=np.float32)\n              for _ in range(n_components)]\n    cat_tf = distributions_py.Categorical(probs=mixture_weights)\n    components_tf = [distributions_py.Normal(loc=mu, scale=sigma)\n                     for (mu, sigma) in zip(means, sigmas)]\n    mixture_tf = distributions_py.Mixture(cat=cat_tf, components=components_tf)\n\n    x_tensor = array_ops.placeholder(shape=psize, dtype=dtypes.float32)\n    xs_to_check = [\n        np.array([1.0, 5.9, -3, 0.0, 0.0], dtype=np.float32),\n        np.random.randn(batch_size).astype(np.float32)\n    ]\n\n    x_cdf_tf = mixture_tf.cdf(x_tensor)\n    x_log_cdf_tf = mixture_tf.log_cdf(x_tensor)\n\n    with self.test_session() as sess:\n      for x_feed in xs_to_check:\n        x_cdf_tf_result, x_log_cdf_tf_result = sess.run(\n            [x_cdf_tf, x_log_cdf_tf],\n            feed_dict={x_tensor: x_feed})\n\n        # Compute the cdf with scipy.\n        scipy_component_cdfs = [stats.norm.cdf(x=x_feed, loc=mu, scale=sigma)\n                                for (mu, sigma) in zip(means, sigmas)]\n        weights_and_cdfs = zip(np.transpose(mixture_weights, axes=[1, 0]),\n                               scipy_component_cdfs)\n        final_cdf_probs_per_component = [\n            np.multiply(c_p_value, d_cdf_value)\n            for (c_p_value, d_cdf_value) in weights_and_cdfs]\n        scipy_cdf_result = np.sum(final_cdf_probs_per_component, axis=0)\n        self.assertAllClose(x_cdf_tf_result, scipy_cdf_result)\n        self.assertAllClose(np.exp(x_log_cdf_tf_result), scipy_cdf_result)\n\n\nclass MixtureBenchmark(test.Benchmark):\n\n  def _runSamplingBenchmark(self, name, create_distribution, use_gpu,\n                            num_components, batch_size, num_features,\n                            sample_size):\n    config = config_pb2.ConfigProto()\n    config.allow_soft_placement = True\n    np.random.seed(127)\n    with session.Session(config=config, graph=ops.Graph()) as sess:\n      random_seed.set_random_seed(0)\n      with ops.device(\"/gpu:0\" if use_gpu else \"/cpu:0\"):\n        mixture = create_distribution(\n            num_components=num_components,\n            batch_size=batch_size,\n            num_features=num_features)\n        sample_op = mixture.sample(sample_size).op\n        sess.run(variables.global_variables_initializer())\n        reported = self.run_op_benchmark(\n            sess,\n            sample_op,\n            min_iters=10,\n            name=(\"%s_%s_components_%d_batch_%d_features_%d_sample_%d\" %\n                  (name, use_gpu, num_components, batch_size, num_features,\n                   sample_size)))\n        logging.vlog(2, \"\\t\".join([\"%s\", \"%d\", \"%d\", \"%d\", \"%d\", \"%g\"]) % (\n            use_gpu, num_components, batch_size, num_features, sample_size,\n            reported[\"wall_time\"]))\n\n  def benchmarkSamplingMVNDiag(self):\n    logging.vlog(\n        2, \"mvn_diag\\tuse_gpu\\tcomponents\\tbatch\\tfeatures\\tsample\\twall_time\")\n\n    def create_distribution(batch_size, num_components, num_features):\n      cat = distributions_py.Categorical(\n          logits=np.random.randn(batch_size, num_components))\n      mus = [\n          variables.Variable(np.random.randn(batch_size, num_features))\n          for _ in range(num_components)\n      ]\n      sigmas = [\n          variables.Variable(np.random.rand(batch_size, num_features))\n          for _ in range(num_components)\n      ]\n      components = list(\n          distributions_py.MultivariateNormalDiag(\n              loc=mu, scale_diag=sigma) for (mu, sigma) in zip(mus, sigmas))\n      return distributions_py.Mixture(cat, components)\n\n    for use_gpu in False, True:\n      if use_gpu and not test.is_gpu_available():\n        continue\n      for num_components in 1, 8, 16:\n        for batch_size in 1, 32:\n          for num_features in 1, 64, 512:\n            for sample_size in 1, 32, 128:\n              self._runSamplingBenchmark(\n                  \"mvn_diag\",\n                  create_distribution=create_distribution,\n                  use_gpu=use_gpu,\n                  num_components=num_components,\n                  batch_size=batch_size,\n                  num_features=num_features,\n                  sample_size=sample_size)\n\n  def benchmarkSamplingMVNFull(self):\n    logging.vlog(\n        2, \"mvn_full\\tuse_gpu\\tcomponents\\tbatch\\tfeatures\\tsample\\twall_time\")\n\n    def psd(x):\n      \"\"\"Construct batch-wise PSD matrices.\"\"\"\n      return np.stack([np.dot(np.transpose(z), z) for z in x])\n\n    def create_distribution(batch_size, num_components, num_features):\n      cat = distributions_py.Categorical(\n          logits=np.random.randn(batch_size, num_components))\n      mus = [\n          variables.Variable(np.random.randn(batch_size, num_features))\n          for _ in range(num_components)\n      ]\n      sigmas = [\n          variables.Variable(\n              psd(np.random.rand(batch_size, num_features, num_features)))\n          for _ in range(num_components)\n      ]\n      components = list(\n          distributions_py.MultivariateNormalTriL(\n              loc=mu, scale_tril=linalg_ops.cholesky(sigma))\n          for (mu, sigma) in zip(mus, sigmas))\n      return distributions_py.Mixture(cat, components)\n\n    for use_gpu in False, True:\n      if use_gpu and not test.is_gpu_available():\n        continue\n      for num_components in 1, 8, 16:\n        for batch_size in 1, 32:\n          for num_features in 1, 64, 512:\n            for sample_size in 1, 32, 128:\n              self._runSamplingBenchmark(\n                  \"mvn_full\",\n                  create_distribution=create_distribution,\n                  use_gpu=use_gpu,\n                  num_components=num_components,\n                  batch_size=batch_size,\n                  num_features=num_features,\n                  sample_size=sample_size)\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "framework": "tensorflow"}
{"repo_name": "suiyuan2009/tensorflow", "file_path": "tensorflow/contrib/distributions/python/kernel_tests/mixture_test.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Mixture distribution.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\n\nimport numpy as np\nfrom scipy import stats\n\nfrom tensorflow.contrib import distributions\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import random_seed\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging as logging\n\ndistributions_py = distributions\n\n\ndef _swap_first_last_axes(array):\n  rank = len(array.shape)\n  transpose = [rank - 1] + list(range(0, rank - 1))\n  return array.transpose(transpose)\n\n\n@contextlib.contextmanager\ndef _test_capture_mvndiag_sample_outputs():\n  \"\"\"Use monkey-patching to capture the output of an MVNDiag _sample_n.\"\"\"\n  data_container = []\n  true_mvndiag_sample_n = distributions_py.MultivariateNormalDiag._sample_n\n\n  def _capturing_mvndiag_sample_n(self, n, seed=None):\n    samples = true_mvndiag_sample_n(self, n=n, seed=seed)\n    data_container.append(samples)\n    return samples\n\n  distributions_py.MultivariateNormalDiag._sample_n = (\n      _capturing_mvndiag_sample_n)\n  yield data_container\n  distributions_py.MultivariateNormalDiag._sample_n = true_mvndiag_sample_n\n\n\n@contextlib.contextmanager\ndef _test_capture_normal_sample_outputs():\n  \"\"\"Use monkey-patching to capture the output of an Normal _sample_n.\"\"\"\n  data_container = []\n  true_normal_sample_n = distributions_py.Normal._sample_n\n\n  def _capturing_normal_sample_n(self, n, seed=None):\n    samples = true_normal_sample_n(self, n=n, seed=seed)\n    data_container.append(samples)\n    return samples\n\n  distributions_py.Normal._sample_n = _capturing_normal_sample_n\n  yield data_container\n  distributions_py.Normal._sample_n = true_normal_sample_n\n\n\ndef make_univariate_mixture(batch_shape, num_components):\n  batch_shape = ops.convert_to_tensor(batch_shape, dtypes.int32)\n  logits = random_ops.random_uniform(\n      array_ops.concat((batch_shape, [num_components]), axis=0),\n      -1, 1, dtype=dtypes.float32) - 50.\n  components = [\n      distributions_py.Normal(\n          loc=random_ops.random_normal(batch_shape),\n          scale=10 * random_ops.random_uniform(batch_shape))\n      for _ in range(num_components)\n  ]\n  cat = distributions_py.Categorical(logits, dtype=dtypes.int32)\n  return distributions_py.Mixture(cat, components)\n\n\ndef make_multivariate_mixture(batch_shape, num_components, event_shape,\n                              batch_shape_tensor=None):\n  if batch_shape_tensor is None:\n    batch_shape_tensor = batch_shape\n  batch_shape_tensor = ops.convert_to_tensor(batch_shape_tensor, dtypes.int32)\n  logits = random_ops.random_uniform(\n      array_ops.concat((batch_shape_tensor, [num_components]), 0),\n      -1, 1, dtype=dtypes.float32) - 50.\n  logits.set_shape(\n      tensor_shape.TensorShape(batch_shape).concatenate(num_components))\n  static_batch_and_event_shape = (\n      tensor_shape.TensorShape(batch_shape).concatenate(event_shape))\n  event_shape = ops.convert_to_tensor(event_shape, dtypes.int32)\n  batch_and_event_shape = array_ops.concat((batch_shape_tensor, event_shape), 0)\n  def create_component():\n    loc = random_ops.random_normal(batch_and_event_shape)\n    scale_diag = 10 * random_ops.random_uniform(batch_and_event_shape)\n    loc.set_shape(static_batch_and_event_shape)\n    scale_diag.set_shape(static_batch_and_event_shape)\n    return distributions_py.MultivariateNormalDiag(\n        loc=loc, scale_diag=scale_diag)\n  components = [create_component() for _ in range(num_components)]\n  cat = distributions_py.Categorical(logits, dtype=dtypes.int32)\n  return distributions_py.Mixture(cat, components)\n\n\nclass MixtureTest(test.TestCase):\n\n  def testShapes(self):\n    with self.test_session():\n      for batch_shape in ([], [1], [2, 3, 4]):\n        dist = make_univariate_mixture(batch_shape, num_components=10)\n        self.assertAllEqual(batch_shape, dist.batch_shape)\n        self.assertAllEqual(batch_shape, dist.batch_shape_tensor().eval())\n        self.assertAllEqual([], dist.event_shape)\n        self.assertAllEqual([], dist.event_shape_tensor().eval())\n\n        for event_shape in ([1], [2]):\n          dist = make_multivariate_mixture(\n              batch_shape, num_components=10, event_shape=event_shape)\n          self.assertAllEqual(batch_shape, dist.batch_shape)\n          self.assertAllEqual(batch_shape, dist.batch_shape_tensor().eval())\n          self.assertAllEqual(event_shape, dist.event_shape)\n          self.assertAllEqual(event_shape, dist.event_shape_tensor().eval())\n\n  def testBrokenShapesStatic(self):\n    with self.assertRaisesWithPredicateMatch(ValueError,\n                                             r\"cat.num_classes != len\"):\n      distributions_py.Mixture(\n          distributions_py.Categorical([0.1, 0.5]),  # 2 classes\n          [distributions_py.Normal(loc=1.0, scale=2.0)])\n    with self.assertRaisesWithPredicateMatch(\n        ValueError, r\"\\(\\) and \\(2,\\) are not compatible\"):\n      # The value error is raised because the batch shapes of the\n      # Normals are not equal.  One is a scalar, the other is a\n      # vector of size (2,).\n      distributions_py.Mixture(\n          distributions_py.Categorical([-0.5, 0.5]),  # scalar batch\n          [\n              distributions_py.Normal(\n                  loc=1.0, scale=2.0),  # scalar dist\n              distributions_py.Normal(\n                  loc=[1.0, 1.0], scale=[2.0, 2.0])\n          ])\n    with self.assertRaisesWithPredicateMatch(ValueError, r\"Could not infer\"):\n      cat_logits = array_ops.placeholder(shape=[1, None], dtype=dtypes.float32)\n      distributions_py.Mixture(\n          distributions_py.Categorical(cat_logits),\n          [distributions_py.Normal(\n              loc=[1.0], scale=[2.0])])\n\n  def testBrokenShapesDynamic(self):\n    with self.test_session():\n      d0_param = array_ops.placeholder(dtype=dtypes.float32)\n      d1_param = array_ops.placeholder(dtype=dtypes.float32)\n      d = distributions_py.Mixture(\n          distributions_py.Categorical([0.1, 0.2]), [\n              distributions_py.Normal(\n                  loc=d0_param, scale=d0_param), distributions_py.Normal(\n                      loc=d1_param, scale=d1_param)\n          ],\n          validate_args=True)\n      with self.assertRaisesOpError(r\"batch shape must match\"):\n        d.sample().eval(feed_dict={d0_param: [2.0, 3.0], d1_param: [1.0]})\n      with self.assertRaisesOpError(r\"batch shape must match\"):\n        d.sample().eval(feed_dict={d0_param: [2.0, 3.0], d1_param: 1.0})\n\n  def testBrokenTypes(self):\n    with self.assertRaisesWithPredicateMatch(TypeError, \"Categorical\"):\n      distributions_py.Mixture(None, [])\n    cat = distributions_py.Categorical([0.3, 0.2])\n    # components must be a list of distributions\n    with self.assertRaisesWithPredicateMatch(\n        TypeError, \"all .* must be Distribution instances\"):\n      distributions_py.Mixture(cat, [None])\n    with self.assertRaisesWithPredicateMatch(TypeError, \"same dtype\"):\n      distributions_py.Mixture(\n          cat, [\n              distributions_py.Normal(loc=[1.0], scale=[2.0]),\n              distributions_py.Normal(loc=[np.float16(1.0)],\n                                      scale=[np.float16(2.0)]),\n          ])\n    with self.assertRaisesWithPredicateMatch(ValueError, \"non-empty list\"):\n      distributions_py.Mixture(distributions_py.Categorical([0.3, 0.2]), None)\n\n    # TODO(ebrevdo): once distribution Domains have been added, add a\n    # test to ensure that the domains of the distributions in a\n    # mixture are checked for equivalence.\n\n  def testMeanUnivariate(self):\n    with self.test_session() as sess:\n      for batch_shape in ((), (2,), (2, 3)):\n        dist = make_univariate_mixture(\n            batch_shape=batch_shape, num_components=2)\n        mean = dist.mean()\n        self.assertEqual(batch_shape, mean.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_means = [d.mean() for d in dist.components]\n\n        mean_value, cat_probs_value, dist_means_value = sess.run(\n            [mean, cat_probs, dist_means])\n        self.assertEqual(batch_shape, mean_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n        true_mean = sum(\n            [c_p * m for (c_p, m) in zip(cat_probs_value, dist_means_value)])\n\n        self.assertAllClose(true_mean, mean_value)\n\n  def testMeanMultivariate(self):\n    with self.test_session() as sess:\n      for batch_shape in ((), (2,), (2, 3)):\n        dist = make_multivariate_mixture(\n            batch_shape=batch_shape, num_components=2, event_shape=(4,))\n        mean = dist.mean()\n        self.assertEqual(batch_shape + (4,), mean.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_means = [d.mean() for d in dist.components]\n\n        mean_value, cat_probs_value, dist_means_value = sess.run(\n            [mean, cat_probs, dist_means])\n        self.assertEqual(batch_shape + (4,), mean_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n\n        # Add a new innermost dimension for broadcasting to mvn vector shape\n        cat_probs_value = [np.expand_dims(c_p, -1) for c_p in cat_probs_value]\n\n        true_mean = sum(\n            [c_p * m for (c_p, m) in zip(cat_probs_value, dist_means_value)])\n\n        self.assertAllClose(true_mean, mean_value)\n\n  def testProbScalarUnivariate(self):\n    with self.test_session() as sess:\n      dist = make_univariate_mixture(batch_shape=[], num_components=2)\n      for x in [\n          np.array(\n              [1.0, 2.0], dtype=np.float32), np.array(\n                  1.0, dtype=np.float32),\n          np.random.randn(3, 4).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n\n        self.assertEqual(x.shape, p_x.get_shape())\n        cat_probs = nn_ops.softmax([dist.cat.logits])[0]\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n        self.assertEqual(x.shape, p_x_value.shape)\n\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testProbScalarMultivariate(self):\n    with self.test_session() as sess:\n      dist = make_multivariate_mixture(\n          batch_shape=[], num_components=2, event_shape=[3])\n      for x in [\n          np.array(\n              [[-1.0, 0.0, 1.0], [0.5, 1.0, -0.3]], dtype=np.float32), np.array(\n                  [-1.0, 0.0, 1.0], dtype=np.float32),\n          np.random.randn(2, 2, 3).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n\n        self.assertEqual(x.shape[:-1], p_x.get_shape())\n\n        cat_probs = nn_ops.softmax([dist.cat.logits])[0]\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n\n        self.assertEqual(x.shape[:-1], p_x_value.shape)\n\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testProbBatchUnivariate(self):\n    with self.test_session() as sess:\n      dist = make_univariate_mixture(batch_shape=[2, 3], num_components=2)\n\n      for x in [\n          np.random.randn(2, 3).astype(np.float32),\n          np.random.randn(4, 2, 3).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n        self.assertEqual(x.shape, p_x.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n        self.assertEqual(x.shape, p_x_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testProbBatchMultivariate(self):\n    with self.test_session() as sess:\n      dist = make_multivariate_mixture(\n          batch_shape=[2, 3], num_components=2, event_shape=[4])\n\n      for x in [\n          np.random.randn(2, 3, 4).astype(np.float32),\n          np.random.randn(4, 2, 3, 4).astype(np.float32)\n      ]:\n        p_x = dist.prob(x)\n        self.assertEqual(x.shape[:-1], p_x.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_probs = [d.prob(x) for d in dist.components]\n\n        p_x_value, cat_probs_value, dist_probs_value = sess.run(\n            [p_x, cat_probs, dist_probs])\n        self.assertEqual(x.shape[:-1], p_x_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n        total_prob = sum(c_p_value * d_p_value\n                         for (c_p_value, d_p_value\n                             ) in zip(cat_probs_value, dist_probs_value))\n\n        self.assertAllClose(total_prob, p_x_value)\n\n  def testSampleScalarBatchUnivariate(self):\n    with self.test_session() as sess:\n      num_components = 3\n      batch_shape = []\n      dist = make_univariate_mixture(\n          batch_shape=batch_shape, num_components=num_components)\n      n = 4\n      with _test_capture_normal_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      self.assertEqual((4,), samples.get_shape())\n      cat_samples = dist.cat.sample(n, seed=123)\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples])\n      self.assertEqual((4,), sample_values.shape)\n\n      for c in range(num_components):\n        which_c = np.where(cat_sample_values == c)[0]\n        size_c = which_c.size\n        # Scalar Batch univariate case: batch_size == 1, rank 1\n        which_dist_samples = dist_sample_values[c][:size_c]\n        self.assertAllClose(which_dist_samples, sample_values[which_c])\n\n  # Test that sampling with the same seed twice gives the same results.\n  def testSampleMultipleTimes(self):\n    # 5 component mixture.\n    logits = [-10.0, -5.0, 0.0, 5.0, 10.0]\n    mus = [-5.0, 0.0, 5.0, 4.0, 20.0]\n    sigmas = [0.1, 5.0, 3.0, 0.2, 4.0]\n\n    with self.test_session():\n      n = 100\n\n      random_seed.set_random_seed(654321)\n      components = [\n          distributions_py.Normal(\n              loc=mu, scale=sigma) for mu, sigma in zip(mus, sigmas)\n      ]\n      cat = distributions_py.Categorical(\n          logits, dtype=dtypes.int32, name=\"cat1\")\n      dist1 = distributions_py.Mixture(cat, components, name=\"mixture1\")\n      samples1 = dist1.sample(n, seed=123456).eval()\n\n      random_seed.set_random_seed(654321)\n      components2 = [\n          distributions_py.Normal(\n              loc=mu, scale=sigma) for mu, sigma in zip(mus, sigmas)\n      ]\n      cat2 = distributions_py.Categorical(\n          logits, dtype=dtypes.int32, name=\"cat2\")\n      dist2 = distributions_py.Mixture(cat2, components2, name=\"mixture2\")\n      samples2 = dist2.sample(n, seed=123456).eval()\n\n      self.assertAllClose(samples1, samples2)\n\n  def testSampleScalarBatchMultivariate(self):\n    with self.test_session() as sess:\n      num_components = 3\n      dist = make_multivariate_mixture(\n          batch_shape=[], num_components=num_components, event_shape=[2])\n      n = 4\n      with _test_capture_mvndiag_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      self.assertEqual((4, 2), samples.get_shape())\n      cat_samples = dist.cat.sample(n, seed=123)\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples])\n      self.assertEqual((4, 2), sample_values.shape)\n      for c in range(num_components):\n        which_c = np.where(cat_sample_values == c)[0]\n        size_c = which_c.size\n        # Scalar Batch multivariate case: batch_size == 1, rank 2\n        which_dist_samples = dist_sample_values[c][:size_c, :]\n        self.assertAllClose(which_dist_samples, sample_values[which_c, :])\n\n  def testSampleBatchUnivariate(self):\n    with self.test_session() as sess:\n      num_components = 3\n      dist = make_univariate_mixture(\n          batch_shape=[2, 3], num_components=num_components)\n      n = 4\n      with _test_capture_normal_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      self.assertEqual((4, 2, 3), samples.get_shape())\n      cat_samples = dist.cat.sample(n, seed=123)\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples])\n      self.assertEqual((4, 2, 3), sample_values.shape)\n      for c in range(num_components):\n        which_c_s, which_c_b0, which_c_b1 = np.where(cat_sample_values == c)\n        size_c = which_c_s.size\n        # Batch univariate case: batch_size == [2, 3], rank 3\n        which_dist_samples = dist_sample_values[c][range(size_c), which_c_b0,\n                                                   which_c_b1]\n        self.assertAllClose(which_dist_samples,\n                            sample_values[which_c_s, which_c_b0, which_c_b1])\n\n  def _testSampleBatchMultivariate(self, fully_known_batch_shape):\n    with self.test_session() as sess:\n      num_components = 3\n      if fully_known_batch_shape:\n        batch_shape = [2, 3]\n        batch_shape_tensor = [2, 3]\n      else:\n        batch_shape = [None, 3]\n        batch_shape_tensor = array_ops.placeholder(dtype=dtypes.int32)\n\n      dist = make_multivariate_mixture(\n          batch_shape=batch_shape,\n          num_components=num_components, event_shape=[4],\n          batch_shape_tensor=batch_shape_tensor)\n      n = 5\n      with _test_capture_mvndiag_sample_outputs() as component_samples:\n        samples = dist.sample(n, seed=123)\n      self.assertEqual(samples.dtype, dtypes.float32)\n      if fully_known_batch_shape:\n        self.assertEqual((5, 2, 3, 4), samples.get_shape())\n      else:\n        self.assertEqual([5, None, 3, 4], samples.get_shape().as_list())\n      cat_samples = dist.cat.sample(n, seed=123)\n      if fully_known_batch_shape:\n        feed_dict = {}\n      else:\n        feed_dict = {batch_shape_tensor: [2, 3]}\n      sample_values, cat_sample_values, dist_sample_values = sess.run(\n          [samples, cat_samples, component_samples],\n          feed_dict=feed_dict)\n      self.assertEqual((5, 2, 3, 4), sample_values.shape)\n\n      for c in range(num_components):\n        which_c_s, which_c_b0, which_c_b1 = np.where(cat_sample_values == c)\n        size_c = which_c_s.size\n        # Batch univariate case: batch_size == [2, 3], rank 4 (multivariate)\n        which_dist_samples = dist_sample_values[c][range(size_c), which_c_b0,\n                                                   which_c_b1, :]\n        self.assertAllClose(which_dist_samples,\n                            sample_values[which_c_s, which_c_b0, which_c_b1, :])\n\n  def testSampleBatchMultivariateFullyKnownBatchShape(self):\n    self._testSampleBatchMultivariate(fully_known_batch_shape=True)\n\n  def testSampleBatchMultivariateNotFullyKnownBatchShape(self):\n    self._testSampleBatchMultivariate(fully_known_batch_shape=False)\n\n  def testEntropyLowerBoundMultivariate(self):\n    with self.test_session() as sess:\n      for batch_shape in ((), (2,), (2, 3)):\n        dist = make_multivariate_mixture(\n            batch_shape=batch_shape, num_components=2, event_shape=(4,))\n        entropy_lower_bound = dist.entropy_lower_bound()\n        self.assertEqual(batch_shape, entropy_lower_bound.get_shape())\n\n        cat_probs = nn_ops.softmax(dist.cat.logits)\n        dist_entropy = [d.entropy() for d in dist.components]\n\n        entropy_lower_bound_value, cat_probs_value, dist_entropy_value = (\n            sess.run([entropy_lower_bound, cat_probs, dist_entropy]))\n        self.assertEqual(batch_shape, entropy_lower_bound_value.shape)\n\n        cat_probs_value = _swap_first_last_axes(cat_probs_value)\n\n        # entropy_lower_bound = sum_i pi_i entropy_i\n        # for i in num_components, batchwise.\n        true_entropy_lower_bound = sum(\n            [c_p * m for (c_p, m) in zip(cat_probs_value, dist_entropy_value)])\n\n        self.assertAllClose(true_entropy_lower_bound, entropy_lower_bound_value)\n\n  def testCdfScalarUnivariate(self):\n    \"\"\"Tests CDF against scipy for a mixture of seven gaussians.\"\"\"\n    # Construct a mixture of gaussians with seven components.\n    n_components = 7\n\n    # pre-softmax mixture probabilities.\n    mixture_weight_logits = np.random.uniform(\n        low=-1, high=1, size=(n_components,)).astype(np.float32)\n\n    def _scalar_univariate_softmax(x):\n      e_x = np.exp(x - np.max(x))\n      return e_x / e_x.sum()\n\n    # Construct the distributions_py.Mixture object.\n    mixture_weights = _scalar_univariate_softmax(mixture_weight_logits)\n    means = [np.random.uniform(low=-10, high=10, size=()).astype(np.float32)\n             for _ in range(n_components)]\n    sigmas = [np.ones(shape=(), dtype=np.float32) for _ in range(n_components)]\n    cat_tf = distributions_py.Categorical(probs=mixture_weights)\n    components_tf = [distributions_py.Normal(loc=mu, scale=sigma)\n                     for (mu, sigma) in zip(means, sigmas)]\n    mixture_tf = distributions_py.Mixture(cat=cat_tf, components=components_tf)\n\n    x_tensor = array_ops.placeholder(shape=(), dtype=dtypes.float32)\n\n    # These are two test cases to verify.\n    xs_to_check = [\n        np.array(1.0, dtype=np.float32),\n        np.array(np.random.randn()).astype(np.float32)\n    ]\n\n    # Carry out the test for both d.cdf and exp(d.log_cdf).\n    x_cdf_tf = mixture_tf.cdf(x_tensor)\n    x_log_cdf_tf = mixture_tf.log_cdf(x_tensor)\n\n    with self.test_session() as sess:\n      for x_feed in xs_to_check:\n        x_cdf_tf_result, x_log_cdf_tf_result = sess.run(\n            [x_cdf_tf, x_log_cdf_tf], feed_dict={x_tensor: x_feed})\n\n        # Compute the cdf with scipy.\n        scipy_component_cdfs = [stats.norm.cdf(x=x_feed, loc=mu, scale=sigma)\n                                for (mu, sigma) in zip(means, sigmas)]\n        scipy_cdf_result = np.dot(mixture_weights,\n                                  np.array(scipy_component_cdfs))\n        self.assertAllClose(x_cdf_tf_result, scipy_cdf_result)\n        self.assertAllClose(np.exp(x_log_cdf_tf_result), scipy_cdf_result)\n\n  def testCdfBatchUnivariate(self):\n    \"\"\"Tests against scipy for a (batch of) mixture(s) of seven gaussians.\"\"\"\n    n_components = 7\n    batch_size = 5\n    mixture_weight_logits = np.random.uniform(\n        low=-1, high=1, size=(batch_size, n_components)).astype(np.float32)\n\n    def _batch_univariate_softmax(x):\n      e_x = np.exp(x)\n      e_x_sum = np.expand_dims(np.sum(e_x, axis=1), axis=1)\n      return e_x / np.tile(e_x_sum, reps=[1, x.shape[1]])\n\n    psize = (batch_size,)\n    mixture_weights = _batch_univariate_softmax(mixture_weight_logits)\n    means = [np.random.uniform(low=-10, high=10, size=psize).astype(np.float32)\n             for _ in range(n_components)]\n    sigmas = [np.ones(shape=psize, dtype=np.float32)\n              for _ in range(n_components)]\n    cat_tf = distributions_py.Categorical(probs=mixture_weights)\n    components_tf = [distributions_py.Normal(loc=mu, scale=sigma)\n                     for (mu, sigma) in zip(means, sigmas)]\n    mixture_tf = distributions_py.Mixture(cat=cat_tf, components=components_tf)\n\n    x_tensor = array_ops.placeholder(shape=psize, dtype=dtypes.float32)\n    xs_to_check = [\n        np.array([1.0, 5.9, -3, 0.0, 0.0], dtype=np.float32),\n        np.random.randn(batch_size).astype(np.float32)\n    ]\n\n    x_cdf_tf = mixture_tf.cdf(x_tensor)\n    x_log_cdf_tf = mixture_tf.log_cdf(x_tensor)\n\n    with self.test_session() as sess:\n      for x_feed in xs_to_check:\n        x_cdf_tf_result, x_log_cdf_tf_result = sess.run(\n            [x_cdf_tf, x_log_cdf_tf],\n            feed_dict={x_tensor: x_feed})\n\n        # Compute the cdf with scipy.\n        scipy_component_cdfs = [stats.norm.cdf(x=x_feed, loc=mu, scale=sigma)\n                                for (mu, sigma) in zip(means, sigmas)]\n        weights_and_cdfs = zip(np.transpose(mixture_weights, axes=[1, 0]),\n                               scipy_component_cdfs)\n        final_cdf_probs_per_component = [\n            np.multiply(c_p_value, d_cdf_value)\n            for (c_p_value, d_cdf_value) in weights_and_cdfs]\n        scipy_cdf_result = np.sum(final_cdf_probs_per_component, axis=0)\n        self.assertAllClose(x_cdf_tf_result, scipy_cdf_result)\n        self.assertAllClose(np.exp(x_log_cdf_tf_result), scipy_cdf_result)\n\n\nclass MixtureBenchmark(test.Benchmark):\n\n  def _runSamplingBenchmark(self, name, create_distribution, use_gpu,\n                            num_components, batch_size, num_features,\n                            sample_size):\n    config = config_pb2.ConfigProto()\n    config.allow_soft_placement = True\n    np.random.seed(127)\n    with session.Session(config=config, graph=ops.Graph()) as sess:\n      random_seed.set_random_seed(0)\n      with ops.device(\"/gpu:0\" if use_gpu else \"/cpu:0\"):\n        mixture = create_distribution(\n            num_components=num_components,\n            batch_size=batch_size,\n            num_features=num_features)\n        sample_op = mixture.sample(sample_size).op\n        sess.run(variables.global_variables_initializer())\n        reported = self.run_op_benchmark(\n            sess,\n            sample_op,\n            min_iters=10,\n            name=(\"%s_%s_components_%d_batch_%d_features_%d_sample_%d\" %\n                  (name, use_gpu, num_components, batch_size, num_features,\n                   sample_size)))\n        logging.vlog(2, \"\\t\".join([\"%s\", \"%d\", \"%d\", \"%d\", \"%d\", \"%g\"]) % (\n            use_gpu, num_components, batch_size, num_features, sample_size,\n            reported[\"wall_time\"]))\n\n  def benchmarkSamplingMVNDiag(self):\n    logging.vlog(\n        2, \"mvn_diag\\tuse_gpu\\tcomponents\\tbatch\\tfeatures\\tsample\\twall_time\")\n\n    def create_distribution(batch_size, num_components, num_features):\n      cat = distributions_py.Categorical(\n          logits=np.random.randn(batch_size, num_components))\n      mus = [\n          variables.Variable(np.random.randn(batch_size, num_features))\n          for _ in range(num_components)\n      ]\n      sigmas = [\n          variables.Variable(np.random.rand(batch_size, num_features))\n          for _ in range(num_components)\n      ]\n      components = list(\n          distributions_py.MultivariateNormalDiag(\n              loc=mu, scale_diag=sigma) for (mu, sigma) in zip(mus, sigmas))\n      return distributions_py.Mixture(cat, components)\n\n    for use_gpu in False, True:\n      if use_gpu and not test.is_gpu_available():\n        continue\n      for num_components in 1, 8, 16:\n        for batch_size in 1, 32:\n          for num_features in 1, 64, 512:\n            for sample_size in 1, 32, 128:\n              self._runSamplingBenchmark(\n                  \"mvn_diag\",\n                  create_distribution=create_distribution,\n                  use_gpu=use_gpu,\n                  num_components=num_components,\n                  batch_size=batch_size,\n                  num_features=num_features,\n                  sample_size=sample_size)\n\n  def benchmarkSamplingMVNFull(self):\n    logging.vlog(\n        2, \"mvn_full\\tuse_gpu\\tcomponents\\tbatch\\tfeatures\\tsample\\twall_time\")\n\n    def psd(x):\n      \"\"\"Construct batch-wise PSD matrices.\"\"\"\n      return np.stack([np.dot(np.transpose(z), z) for z in x])\n\n    def create_distribution(batch_size, num_components, num_features):\n      cat = distributions_py.Categorical(\n          logits=np.random.randn(batch_size, num_components))\n      mus = [\n          variables.Variable(np.random.randn(batch_size, num_features))\n          for _ in range(num_components)\n      ]\n      sigmas = [\n          variables.Variable(\n              psd(np.random.rand(batch_size, num_features, num_features)))\n          for _ in range(num_components)\n      ]\n      components = list(\n          distributions_py.MultivariateNormalTriL(\n              loc=mu, scale_tril=linalg_ops.cholesky(sigma))\n          for (mu, sigma) in zip(mus, sigmas))\n      return distributions_py.Mixture(cat, components)\n\n    for use_gpu in False, True:\n      if use_gpu and not test.is_gpu_available():\n        continue\n      for num_components in 1, 8, 16:\n        for batch_size in 1, 32:\n          for num_features in 1, 64, 512:\n            for sample_size in 1, 32, 128:\n              self._runSamplingBenchmark(\n                  \"mvn_full\",\n                  create_distribution=create_distribution,\n                  use_gpu=use_gpu,\n                  num_components=num_components,\n                  batch_size=batch_size,\n                  num_features=num_features,\n                  sample_size=sample_size)\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "framework": "tensorflow"}
{"repo_name": "nanditav/15712-TensorFlow", "file_path": "tensorflow/contrib/grid_rnn/python/kernel_tests/grid_rnn_test.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for GridRNN cells.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass GridRNNCellTest(tf.test.TestCase):\n\n  def testGrid2BasicLSTMCell(self):\n    with self.test_session() as sess:\n      with tf.variable_scope(\n          'root', initializer=tf.constant_initializer(0.2)) as root_scope:\n        x = tf.zeros([1, 3])\n        m = tf.zeros([1, 8])\n        cell = tf.contrib.grid_rnn.Grid2BasicLSTMCell(2)\n        self.assertEqual(cell.state_size, 8)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (1, 2))\n        self.assertEqual(s.get_shape(), (1, 8))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run(\n            [g, s], {x: np.array([[1., 1., 1.]]),\n                     m: np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]])})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 8))\n        self.assertAllClose(res[0], [[0.36617181, 0.36617181]])\n        self.assertAllClose(res[1], [[0.71053141, 0.71053141, 0.36617181,\n                                      0.36617181, 0.72320831, 0.80555487,\n                                      0.39102408, 0.42150158]])\n\n        # emulate a loop through the input sequence,\n        # where we call cell() multiple times\n        root_scope.reuse_variables()\n        g2, s2 = cell(x, m)\n        self.assertEqual(g2.get_shape(), (1, 2))\n        self.assertEqual(s2.get_shape(), (1, 8))\n\n        res = sess.run([g2, s2], {x: np.array([[2., 2., 2.]]), m: res[1]})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 8))\n        self.assertAllClose(res[0], [[0.58847463, 0.58847463]])\n        self.assertAllClose(res[1], [[1.40469193, 1.40469193, 0.58847463,\n                                      0.58847463, 0.97726452, 1.04626071,\n                                      0.4927212, 0.51137757]])\n\n  def testGrid2BasicLSTMCellTied(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.2)):\n        x = tf.zeros([1, 3])\n        m = tf.zeros([1, 8])\n        cell = tf.contrib.grid_rnn.Grid2BasicLSTMCell(2, tied=True)\n        self.assertEqual(cell.state_size, 8)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (1, 2))\n        self.assertEqual(s.get_shape(), (1, 8))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run(\n            [g, s], {x: np.array([[1., 1., 1.]]),\n                     m: np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]])})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 8))\n        self.assertAllClose(res[0], [[0.36617181, 0.36617181]])\n        self.assertAllClose(res[1], [[0.71053141, 0.71053141, 0.36617181,\n                                      0.36617181, 0.72320831, 0.80555487,\n                                      0.39102408, 0.42150158]])\n\n        res = sess.run([g, s], {x: np.array([[1., 1., 1.]]), m: res[1]})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 8))\n        self.assertAllClose(res[0], [[0.36703536, 0.36703536]])\n        self.assertAllClose(res[1], [[0.71200621, 0.71200621, 0.36703536,\n                                      0.36703536, 0.80941606, 0.87550586,\n                                      0.40108523, 0.42199609]])\n\n  def testGrid2BasicLSTMCellWithRelu(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.2)):\n        x = tf.zeros([1, 3])\n        m = tf.zeros([1, 4])\n        cell = tf.contrib.grid_rnn.Grid2BasicLSTMCell(\n            2, tied=False, non_recurrent_fn=tf.nn.relu)\n        self.assertEqual(cell.state_size, 4)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (1, 2))\n        self.assertEqual(s.get_shape(), (1, 4))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run([g, s], {x: np.array([[1., 1., 1.]]),\n                                m: np.array([[0.1, 0.2, 0.3, 0.4]])})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 4))\n        self.assertAllClose(res[0], [[0.31667367, 0.31667367]])\n        self.assertAllClose(res[1],\n                            [[0.29530135, 0.37520045, 0.17044567, 0.21292259]])\n\n  \"\"\"LSTMCell\n  \"\"\"\n\n  def testGrid2LSTMCell(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n        x = tf.zeros([1, 3])\n        m = tf.zeros([1, 8])\n        cell = tf.contrib.grid_rnn.Grid2LSTMCell(2, use_peepholes=True)\n        self.assertEqual(cell.state_size, 8)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (1, 2))\n        self.assertEqual(s.get_shape(), (1, 8))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run(\n            [g, s], {x: np.array([[1., 1., 1.]]),\n                     m: np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]])})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 8))\n        self.assertAllClose(res[0], [[0.95686918, 0.95686918]])\n        self.assertAllClose(res[1], [[2.41515064, 2.41515064, 0.95686918,\n                                      0.95686918, 1.38917875, 1.49043763,\n                                      0.83884692, 0.86036491]])\n\n  def testGrid2LSTMCellTied(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n        x = tf.zeros([1, 3])\n        m = tf.zeros([1, 8])\n        cell = tf.contrib.grid_rnn.Grid2LSTMCell(\n            2, tied=True, use_peepholes=True)\n        self.assertEqual(cell.state_size, 8)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (1, 2))\n        self.assertEqual(s.get_shape(), (1, 8))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run(\n            [g, s], {x: np.array([[1., 1., 1.]]),\n                     m: np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]])})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 8))\n        self.assertAllClose(res[0], [[0.95686918, 0.95686918]])\n        self.assertAllClose(res[1], [[2.41515064, 2.41515064, 0.95686918,\n                                      0.95686918, 1.38917875, 1.49043763,\n                                      0.83884692, 0.86036491]])\n\n  def testGrid2LSTMCellWithRelu(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n        x = tf.zeros([1, 3])\n        m = tf.zeros([1, 4])\n        cell = tf.contrib.grid_rnn.Grid2LSTMCell(\n            2, use_peepholes=True, non_recurrent_fn=tf.nn.relu)\n        self.assertEqual(cell.state_size, 4)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (1, 2))\n        self.assertEqual(s.get_shape(), (1, 4))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run([g, s], {x: np.array([[1., 1., 1.]]),\n                                m: np.array([[0.1, 0.2, 0.3, 0.4]])})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 4))\n        self.assertAllClose(res[0], [[2.1831727, 2.1831727]])\n        self.assertAllClose(res[1],\n                            [[0.92270052, 1.02325559, 0.66159075, 0.70475441]])\n\n  \"\"\"RNNCell\n  \"\"\"\n\n  def testGrid2BasicRNNCell(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n        x = tf.zeros([2, 2])\n        m = tf.zeros([2, 4])\n        cell = tf.contrib.grid_rnn.Grid2BasicRNNCell(2)\n        self.assertEqual(cell.state_size, 4)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (2, 2))\n        self.assertEqual(s.get_shape(), (2, 4))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run(\n            [g, s], {x: np.array([[1., 1.], [2., 2.]]),\n                     m: np.array([[0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2]])})\n        self.assertEqual(res[0].shape, (2, 2))\n        self.assertEqual(res[1].shape, (2, 4))\n        self.assertAllClose(res[0], [[0.94685763, 0.94685763],\n                                     [0.99480951, 0.99480951]])\n        self.assertAllClose(res[1],\n                            [[0.94685763, 0.94685763, 0.80049908, 0.80049908],\n                             [0.99480951, 0.99480951, 0.97574311, 0.97574311]])\n\n  def testGrid2BasicRNNCellTied(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n        x = tf.zeros([2, 2])\n        m = tf.zeros([2, 4])\n        cell = tf.contrib.grid_rnn.Grid2BasicRNNCell(2, tied=True)\n        self.assertEqual(cell.state_size, 4)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (2, 2))\n        self.assertEqual(s.get_shape(), (2, 4))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run(\n            [g, s], {x: np.array([[1., 1.], [2., 2.]]),\n                     m: np.array([[0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2]])})\n        self.assertEqual(res[0].shape, (2, 2))\n        self.assertEqual(res[1].shape, (2, 4))\n        self.assertAllClose(res[0], [[0.94685763, 0.94685763],\n                                     [0.99480951, 0.99480951]])\n        self.assertAllClose(res[1],\n                            [[0.94685763, 0.94685763, 0.80049908, 0.80049908],\n                             [0.99480951, 0.99480951, 0.97574311, 0.97574311]])\n\n  def testGrid2BasicRNNCellWithRelu(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n        x = tf.zeros([1, 2])\n        m = tf.zeros([1, 2])\n        cell = tf.contrib.grid_rnn.Grid2BasicRNNCell(\n            2, non_recurrent_fn=tf.nn.relu)\n        self.assertEqual(cell.state_size, 2)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (1, 2))\n        self.assertEqual(s.get_shape(), (1, 2))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run([g, s], {x: np.array([[1., 1.]]),\n                                m: np.array([[0.1, 0.1]])})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 2))\n        self.assertAllClose(res[0], [[1.80049896, 1.80049896]])\n        self.assertAllClose(res[1], [[0.80049896, 0.80049896]])\n\n  \"\"\"1-LSTM\n  \"\"\"\n\n  def testGrid1LSTMCell(self):\n    with self.test_session() as sess:\n      with tf.variable_scope(\n          'root', initializer=tf.constant_initializer(0.5)) as root_scope:\n        x = tf.zeros([1, 3])\n        m = tf.zeros([1, 4])\n        cell = tf.contrib.grid_rnn.Grid1LSTMCell(2, use_peepholes=True)\n        self.assertEqual(cell.state_size, 4)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (1, 2))\n        self.assertEqual(s.get_shape(), (1, 4))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run([g, s], {x: np.array([[1., 1., 1.]]),\n                                m: np.array([[0.1, 0.2, 0.3, 0.4]])})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 4))\n        self.assertAllClose(res[0], [[0.91287315, 0.91287315]])\n        self.assertAllClose(res[1],\n                            [[2.26285243, 2.26285243, 0.91287315, 0.91287315]])\n\n        root_scope.reuse_variables()\n\n        x2 = tf.zeros([0, 0])\n        g2, s2 = cell(x2, m)\n        self.assertEqual(g2.get_shape(), (1, 2))\n        self.assertEqual(s2.get_shape(), (1, 4))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run([g2, s2], {m: res[1]})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 4))\n        self.assertAllClose(res[0], [[0.9032144, 0.9032144]])\n        self.assertAllClose(res[1],\n                            [[2.79966092, 2.79966092, 0.9032144, 0.9032144]])\n\n        g3, s3 = cell(x2, m)\n        self.assertEqual(g3.get_shape(), (1, 2))\n        self.assertEqual(s3.get_shape(), (1, 4))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run([g3, s3], {m: res[1]})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 4))\n        self.assertAllClose(res[0], [[0.92727238, 0.92727238]])\n        self.assertAllClose(res[1],\n                            [[3.3529923, 3.3529923, 0.92727238, 0.92727238]])\n\n  \"\"\"3-LSTM\n  \"\"\"\n\n  def testGrid3LSTMCell(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n        x = tf.zeros([1, 3])\n        m = tf.zeros([1, 12])\n        cell = tf.contrib.grid_rnn.Grid3LSTMCell(2, use_peepholes=True)\n        self.assertEqual(cell.state_size, 12)\n\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (1, 2))\n        self.assertEqual(s.get_shape(), (1, 12))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run([g, s], {x: np.array([[1., 1., 1.]]),\n                                m: np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,\n                                              0.8, -0.1, -0.2, -0.3, -0.4]])})\n        self.assertEqual(res[0].shape, (1, 2))\n        self.assertEqual(res[1].shape, (1, 12))\n\n        self.assertAllClose(res[0], [[0.96892911, 0.96892911]])\n        self.assertAllClose(res[1], [[2.45227885, 2.45227885, 0.96892911,\n                                      0.96892911, 1.33592629, 1.4373529,\n                                      0.80867189, 0.83247656, 0.7317788,\n                                      0.63205892, 0.56548983, 0.50446129]])\n\n  \"\"\"Edge cases\n  \"\"\"\n\n  def testGridRNNEdgeCasesLikeRelu(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n        x = tf.zeros([3, 2])\n        m = tf.zeros([0, 0])\n\n        # this is equivalent to relu\n        cell = tf.contrib.grid_rnn.GridRNNCell(\n            num_units=2,\n            num_dims=1,\n            input_dims=0,\n            output_dims=0,\n            non_recurrent_dims=0,\n            non_recurrent_fn=tf.nn.relu)\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (3, 2))\n        self.assertEqual(s.get_shape(), (0, 0))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run([g, s], {x: np.array([[1., -1.], [-2, 1], [2, -1]])})\n        self.assertEqual(res[0].shape, (3, 2))\n        self.assertEqual(res[1].shape, (0, 0))\n        self.assertAllClose(res[0], [[0, 0], [0, 0], [0.5, 0.5]])\n\n  def testGridRNNEdgeCasesNoOutput(self):\n    with self.test_session() as sess:\n      with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n        x = tf.zeros([1, 2])\n        m = tf.zeros([1, 4])\n\n        # This cell produces no output\n        cell = tf.contrib.grid_rnn.GridRNNCell(\n            num_units=2,\n            num_dims=2,\n            input_dims=0,\n            output_dims=None,\n            non_recurrent_dims=0,\n            non_recurrent_fn=tf.nn.relu)\n        g, s = cell(x, m)\n        self.assertEqual(g.get_shape(), (0, 0))\n        self.assertEqual(s.get_shape(), (1, 4))\n\n        sess.run([tf.global_variables_initializer()])\n        res = sess.run([g, s], {x: np.array([[1., 1.]]),\n                                m: np.array([[0.1, 0.1, 0.1, 0.1]])})\n        self.assertEqual(res[0].shape, (0, 0))\n        self.assertEqual(res[1].shape, (1, 4))\n\n  \"\"\"Test with tf.nn.rnn\n  \"\"\"\n\n  def testGrid2LSTMCellWithRNN(self):\n    batch_size = 3\n    input_size = 5\n    max_length = 6  # unrolled up to this length\n    num_units = 2\n\n    with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n      cell = tf.contrib.grid_rnn.Grid2LSTMCell(num_units=num_units)\n\n      inputs = max_length * [\n          tf.placeholder(\n              tf.float32, shape=(batch_size, input_size))\n      ]\n\n      outputs, state = tf.nn.rnn(cell, inputs, dtype=tf.float32)\n\n    self.assertEqual(len(outputs), len(inputs))\n    self.assertEqual(state.get_shape(), (batch_size, 8))\n\n    for out, inp in zip(outputs, inputs):\n      self.assertEqual(out.get_shape()[0], inp.get_shape()[0])\n      self.assertEqual(out.get_shape()[1], num_units)\n      self.assertEqual(out.dtype, inp.dtype)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      input_value = np.ones((batch_size, input_size))\n      values = sess.run(outputs + [state], feed_dict={inputs[0]: input_value})\n      for v in values:\n        self.assertTrue(np.all(np.isfinite(v)))\n\n  def testGrid2LSTMCellReLUWithRNN(self):\n    batch_size = 3\n    input_size = 5\n    max_length = 6  # unrolled up to this length\n    num_units = 2\n\n    with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n      cell = tf.contrib.grid_rnn.Grid2LSTMCell(\n          num_units=num_units, non_recurrent_fn=tf.nn.relu)\n\n      inputs = max_length * [\n          tf.placeholder(\n              tf.float32, shape=(batch_size, input_size))\n      ]\n\n      outputs, state = tf.nn.rnn(cell, inputs, dtype=tf.float32)\n\n    self.assertEqual(len(outputs), len(inputs))\n    self.assertEqual(state.get_shape(), (batch_size, 4))\n\n    for out, inp in zip(outputs, inputs):\n      self.assertEqual(out.get_shape()[0], inp.get_shape()[0])\n      self.assertEqual(out.get_shape()[1], num_units)\n      self.assertEqual(out.dtype, inp.dtype)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      input_value = np.ones((batch_size, input_size))\n      values = sess.run(outputs + [state], feed_dict={inputs[0]: input_value})\n      for v in values:\n        self.assertTrue(np.all(np.isfinite(v)))\n\n  def testGrid3LSTMCellReLUWithRNN(self):\n    batch_size = 3\n    input_size = 5\n    max_length = 6  # unrolled up to this length\n    num_units = 2\n\n    with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n      cell = tf.contrib.grid_rnn.Grid3LSTMCell(\n          num_units=num_units, non_recurrent_fn=tf.nn.relu)\n\n      inputs = max_length * [\n          tf.placeholder(\n              tf.float32, shape=(batch_size, input_size))\n      ]\n\n      outputs, state = tf.nn.rnn(cell, inputs, dtype=tf.float32)\n\n    self.assertEqual(len(outputs), len(inputs))\n    self.assertEqual(state.get_shape(), (batch_size, 8))\n\n    for out, inp in zip(outputs, inputs):\n      self.assertEqual(out.get_shape()[0], inp.get_shape()[0])\n      self.assertEqual(out.get_shape()[1], num_units)\n      self.assertEqual(out.dtype, inp.dtype)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      input_value = np.ones((batch_size, input_size))\n      values = sess.run(outputs + [state], feed_dict={inputs[0]: input_value})\n      for v in values:\n        self.assertTrue(np.all(np.isfinite(v)))\n\n  def testGrid1LSTMCellWithRNN(self):\n    batch_size = 3\n    input_size = 5\n    max_length = 6  # unrolled up to this length\n    num_units = 2\n\n    with tf.variable_scope('root', initializer=tf.constant_initializer(0.5)):\n      cell = tf.contrib.grid_rnn.Grid1LSTMCell(num_units=num_units)\n\n      # for 1-LSTM, we only feed the first step\n      inputs = ([tf.placeholder(tf.float32, shape=(batch_size, input_size))]\n                + (max_length - 1) * [tf.zeros([batch_size, input_size])])\n\n      outputs, state = tf.nn.rnn(cell, inputs, dtype=tf.float32)\n\n    self.assertEqual(len(outputs), len(inputs))\n    self.assertEqual(state.get_shape(), (batch_size, 4))\n\n    for out, inp in zip(outputs, inputs):\n      self.assertEqual(out.get_shape(), (3, num_units))\n      self.assertEqual(out.dtype, inp.dtype)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      input_value = np.ones((batch_size, input_size))\n      values = sess.run(outputs + [state], feed_dict={inputs[0]: input_value})\n      for v in values:\n        self.assertTrue(np.all(np.isfinite(v)))\n\n\nif __name__ == '__main__':\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "nanditav/15712-TensorFlow", "file_path": "tensorflow/contrib/learn/python/learn/estimators/classifier_test.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Tests for Classifier.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport tempfile\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.contrib import learn\nfrom tensorflow.contrib.learn.python.learn.estimators import _sklearn\nfrom tensorflow.contrib.session_bundle import manifest_pb2\n\n\ndef iris_input_fn(num_epochs=None):\n  iris = tf.contrib.learn.datasets.load_iris()\n  features = tf.train.limit_epochs(\n      tf.reshape(tf.constant(iris.data), [-1, 4]), num_epochs=num_epochs)\n  labels = tf.reshape(tf.constant(iris.target), [-1])\n  return features, labels\n\n\ndef logistic_model_fn(features, labels, unused_mode):\n  labels = tf.one_hot(labels, 3, 1, 0)\n  prediction, loss = tf.contrib.learn.models.logistic_regression_zero_init(\n      features, labels)\n  train_op = tf.contrib.layers.optimize_loss(\n      loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n      learning_rate=0.1)\n  return prediction, loss, train_op\n\n\ndef logistic_model_params_fn(features, labels, unused_mode, params):\n  labels = tf.one_hot(labels, 3, 1, 0)\n  prediction, loss = tf.contrib.learn.models.logistic_regression_zero_init(\n      features, labels)\n  train_op = tf.contrib.layers.optimize_loss(\n      loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n      learning_rate=params['learning_rate'])\n  return prediction, loss, train_op\n\n\nclass ClassifierTest(tf.test.TestCase):\n\n  def testIrisAll(self):\n    est = tf.contrib.learn.Classifier(model_fn=logistic_model_fn, n_classes=3)\n    self._runIrisAll(est)\n\n  def testIrisAllWithParams(self):\n    est = tf.contrib.learn.Classifier(model_fn=logistic_model_params_fn,\n                                      n_classes=3,\n                                      params={'learning_rate': 0.01})\n    self._runIrisAll(est)\n\n  def testIrisInputFn(self):\n    iris = tf.contrib.learn.datasets.load_iris()\n    est = tf.contrib.learn.Classifier(model_fn=logistic_model_fn, n_classes=3)\n    est.fit(input_fn=iris_input_fn, steps=100)\n    est.evaluate(input_fn=iris_input_fn, steps=1, name='eval')\n    predict_input_fn = functools.partial(iris_input_fn, num_epochs=1)\n    predictions = list(est.predict(input_fn=predict_input_fn))\n    self.assertEqual(len(predictions), iris.target.shape[0])\n\n  def _runIrisAll(self, est):\n    iris = tf.contrib.learn.datasets.load_iris()\n    est.fit(iris.data, iris.target, steps=100)\n    scores = est.evaluate(x=iris.data, y=iris.target, name='eval')\n    predictions = list(est.predict(x=iris.data))\n    predictions_proba = list(est.predict_proba(x=iris.data))\n    self.assertEqual(len(predictions), iris.target.shape[0])\n    self.assertAllEqual(predictions, np.argmax(predictions_proba, axis=1))\n    other_score = _sklearn.accuracy_score(iris.target, predictions)\n    self.assertAllClose(other_score, scores['accuracy'])\n\n  def _get_default_signature(self, export_meta_filename):\n    \"\"\"Gets the default signature from the export.meta file.\"\"\"\n    with tf.Session():\n      save = tf.train.import_meta_graph(export_meta_filename)\n      meta_graph_def = save.export_meta_graph()\n      collection_def = meta_graph_def.collection_def\n\n      signatures_any = collection_def['serving_signatures'].any_list.value\n      self.assertEquals(len(signatures_any), 1)\n      signatures = manifest_pb2.Signatures()\n      signatures_any[0].Unpack(signatures)\n      default_signature = signatures.default_signature\n      return default_signature\n\n  # Disable this test case until b/31032996 is fixed.\n  def _testExportMonitorRegressionSignature(self):\n    iris = tf.contrib.learn.datasets.load_iris()\n    est = tf.contrib.learn.Classifier(model_fn=logistic_model_fn, n_classes=3)\n    export_dir = tempfile.mkdtemp() + 'export/'\n    export_monitor = learn.monitors.ExportMonitor(\n        every_n_steps=1,\n        export_dir=export_dir,\n        exports_to_keep=1,\n        signature_fn=tf.contrib.learn.classifier.classification_signature_fn)\n    est.fit(iris.data, iris.target, steps=2, monitors=[export_monitor])\n\n    self.assertTrue(tf.gfile.Exists(export_dir))\n    self.assertFalse(tf.gfile.Exists(export_dir + '00000000/export'))\n    self.assertTrue(tf.gfile.Exists(export_dir + '00000002/export'))\n    # Validate the signature\n    signature = self._get_default_signature(export_dir + '00000002/export.meta')\n    self.assertTrue(signature.HasField('classification_signature'))\n\n\nif __name__ == '__main__':\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "aselle/tensorflow", "file_path": "tensorflow/contrib/learn/python/learn/estimators/head.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Abstractions for the head(s) of a model (deprecated).\n\nThis module and all its submodules are deprecated. See\n[contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\nfor migration instructions.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nimport six\n\nfrom tensorflow.contrib import framework as framework_lib\nfrom tensorflow.contrib import layers as layers_lib\nfrom tensorflow.contrib.learn.python.learn.estimators import constants\nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn\nfrom tensorflow.contrib.learn.python.learn.estimators import prediction_key\nfrom tensorflow.contrib.learn.python.learn.estimators.metric_key import MetricKey as mkey\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import lookup_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import metrics as metrics_lib\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import weights_broadcast_ops\nfrom tensorflow.python.ops.losses import losses as losses_lib\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.summary import summary\nfrom tensorflow.python.training import training\nfrom tensorflow.python.util import tf_decorator\nfrom tensorflow.python.util import tf_inspect\nfrom tensorflow.python.util.deprecation import deprecated\n\n\nclass Head(object):\n  \"\"\"Interface for the head/top of a model.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Given logits (or output of a hidden layer), a Head knows how to compute\n  predictions, loss, default metric and export signature. It is meant to,\n\n  1) Simplify writing model_fn and to make model_fn more configurable\n  2) Support wide range of machine learning models. Since most heads can work\n      with logits, they can support DNN, RNN, Wide, Wide&Deep,\n      Global objectives, Gradient boosted trees and many other types\n      of machine learning models.\n  2) To allow users to seamlessly switch between 1 to n heads for multi\n  objective learning (See _MultiHead implementation for more details)\n\n  Common usage:\n  Here is simplified model_fn to build a multiclass DNN model.\n    ```python\n    def _my_dnn_model_fn(features, labels, mode, params, config=None):\n      # Optionally your callers can pass head to model_fn as a param.\n      head = tf.contrib.learn.multi_class_head(...)\n      input = tf.contrib.layers.input_from_feature_columns(features, ...)\n      last_hidden_layer_out = tf.contrib.layers.stack(\n          input, tf.contrib.layers.fully_connected, [1000, 500])\n      logits = tf.contrib.layers.fully_connected(\n          last_hidden_layer_out, head.logits_dimension, activation_fn=None)\n\n      def _train_op_fn(loss):\n        return optimizer.minimize(loss)\n\n      return head.create_model_fn_ops(\n          features=features,\n          labels=labels,\n          mode=mode,\n          train_op_fn=_train_op_fn,\n          logits=logits,\n          scope=...)\n    ```\n\n  Most heads also support logits_input which is typically the output of the last\n  hidden layer. Some heads (like heads responsible for candidate sampling or\n  hierarchical softmax) intrinsically will not support logits and you have\n  to pass logits_input. Here is a common usage,\n    ```python\n    return head.create_model_fn_ops(\n        features=features,\n        labels=labels,\n        mode=mode,\n        train_op_fn=_train_op_fn,\n        logits_input=last_hidden_layer_out,\n        scope=...)\n    ```python\n\n  There are cases where computing and applying gradients can not be meaningfully\n  captured with train_op_fn we support (for example, with sync optimizer). In\n  such case, you can take the responsibility on your own. Here is a common\n  use case,\n    ```python\n    model_fn_ops = head.create_model_fn_ops(\n        features=features,\n        labels=labels,\n        mode=mode,\n        train_op_fn=tf.contrib.learn.no_op_train_fn,\n        logits=logits,\n        scope=...)\n    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n      optimizer = ...\n      sync = tf.train.SyncReplicasOptimizer(opt=optimizer, ...)\n      update_op = tf.contrib.layers.optimize_loss(optimizer=sync,\n                                                  loss=model_fn_ops.loss, ...)\n      hooks = [sync.make_session_run_hook(is_chief)]\n      ... update train_op and hooks in ModelFnOps and return\n    ```\n  \"\"\"\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractproperty\n  def logits_dimension(self):\n    \"\"\"Size of the last dimension of the logits `Tensor`.\n\n    Typically, logits is of shape `[batch_size, logits_dimension]`.\n\n    Returns:\n      The expected size of the `logits` tensor.\n    \"\"\"\n    raise NotImplementedError(\"Calling an abstract method.\")\n\n  @abc.abstractmethod\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"Returns `ModelFnOps` that a model_fn can return.\n\n    Please note that,\n    + Exactly one of `logits` and `logits_input` must be provided.\n    + All args must be passed via name.\n\n    Args:\n      features: Input `dict` of `Tensor` objects.\n      mode: Estimator's `ModeKeys`.\n      labels: Labels `Tensor`, or `dict` of same.\n      train_op_fn: Function that takes a scalar loss `Tensor` and returns an op\n          to optimize the model with the loss. This is used in TRAIN mode and\n          must not be None. None is allowed in other modes. If you want to\n          optimize loss yourself you can pass `no_op_train_fn` and then use\n          ModeFnOps.loss to compute and apply gradients.\n      logits: logits `Tensor` to be used by the head.\n      logits_input: `Tensor` from which to build logits, often needed when you\n        don't want to compute the logits. Typically this is the activation of\n        the last hidden layer in a DNN. Some heads (like the ones responsible\n        for candidate sampling) intrinsically avoid computing full logits and\n        only accepts logits_input.\n      scope: Optional scope for `variable_scope`.\n\n    Returns:\n      An instance of `ModelFnOps`.\n\n    Raises:\n      ValueError: If `mode` is not recognized.\n      ValueError: If neither or both of `logits` and `logits_input` is provided.\n    \"\"\"\n    raise NotImplementedError(\"Calling an abstract method.\")\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef regression_head(label_name=None,\n                    weight_column_name=None,\n                    label_dimension=1,\n                    enable_centered_bias=False,\n                    head_name=None,\n                    link_fn=None):\n  \"\"\"Creates a `Head` for linear regression.\n\n  Args:\n    label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    label_dimension: Number of regression labels per example. This is the size\n      of the last dimension of the labels `Tensor` (typically, this has shape\n      `[batch_size, label_dimension]`).\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n    link_fn: link function to convert logits to predictions. If provided,\n      this link function will be used instead of identity.\n\n  Returns:\n    An instance of `Head` for linear regression.\n  \"\"\"\n  return _RegressionHead(\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      label_dimension=label_dimension,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      loss_fn=_mean_squared_loss,\n      link_fn=(link_fn if link_fn is not None else array_ops.identity))\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef poisson_regression_head(label_name=None,\n                            weight_column_name=None,\n                            label_dimension=1,\n                            enable_centered_bias=False,\n                            head_name=None):\n  \"\"\"Creates a `Head` for poisson regression.\n\n  Args:\n    label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    label_dimension: Number of regression labels per example. This is the size\n      of the last dimension of the labels `Tensor` (typically, this has shape\n      `[batch_size, label_dimension]`).\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n\n  Returns:\n    An instance of `Head` for poisson regression.\n  \"\"\"\n  return _RegressionHead(\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      label_dimension=label_dimension,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      loss_fn=_poisson_loss,\n      link_fn=math_ops.exp)\n\n# TODO(zakaria): Consider adding a _RegressionHead for logistic_regression\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef multi_class_head(n_classes,\n                     label_name=None,\n                     weight_column_name=None,\n                     enable_centered_bias=False,\n                     head_name=None,\n                     thresholds=None,\n                     metric_class_ids=None,\n                     loss_fn=None,\n                     label_keys=None):\n  \"\"\"Creates a `Head` for multi class single label classification.\n\n  The Head uses softmax cross entropy loss.\n\n  This head expects to be fed integer labels specifying the class index. But\n  if `label_keys` is specified, then labels must be strings from this\n  vocabulary, and the predicted classes will be strings from the same\n  vocabulary.\n\n  Args:\n    n_classes: Integer, number of classes, must be >= 2\n    label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n    thresholds: thresholds for eval metrics, defaults to [.5]\n    metric_class_ids: List of class IDs for which we should report per-class\n      metrics. Must all be in the range `[0, n_classes)`. Invalid if\n      `n_classes` is 2.\n    loss_fn: Optional function that takes (`labels`, `logits`, `weights`) as\n      parameter and returns a weighted scalar loss. `weights` should be\n      optional. See `tf.losses`\n    label_keys: Optional list of strings with size `[n_classes]` defining the\n      label vocabulary. Only supported for `n_classes` > 2.\n\n  Returns:\n    An instance of `Head` for multi class classification.\n\n  Raises:\n    ValueError: if `n_classes` is < 2.\n    ValueError: If `metric_class_ids` is provided when `n_classes` is 2.\n    ValueError: If `len(label_keys) != n_classes`.\n  \"\"\"\n  if (n_classes is None) or (n_classes < 2):\n    raise ValueError(\"n_classes must be > 1 for classification: %s.\" %\n                     n_classes)\n  if loss_fn:\n    _verify_loss_fn_args(loss_fn)\n\n  loss_fn = _wrap_custom_loss_fn(loss_fn) if loss_fn else None\n  if n_classes == 2:\n    if metric_class_ids:\n      raise ValueError(\"metric_class_ids invalid for n_classes==2.\")\n    if label_keys:\n      raise ValueError(\"label_keys is not supported for n_classes=2.\")\n    return _BinaryLogisticHead(\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        enable_centered_bias=enable_centered_bias,\n        head_name=head_name,\n        thresholds=thresholds,\n        loss_fn=loss_fn)\n\n  return _MultiClassHead(\n      n_classes=n_classes,\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      thresholds=thresholds,\n      metric_class_ids=metric_class_ids,\n      loss_fn=loss_fn,\n      label_keys=label_keys)\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef binary_svm_head(\n    label_name=None,\n    weight_column_name=None,\n    enable_centered_bias=False,\n    head_name=None,\n    thresholds=None,):\n  \"\"\"Creates a `Head` for binary classification with SVMs.\n\n  The head uses binary hinge loss.\n\n  Args:\n    label_name: String, name of the key in label dict. Can be null if label\n      is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n    thresholds: thresholds for eval metrics, defaults to [.5]\n\n  Returns:\n    An instance of `Head` for binary classification with SVM.\n  \"\"\"\n  return _BinarySvmHead(\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      thresholds=thresholds)\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef multi_label_head(n_classes,\n                     label_name=None,\n                     weight_column_name=None,\n                     enable_centered_bias=False,\n                     head_name=None,\n                     thresholds=None,\n                     metric_class_ids=None,\n                     loss_fn=None):\n  \"\"\"Creates a Head for multi label classification.\n\n  Multi-label classification handles the case where each example may have zero\n  or more associated labels, from a discrete set.  This is distinct from\n  `multi_class_head` which has exactly one label from a discrete set.\n\n  This head by default uses sigmoid cross entropy loss, which expects as input\n  a multi-hot tensor of shape `(batch_size, num_classes)`.\n\n  Args:\n    n_classes: Integer, number of classes, must be >= 2\n    label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n    thresholds: thresholds for eval metrics, defaults to [.5]\n    metric_class_ids: List of class IDs for which we should report per-class\n      metrics. Must all be in the range `[0, n_classes)`.\n    loss_fn: Optional function that takes (`labels`, `logits`, `weights`) as\n      parameter and returns a weighted scalar loss. `weights` should be\n      optional. See `tf.losses`\n\n  Returns:\n    An instance of `Head` for multi label classification.\n\n  Raises:\n    ValueError: If n_classes is < 2\n    ValueError: If loss_fn does not have expected signature.\n  \"\"\"\n  if n_classes < 2:\n    raise ValueError(\"n_classes must be > 1 for classification.\")\n  if loss_fn:\n    _verify_loss_fn_args(loss_fn)\n\n  return _MultiLabelHead(\n      n_classes=n_classes,\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      thresholds=thresholds,\n      metric_class_ids=metric_class_ids,\n      loss_fn=_wrap_custom_loss_fn(loss_fn) if loss_fn else None)\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef loss_only_head(loss_fn, head_name=None):\n  \"\"\"Creates a Head that contains only loss terms.\n\n  Loss only head holds additional loss terms to be added to other heads and\n  usually represents additional regularization terms in the objective function.\n\n  Args:\n    loss_fn: a function that takes no argument and returns a list of\n        scalar tensors.\n    head_name: a name for the head.\n\n  Returns:\n    An instance of `Head` to hold the additional losses.\n  \"\"\"\n  return _LossOnlyHead(loss_fn, head_name=head_name)\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef multi_head(heads, loss_weights=None):\n  \"\"\"Creates a MultiHead stemming from same logits/hidden layer.\n\n  Args:\n    heads: list of Head objects.\n    loss_weights: optional list of weights to be used to merge losses from\n        each head. All losses are weighted equally if not provided.\n\n  Returns:\n    A instance of `Head` that merges multiple heads.\n\n  Raises:\n    ValueError: if heads and loss_weights have different size.\n  \"\"\"\n  if loss_weights:\n    if len(loss_weights) != len(heads):\n      raise ValueError(\"heads and loss_weights must have same size\")\n\n  def _weighted_loss_merger(losses):\n    if loss_weights:\n      if len(losses) != len(loss_weights):\n        raise ValueError(\"losses and loss_weights must have same size\")\n      weighted_losses = []\n      for loss, weight in zip(losses, loss_weights):\n        weighted_losses.append(math_ops.multiply(loss, weight))\n      return math_ops.add_n(weighted_losses)\n    else:\n      return math_ops.add_n(losses)\n\n  return _MultiHead(heads, loss_merger=_weighted_loss_merger)\n\n\n@deprecated(None, \"Use 'lambda _: tf.no_op()'.\")\ndef no_op_train_fn(loss):\n  del loss\n  return control_flow_ops.no_op()\n\n\nclass _SingleHead(Head):\n  \"\"\"Interface for a single head/top of a model.\"\"\"\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(\n      self, problem_type, logits_dimension, label_name=None,\n      weight_column_name=None, head_name=None):\n    if problem_type is None:\n      raise ValueError(\"Invalid problem_type %s.\" % problem_type)\n    if logits_dimension is None or logits_dimension < 1:\n      raise ValueError(\"Invalid logits_dimension %s.\" % logits_dimension)\n    self._problem_type = problem_type\n    self._logits_dimension = logits_dimension\n    self._label_name = label_name\n    self._weight_column_name = weight_column_name\n    self._head_name = head_name\n\n  @property\n  def logits_dimension(self):\n    return self._logits_dimension\n\n  @property\n  def label_name(self):\n    return self._label_name\n\n  @property\n  def weight_column_name(self):\n    return self._weight_column_name\n\n  @property\n  def head_name(self):\n    return self._head_name\n\n  def _create_output_alternatives(self, predictions):\n    \"\"\"Creates output alternative for the Head.\n\n    Args:\n      predictions: a dict of {tensor_name: Tensor}, where 'tensor_name' is a\n        symbolic name for an output Tensor possibly but not necessarily taken\n        from `PredictionKey`, and 'Tensor' is the corresponding output Tensor\n        itself.\n\n    Returns:\n      `dict` of {submodel_name: (problem_type, {tensor_name: Tensor})}, where\n      'submodel_name' is a submodel identifier that should be consistent across\n      the pipeline (here likely taken from the head_name),\n      'problem_type' is a `ProblemType`,\n      'tensor_name' is a symbolic name for an output Tensor possibly but not\n       necessarily taken from `PredictionKey`, and\n      'Tensor' is the corresponding output Tensor itself.\n    \"\"\"\n    return {self._head_name: (self._problem_type, predictions)}\n\n\n# TODO(zakaria): use contrib losses.\ndef _mean_squared_loss(labels, logits, weights=None):\n  with ops.name_scope(None, \"mean_squared_loss\", (logits, labels)) as name:\n    logits = ops.convert_to_tensor(logits)\n    labels = ops.convert_to_tensor(labels)\n    # To prevent broadcasting inside \"-\".\n    if len(labels.get_shape()) == 1:\n      labels = array_ops.expand_dims(labels, axis=(1,))\n    # TODO(zakaria): make sure it does not recreate the broadcast bug.\n    if len(logits.get_shape()) == 1:\n      logits = array_ops.expand_dims(logits, axis=(1,))\n    logits.get_shape().assert_is_compatible_with(labels.get_shape())\n    loss = math_ops.square(logits - math_ops.to_float(labels), name=name)\n    return _compute_weighted_loss(loss, weights)\n\n\ndef _poisson_loss(labels, logits, weights=None):\n  \"\"\"Computes poisson loss from logits.\"\"\"\n  with ops.name_scope(None, \"_poisson_loss\", (logits, labels)) as name:\n    logits = ops.convert_to_tensor(logits)\n    labels = ops.convert_to_tensor(labels)\n    # To prevent broadcasting inside \"-\".\n    if len(labels.get_shape()) == 1:\n      labels = array_ops.expand_dims(labels, axis=(1,))\n    # TODO(zakaria): make sure it does not recreate the broadcast bug.\n    if len(logits.get_shape()) == 1:\n      logits = array_ops.expand_dims(logits, axis=(1,))\n    logits.get_shape().assert_is_compatible_with(labels.get_shape())\n    loss = nn.log_poisson_loss(labels, logits, compute_full_loss=True,\n                               name=name)\n    return _compute_weighted_loss(loss, weights)\n\n\ndef _logits(logits_input, logits, logits_dimension):\n  \"\"\"Validate logits args, and create `logits` if necessary.\n\n  Exactly one of `logits_input` and `logits` must be provided.\n\n  Args:\n    logits_input: `Tensor` input to `logits`.\n    logits: `Tensor` output.\n    logits_dimension: Integer, last dimension of `logits`. This is used to\n      create `logits` from `logits_input` if `logits` is `None`; otherwise, it's\n      used to validate `logits`.\n\n  Returns:\n    `logits` `Tensor`.\n\n  Raises:\n    ValueError: if neither or both of `logits` and `logits_input` are supplied.\n  \"\"\"\n  if (logits_dimension is None) or (logits_dimension < 1):\n    raise ValueError(\"Invalid logits_dimension %s.\" % logits_dimension)\n\n  # If not provided, create logits.\n  if logits is None:\n    if logits_input is None:\n      raise ValueError(\"Neither logits nor logits_input supplied.\")\n    return layers_lib.linear(logits_input, logits_dimension, scope=\"logits\")\n\n  if logits_input is not None:\n    raise ValueError(\"Both logits and logits_input supplied.\")\n\n  logits = ops.convert_to_tensor(logits, name=\"logits\")\n  logits_dims = logits.get_shape().dims\n  if logits_dims is not None:\n    logits_dims[-1].assert_is_compatible_with(logits_dimension)\n\n  return logits\n\n\ndef _create_model_fn_ops(features,\n                         mode,\n                         loss_fn,\n                         logits_to_predictions_fn,\n                         metrics_fn,\n                         create_output_alternatives_fn,\n                         labels=None,\n                         train_op_fn=None,\n                         logits=None,\n                         logits_dimension=None,\n                         head_name=None,\n                         weight_column_name=None,\n                         enable_centered_bias=False):\n  \"\"\"Returns a `ModelFnOps` object.\"\"\"\n  _check_mode_valid(mode)\n\n  centered_bias = None\n  if enable_centered_bias:\n    centered_bias = _centered_bias(logits_dimension, head_name)\n    logits = nn.bias_add(logits, centered_bias)\n\n  predictions = logits_to_predictions_fn(logits)\n  loss = None\n  train_op = None\n  eval_metric_ops = None\n  if (mode != model_fn.ModeKeys.INFER) and (labels is not None):\n    weight_tensor = _weight_tensor(features, weight_column_name)\n    loss, weighted_average_loss = loss_fn(labels, logits, weight_tensor)\n    # The name_scope escapism is needed to maintain the same summary tag\n    # after switching away from the now unsupported API.\n    with ops.name_scope(\"\"):\n      summary_loss = array_ops.identity(weighted_average_loss)\n      summary.scalar(_summary_key(head_name, mkey.LOSS), summary_loss)\n\n    if mode == model_fn.ModeKeys.TRAIN:\n      if train_op_fn is None:\n        raise ValueError(\"train_op_fn can not be None in TRAIN mode\")\n      batch_size = array_ops.shape(logits)[0]\n      train_op = _train_op(loss, labels, train_op_fn, centered_bias,\n                           batch_size, loss_fn, weight_tensor)\n    eval_metric_ops = metrics_fn(\n        weighted_average_loss, predictions, labels, weight_tensor)\n  return model_fn.ModelFnOps(\n      mode=mode,\n      predictions=predictions,\n      loss=loss,\n      train_op=train_op,\n      eval_metric_ops=eval_metric_ops,\n      output_alternatives=create_output_alternatives_fn(predictions))\n\n\nclass _RegressionHead(_SingleHead):\n  \"\"\"`Head` for regression with a generalized linear model.\"\"\"\n\n  def __init__(self,\n               label_dimension,\n               loss_fn,\n               link_fn,\n               logits_dimension=None,\n               label_name=None,\n               weight_column_name=None,\n               enable_centered_bias=False,\n               head_name=None):\n    \"\"\"`Head` for regression.\n\n    Args:\n      label_dimension: Number of regression labels per example. This is the\n        size of the last dimension of the labels `Tensor` (typically, this has\n        shape `[batch_size, label_dimension]`).\n      loss_fn: Loss function, takes logits and labels and returns loss.\n      link_fn: Link function, takes a logits tensor and returns the output.\n      logits_dimension: Number of logits per example. This is the\n        size of the last dimension of the logits `Tensor` (typically, this has\n        shape `[batch_size, label_dimension]`).\n        Default value: `label_dimension`.\n      label_name: String, name of the key in label dict. Can be null if label\n          is a tensor (single headed models).\n      weight_column_name: A string defining feature column name representing\n        weights. It is used to down weight or boost examples during training. It\n        will be multiplied by the loss of the example.\n      enable_centered_bias: A bool. If True, estimator will learn a centered\n        bias variable for each class. Rest of the model structure learns the\n        residual after centered bias.\n      head_name: name of the head. Predictions, summary and metrics keys are\n        suffixed by `\"/\" + head_name` and the default variable scope is\n        `head_name`.\n    \"\"\"\n    super(_RegressionHead, self).__init__(\n        problem_type=constants.ProblemType.LINEAR_REGRESSION,\n        logits_dimension=(logits_dimension if logits_dimension is not None\n                          else label_dimension),\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n\n    self._loss_fn = loss_fn\n    self._link_fn = link_fn\n    self._enable_centered_bias = enable_centered_bias\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"regression_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          create_output_alternatives_fn=self._create_output_alternatives,\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Applies transformations to labels tensor.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    _check_no_sparse_tensor(labels_tensor)\n    return labels_tensor\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"Returns a dict of predictions.\n\n    Args:\n      logits: logits `Tensor` after applying possible centered bias.\n\n    Returns:\n      Dict of prediction `Tensor` keyed by `PredictionKey`.\n    \"\"\"\n    key = prediction_key.PredictionKey.SCORES\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      if self.logits_dimension == 1:\n        logits = array_ops.squeeze(logits, axis=(1,), name=key)\n      return {key: self._link_fn(logits)}\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"Returns a dict of metrics keyed by name.\"\"\"\n    del predictions, labels, weights  # Unused by this head.\n    with ops.name_scope(\"metrics\", values=[eval_loss]):\n      return {\n          _summary_key(self.head_name, mkey.LOSS):\n              metrics_lib.mean(eval_loss)}\n\n\ndef _log_loss_with_two_classes(labels, logits, weights=None):\n  with ops.name_scope(None, \"log_loss_with_two_classes\",\n                      (logits, labels)) as name:\n    logits = ops.convert_to_tensor(logits)\n    labels = math_ops.to_float(labels)\n    # TODO(ptucker): This will break for dynamic shapes.\n    # sigmoid_cross_entropy_with_logits requires [batch_size, 1] labels.\n    if len(labels.get_shape()) == 1:\n      labels = array_ops.expand_dims(labels, axis=(1,))\n    loss = nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits,\n                                                name=name)\n    return _compute_weighted_loss(loss, weights)\n\n\ndef _one_class_to_two_class_logits(logits):\n  return array_ops.concat((array_ops.zeros_like(logits), logits), 1)\n\n\nclass _BinaryLogisticHead(_SingleHead):\n  \"\"\"`Head` for binary classification with logistic regression.\"\"\"\n\n  def __init__(self,\n               label_name=None,\n               weight_column_name=None,\n               enable_centered_bias=False,\n               head_name=None,\n               loss_fn=None,\n               thresholds=None):\n    \"\"\"`Head` for binary classification with logistic regression.\n\n    Args:\n      label_name: String, name of the key in label dict. Can be `None` if label\n          is a tensor (single headed models).\n      weight_column_name: A string defining feature column name representing\n        weights. It is used to down weight or boost examples during training. It\n        will be multiplied by the loss of the example.\n      enable_centered_bias: A bool. If True, estimator will learn a centered\n        bias variable for each class. Rest of the model structure learns the\n        residual after centered bias.\n      head_name: name of the head. Predictions, summary, metrics keys are\n        suffixed by `\"/\" + head_name` and the default variable scope is\n        `head_name`.\n      loss_fn: Loss function.\n      thresholds: thresholds for eval.\n\n    Raises:\n      ValueError: if n_classes is invalid.\n    \"\"\"\n    super(_BinaryLogisticHead, self).__init__(\n        problem_type=constants.ProblemType.LOGISTIC_REGRESSION,\n        logits_dimension=1,\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n    self._thresholds = thresholds if thresholds else (.5,)\n    self._loss_fn = loss_fn if loss_fn else _log_loss_with_two_classes\n    self._enable_centered_bias = enable_centered_bias\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"binary_logistic_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          create_output_alternatives_fn=_classification_output_alternatives(\n              self.head_name, self._problem_type),\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Applies transformations to labels tensor.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    _check_no_sparse_tensor(labels_tensor)\n    return labels_tensor\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"Returns a dict of predictions.\n\n    Args:\n      logits: logits `Output` after applying possible centered bias.\n\n    Returns:\n      Dict of prediction `Output` keyed by `PredictionKey`.\n    \"\"\"\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      two_class_logits = _one_class_to_two_class_logits(logits)\n      return {\n          prediction_key.PredictionKey.LOGITS:\n              logits,\n          prediction_key.PredictionKey.LOGISTIC:\n              math_ops.sigmoid(\n                  logits, name=prediction_key.PredictionKey.LOGISTIC),\n          prediction_key.PredictionKey.PROBABILITIES:\n              nn.softmax(\n                  two_class_logits,\n                  name=prediction_key.PredictionKey.PROBABILITIES),\n          prediction_key.PredictionKey.CLASSES:\n              math_ops.argmax(\n                  two_class_logits,\n                  1,\n                  name=prediction_key.PredictionKey.CLASSES)\n      }\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"Returns a dict of metrics keyed by name.\"\"\"\n    with ops.name_scope(\"metrics\", values=(\n        [eval_loss, labels, weights] + list(six.itervalues(predictions)))):\n      classes = predictions[prediction_key.PredictionKey.CLASSES]\n      logistic = predictions[prediction_key.PredictionKey.LOGISTIC]\n\n      metrics = {_summary_key(self.head_name, mkey.LOSS):\n                 metrics_lib.mean(eval_loss)}\n      # TODO(b/29366811): This currently results in both an \"accuracy\" and an\n      # \"accuracy/threshold_0.500000_mean\" metric for binary classification.\n      metrics[_summary_key(self.head_name, mkey.ACCURACY)] = (\n          metrics_lib.accuracy(labels, classes, weights))\n      metrics[_summary_key(self.head_name, mkey.PREDICTION_MEAN)] = (\n          _predictions_streaming_mean(logistic, weights))\n      metrics[_summary_key(self.head_name, mkey.LABEL_MEAN)] = (\n          _indicator_labels_streaming_mean(labels, weights))\n\n      # Also include the streaming mean of the label as an accuracy baseline, as\n      # a reminder to users.\n      metrics[_summary_key(self.head_name, mkey.ACCURACY_BASELINE)] = (\n          _indicator_labels_streaming_mean(labels, weights))\n      metrics[_summary_key(self.head_name, mkey.AUC)] = (\n          _streaming_auc(logistic, labels, weights))\n      metrics[_summary_key(self.head_name, mkey.AUC_PR)] = (\n          _streaming_auc(logistic, labels, weights, curve=\"PR\"))\n\n      for threshold in self._thresholds:\n        metrics[_summary_key(\n            self.head_name, mkey.ACCURACY_MEAN % threshold)] = (\n                _streaming_accuracy_at_threshold(logistic, labels, weights,\n                                                 threshold))\n        # Precision for positive examples.\n        metrics[_summary_key(\n            self.head_name, mkey.PRECISION_MEAN % threshold)] = (\n                _streaming_precision_at_threshold(logistic, labels, weights,\n                                                  threshold))\n        # Recall for positive examples.\n        metrics[_summary_key(\n            self.head_name, mkey.RECALL_MEAN % threshold)] = (\n                _streaming_recall_at_threshold(logistic, labels, weights,\n                                               threshold))\n\n    return metrics\n\n\ndef _softmax_cross_entropy_loss(labels, logits, weights=None):\n  with ops.name_scope(\n      None, \"softmax_cross_entropy_loss\", (logits, labels,)) as name:\n    labels = ops.convert_to_tensor(labels)\n    # Check that we got integer for classification.\n    if not labels.dtype.is_integer:\n      raise ValueError(\"Labels dtype should be integer \"\n                       \"Instead got %s.\" % labels.dtype)\n\n    # sparse_softmax_cross_entropy_with_logits requires [batch_size] labels.\n    is_squeezed_labels = False\n    # TODO(ptucker): This will break for dynamic shapes.\n    if len(labels.get_shape()) == 2:\n      labels = array_ops.squeeze(labels, axis=(1,))\n      is_squeezed_labels = True\n\n    loss = nn.sparse_softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits, name=name)\n\n    # Restore squeezed dimension, if necessary, so loss matches weights shape.\n    if is_squeezed_labels:\n      loss = array_ops.expand_dims(loss, axis=(1,))\n\n    return _compute_weighted_loss(loss, weights)\n\n\nclass _MultiClassHead(_SingleHead):\n  \"\"\"'Head' for multi class classification.\"\"\"\n\n  def __init__(self,\n               n_classes,\n               label_name=None,\n               weight_column_name=None,\n               enable_centered_bias=False,\n               head_name=None,\n               loss_fn=None,\n               thresholds=None,\n               metric_class_ids=None,\n               label_keys=None):\n    \"\"\"'Head' for multi class classification.\n\n    This head expects to be fed integer labels specifying the class index. But\n    if `label_keys` is specified, then labels must be strings from this\n    vocabulary, and the predicted classes will be strings from the same\n    vocabulary.\n\n    Args:\n      n_classes: Number of classes, must be greater than 2 (for 2 classes, use\n        `_BinaryLogisticHead`).\n      label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n      weight_column_name: A string defining feature column name representing\n        weights. It is used to down weight or boost examples during training. It\n        will be multiplied by the loss of the example.\n      enable_centered_bias: A bool. If True, estimator will learn a centered\n        bias variable for each class. Rest of the model structure learns the\n        residual after centered bias.\n      head_name: name of the head. If provided, predictions, summary, metrics\n        keys will be suffixed by `\"/\" + head_name` and the default variable\n        scope will be `head_name`.\n      loss_fn: Loss function. Defaults to softmax cross entropy loss.\n      thresholds: thresholds for eval.\n      metric_class_ids: List of class IDs for which we should report per-class\n        metrics. Must all be in the range `[0, n_classes)`.\n      label_keys: Optional list of strings with size `[n_classes]` defining the\n        label vocabulary.\n\n    Raises:\n      ValueError: if `n_classes`, `metric_class_ids` or `label_keys` is invalid.\n    \"\"\"\n    super(_MultiClassHead, self).__init__(\n        problem_type=constants.ProblemType.CLASSIFICATION,\n        logits_dimension=n_classes,\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n\n    if (n_classes is None) or (n_classes <= 2):\n      raise ValueError(\"n_classes must be > 2: %s.\" % n_classes)\n    self._thresholds = thresholds if thresholds else (.5,)\n    self._loss_fn = loss_fn if loss_fn else _softmax_cross_entropy_loss\n    self._enable_centered_bias = enable_centered_bias\n    self._metric_class_ids = tuple([] if metric_class_ids is None else\n                                   metric_class_ids)\n    for class_id in self._metric_class_ids:\n      if (class_id < 0) or (class_id >= n_classes):\n        raise ValueError(\"Class ID %s not in [0, %s).\" % (class_id, n_classes))\n    if label_keys and len(label_keys) != n_classes:\n      raise ValueError(\"Length of label_keys must equal n_classes.\")\n    self._label_keys = label_keys\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"multi_class_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._wrapped_loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          create_output_alternatives_fn=_classification_output_alternatives(\n              self.head_name, self._problem_type, self._label_keys),\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Returns a dict that contains both the original labels and label IDs.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    _check_no_sparse_tensor(labels_tensor)\n    if self._label_keys:\n      table = lookup_ops.index_table_from_tensor(\n          self._label_keys, name=\"label_id_lookup\")\n      return {\n          \"labels\": labels_tensor,\n          \"label_ids\": table.lookup(labels_tensor),\n      }\n    return {\n        \"labels\": labels_tensor,\n        \"label_ids\": labels_tensor,\n    }\n\n  def _labels(self, labels_dict):\n    \"\"\"Returns labels `Tensor` of the same type as classes.\"\"\"\n    return labels_dict[\"labels\"]\n\n  def _label_ids(self, labels_dict):\n    \"\"\"Returns integer label ID `Tensor`.\"\"\"\n    return labels_dict[\"label_ids\"]\n\n  def _wrapped_loss_fn(self, labels, logits, weights=None):\n    return self._loss_fn(self._label_ids(labels), logits, weights=weights)\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"Returns a dict of predictions.\n\n    Args:\n      logits: logits `Tensor` after applying possible centered bias.\n\n    Returns:\n      Dict of prediction `Tensor` keyed by `PredictionKey`.\n    \"\"\"\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      class_ids = math_ops.argmax(\n          logits, 1, name=prediction_key.PredictionKey.CLASSES)\n      if self._label_keys:\n        table = lookup_ops.index_to_string_table_from_tensor(\n            self._label_keys, name=\"class_string_lookup\")\n        classes = table.lookup(class_ids)\n      else:\n        classes = class_ids\n      return {\n          prediction_key.PredictionKey.LOGITS: logits,\n          prediction_key.PredictionKey.PROBABILITIES:\n              nn.softmax(\n                  logits, name=prediction_key.PredictionKey.PROBABILITIES),\n          prediction_key.PredictionKey.CLASSES: classes\n      }\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"Returns a dict of metrics keyed by name.\"\"\"\n    with ops.name_scope(\n        \"metrics\",\n        values=((eval_loss, self._labels(labels), self._label_ids(labels),\n                 weights) + tuple(six.itervalues(predictions)))):\n      logits = predictions[prediction_key.PredictionKey.LOGITS]\n      probabilities = predictions[prediction_key.PredictionKey.PROBABILITIES]\n      classes = predictions[prediction_key.PredictionKey.CLASSES]\n\n      metrics = {_summary_key(self.head_name, mkey.LOSS):\n                 metrics_lib.mean(eval_loss)}\n      # TODO(b/29366811): This currently results in both an \"accuracy\" and an\n      # \"accuracy/threshold_0.500000_mean\" metric for binary classification.\n      metrics[_summary_key(self.head_name, mkey.ACCURACY)] = (\n          metrics_lib.accuracy(self._labels(labels), classes, weights))\n\n      if not self._label_keys:\n        # Classes are IDs. Add some metrics.\n        for class_id in self._metric_class_ids:\n          metrics[_summary_key(\n              self.head_name, mkey.CLASS_PREDICTION_MEAN % class_id)] = (\n                  _class_predictions_streaming_mean(classes, weights, class_id))\n          # TODO(ptucker): Add per-class accuracy, precision, recall.\n          metrics[_summary_key(\n              self.head_name, mkey.CLASS_LABEL_MEAN % class_id)] = (\n                  _class_labels_streaming_mean(\n                      self._label_ids(labels), weights, class_id))\n          metrics[_summary_key(\n              self.head_name, mkey.CLASS_PROBABILITY_MEAN % class_id)] = (\n                  _predictions_streaming_mean(probabilities, weights, class_id))\n          metrics[_summary_key(\n              self.head_name, mkey.CLASS_LOGITS_MEAN % class_id)] = (\n                  _predictions_streaming_mean(logits, weights, class_id))\n\n    return metrics\n\n\ndef _to_labels_tensor(labels, label_name):\n  \"\"\"Returns label as a tensor.\n\n  Args:\n    labels: Label `Tensor` or `SparseTensor` or a dict containing labels.\n    label_name: Label name if labels is a dict.\n\n  Returns:\n    Label `Tensor` or `SparseTensor`.\n  \"\"\"\n  labels = labels[label_name] if isinstance(labels, dict) else labels\n  return framework_lib.convert_to_tensor_or_sparse_tensor(labels)\n\n\ndef _check_no_sparse_tensor(x):\n  \"\"\"Raises ValueError if the given tensor is `SparseTensor`.\"\"\"\n  if isinstance(x, sparse_tensor.SparseTensor):\n    raise ValueError(\"SparseTensor is not supported.\")\n\n\ndef _sparse_labels_to_indicator(labels, num_classes):\n  \"\"\"If labels is `SparseTensor`, converts it to indicator `Tensor`.\n\n  Args:\n    labels: Label `Tensor` or `SparseTensor`.\n    num_classes: Number of classes.\n\n  Returns:\n    Dense label `Tensor`.\n\n  Raises:\n    ValueError: If labels is `SparseTensor` and `num_classes` < 2.\n  \"\"\"\n  if isinstance(labels, sparse_tensor.SparseTensor):\n    if num_classes < 2:\n      raise ValueError(\"Must set num_classes >= 2 when passing labels as a \"\n                       \"SparseTensor.\")\n    return math_ops.to_int64(\n        sparse_ops.sparse_to_indicator(labels, num_classes))\n  return labels\n\n\ndef _assert_labels_rank(labels):\n  return control_flow_ops.Assert(\n      math_ops.less_equal(array_ops.rank(labels), 2),\n      (\"labels shape should be either [batch_size, 1] or [batch_size]\",))\n\n\nclass _BinarySvmHead(_SingleHead):\n  \"\"\"`Head` for binary classification using SVM.\"\"\"\n\n  def __init__(self, label_name, weight_column_name, enable_centered_bias,\n               head_name, thresholds):\n\n    def _loss_fn(labels, logits, weights=None):\n      with ops.name_scope(None, \"hinge_loss\", (logits, labels)) as name:\n        with ops.control_dependencies((_assert_labels_rank(labels),)):\n          labels = array_ops.reshape(labels, shape=(-1, 1))\n        loss = losses_lib.hinge_loss(labels=labels, logits=logits, scope=name,\n                                     reduction=losses_lib.Reduction.NONE)\n        return _compute_weighted_loss(loss, weights)\n\n    super(_BinarySvmHead, self).__init__(\n        problem_type=constants.ProblemType.LOGISTIC_REGRESSION,\n        logits_dimension=1,\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n    self._thresholds = thresholds if thresholds else (.5,)\n    self._loss_fn = _loss_fn\n    self._enable_centered_bias = enable_centered_bias\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"binary_svm_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          # TODO(zakaria): Handle labels for export.\n          create_output_alternatives_fn=self._create_output_alternatives,\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Applies transformations to labels tensor.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    _check_no_sparse_tensor(labels_tensor)\n    return labels_tensor\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"See `_MultiClassHead`.\"\"\"\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      return {\n          prediction_key.PredictionKey.LOGITS:\n              logits,\n          prediction_key.PredictionKey.CLASSES:\n              math_ops.argmax(\n                  _one_class_to_two_class_logits(logits),\n                  1,\n                  name=prediction_key.PredictionKey.CLASSES)\n      }\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"See `_MultiClassHead`.\"\"\"\n    with ops.name_scope(\"metrics\", values=(\n        [eval_loss, labels, weights] + list(six.itervalues(predictions)))):\n      metrics = {_summary_key(self.head_name, mkey.LOSS):\n                 metrics_lib.mean(eval_loss)}\n\n      # TODO(b/29366811): This currently results in both an \"accuracy\" and an\n      # \"accuracy/threshold_0.500000_mean\" metric for binary classification.\n      classes = predictions[prediction_key.PredictionKey.CLASSES]\n      metrics[_summary_key(self.head_name, mkey.ACCURACY)] = (\n          metrics_lib.accuracy(labels, classes, weights))\n      # TODO(sibyl-vie3Poto): add more metrics relevant for svms.\n\n    return metrics\n\n\nclass _MultiLabelHead(_SingleHead):\n  \"\"\"`Head` for multi-label classification.\"\"\"\n\n  # TODO(zakaria): add signature and metric for multilabel.\n  def __init__(self,\n               n_classes,\n               label_name,\n               weight_column_name,\n               enable_centered_bias,\n               head_name,\n               thresholds,\n               metric_class_ids=None,\n               loss_fn=None):\n\n    super(_MultiLabelHead, self).__init__(\n        problem_type=constants.ProblemType.CLASSIFICATION,\n        logits_dimension=n_classes,\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n\n    self._thresholds = thresholds if thresholds else (.5,)\n    self._loss_fn = loss_fn if loss_fn else _sigmoid_cross_entropy_loss\n    self._enable_centered_bias = enable_centered_bias\n    self._metric_class_ids = tuple([] if metric_class_ids is None else\n                                   metric_class_ids)\n    for class_id in self._metric_class_ids:\n      if (class_id < 0) or (class_id >= n_classes):\n        raise ValueError(\"Class ID %s not in [0, %s).\" % (class_id, n_classes))\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"multi_label_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          create_output_alternatives_fn=_classification_output_alternatives(\n              self.head_name, self._problem_type),\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Applies transformations to labels tensor.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    labels_tensor = _sparse_labels_to_indicator(labels_tensor,\n                                                self._logits_dimension)\n    return labels_tensor\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"See `_MultiClassHead`.\"\"\"\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      return {\n          prediction_key.PredictionKey.LOGITS:\n              logits,\n          prediction_key.PredictionKey.PROBABILITIES:\n              math_ops.sigmoid(\n                  logits, name=prediction_key.PredictionKey.PROBABILITIES),\n          prediction_key.PredictionKey.CLASSES:\n              math_ops.to_int64(\n                  math_ops.greater(logits, 0),\n                  name=prediction_key.PredictionKey.CLASSES)\n      }\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"Returns a dict of metrics keyed by name.\"\"\"\n    with ops.name_scope(\"metrics\", values=(\n        [eval_loss, labels, weights] + list(six.itervalues(predictions)))):\n      classes = predictions[prediction_key.PredictionKey.CLASSES]\n      probabilities = predictions[prediction_key.PredictionKey.PROBABILITIES]\n      logits = predictions[prediction_key.PredictionKey.LOGITS]\n\n      metrics = {_summary_key(self.head_name, mkey.LOSS):\n                 metrics_lib.mean(eval_loss)}\n      # TODO(b/29366811): This currently results in both an \"accuracy\" and an\n      # \"accuracy/threshold_0.500000_mean\" metric for binary classification.\n      metrics[_summary_key(self.head_name, mkey.ACCURACY)] = (\n          metrics_lib.accuracy(labels, classes, weights))\n      metrics[_summary_key(self.head_name, mkey.AUC)] = _streaming_auc(\n          probabilities, labels, weights)\n      metrics[_summary_key(self.head_name, mkey.AUC_PR)] = _streaming_auc(\n          probabilities, labels, weights, curve=\"PR\")\n\n      for class_id in self._metric_class_ids:\n        # TODO(ptucker): Add per-class accuracy, precision, recall.\n        metrics[_summary_key(\n            self.head_name, mkey.CLASS_PREDICTION_MEAN % class_id)] = (\n                _predictions_streaming_mean(classes, weights, class_id))\n        metrics[_summary_key(\n            self.head_name, mkey.CLASS_LABEL_MEAN % class_id)] = (\n                _indicator_labels_streaming_mean(labels, weights, class_id))\n        metrics[_summary_key(\n            self.head_name, mkey.CLASS_PROBABILITY_MEAN % class_id)] = (\n                _predictions_streaming_mean(probabilities, weights, class_id))\n        metrics[_summary_key(\n            self.head_name, mkey.CLASS_LOGITS_MEAN % class_id)] = (\n                _predictions_streaming_mean(logits, weights, class_id))\n        metrics[_summary_key(self.head_name, mkey.CLASS_AUC % class_id)] = (\n            _streaming_auc(probabilities, labels, weights, class_id))\n        metrics[_summary_key(self.head_name, mkey.CLASS_AUC_PR % class_id)] = (\n            _streaming_auc(probabilities, labels, weights, class_id,\n                           curve=\"PR\"))\n\n    return metrics\n\n\nclass _LossOnlyHead(Head):\n  \"\"\"`Head` implementation for additional loss terms.\n\n  This class only holds loss terms unrelated to any other heads (labels),\n  e.g. regularization.\n\n  Common usage:\n  This is oftem combine with other heads in a multi head setup.\n    ```python\n    head = multi_head([\n        head1, head2, loss_only_head('regularizer', regularizer)])\n    ```\n  \"\"\"\n\n  def __init__(self, loss_fn, head_name=None):\n    self._loss_fn = loss_fn\n    self.head_name = head_name or \"loss_only_head\"\n\n  @property\n  def logits_dimension(self):\n    return 0\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `_Head.create_model_fn_ops`.\n\n    Args:\n      features: Not been used.\n      mode: Estimator's `ModeKeys`.\n      labels: Labels `Tensor`, or `dict` of same.\n      train_op_fn: Function that takes a scalar loss and returns an op to\n          optimize with the loss.\n      logits: Not been used.\n      logits_input: Not been used.\n      scope: Optional scope for variable_scope. If provided, will be passed to\n          all heads. Most users will want to set this to `None`, so each head\n          constructs a separate variable_scope according to its `head_name`.\n\n    Returns:\n      A `ModelFnOps` object.\n\n    Raises:\n      ValueError: if `mode` is not recognition.\n    \"\"\"\n    _check_mode_valid(mode)\n    loss = None\n    train_op = None\n    if mode != model_fn.ModeKeys.INFER:\n      with variable_scope.variable_scope(scope, default_name=self.head_name):\n        loss = self._loss_fn()\n        if isinstance(loss, list):\n          loss = math_ops.add_n(loss)\n        # The name_scope escapism is needed to maintain the same summary tag\n        # after switching away from the now unsupported API.\n        with ops.name_scope(\"\"):\n          summary_loss = array_ops.identity(loss)\n          summary.scalar(_summary_key(self.head_name, mkey.LOSS),\n                         summary_loss)\n        if mode == model_fn.ModeKeys.TRAIN:\n          if train_op_fn is None:\n            raise ValueError(\"train_op_fn can not be None in TRAIN mode\")\n          with ops.name_scope(None, \"train_op\", (loss,)):\n            train_op = train_op_fn(loss)\n\n    return model_fn.ModelFnOps(\n        mode=mode,\n        loss=loss,\n        train_op=train_op,\n        predictions={},\n        eval_metric_ops={})\n\n\nclass _MultiHead(Head):\n  \"\"\"`Head` implementation for multi objective learning.\n\n  This class is responsible for using and merging the output of multiple\n  `Head` objects.\n\n  All heads stem from the same logits/logit_input tensor.\n\n  Common usage:\n  For simple use cases you can pass the activation of hidden layer like\n  this from your model_fn,\n    ```python\n    last_hidden_layer_activation = ... Build your model.\n    multi_head = ...\n    return multi_head.create_model_fn_ops(\n        ..., logits_input=last_hidden_layer_activation, ...)\n    ```\n\n  Or you can create a logits tensor of\n  [batch_size, multi_head.logits_dimension] shape. _MultiHead will split the\n  logits for you.\n    return multi_head.create_model_fn_ops(..., logits=logits, ...)\n\n  For more complex use cases like a multi-task/multi-tower model or when logits\n  for each head has to be created separately, you can pass a dict of logits\n  where the keys match the name of the single heads.\n    ```python\n    logits = {\"head1\": logits1, \"head2\": logits2}\n    return multi_head.create_model_fn_ops(..., logits=logits, ...)\n    ```\n\n  Here is what this class does,\n  + For training, merges losses of each heads according a function provided by\n      user, calls user provided train_op_fn with this final loss.\n  + For eval, merges metrics by adding head_name suffix to the keys in eval\n      metrics.\n  + For inference, updates keys in prediction dict to a 2-tuple,\n      (head_name, prediction_key)\n  \"\"\"\n\n  def __init__(self, heads, loss_merger):\n    \"\"\"_Head to merges multiple _Head objects.\n\n    Args:\n      heads: list of _Head objects.\n      loss_merger: function that takes a list of loss tensors for the heads\n        and returns the final loss tensor for the multi head.\n\n    Raises:\n      ValueError: if any head does not have a name.\n    \"\"\"\n    self._logits_dimension = 0\n    for head in heads:\n      if not head.head_name:\n        raise ValueError(\"Members of MultiHead must have names.\")\n      self._logits_dimension += head.logits_dimension\n\n    self._heads = heads\n    self._loss_merger = loss_merger\n\n  @property\n  def logits_dimension(self):\n    return self._logits_dimension\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `_Head.create_model_fn_ops`.\n\n    Args:\n      features: Input `dict` of `Tensor` objects.\n      mode: Estimator's `ModeKeys`.\n      labels: Labels `Tensor`, or `dict` of same.\n      train_op_fn: Function that takes a scalar loss and returns an op to\n          optimize with the loss.\n      logits: Concatenated logits for all heads or a dict of head name to logits\n          tensor. If concatenated logits, it should have (batchsize, x) shape\n          where x is the sum of `logits_dimension` of all the heads,\n          i.e., same as `logits_dimension` of this class. create_model_fn_ops\n          will split the logits tensor and pass logits of proper size to each\n          head. This is useful if we want to be agnostic about whether you\n          creating a single versus multihead. logits can also be a dict for\n          convenience where you are creating the head specific logits explicitly\n          and don't want to concatenate them yourself.\n      logits_input: tensor to build logits from.\n      scope: Optional scope for variable_scope. If provided, will be passed to\n        all heads. Most users will want to set this to `None`, so each head\n        constructs a separate variable_scope according to its `head_name`.\n\n    Returns:\n      `ModelFnOps`.\n\n    Raises:\n      ValueError: if `mode` is not recognized, or neither or both of `logits`\n          and `logits_input` is provided.\n    \"\"\"\n    _check_mode_valid(mode)\n    all_model_fn_ops = []\n    if logits is None:\n      # Use logits_input.\n      for head in self._heads:\n        all_model_fn_ops.append(\n            head.create_model_fn_ops(\n                features=features,\n                mode=mode,\n                labels=labels,\n                train_op_fn=no_op_train_fn,\n                logits_input=logits_input,\n                scope=scope))\n    else:\n      head_logits_pairs = []\n      if isinstance(logits, dict):\n        head_logits_pairs = []\n        for head in self._heads:\n          if isinstance(head, _LossOnlyHead):\n            head_logits_pairs.append((head, None))\n          else:\n            head_logits_pairs.append((head, logits[head.head_name]))\n      else:\n        # Split logits for each head.\n        head_logits_pairs = zip(self._heads, self._split_logits(logits))\n\n      for head, head_logits in head_logits_pairs:\n        all_model_fn_ops.append(\n            head.create_model_fn_ops(\n                features=features,\n                mode=mode,\n                labels=labels,\n                train_op_fn=no_op_train_fn,\n                logits=head_logits,\n                scope=scope))\n\n    if mode == model_fn.ModeKeys.TRAIN:\n      if train_op_fn is None:\n        raise ValueError(\"train_op_fn can not be None in TRAIN mode.\")\n      return self._merge_train(all_model_fn_ops, train_op_fn)\n    if mode == model_fn.ModeKeys.INFER:\n      return self._merge_infer(all_model_fn_ops)\n    if mode == model_fn.ModeKeys.EVAL:\n      return self._merge_eval(all_model_fn_ops)\n    raise ValueError(\"mode=%s unrecognized\" % str(mode))\n\n  def _split_logits(self, logits):\n    \"\"\"Splits logits for heads.\n\n    Args:\n      logits: the logits tensor.\n\n    Returns:\n      A list of logits for the individual heads.\n    \"\"\"\n    all_logits = []\n    begin = 0\n    for head in self._heads:\n      current_logits_size = head.logits_dimension\n      current_logits = array_ops.slice(logits, [0, begin],\n                                       [-1, current_logits_size])\n      all_logits.append(current_logits)\n      begin += current_logits_size\n    return all_logits\n\n  def _merge_train(self, all_model_fn_ops, train_op_fn):\n    \"\"\"Merges list of ModelFnOps for training.\n\n    Args:\n      all_model_fn_ops: list of ModelFnOps for the individual heads.\n      train_op_fn: Function to create train op. See `create_model_fn_ops`\n          documentation for more details.\n\n    Returns:\n      ModelFnOps that merges all heads for TRAIN.\n    \"\"\"\n    losses = []\n    metrics = {}\n    additional_train_ops = []\n    for m in all_model_fn_ops:\n      losses.append(m.loss)\n      if m.eval_metric_ops is not None:\n        for k, v in six.iteritems(m.eval_metric_ops):\n          # metrics[\"%s/%s\" % (k, head_name)] = v\n          metrics[k] = v\n      additional_train_ops.append(m.train_op)\n    loss = self._loss_merger(losses)\n\n    train_op = train_op_fn(loss)\n    train_op = control_flow_ops.group(train_op, *additional_train_ops)\n    return model_fn.ModelFnOps(\n        mode=model_fn.ModeKeys.TRAIN,\n        loss=loss,\n        train_op=train_op,\n        eval_metric_ops=metrics)\n\n  def _merge_infer(self, all_model_fn_ops):\n    \"\"\"Merges list of ModelFnOps for inference.\n\n    Args:\n      all_model_fn_ops: list of ModelFnOps for the individual heads.\n\n    Returns:\n      ModelFnOps that Merges all the heads for INFER.\n    \"\"\"\n    predictions = {}\n    output_alternatives = {}\n    for head, m in zip(self._heads, all_model_fn_ops):\n      if isinstance(head, _LossOnlyHead):\n        continue\n      head_name = head.head_name\n      output_alternatives[head_name] = m.output_alternatives[head_name]\n      for k, v in m.predictions.items():\n        predictions[(head_name, k)] = v\n\n    return model_fn.ModelFnOps(\n        mode=model_fn.ModeKeys.INFER,\n        predictions=predictions,\n        output_alternatives=output_alternatives)\n\n  def _merge_eval(self, all_model_fn_ops):\n    \"\"\"Merges list of ModelFnOps for eval.\n\n    Args:\n      all_model_fn_ops: list of ModelFnOps for the individual heads.\n\n    Returns:\n      ModelFnOps that merges all the heads for EVAL.\n    \"\"\"\n    predictions = {}\n    metrics = {}\n    losses = []\n    for head, m in zip(self._heads, all_model_fn_ops):\n      losses.append(m.loss)\n      head_name = head.head_name\n      for k, v in m.predictions.items():\n        predictions[(head_name, k)] = v\n      for k, v in m.eval_metric_ops.items():\n        # metrics[\"%s/%s\" % (k, head_name)] = v\n        metrics[k] = v\n    loss = self._loss_merger(losses)\n\n    return model_fn.ModelFnOps(\n        mode=model_fn.ModeKeys.EVAL,\n        predictions=predictions,\n        loss=loss,\n        eval_metric_ops=metrics)\n\n\ndef _weight_tensor(features, weight_column_name):\n  \"\"\"Returns weights as `Tensor` of rank 0, or at least 2.\"\"\"\n  if not weight_column_name:\n    return None\n  if weight_column_name not in features:\n    raise ValueError(\"Weights {} missing from features.\".format(\n        weight_column_name))\n  with ops.name_scope(None, \"weight_tensor\", tuple(six.itervalues(features))):\n    weight_tensor = math_ops.to_float(features[weight_column_name])\n    shape = weight_tensor.get_shape()\n    rank = shape.ndims\n    # We don't bother with expanding dims of non-staticly shaped tensors or\n    # scalars, and >1d is already in a good format.\n    if rank == 1:\n      logging.warning(\"Weights {} has shape {}, expanding to make it 2d.\".\n                      format(weight_column_name, shape))\n      return (\n          sparse_ops.sparse_reshape(weight_tensor, (-1, 1))\n          if isinstance(weight_tensor, sparse_tensor.SparseTensor) else\n          array_ops.reshape(weight_tensor, (-1, 1)))\n    return weight_tensor\n\n\n# TODO(zakaria): This function is needed for backward compatibility and should\n#   be removed when we migrate to core.\ndef _compute_weighted_loss(loss_unweighted, weight, name=\"loss\"):\n  \"\"\"Returns a tuple of (loss_train, loss_report).\n\n  loss is used for gradient descent while weighted_average_loss is used for\n  summaries to be backward compatible.\n\n  loss is different from the loss reported on the tensorboard as we\n  should respect the example weights when computing the gradient.\n\n    L = sum_{i} w_{i} * l_{i} / B\n\n  where B is the number of examples in the batch, l_{i}, w_{i} are individual\n  losses, and example weight.\n\n  Args:\n    loss_unweighted: Unweighted loss\n    weight: Weight tensor\n    name: Optional name\n\n  Returns:\n    A tuple of losses. First one for training and the second one for reporting.\n  \"\"\"\n  with ops.name_scope(name, values=(loss_unweighted, weight)) as name_scope:\n    if weight is None:\n      loss = math_ops.reduce_mean(loss_unweighted, name=name_scope)\n      return loss, loss\n    weight = weights_broadcast_ops.broadcast_weights(weight, loss_unweighted)\n    with ops.name_scope(None, \"weighted_loss\",\n                        (loss_unweighted, weight)) as name:\n      weighted_loss = math_ops.multiply(loss_unweighted, weight, name=name)\n    weighted_loss_mean = math_ops.reduce_mean(weighted_loss, name=name_scope)\n    weighted_loss_normalized = math_ops.div(\n        math_ops.reduce_sum(weighted_loss),\n        math_ops.to_float(math_ops.reduce_sum(weight)),\n        name=\"weighted_average_loss\")\n\n    return weighted_loss_mean, weighted_loss_normalized\n\n\ndef _wrap_custom_loss_fn(loss_fn):\n  def _wrapper(labels, logits, weights=None):\n    if weights is None:\n      loss = loss_fn(labels, logits)\n    else:\n      loss = loss_fn(labels, logits, weights)\n    return loss, loss\n  return _wrapper\n\n\ndef _check_mode_valid(mode):\n  \"\"\"Raises ValueError if the given mode is invalid.\"\"\"\n  if (mode != model_fn.ModeKeys.TRAIN and mode != model_fn.ModeKeys.INFER and\n      mode != model_fn.ModeKeys.EVAL):\n    raise ValueError(\"mode=%s unrecognized.\" % str(mode))\n\n\ndef _get_arguments(func):\n  \"\"\"Returns a spec of given func.\"\"\"\n  _, func = tf_decorator.unwrap(func)\n  if hasattr(func, \"__code__\"):\n    # Regular function.\n    return tf_inspect.getargspec(func)\n  elif hasattr(func, \"func\"):\n    # Partial function.\n    return _get_arguments(func.func)\n  elif hasattr(func, \"__call__\"):\n    # Callable object.\n    return _get_arguments(func.__call__)\n\n\ndef _verify_loss_fn_args(loss_fn):\n  args = _get_arguments(loss_fn).args\n  for arg_name in [\"labels\", \"logits\", \"weights\"]:\n    if arg_name not in args:\n      raise ValueError(\"Argument %s not found in loss_fn.\" % arg_name)\n\n\ndef _centered_bias(logits_dimension, head_name=None):\n  \"\"\"Returns centered_bias `Variable`.\n\n  Args:\n    logits_dimension: Last dimension of `logits`. Must be >= 1.\n    head_name: Optional name of the head.\n\n  Returns:\n    `Variable` with shape `[logits_dimension]`.\n\n  Raises:\n    ValueError: if `logits_dimension` is invalid.\n  \"\"\"\n  if (logits_dimension is None) or (logits_dimension < 1):\n    raise ValueError(\"Invalid logits_dimension %s.\" % logits_dimension)\n  # Do not create a variable with variable_scope.get_variable, because that may\n  # create a PartitionedVariable, which does not support indexing, so\n  # summary.scalar will not work.\n  centered_bias = variable_scope.variable(\n      name=\"centered_bias_weight\",\n      initial_value=array_ops.zeros(shape=(logits_dimension,)),\n      trainable=True)\n  for dim in range(logits_dimension):\n    if head_name:\n      summary.scalar(\"centered_bias/bias_%d/%s\" % (dim, head_name),\n                     centered_bias[dim])\n    else:\n      summary.scalar(\"centered_bias/bias_%d\" % dim, centered_bias[dim])\n  return centered_bias\n\n\ndef _centered_bias_step(centered_bias, batch_size, labels, loss_fn, weights):\n  \"\"\"Creates and returns training op for centered bias.\"\"\"\n  with ops.name_scope(None, \"centered_bias_step\", (labels,)) as name:\n    logits_dimension = array_ops.shape(centered_bias)[0]\n    logits = array_ops.reshape(\n        array_ops.tile(centered_bias, (batch_size,)),\n        (batch_size, logits_dimension))\n    with ops.name_scope(None, \"centered_bias\", (labels, logits)):\n      centered_bias_loss = math_ops.reduce_mean(\n          loss_fn(labels, logits, weights), name=\"training_loss\")\n  # Learn central bias by an optimizer. 0.1 is a convervative lr for a\n  # single variable.\n  return training.AdagradOptimizer(0.1).minimize(\n      centered_bias_loss, var_list=(centered_bias,), name=name)\n\n\ndef _summary_key(head_name, val):\n  return \"%s/%s\" % (val, head_name) if head_name else val\n\n\ndef _train_op(loss, labels, train_op_fn, centered_bias, batch_size, loss_fn,\n              weights):\n  \"\"\"Returns op for the training step.\"\"\"\n  if centered_bias is not None:\n    centered_bias_step = _centered_bias_step(\n        centered_bias=centered_bias,\n        batch_size=batch_size,\n        labels=labels,\n        loss_fn=loss_fn,\n        weights=weights)\n  else:\n    centered_bias_step = None\n  with ops.name_scope(None, \"train_op\", (loss, labels)):\n    train_op = train_op_fn(loss)\n    if centered_bias_step is not None:\n      train_op = control_flow_ops.group(train_op, centered_bias_step)\n    return train_op\n\n\ndef _sigmoid_cross_entropy_loss(labels, logits, weights=None):\n  with ops.name_scope(None, \"sigmoid_cross_entropy_loss\",\n                      (logits, labels)) as name:\n    # sigmoid_cross_entropy_with_logits requires [batch_size, n_classes] labels.\n    loss = nn.sigmoid_cross_entropy_with_logits(\n        labels=math_ops.to_float(labels), logits=logits, name=name)\n    return _compute_weighted_loss(loss, weights)\n\n\ndef _float_weights_or_none(weights):\n  if weights is None:\n    return None\n  with ops.name_scope(None, \"float_weights\", (weights,)) as name:\n    return math_ops.to_float(weights, name=name)\n\n\ndef _indicator_labels_streaming_mean(labels, weights=None, class_id=None):\n  labels = math_ops.to_float(labels)\n  weights = _float_weights_or_none(weights)\n  if weights is not None:\n    weights = weights_broadcast_ops.broadcast_weights(weights, labels)\n  if class_id is not None:\n    if weights is not None:\n      weights = weights[:, class_id]\n    labels = labels[:, class_id]\n  return metrics_lib.mean(labels, weights)\n\n\ndef _predictions_streaming_mean(predictions,\n                                weights=None,\n                                class_id=None):\n  predictions = math_ops.to_float(predictions)\n  weights = _float_weights_or_none(weights)\n  if weights is not None:\n    weights = weights_broadcast_ops.broadcast_weights(weights, predictions)\n  if class_id is not None:\n    if weights is not None:\n      weights = weights[:, class_id]\n    predictions = predictions[:, class_id]\n  return metrics_lib.mean(predictions, weights)\n\n\n# TODO(ptucker): Add support for SparseTensor labels.\ndef _class_id_labels_to_indicator(labels, num_classes):\n  if (num_classes is None) or (num_classes < 2):\n    raise ValueError(\"Invalid num_classes %s.\" % num_classes)\n  with ops.control_dependencies((_assert_labels_rank(labels),)):\n    labels = array_ops.reshape(labels, (-1,))\n  return array_ops.one_hot(labels, depth=num_classes, axis=-1)\n\n\ndef _class_predictions_streaming_mean(predictions, weights, class_id):\n  return metrics_lib.mean(\n      array_ops.where(\n          math_ops.equal(\n              math_ops.to_int32(class_id), math_ops.to_int32(predictions)),\n          array_ops.ones_like(predictions),\n          array_ops.zeros_like(predictions)),\n      weights=weights)\n\n\ndef _class_labels_streaming_mean(labels, weights, class_id):\n  return metrics_lib.mean(\n      array_ops.where(\n          math_ops.equal(\n              math_ops.to_int32(class_id), math_ops.to_int32(labels)),\n          array_ops.ones_like(labels), array_ops.zeros_like(labels)),\n      weights=weights)\n\n\ndef _streaming_auc(predictions, labels, weights=None, class_id=None,\n                   curve=\"ROC\"):\n  # pylint: disable=missing-docstring\n  predictions = math_ops.to_float(predictions)\n  if labels.dtype.base_dtype != dtypes.bool:\n    logging.warning(\"Casting %s labels to bool.\", labels.dtype)\n    labels = math_ops.cast(labels, dtypes.bool)\n  weights = _float_weights_or_none(weights)\n  if weights is not None:\n    weights = weights_broadcast_ops.broadcast_weights(weights, predictions)\n  if class_id is not None:\n    if weights is not None:\n      weights = weights[:, class_id]\n    predictions = predictions[:, class_id]\n    labels = labels[:, class_id]\n  return metrics_lib.auc(labels, predictions, weights, curve=curve)\n\n\ndef _assert_class_id(class_id, num_classes=None):\n  \"\"\"Average label value for class `class_id`.\"\"\"\n  if (class_id is None) or (class_id < 0):\n    raise ValueError(\"Invalid class_id %s.\" % class_id)\n  if num_classes is not None:\n    if num_classes < 2:\n      raise ValueError(\"Invalid num_classes %s.\" % num_classes)\n    if class_id >= num_classes:\n      raise ValueError(\"Invalid class_id %s.\" % class_id)\n\n\ndef _streaming_accuracy_at_threshold(predictions, labels, weights, threshold):\n  threshold_predictions = math_ops.to_float(\n      math_ops.greater_equal(predictions, threshold))\n  return metrics_lib.accuracy(labels, threshold_predictions, weights)\n\n\ndef _streaming_precision_at_threshold(predictions, labels, weights, threshold):\n  precision_tensor, update_op = metrics_lib.precision_at_thresholds(\n      labels, predictions, (threshold,), _float_weights_or_none(weights))\n  return array_ops.squeeze(precision_tensor), array_ops.squeeze(update_op)\n\n\ndef _streaming_recall_at_threshold(predictions, labels, weights, threshold):\n  precision_tensor, update_op = metrics_lib.recall_at_thresholds(\n      labels, predictions, (threshold,), _float_weights_or_none(weights))\n  return array_ops.squeeze(precision_tensor), array_ops.squeeze(update_op)\n\n\ndef _classification_output_alternatives(head_name, problem_type,\n                                        label_keys=None):\n  \"\"\"Creates a func to generate output alternatives for classification.\n\n  Servo expects classes to be a string tensor, and have the same dimensions\n  as the probabilities tensor. It should contain the labels of the corresponding\n  entries in probabilities. This function creates a new classes tensor that\n  satisfies these conditions and can be exported.\n\n  Args:\n    head_name: Name of the head.\n    problem_type: `ProblemType`\n    label_keys: Optional label keys\n\n  Returns:\n    A function to generate output alternatives.\n  \"\"\"\n  def _create_output_alternatives(predictions):\n    \"\"\"Creates output alternative for the Head.\n\n    Args:\n      predictions: a dict of {tensor_name: Tensor}, where 'tensor_name' is a\n        symbolic name for an output Tensor possibly but not necessarily taken\n        from `PredictionKey`, and 'Tensor' is the corresponding output Tensor\n        itself.\n\n    Returns:\n      `dict` of {submodel_name: (problem_type, {tensor_name: Tensor})}, where\n      'submodel_name' is a submodel identifier that should be consistent across\n      the pipeline (here likely taken from the head_name),\n      'problem_type' is a `ProblemType`,\n      'tensor_name' is a symbolic name for an output Tensor possibly but not\n       necessarily taken from `PredictionKey`, and\n      'Tensor' is the corresponding output Tensor itself.\n\n    Raises:\n      ValueError: if predictions does not have PredictionKey.PROBABILITIES key.\n    \"\"\"\n    probabilities = predictions.get(prediction_key.PredictionKey.PROBABILITIES)\n    if probabilities is None:\n      raise ValueError(\"%s missing in predictions\" %\n                       prediction_key.PredictionKey.PROBABILITIES)\n\n    with ops.name_scope(None, \"_classification_output_alternatives\",\n                        (probabilities,)):\n      batch_size = array_ops.shape(probabilities)[0]\n      if label_keys:\n        classes = array_ops.tile(\n            input=array_ops.expand_dims(input=label_keys, axis=0),\n            multiples=[batch_size, 1],\n            name=\"classes_tensor\")\n      else:\n        n = array_ops.shape(probabilities)[1]\n        classes = array_ops.tile(\n            input=array_ops.expand_dims(input=math_ops.range(n), axis=0),\n            multiples=[batch_size, 1])\n        classes = string_ops.as_string(classes, name=\"classes_tensor\")\n\n    exported_predictions = {\n        prediction_key.PredictionKey.PROBABILITIES: probabilities,\n        prediction_key.PredictionKey.CLASSES: classes}\n    return {head_name: (problem_type, exported_predictions)}\n\n  return _create_output_alternatives\n\n# Aliases\n# TODO(zakaria): Remove these aliases, See b/34751732\n_regression_head = regression_head\n_poisson_regression_head = poisson_regression_head\n_multi_class_head = multi_class_head\n_binary_svm_head = binary_svm_head\n_multi_label_head = multi_label_head\n_multi_head = multi_head\n_Head = Head\n", "framework": "tensorflow"}
{"repo_name": "AnishShah/tensorflow", "file_path": "tensorflow/contrib/learn/python/learn/estimators/head.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Abstractions for the head(s) of a model (deprecated).\n\nThis module and all its submodules are deprecated. See\n[contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\nfor migration instructions.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nimport six\n\nfrom tensorflow.contrib import framework as framework_lib\nfrom tensorflow.contrib import layers as layers_lib\nfrom tensorflow.contrib.learn.python.learn.estimators import constants\nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn\nfrom tensorflow.contrib.learn.python.learn.estimators import prediction_key\nfrom tensorflow.contrib.learn.python.learn.estimators.metric_key import MetricKey as mkey\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import lookup_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import metrics as metrics_lib\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import weights_broadcast_ops\nfrom tensorflow.python.ops.losses import losses as losses_lib\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.summary import summary\nfrom tensorflow.python.training import training\nfrom tensorflow.python.util import tf_decorator\nfrom tensorflow.python.util import tf_inspect\nfrom tensorflow.python.util.deprecation import deprecated\n\n\nclass Head(object):\n  \"\"\"Interface for the head/top of a model.\n\n  THIS CLASS IS DEPRECATED. See\n  [contrib/learn/README.md](https://www.tensorflow.org/code/tensorflow/contrib/learn/README.md)\n  for general migration instructions.\n\n  Given logits (or output of a hidden layer), a Head knows how to compute\n  predictions, loss, default metric and export signature. It is meant to,\n\n  1) Simplify writing model_fn and to make model_fn more configurable\n  2) Support wide range of machine learning models. Since most heads can work\n      with logits, they can support DNN, RNN, Wide, Wide&Deep,\n      Global objectives, Gradient boosted trees and many other types\n      of machine learning models.\n  2) To allow users to seamlessly switch between 1 to n heads for multi\n  objective learning (See _MultiHead implementation for more details)\n\n  Common usage:\n  Here is simplified model_fn to build a multiclass DNN model.\n    ```python\n    def _my_dnn_model_fn(features, labels, mode, params, config=None):\n      # Optionally your callers can pass head to model_fn as a param.\n      head = tf.contrib.learn.multi_class_head(...)\n      input = tf.contrib.layers.input_from_feature_columns(features, ...)\n      last_hidden_layer_out = tf.contrib.layers.stack(\n          input, tf.contrib.layers.fully_connected, [1000, 500])\n      logits = tf.contrib.layers.fully_connected(\n          last_hidden_layer_out, head.logits_dimension, activation_fn=None)\n\n      def _train_op_fn(loss):\n        return optimizer.minimize(loss)\n\n      return head.create_model_fn_ops(\n          features=features,\n          labels=labels,\n          mode=mode,\n          train_op_fn=_train_op_fn,\n          logits=logits,\n          scope=...)\n    ```\n\n  Most heads also support logits_input which is typically the output of the last\n  hidden layer. Some heads (like heads responsible for candidate sampling or\n  hierarchical softmax) intrinsically will not support logits and you have\n  to pass logits_input. Here is a common usage,\n    ```python\n    return head.create_model_fn_ops(\n        features=features,\n        labels=labels,\n        mode=mode,\n        train_op_fn=_train_op_fn,\n        logits_input=last_hidden_layer_out,\n        scope=...)\n    ```python\n\n  There are cases where computing and applying gradients can not be meaningfully\n  captured with train_op_fn we support (for example, with sync optimizer). In\n  such case, you can take the responsibility on your own. Here is a common\n  use case,\n    ```python\n    model_fn_ops = head.create_model_fn_ops(\n        features=features,\n        labels=labels,\n        mode=mode,\n        train_op_fn=tf.contrib.learn.no_op_train_fn,\n        logits=logits,\n        scope=...)\n    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n      optimizer = ...\n      sync = tf.train.SyncReplicasOptimizer(opt=optimizer, ...)\n      update_op = tf.contrib.layers.optimize_loss(optimizer=sync,\n                                                  loss=model_fn_ops.loss, ...)\n      hooks = [sync.make_session_run_hook(is_chief)]\n      ... update train_op and hooks in ModelFnOps and return\n    ```\n  \"\"\"\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractproperty\n  def logits_dimension(self):\n    \"\"\"Size of the last dimension of the logits `Tensor`.\n\n    Typically, logits is of shape `[batch_size, logits_dimension]`.\n\n    Returns:\n      The expected size of the `logits` tensor.\n    \"\"\"\n    raise NotImplementedError(\"Calling an abstract method.\")\n\n  @abc.abstractmethod\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"Returns `ModelFnOps` that a model_fn can return.\n\n    Please note that,\n    + Exactly one of `logits` and `logits_input` must be provided.\n    + All args must be passed via name.\n\n    Args:\n      features: Input `dict` of `Tensor` objects.\n      mode: Estimator's `ModeKeys`.\n      labels: Labels `Tensor`, or `dict` of same.\n      train_op_fn: Function that takes a scalar loss `Tensor` and returns an op\n          to optimize the model with the loss. This is used in TRAIN mode and\n          must not be None. None is allowed in other modes. If you want to\n          optimize loss yourself you can pass `no_op_train_fn` and then use\n          ModeFnOps.loss to compute and apply gradients.\n      logits: logits `Tensor` to be used by the head.\n      logits_input: `Tensor` from which to build logits, often needed when you\n        don't want to compute the logits. Typically this is the activation of\n        the last hidden layer in a DNN. Some heads (like the ones responsible\n        for candidate sampling) intrinsically avoid computing full logits and\n        only accepts logits_input.\n      scope: Optional scope for `variable_scope`.\n\n    Returns:\n      An instance of `ModelFnOps`.\n\n    Raises:\n      ValueError: If `mode` is not recognized.\n      ValueError: If neither or both of `logits` and `logits_input` is provided.\n    \"\"\"\n    raise NotImplementedError(\"Calling an abstract method.\")\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef regression_head(label_name=None,\n                    weight_column_name=None,\n                    label_dimension=1,\n                    enable_centered_bias=False,\n                    head_name=None,\n                    link_fn=None):\n  \"\"\"Creates a `Head` for linear regression.\n\n  Args:\n    label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    label_dimension: Number of regression labels per example. This is the size\n      of the last dimension of the labels `Tensor` (typically, this has shape\n      `[batch_size, label_dimension]`).\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n    link_fn: link function to convert logits to predictions. If provided,\n      this link function will be used instead of identity.\n\n  Returns:\n    An instance of `Head` for linear regression.\n  \"\"\"\n  return _RegressionHead(\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      label_dimension=label_dimension,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      loss_fn=_mean_squared_loss,\n      link_fn=(link_fn if link_fn is not None else array_ops.identity))\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef poisson_regression_head(label_name=None,\n                            weight_column_name=None,\n                            label_dimension=1,\n                            enable_centered_bias=False,\n                            head_name=None):\n  \"\"\"Creates a `Head` for poisson regression.\n\n  Args:\n    label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    label_dimension: Number of regression labels per example. This is the size\n      of the last dimension of the labels `Tensor` (typically, this has shape\n      `[batch_size, label_dimension]`).\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n\n  Returns:\n    An instance of `Head` for poisson regression.\n  \"\"\"\n  return _RegressionHead(\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      label_dimension=label_dimension,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      loss_fn=_poisson_loss,\n      link_fn=math_ops.exp)\n\n# TODO(zakaria): Consider adding a _RegressionHead for logistic_regression\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef multi_class_head(n_classes,\n                     label_name=None,\n                     weight_column_name=None,\n                     enable_centered_bias=False,\n                     head_name=None,\n                     thresholds=None,\n                     metric_class_ids=None,\n                     loss_fn=None,\n                     label_keys=None):\n  \"\"\"Creates a `Head` for multi class single label classification.\n\n  The Head uses softmax cross entropy loss.\n\n  This head expects to be fed integer labels specifying the class index. But\n  if `label_keys` is specified, then labels must be strings from this\n  vocabulary, and the predicted classes will be strings from the same\n  vocabulary.\n\n  Args:\n    n_classes: Integer, number of classes, must be >= 2\n    label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n    thresholds: thresholds for eval metrics, defaults to [.5]\n    metric_class_ids: List of class IDs for which we should report per-class\n      metrics. Must all be in the range `[0, n_classes)`. Invalid if\n      `n_classes` is 2.\n    loss_fn: Optional function that takes (`labels`, `logits`, `weights`) as\n      parameter and returns a weighted scalar loss. `weights` should be\n      optional. See `tf.losses`\n    label_keys: Optional list of strings with size `[n_classes]` defining the\n      label vocabulary. Only supported for `n_classes` > 2.\n\n  Returns:\n    An instance of `Head` for multi class classification.\n\n  Raises:\n    ValueError: if `n_classes` is < 2.\n    ValueError: If `metric_class_ids` is provided when `n_classes` is 2.\n    ValueError: If `len(label_keys) != n_classes`.\n  \"\"\"\n  if (n_classes is None) or (n_classes < 2):\n    raise ValueError(\"n_classes must be > 1 for classification: %s.\" %\n                     n_classes)\n  if loss_fn:\n    _verify_loss_fn_args(loss_fn)\n\n  loss_fn = _wrap_custom_loss_fn(loss_fn) if loss_fn else None\n  if n_classes == 2:\n    if metric_class_ids:\n      raise ValueError(\"metric_class_ids invalid for n_classes==2.\")\n    if label_keys:\n      raise ValueError(\"label_keys is not supported for n_classes=2.\")\n    return _BinaryLogisticHead(\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        enable_centered_bias=enable_centered_bias,\n        head_name=head_name,\n        thresholds=thresholds,\n        loss_fn=loss_fn)\n\n  return _MultiClassHead(\n      n_classes=n_classes,\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      thresholds=thresholds,\n      metric_class_ids=metric_class_ids,\n      loss_fn=loss_fn,\n      label_keys=label_keys)\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef binary_svm_head(\n    label_name=None,\n    weight_column_name=None,\n    enable_centered_bias=False,\n    head_name=None,\n    thresholds=None,):\n  \"\"\"Creates a `Head` for binary classification with SVMs.\n\n  The head uses binary hinge loss.\n\n  Args:\n    label_name: String, name of the key in label dict. Can be null if label\n      is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n    thresholds: thresholds for eval metrics, defaults to [.5]\n\n  Returns:\n    An instance of `Head` for binary classification with SVM.\n  \"\"\"\n  return _BinarySvmHead(\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      thresholds=thresholds)\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef multi_label_head(n_classes,\n                     label_name=None,\n                     weight_column_name=None,\n                     enable_centered_bias=False,\n                     head_name=None,\n                     thresholds=None,\n                     metric_class_ids=None,\n                     loss_fn=None):\n  \"\"\"Creates a Head for multi label classification.\n\n  Multi-label classification handles the case where each example may have zero\n  or more associated labels, from a discrete set.  This is distinct from\n  `multi_class_head` which has exactly one label from a discrete set.\n\n  This head by default uses sigmoid cross entropy loss, which expects as input\n  a multi-hot tensor of shape `(batch_size, num_classes)`.\n\n  Args:\n    n_classes: Integer, number of classes, must be >= 2\n    label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    enable_centered_bias: A bool. If True, estimator will learn a centered\n      bias variable for each class. Rest of the model structure learns the\n      residual after centered bias.\n    head_name: name of the head. If provided, predictions, summary and metrics\n      keys will be suffixed by `\"/\" + head_name` and the default variable scope\n      will be `head_name`.\n    thresholds: thresholds for eval metrics, defaults to [.5]\n    metric_class_ids: List of class IDs for which we should report per-class\n      metrics. Must all be in the range `[0, n_classes)`.\n    loss_fn: Optional function that takes (`labels`, `logits`, `weights`) as\n      parameter and returns a weighted scalar loss. `weights` should be\n      optional. See `tf.losses`\n\n  Returns:\n    An instance of `Head` for multi label classification.\n\n  Raises:\n    ValueError: If n_classes is < 2\n    ValueError: If loss_fn does not have expected signature.\n  \"\"\"\n  if n_classes < 2:\n    raise ValueError(\"n_classes must be > 1 for classification.\")\n  if loss_fn:\n    _verify_loss_fn_args(loss_fn)\n\n  return _MultiLabelHead(\n      n_classes=n_classes,\n      label_name=label_name,\n      weight_column_name=weight_column_name,\n      enable_centered_bias=enable_centered_bias,\n      head_name=head_name,\n      thresholds=thresholds,\n      metric_class_ids=metric_class_ids,\n      loss_fn=_wrap_custom_loss_fn(loss_fn) if loss_fn else None)\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef loss_only_head(loss_fn, head_name=None):\n  \"\"\"Creates a Head that contains only loss terms.\n\n  Loss only head holds additional loss terms to be added to other heads and\n  usually represents additional regularization terms in the objective function.\n\n  Args:\n    loss_fn: a function that takes no argument and returns a list of\n        scalar tensors.\n    head_name: a name for the head.\n\n  Returns:\n    An instance of `Head` to hold the additional losses.\n  \"\"\"\n  return _LossOnlyHead(loss_fn, head_name=head_name)\n\n\n@deprecated(None, \"Please switch to tf.contrib.estimator.*_head.\")\ndef multi_head(heads, loss_weights=None):\n  \"\"\"Creates a MultiHead stemming from same logits/hidden layer.\n\n  Args:\n    heads: list of Head objects.\n    loss_weights: optional list of weights to be used to merge losses from\n        each head. All losses are weighted equally if not provided.\n\n  Returns:\n    A instance of `Head` that merges multiple heads.\n\n  Raises:\n    ValueError: if heads and loss_weights have different size.\n  \"\"\"\n  if loss_weights:\n    if len(loss_weights) != len(heads):\n      raise ValueError(\"heads and loss_weights must have same size\")\n\n  def _weighted_loss_merger(losses):\n    if loss_weights:\n      if len(losses) != len(loss_weights):\n        raise ValueError(\"losses and loss_weights must have same size\")\n      weighted_losses = []\n      for loss, weight in zip(losses, loss_weights):\n        weighted_losses.append(math_ops.multiply(loss, weight))\n      return math_ops.add_n(weighted_losses)\n    else:\n      return math_ops.add_n(losses)\n\n  return _MultiHead(heads, loss_merger=_weighted_loss_merger)\n\n\n@deprecated(None, \"Use 'lambda _: tf.no_op()'.\")\ndef no_op_train_fn(loss):\n  del loss\n  return control_flow_ops.no_op()\n\n\nclass _SingleHead(Head):\n  \"\"\"Interface for a single head/top of a model.\"\"\"\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(\n      self, problem_type, logits_dimension, label_name=None,\n      weight_column_name=None, head_name=None):\n    if problem_type is None:\n      raise ValueError(\"Invalid problem_type %s.\" % problem_type)\n    if logits_dimension is None or logits_dimension < 1:\n      raise ValueError(\"Invalid logits_dimension %s.\" % logits_dimension)\n    self._problem_type = problem_type\n    self._logits_dimension = logits_dimension\n    self._label_name = label_name\n    self._weight_column_name = weight_column_name\n    self._head_name = head_name\n\n  @property\n  def logits_dimension(self):\n    return self._logits_dimension\n\n  @property\n  def label_name(self):\n    return self._label_name\n\n  @property\n  def weight_column_name(self):\n    return self._weight_column_name\n\n  @property\n  def head_name(self):\n    return self._head_name\n\n  def _create_output_alternatives(self, predictions):\n    \"\"\"Creates output alternative for the Head.\n\n    Args:\n      predictions: a dict of {tensor_name: Tensor}, where 'tensor_name' is a\n        symbolic name for an output Tensor possibly but not necessarily taken\n        from `PredictionKey`, and 'Tensor' is the corresponding output Tensor\n        itself.\n\n    Returns:\n      `dict` of {submodel_name: (problem_type, {tensor_name: Tensor})}, where\n      'submodel_name' is a submodel identifier that should be consistent across\n      the pipeline (here likely taken from the head_name),\n      'problem_type' is a `ProblemType`,\n      'tensor_name' is a symbolic name for an output Tensor possibly but not\n       necessarily taken from `PredictionKey`, and\n      'Tensor' is the corresponding output Tensor itself.\n    \"\"\"\n    return {self._head_name: (self._problem_type, predictions)}\n\n\n# TODO(zakaria): use contrib losses.\ndef _mean_squared_loss(labels, logits, weights=None):\n  with ops.name_scope(None, \"mean_squared_loss\", (logits, labels)) as name:\n    logits = ops.convert_to_tensor(logits)\n    labels = ops.convert_to_tensor(labels)\n    # To prevent broadcasting inside \"-\".\n    if len(labels.get_shape()) == 1:\n      labels = array_ops.expand_dims(labels, axis=(1,))\n    # TODO(zakaria): make sure it does not recreate the broadcast bug.\n    if len(logits.get_shape()) == 1:\n      logits = array_ops.expand_dims(logits, axis=(1,))\n    logits.get_shape().assert_is_compatible_with(labels.get_shape())\n    loss = math_ops.square(logits - math_ops.to_float(labels), name=name)\n    return _compute_weighted_loss(loss, weights)\n\n\ndef _poisson_loss(labels, logits, weights=None):\n  \"\"\"Computes poisson loss from logits.\"\"\"\n  with ops.name_scope(None, \"_poisson_loss\", (logits, labels)) as name:\n    logits = ops.convert_to_tensor(logits)\n    labels = ops.convert_to_tensor(labels)\n    # To prevent broadcasting inside \"-\".\n    if len(labels.get_shape()) == 1:\n      labels = array_ops.expand_dims(labels, axis=(1,))\n    # TODO(zakaria): make sure it does not recreate the broadcast bug.\n    if len(logits.get_shape()) == 1:\n      logits = array_ops.expand_dims(logits, axis=(1,))\n    logits.get_shape().assert_is_compatible_with(labels.get_shape())\n    loss = nn.log_poisson_loss(labels, logits, compute_full_loss=True,\n                               name=name)\n    return _compute_weighted_loss(loss, weights)\n\n\ndef _logits(logits_input, logits, logits_dimension):\n  \"\"\"Validate logits args, and create `logits` if necessary.\n\n  Exactly one of `logits_input` and `logits` must be provided.\n\n  Args:\n    logits_input: `Tensor` input to `logits`.\n    logits: `Tensor` output.\n    logits_dimension: Integer, last dimension of `logits`. This is used to\n      create `logits` from `logits_input` if `logits` is `None`; otherwise, it's\n      used to validate `logits`.\n\n  Returns:\n    `logits` `Tensor`.\n\n  Raises:\n    ValueError: if neither or both of `logits` and `logits_input` are supplied.\n  \"\"\"\n  if (logits_dimension is None) or (logits_dimension < 1):\n    raise ValueError(\"Invalid logits_dimension %s.\" % logits_dimension)\n\n  # If not provided, create logits.\n  if logits is None:\n    if logits_input is None:\n      raise ValueError(\"Neither logits nor logits_input supplied.\")\n    return layers_lib.linear(logits_input, logits_dimension, scope=\"logits\")\n\n  if logits_input is not None:\n    raise ValueError(\"Both logits and logits_input supplied.\")\n\n  logits = ops.convert_to_tensor(logits, name=\"logits\")\n  logits_dims = logits.get_shape().dims\n  if logits_dims is not None:\n    logits_dims[-1].assert_is_compatible_with(logits_dimension)\n\n  return logits\n\n\ndef _create_model_fn_ops(features,\n                         mode,\n                         loss_fn,\n                         logits_to_predictions_fn,\n                         metrics_fn,\n                         create_output_alternatives_fn,\n                         labels=None,\n                         train_op_fn=None,\n                         logits=None,\n                         logits_dimension=None,\n                         head_name=None,\n                         weight_column_name=None,\n                         enable_centered_bias=False):\n  \"\"\"Returns a `ModelFnOps` object.\"\"\"\n  _check_mode_valid(mode)\n\n  centered_bias = None\n  if enable_centered_bias:\n    centered_bias = _centered_bias(logits_dimension, head_name)\n    logits = nn.bias_add(logits, centered_bias)\n\n  predictions = logits_to_predictions_fn(logits)\n  loss = None\n  train_op = None\n  eval_metric_ops = None\n  if (mode != model_fn.ModeKeys.INFER) and (labels is not None):\n    weight_tensor = _weight_tensor(features, weight_column_name)\n    loss, weighted_average_loss = loss_fn(labels, logits, weight_tensor)\n    # The name_scope escapism is needed to maintain the same summary tag\n    # after switching away from the now unsupported API.\n    with ops.name_scope(\"\"):\n      summary_loss = array_ops.identity(weighted_average_loss)\n      summary.scalar(_summary_key(head_name, mkey.LOSS), summary_loss)\n\n    if mode == model_fn.ModeKeys.TRAIN:\n      if train_op_fn is None:\n        raise ValueError(\"train_op_fn can not be None in TRAIN mode\")\n      batch_size = array_ops.shape(logits)[0]\n      train_op = _train_op(loss, labels, train_op_fn, centered_bias,\n                           batch_size, loss_fn, weight_tensor)\n    eval_metric_ops = metrics_fn(\n        weighted_average_loss, predictions, labels, weight_tensor)\n  return model_fn.ModelFnOps(\n      mode=mode,\n      predictions=predictions,\n      loss=loss,\n      train_op=train_op,\n      eval_metric_ops=eval_metric_ops,\n      output_alternatives=create_output_alternatives_fn(predictions))\n\n\nclass _RegressionHead(_SingleHead):\n  \"\"\"`Head` for regression with a generalized linear model.\"\"\"\n\n  def __init__(self,\n               label_dimension,\n               loss_fn,\n               link_fn,\n               logits_dimension=None,\n               label_name=None,\n               weight_column_name=None,\n               enable_centered_bias=False,\n               head_name=None):\n    \"\"\"`Head` for regression.\n\n    Args:\n      label_dimension: Number of regression labels per example. This is the\n        size of the last dimension of the labels `Tensor` (typically, this has\n        shape `[batch_size, label_dimension]`).\n      loss_fn: Loss function, takes logits and labels and returns loss.\n      link_fn: Link function, takes a logits tensor and returns the output.\n      logits_dimension: Number of logits per example. This is the\n        size of the last dimension of the logits `Tensor` (typically, this has\n        shape `[batch_size, label_dimension]`).\n        Default value: `label_dimension`.\n      label_name: String, name of the key in label dict. Can be null if label\n          is a tensor (single headed models).\n      weight_column_name: A string defining feature column name representing\n        weights. It is used to down weight or boost examples during training. It\n        will be multiplied by the loss of the example.\n      enable_centered_bias: A bool. If True, estimator will learn a centered\n        bias variable for each class. Rest of the model structure learns the\n        residual after centered bias.\n      head_name: name of the head. Predictions, summary and metrics keys are\n        suffixed by `\"/\" + head_name` and the default variable scope is\n        `head_name`.\n    \"\"\"\n    super(_RegressionHead, self).__init__(\n        problem_type=constants.ProblemType.LINEAR_REGRESSION,\n        logits_dimension=(logits_dimension if logits_dimension is not None\n                          else label_dimension),\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n\n    self._loss_fn = loss_fn\n    self._link_fn = link_fn\n    self._enable_centered_bias = enable_centered_bias\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"regression_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          create_output_alternatives_fn=self._create_output_alternatives,\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Applies transformations to labels tensor.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    _check_no_sparse_tensor(labels_tensor)\n    return labels_tensor\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"Returns a dict of predictions.\n\n    Args:\n      logits: logits `Tensor` after applying possible centered bias.\n\n    Returns:\n      Dict of prediction `Tensor` keyed by `PredictionKey`.\n    \"\"\"\n    key = prediction_key.PredictionKey.SCORES\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      if self.logits_dimension == 1:\n        logits = array_ops.squeeze(logits, axis=(1,), name=key)\n      return {key: self._link_fn(logits)}\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"Returns a dict of metrics keyed by name.\"\"\"\n    del predictions, labels, weights  # Unused by this head.\n    with ops.name_scope(\"metrics\", values=[eval_loss]):\n      return {\n          _summary_key(self.head_name, mkey.LOSS):\n              metrics_lib.mean(eval_loss)}\n\n\ndef _log_loss_with_two_classes(labels, logits, weights=None):\n  with ops.name_scope(None, \"log_loss_with_two_classes\",\n                      (logits, labels)) as name:\n    logits = ops.convert_to_tensor(logits)\n    labels = math_ops.to_float(labels)\n    # TODO(ptucker): This will break for dynamic shapes.\n    # sigmoid_cross_entropy_with_logits requires [batch_size, 1] labels.\n    if len(labels.get_shape()) == 1:\n      labels = array_ops.expand_dims(labels, axis=(1,))\n    loss = nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits,\n                                                name=name)\n    return _compute_weighted_loss(loss, weights)\n\n\ndef _one_class_to_two_class_logits(logits):\n  return array_ops.concat((array_ops.zeros_like(logits), logits), 1)\n\n\nclass _BinaryLogisticHead(_SingleHead):\n  \"\"\"`Head` for binary classification with logistic regression.\"\"\"\n\n  def __init__(self,\n               label_name=None,\n               weight_column_name=None,\n               enable_centered_bias=False,\n               head_name=None,\n               loss_fn=None,\n               thresholds=None):\n    \"\"\"`Head` for binary classification with logistic regression.\n\n    Args:\n      label_name: String, name of the key in label dict. Can be `None` if label\n          is a tensor (single headed models).\n      weight_column_name: A string defining feature column name representing\n        weights. It is used to down weight or boost examples during training. It\n        will be multiplied by the loss of the example.\n      enable_centered_bias: A bool. If True, estimator will learn a centered\n        bias variable for each class. Rest of the model structure learns the\n        residual after centered bias.\n      head_name: name of the head. Predictions, summary, metrics keys are\n        suffixed by `\"/\" + head_name` and the default variable scope is\n        `head_name`.\n      loss_fn: Loss function.\n      thresholds: thresholds for eval.\n\n    Raises:\n      ValueError: if n_classes is invalid.\n    \"\"\"\n    super(_BinaryLogisticHead, self).__init__(\n        problem_type=constants.ProblemType.LOGISTIC_REGRESSION,\n        logits_dimension=1,\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n    self._thresholds = thresholds if thresholds else (.5,)\n    self._loss_fn = loss_fn if loss_fn else _log_loss_with_two_classes\n    self._enable_centered_bias = enable_centered_bias\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"binary_logistic_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          create_output_alternatives_fn=_classification_output_alternatives(\n              self.head_name, self._problem_type),\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Applies transformations to labels tensor.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    _check_no_sparse_tensor(labels_tensor)\n    return labels_tensor\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"Returns a dict of predictions.\n\n    Args:\n      logits: logits `Output` after applying possible centered bias.\n\n    Returns:\n      Dict of prediction `Output` keyed by `PredictionKey`.\n    \"\"\"\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      two_class_logits = _one_class_to_two_class_logits(logits)\n      return {\n          prediction_key.PredictionKey.LOGITS:\n              logits,\n          prediction_key.PredictionKey.LOGISTIC:\n              math_ops.sigmoid(\n                  logits, name=prediction_key.PredictionKey.LOGISTIC),\n          prediction_key.PredictionKey.PROBABILITIES:\n              nn.softmax(\n                  two_class_logits,\n                  name=prediction_key.PredictionKey.PROBABILITIES),\n          prediction_key.PredictionKey.CLASSES:\n              math_ops.argmax(\n                  two_class_logits,\n                  1,\n                  name=prediction_key.PredictionKey.CLASSES)\n      }\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"Returns a dict of metrics keyed by name.\"\"\"\n    with ops.name_scope(\"metrics\", values=(\n        [eval_loss, labels, weights] + list(six.itervalues(predictions)))):\n      classes = predictions[prediction_key.PredictionKey.CLASSES]\n      logistic = predictions[prediction_key.PredictionKey.LOGISTIC]\n\n      metrics = {_summary_key(self.head_name, mkey.LOSS):\n                 metrics_lib.mean(eval_loss)}\n      # TODO(b/29366811): This currently results in both an \"accuracy\" and an\n      # \"accuracy/threshold_0.500000_mean\" metric for binary classification.\n      metrics[_summary_key(self.head_name, mkey.ACCURACY)] = (\n          metrics_lib.accuracy(labels, classes, weights))\n      metrics[_summary_key(self.head_name, mkey.PREDICTION_MEAN)] = (\n          _predictions_streaming_mean(logistic, weights))\n      metrics[_summary_key(self.head_name, mkey.LABEL_MEAN)] = (\n          _indicator_labels_streaming_mean(labels, weights))\n\n      # Also include the streaming mean of the label as an accuracy baseline, as\n      # a reminder to users.\n      metrics[_summary_key(self.head_name, mkey.ACCURACY_BASELINE)] = (\n          _indicator_labels_streaming_mean(labels, weights))\n      metrics[_summary_key(self.head_name, mkey.AUC)] = (\n          _streaming_auc(logistic, labels, weights))\n      metrics[_summary_key(self.head_name, mkey.AUC_PR)] = (\n          _streaming_auc(logistic, labels, weights, curve=\"PR\"))\n\n      for threshold in self._thresholds:\n        metrics[_summary_key(\n            self.head_name, mkey.ACCURACY_MEAN % threshold)] = (\n                _streaming_accuracy_at_threshold(logistic, labels, weights,\n                                                 threshold))\n        # Precision for positive examples.\n        metrics[_summary_key(\n            self.head_name, mkey.PRECISION_MEAN % threshold)] = (\n                _streaming_precision_at_threshold(logistic, labels, weights,\n                                                  threshold))\n        # Recall for positive examples.\n        metrics[_summary_key(\n            self.head_name, mkey.RECALL_MEAN % threshold)] = (\n                _streaming_recall_at_threshold(logistic, labels, weights,\n                                               threshold))\n\n    return metrics\n\n\ndef _softmax_cross_entropy_loss(labels, logits, weights=None):\n  with ops.name_scope(\n      None, \"softmax_cross_entropy_loss\", (logits, labels,)) as name:\n    labels = ops.convert_to_tensor(labels)\n    # Check that we got integer for classification.\n    if not labels.dtype.is_integer:\n      raise ValueError(\"Labels dtype should be integer \"\n                       \"Instead got %s.\" % labels.dtype)\n\n    # sparse_softmax_cross_entropy_with_logits requires [batch_size] labels.\n    is_squeezed_labels = False\n    # TODO(ptucker): This will break for dynamic shapes.\n    if len(labels.get_shape()) == 2:\n      labels = array_ops.squeeze(labels, axis=(1,))\n      is_squeezed_labels = True\n\n    loss = nn.sparse_softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits, name=name)\n\n    # Restore squeezed dimension, if necessary, so loss matches weights shape.\n    if is_squeezed_labels:\n      loss = array_ops.expand_dims(loss, axis=(1,))\n\n    return _compute_weighted_loss(loss, weights)\n\n\nclass _MultiClassHead(_SingleHead):\n  \"\"\"'Head' for multi class classification.\"\"\"\n\n  def __init__(self,\n               n_classes,\n               label_name=None,\n               weight_column_name=None,\n               enable_centered_bias=False,\n               head_name=None,\n               loss_fn=None,\n               thresholds=None,\n               metric_class_ids=None,\n               label_keys=None):\n    \"\"\"'Head' for multi class classification.\n\n    This head expects to be fed integer labels specifying the class index. But\n    if `label_keys` is specified, then labels must be strings from this\n    vocabulary, and the predicted classes will be strings from the same\n    vocabulary.\n\n    Args:\n      n_classes: Number of classes, must be greater than 2 (for 2 classes, use\n        `_BinaryLogisticHead`).\n      label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n      weight_column_name: A string defining feature column name representing\n        weights. It is used to down weight or boost examples during training. It\n        will be multiplied by the loss of the example.\n      enable_centered_bias: A bool. If True, estimator will learn a centered\n        bias variable for each class. Rest of the model structure learns the\n        residual after centered bias.\n      head_name: name of the head. If provided, predictions, summary, metrics\n        keys will be suffixed by `\"/\" + head_name` and the default variable\n        scope will be `head_name`.\n      loss_fn: Loss function. Defaults to softmax cross entropy loss.\n      thresholds: thresholds for eval.\n      metric_class_ids: List of class IDs for which we should report per-class\n        metrics. Must all be in the range `[0, n_classes)`.\n      label_keys: Optional list of strings with size `[n_classes]` defining the\n        label vocabulary.\n\n    Raises:\n      ValueError: if `n_classes`, `metric_class_ids` or `label_keys` is invalid.\n    \"\"\"\n    super(_MultiClassHead, self).__init__(\n        problem_type=constants.ProblemType.CLASSIFICATION,\n        logits_dimension=n_classes,\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n\n    if (n_classes is None) or (n_classes <= 2):\n      raise ValueError(\"n_classes must be > 2: %s.\" % n_classes)\n    self._thresholds = thresholds if thresholds else (.5,)\n    self._loss_fn = loss_fn if loss_fn else _softmax_cross_entropy_loss\n    self._enable_centered_bias = enable_centered_bias\n    self._metric_class_ids = tuple([] if metric_class_ids is None else\n                                   metric_class_ids)\n    for class_id in self._metric_class_ids:\n      if (class_id < 0) or (class_id >= n_classes):\n        raise ValueError(\"Class ID %s not in [0, %s).\" % (class_id, n_classes))\n    if label_keys and len(label_keys) != n_classes:\n      raise ValueError(\"Length of label_keys must equal n_classes.\")\n    self._label_keys = label_keys\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"multi_class_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._wrapped_loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          create_output_alternatives_fn=_classification_output_alternatives(\n              self.head_name, self._problem_type, self._label_keys),\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Returns a dict that contains both the original labels and label IDs.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    _check_no_sparse_tensor(labels_tensor)\n    if self._label_keys:\n      table = lookup_ops.index_table_from_tensor(\n          self._label_keys, name=\"label_id_lookup\")\n      return {\n          \"labels\": labels_tensor,\n          \"label_ids\": table.lookup(labels_tensor),\n      }\n    return {\n        \"labels\": labels_tensor,\n        \"label_ids\": labels_tensor,\n    }\n\n  def _labels(self, labels_dict):\n    \"\"\"Returns labels `Tensor` of the same type as classes.\"\"\"\n    return labels_dict[\"labels\"]\n\n  def _label_ids(self, labels_dict):\n    \"\"\"Returns integer label ID `Tensor`.\"\"\"\n    return labels_dict[\"label_ids\"]\n\n  def _wrapped_loss_fn(self, labels, logits, weights=None):\n    return self._loss_fn(self._label_ids(labels), logits, weights=weights)\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"Returns a dict of predictions.\n\n    Args:\n      logits: logits `Tensor` after applying possible centered bias.\n\n    Returns:\n      Dict of prediction `Tensor` keyed by `PredictionKey`.\n    \"\"\"\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      class_ids = math_ops.argmax(\n          logits, 1, name=prediction_key.PredictionKey.CLASSES)\n      if self._label_keys:\n        table = lookup_ops.index_to_string_table_from_tensor(\n            self._label_keys, name=\"class_string_lookup\")\n        classes = table.lookup(class_ids)\n      else:\n        classes = class_ids\n      return {\n          prediction_key.PredictionKey.LOGITS: logits,\n          prediction_key.PredictionKey.PROBABILITIES:\n              nn.softmax(\n                  logits, name=prediction_key.PredictionKey.PROBABILITIES),\n          prediction_key.PredictionKey.CLASSES: classes\n      }\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"Returns a dict of metrics keyed by name.\"\"\"\n    with ops.name_scope(\n        \"metrics\",\n        values=((eval_loss, self._labels(labels), self._label_ids(labels),\n                 weights) + tuple(six.itervalues(predictions)))):\n      logits = predictions[prediction_key.PredictionKey.LOGITS]\n      probabilities = predictions[prediction_key.PredictionKey.PROBABILITIES]\n      classes = predictions[prediction_key.PredictionKey.CLASSES]\n\n      metrics = {_summary_key(self.head_name, mkey.LOSS):\n                 metrics_lib.mean(eval_loss)}\n      # TODO(b/29366811): This currently results in both an \"accuracy\" and an\n      # \"accuracy/threshold_0.500000_mean\" metric for binary classification.\n      metrics[_summary_key(self.head_name, mkey.ACCURACY)] = (\n          metrics_lib.accuracy(self._labels(labels), classes, weights))\n\n      if not self._label_keys:\n        # Classes are IDs. Add some metrics.\n        for class_id in self._metric_class_ids:\n          metrics[_summary_key(\n              self.head_name, mkey.CLASS_PREDICTION_MEAN % class_id)] = (\n                  _class_predictions_streaming_mean(classes, weights, class_id))\n          # TODO(ptucker): Add per-class accuracy, precision, recall.\n          metrics[_summary_key(\n              self.head_name, mkey.CLASS_LABEL_MEAN % class_id)] = (\n                  _class_labels_streaming_mean(\n                      self._label_ids(labels), weights, class_id))\n          metrics[_summary_key(\n              self.head_name, mkey.CLASS_PROBABILITY_MEAN % class_id)] = (\n                  _predictions_streaming_mean(probabilities, weights, class_id))\n          metrics[_summary_key(\n              self.head_name, mkey.CLASS_LOGITS_MEAN % class_id)] = (\n                  _predictions_streaming_mean(logits, weights, class_id))\n\n    return metrics\n\n\ndef _to_labels_tensor(labels, label_name):\n  \"\"\"Returns label as a tensor.\n\n  Args:\n    labels: Label `Tensor` or `SparseTensor` or a dict containing labels.\n    label_name: Label name if labels is a dict.\n\n  Returns:\n    Label `Tensor` or `SparseTensor`.\n  \"\"\"\n  labels = labels[label_name] if isinstance(labels, dict) else labels\n  return framework_lib.convert_to_tensor_or_sparse_tensor(labels)\n\n\ndef _check_no_sparse_tensor(x):\n  \"\"\"Raises ValueError if the given tensor is `SparseTensor`.\"\"\"\n  if isinstance(x, sparse_tensor.SparseTensor):\n    raise ValueError(\"SparseTensor is not supported.\")\n\n\ndef _sparse_labels_to_indicator(labels, num_classes):\n  \"\"\"If labels is `SparseTensor`, converts it to indicator `Tensor`.\n\n  Args:\n    labels: Label `Tensor` or `SparseTensor`.\n    num_classes: Number of classes.\n\n  Returns:\n    Dense label `Tensor`.\n\n  Raises:\n    ValueError: If labels is `SparseTensor` and `num_classes` < 2.\n  \"\"\"\n  if isinstance(labels, sparse_tensor.SparseTensor):\n    if num_classes < 2:\n      raise ValueError(\"Must set num_classes >= 2 when passing labels as a \"\n                       \"SparseTensor.\")\n    return math_ops.to_int64(\n        sparse_ops.sparse_to_indicator(labels, num_classes))\n  return labels\n\n\ndef _assert_labels_rank(labels):\n  return control_flow_ops.Assert(\n      math_ops.less_equal(array_ops.rank(labels), 2),\n      (\"labels shape should be either [batch_size, 1] or [batch_size]\",))\n\n\nclass _BinarySvmHead(_SingleHead):\n  \"\"\"`Head` for binary classification using SVM.\"\"\"\n\n  def __init__(self, label_name, weight_column_name, enable_centered_bias,\n               head_name, thresholds):\n\n    def _loss_fn(labels, logits, weights=None):\n      with ops.name_scope(None, \"hinge_loss\", (logits, labels)) as name:\n        with ops.control_dependencies((_assert_labels_rank(labels),)):\n          labels = array_ops.reshape(labels, shape=(-1, 1))\n        loss = losses_lib.hinge_loss(labels=labels, logits=logits, scope=name,\n                                     reduction=losses_lib.Reduction.NONE)\n        return _compute_weighted_loss(loss, weights)\n\n    super(_BinarySvmHead, self).__init__(\n        problem_type=constants.ProblemType.LOGISTIC_REGRESSION,\n        logits_dimension=1,\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n    self._thresholds = thresholds if thresholds else (.5,)\n    self._loss_fn = _loss_fn\n    self._enable_centered_bias = enable_centered_bias\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"binary_svm_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          # TODO(zakaria): Handle labels for export.\n          create_output_alternatives_fn=self._create_output_alternatives,\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Applies transformations to labels tensor.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    _check_no_sparse_tensor(labels_tensor)\n    return labels_tensor\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"See `_MultiClassHead`.\"\"\"\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      return {\n          prediction_key.PredictionKey.LOGITS:\n              logits,\n          prediction_key.PredictionKey.CLASSES:\n              math_ops.argmax(\n                  _one_class_to_two_class_logits(logits),\n                  1,\n                  name=prediction_key.PredictionKey.CLASSES)\n      }\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"See `_MultiClassHead`.\"\"\"\n    with ops.name_scope(\"metrics\", values=(\n        [eval_loss, labels, weights] + list(six.itervalues(predictions)))):\n      metrics = {_summary_key(self.head_name, mkey.LOSS):\n                 metrics_lib.mean(eval_loss)}\n\n      # TODO(b/29366811): This currently results in both an \"accuracy\" and an\n      # \"accuracy/threshold_0.500000_mean\" metric for binary classification.\n      classes = predictions[prediction_key.PredictionKey.CLASSES]\n      metrics[_summary_key(self.head_name, mkey.ACCURACY)] = (\n          metrics_lib.accuracy(labels, classes, weights))\n      # TODO(sibyl-vie3Poto): add more metrics relevant for svms.\n\n    return metrics\n\n\nclass _MultiLabelHead(_SingleHead):\n  \"\"\"`Head` for multi-label classification.\"\"\"\n\n  # TODO(zakaria): add signature and metric for multilabel.\n  def __init__(self,\n               n_classes,\n               label_name,\n               weight_column_name,\n               enable_centered_bias,\n               head_name,\n               thresholds,\n               metric_class_ids=None,\n               loss_fn=None):\n\n    super(_MultiLabelHead, self).__init__(\n        problem_type=constants.ProblemType.CLASSIFICATION,\n        logits_dimension=n_classes,\n        label_name=label_name,\n        weight_column_name=weight_column_name,\n        head_name=head_name)\n\n    self._thresholds = thresholds if thresholds else (.5,)\n    self._loss_fn = loss_fn if loss_fn else _sigmoid_cross_entropy_loss\n    self._enable_centered_bias = enable_centered_bias\n    self._metric_class_ids = tuple([] if metric_class_ids is None else\n                                   metric_class_ids)\n    for class_id in self._metric_class_ids:\n      if (class_id < 0) or (class_id >= n_classes):\n        raise ValueError(\"Class ID %s not in [0, %s).\" % (class_id, n_classes))\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `Head`.\"\"\"\n    with variable_scope.variable_scope(\n        scope,\n        default_name=self.head_name or \"multi_label_head\",\n        values=(tuple(six.itervalues(features)) +\n                (labels, logits, logits_input))):\n      labels = self._transform_labels(mode=mode, labels=labels)\n      logits = _logits(logits_input, logits, self.logits_dimension)\n      return _create_model_fn_ops(\n          features=features,\n          mode=mode,\n          loss_fn=self._loss_fn,\n          logits_to_predictions_fn=self._logits_to_predictions,\n          metrics_fn=self._metrics,\n          create_output_alternatives_fn=_classification_output_alternatives(\n              self.head_name, self._problem_type),\n          labels=labels,\n          train_op_fn=train_op_fn,\n          logits=logits,\n          logits_dimension=self.logits_dimension,\n          head_name=self.head_name,\n          weight_column_name=self.weight_column_name,\n          enable_centered_bias=self._enable_centered_bias)\n\n  def _transform_labels(self, mode, labels):\n    \"\"\"Applies transformations to labels tensor.\"\"\"\n    if (mode == model_fn.ModeKeys.INFER) or (labels is None):\n      return None\n    labels_tensor = _to_labels_tensor(labels, self._label_name)\n    labels_tensor = _sparse_labels_to_indicator(labels_tensor,\n                                                self._logits_dimension)\n    return labels_tensor\n\n  def _logits_to_predictions(self, logits):\n    \"\"\"See `_MultiClassHead`.\"\"\"\n    with ops.name_scope(None, \"predictions\", (logits,)):\n      return {\n          prediction_key.PredictionKey.LOGITS:\n              logits,\n          prediction_key.PredictionKey.PROBABILITIES:\n              math_ops.sigmoid(\n                  logits, name=prediction_key.PredictionKey.PROBABILITIES),\n          prediction_key.PredictionKey.CLASSES:\n              math_ops.to_int64(\n                  math_ops.greater(logits, 0),\n                  name=prediction_key.PredictionKey.CLASSES)\n      }\n\n  def _metrics(self, eval_loss, predictions, labels, weights):\n    \"\"\"Returns a dict of metrics keyed by name.\"\"\"\n    with ops.name_scope(\"metrics\", values=(\n        [eval_loss, labels, weights] + list(six.itervalues(predictions)))):\n      classes = predictions[prediction_key.PredictionKey.CLASSES]\n      probabilities = predictions[prediction_key.PredictionKey.PROBABILITIES]\n      logits = predictions[prediction_key.PredictionKey.LOGITS]\n\n      metrics = {_summary_key(self.head_name, mkey.LOSS):\n                 metrics_lib.mean(eval_loss)}\n      # TODO(b/29366811): This currently results in both an \"accuracy\" and an\n      # \"accuracy/threshold_0.500000_mean\" metric for binary classification.\n      metrics[_summary_key(self.head_name, mkey.ACCURACY)] = (\n          metrics_lib.accuracy(labels, classes, weights))\n      metrics[_summary_key(self.head_name, mkey.AUC)] = _streaming_auc(\n          probabilities, labels, weights)\n      metrics[_summary_key(self.head_name, mkey.AUC_PR)] = _streaming_auc(\n          probabilities, labels, weights, curve=\"PR\")\n\n      for class_id in self._metric_class_ids:\n        # TODO(ptucker): Add per-class accuracy, precision, recall.\n        metrics[_summary_key(\n            self.head_name, mkey.CLASS_PREDICTION_MEAN % class_id)] = (\n                _predictions_streaming_mean(classes, weights, class_id))\n        metrics[_summary_key(\n            self.head_name, mkey.CLASS_LABEL_MEAN % class_id)] = (\n                _indicator_labels_streaming_mean(labels, weights, class_id))\n        metrics[_summary_key(\n            self.head_name, mkey.CLASS_PROBABILITY_MEAN % class_id)] = (\n                _predictions_streaming_mean(probabilities, weights, class_id))\n        metrics[_summary_key(\n            self.head_name, mkey.CLASS_LOGITS_MEAN % class_id)] = (\n                _predictions_streaming_mean(logits, weights, class_id))\n        metrics[_summary_key(self.head_name, mkey.CLASS_AUC % class_id)] = (\n            _streaming_auc(probabilities, labels, weights, class_id))\n        metrics[_summary_key(self.head_name, mkey.CLASS_AUC_PR % class_id)] = (\n            _streaming_auc(probabilities, labels, weights, class_id,\n                           curve=\"PR\"))\n\n    return metrics\n\n\nclass _LossOnlyHead(Head):\n  \"\"\"`Head` implementation for additional loss terms.\n\n  This class only holds loss terms unrelated to any other heads (labels),\n  e.g. regularization.\n\n  Common usage:\n  This is oftem combine with other heads in a multi head setup.\n    ```python\n    head = multi_head([\n        head1, head2, loss_only_head('regularizer', regularizer)])\n    ```\n  \"\"\"\n\n  def __init__(self, loss_fn, head_name=None):\n    self._loss_fn = loss_fn\n    self.head_name = head_name or \"loss_only_head\"\n\n  @property\n  def logits_dimension(self):\n    return 0\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `_Head.create_model_fn_ops`.\n\n    Args:\n      features: Not been used.\n      mode: Estimator's `ModeKeys`.\n      labels: Labels `Tensor`, or `dict` of same.\n      train_op_fn: Function that takes a scalar loss and returns an op to\n          optimize with the loss.\n      logits: Not been used.\n      logits_input: Not been used.\n      scope: Optional scope for variable_scope. If provided, will be passed to\n          all heads. Most users will want to set this to `None`, so each head\n          constructs a separate variable_scope according to its `head_name`.\n\n    Returns:\n      A `ModelFnOps` object.\n\n    Raises:\n      ValueError: if `mode` is not recognition.\n    \"\"\"\n    _check_mode_valid(mode)\n    loss = None\n    train_op = None\n    if mode != model_fn.ModeKeys.INFER:\n      with variable_scope.variable_scope(scope, default_name=self.head_name):\n        loss = self._loss_fn()\n        if isinstance(loss, list):\n          loss = math_ops.add_n(loss)\n        # The name_scope escapism is needed to maintain the same summary tag\n        # after switching away from the now unsupported API.\n        with ops.name_scope(\"\"):\n          summary_loss = array_ops.identity(loss)\n          summary.scalar(_summary_key(self.head_name, mkey.LOSS),\n                         summary_loss)\n        if mode == model_fn.ModeKeys.TRAIN:\n          if train_op_fn is None:\n            raise ValueError(\"train_op_fn can not be None in TRAIN mode\")\n          with ops.name_scope(None, \"train_op\", (loss,)):\n            train_op = train_op_fn(loss)\n\n    return model_fn.ModelFnOps(\n        mode=mode,\n        loss=loss,\n        train_op=train_op,\n        predictions={},\n        eval_metric_ops={})\n\n\nclass _MultiHead(Head):\n  \"\"\"`Head` implementation for multi objective learning.\n\n  This class is responsible for using and merging the output of multiple\n  `Head` objects.\n\n  All heads stem from the same logits/logit_input tensor.\n\n  Common usage:\n  For simple use cases you can pass the activation of hidden layer like\n  this from your model_fn,\n    ```python\n    last_hidden_layer_activation = ... Build your model.\n    multi_head = ...\n    return multi_head.create_model_fn_ops(\n        ..., logits_input=last_hidden_layer_activation, ...)\n    ```\n\n  Or you can create a logits tensor of\n  [batch_size, multi_head.logits_dimension] shape. _MultiHead will split the\n  logits for you.\n    return multi_head.create_model_fn_ops(..., logits=logits, ...)\n\n  For more complex use cases like a multi-task/multi-tower model or when logits\n  for each head has to be created separately, you can pass a dict of logits\n  where the keys match the name of the single heads.\n    ```python\n    logits = {\"head1\": logits1, \"head2\": logits2}\n    return multi_head.create_model_fn_ops(..., logits=logits, ...)\n    ```\n\n  Here is what this class does,\n  + For training, merges losses of each heads according a function provided by\n      user, calls user provided train_op_fn with this final loss.\n  + For eval, merges metrics by adding head_name suffix to the keys in eval\n      metrics.\n  + For inference, updates keys in prediction dict to a 2-tuple,\n      (head_name, prediction_key)\n  \"\"\"\n\n  def __init__(self, heads, loss_merger):\n    \"\"\"_Head to merges multiple _Head objects.\n\n    Args:\n      heads: list of _Head objects.\n      loss_merger: function that takes a list of loss tensors for the heads\n        and returns the final loss tensor for the multi head.\n\n    Raises:\n      ValueError: if any head does not have a name.\n    \"\"\"\n    self._logits_dimension = 0\n    for head in heads:\n      if not head.head_name:\n        raise ValueError(\"Members of MultiHead must have names.\")\n      self._logits_dimension += head.logits_dimension\n\n    self._heads = heads\n    self._loss_merger = loss_merger\n\n  @property\n  def logits_dimension(self):\n    return self._logits_dimension\n\n  def create_model_fn_ops(self,\n                          features,\n                          mode,\n                          labels=None,\n                          train_op_fn=None,\n                          logits=None,\n                          logits_input=None,\n                          scope=None):\n    \"\"\"See `_Head.create_model_fn_ops`.\n\n    Args:\n      features: Input `dict` of `Tensor` objects.\n      mode: Estimator's `ModeKeys`.\n      labels: Labels `Tensor`, or `dict` of same.\n      train_op_fn: Function that takes a scalar loss and returns an op to\n          optimize with the loss.\n      logits: Concatenated logits for all heads or a dict of head name to logits\n          tensor. If concatenated logits, it should have (batchsize, x) shape\n          where x is the sum of `logits_dimension` of all the heads,\n          i.e., same as `logits_dimension` of this class. create_model_fn_ops\n          will split the logits tensor and pass logits of proper size to each\n          head. This is useful if we want to be agnostic about whether you\n          creating a single versus multihead. logits can also be a dict for\n          convenience where you are creating the head specific logits explicitly\n          and don't want to concatenate them yourself.\n      logits_input: tensor to build logits from.\n      scope: Optional scope for variable_scope. If provided, will be passed to\n        all heads. Most users will want to set this to `None`, so each head\n        constructs a separate variable_scope according to its `head_name`.\n\n    Returns:\n      `ModelFnOps`.\n\n    Raises:\n      ValueError: if `mode` is not recognized, or neither or both of `logits`\n          and `logits_input` is provided.\n    \"\"\"\n    _check_mode_valid(mode)\n    all_model_fn_ops = []\n    if logits is None:\n      # Use logits_input.\n      for head in self._heads:\n        all_model_fn_ops.append(\n            head.create_model_fn_ops(\n                features=features,\n                mode=mode,\n                labels=labels,\n                train_op_fn=no_op_train_fn,\n                logits_input=logits_input,\n                scope=scope))\n    else:\n      head_logits_pairs = []\n      if isinstance(logits, dict):\n        head_logits_pairs = []\n        for head in self._heads:\n          if isinstance(head, _LossOnlyHead):\n            head_logits_pairs.append((head, None))\n          else:\n            head_logits_pairs.append((head, logits[head.head_name]))\n      else:\n        # Split logits for each head.\n        head_logits_pairs = zip(self._heads, self._split_logits(logits))\n\n      for head, head_logits in head_logits_pairs:\n        all_model_fn_ops.append(\n            head.create_model_fn_ops(\n                features=features,\n                mode=mode,\n                labels=labels,\n                train_op_fn=no_op_train_fn,\n                logits=head_logits,\n                scope=scope))\n\n    if mode == model_fn.ModeKeys.TRAIN:\n      if train_op_fn is None:\n        raise ValueError(\"train_op_fn can not be None in TRAIN mode.\")\n      return self._merge_train(all_model_fn_ops, train_op_fn)\n    if mode == model_fn.ModeKeys.INFER:\n      return self._merge_infer(all_model_fn_ops)\n    if mode == model_fn.ModeKeys.EVAL:\n      return self._merge_eval(all_model_fn_ops)\n    raise ValueError(\"mode=%s unrecognized\" % str(mode))\n\n  def _split_logits(self, logits):\n    \"\"\"Splits logits for heads.\n\n    Args:\n      logits: the logits tensor.\n\n    Returns:\n      A list of logits for the individual heads.\n    \"\"\"\n    all_logits = []\n    begin = 0\n    for head in self._heads:\n      current_logits_size = head.logits_dimension\n      current_logits = array_ops.slice(logits, [0, begin],\n                                       [-1, current_logits_size])\n      all_logits.append(current_logits)\n      begin += current_logits_size\n    return all_logits\n\n  def _merge_train(self, all_model_fn_ops, train_op_fn):\n    \"\"\"Merges list of ModelFnOps for training.\n\n    Args:\n      all_model_fn_ops: list of ModelFnOps for the individual heads.\n      train_op_fn: Function to create train op. See `create_model_fn_ops`\n          documentation for more details.\n\n    Returns:\n      ModelFnOps that merges all heads for TRAIN.\n    \"\"\"\n    losses = []\n    metrics = {}\n    additional_train_ops = []\n    for m in all_model_fn_ops:\n      losses.append(m.loss)\n      if m.eval_metric_ops is not None:\n        for k, v in six.iteritems(m.eval_metric_ops):\n          # metrics[\"%s/%s\" % (k, head_name)] = v\n          metrics[k] = v\n      additional_train_ops.append(m.train_op)\n    loss = self._loss_merger(losses)\n\n    train_op = train_op_fn(loss)\n    train_op = control_flow_ops.group(train_op, *additional_train_ops)\n    return model_fn.ModelFnOps(\n        mode=model_fn.ModeKeys.TRAIN,\n        loss=loss,\n        train_op=train_op,\n        eval_metric_ops=metrics)\n\n  def _merge_infer(self, all_model_fn_ops):\n    \"\"\"Merges list of ModelFnOps for inference.\n\n    Args:\n      all_model_fn_ops: list of ModelFnOps for the individual heads.\n\n    Returns:\n      ModelFnOps that Merges all the heads for INFER.\n    \"\"\"\n    predictions = {}\n    output_alternatives = {}\n    for head, m in zip(self._heads, all_model_fn_ops):\n      if isinstance(head, _LossOnlyHead):\n        continue\n      head_name = head.head_name\n      output_alternatives[head_name] = m.output_alternatives[head_name]\n      for k, v in m.predictions.items():\n        predictions[(head_name, k)] = v\n\n    return model_fn.ModelFnOps(\n        mode=model_fn.ModeKeys.INFER,\n        predictions=predictions,\n        output_alternatives=output_alternatives)\n\n  def _merge_eval(self, all_model_fn_ops):\n    \"\"\"Merges list of ModelFnOps for eval.\n\n    Args:\n      all_model_fn_ops: list of ModelFnOps for the individual heads.\n\n    Returns:\n      ModelFnOps that merges all the heads for EVAL.\n    \"\"\"\n    predictions = {}\n    metrics = {}\n    losses = []\n    for head, m in zip(self._heads, all_model_fn_ops):\n      losses.append(m.loss)\n      head_name = head.head_name\n      for k, v in m.predictions.items():\n        predictions[(head_name, k)] = v\n      for k, v in m.eval_metric_ops.items():\n        # metrics[\"%s/%s\" % (k, head_name)] = v\n        metrics[k] = v\n    loss = self._loss_merger(losses)\n\n    return model_fn.ModelFnOps(\n        mode=model_fn.ModeKeys.EVAL,\n        predictions=predictions,\n        loss=loss,\n        eval_metric_ops=metrics)\n\n\ndef _weight_tensor(features, weight_column_name):\n  \"\"\"Returns weights as `Tensor` of rank 0, or at least 2.\"\"\"\n  if not weight_column_name:\n    return None\n  if weight_column_name not in features:\n    raise ValueError(\"Weights {} missing from features.\".format(\n        weight_column_name))\n  with ops.name_scope(None, \"weight_tensor\", tuple(six.itervalues(features))):\n    weight_tensor = math_ops.to_float(features[weight_column_name])\n    shape = weight_tensor.get_shape()\n    rank = shape.ndims\n    # We don't bother with expanding dims of non-staticly shaped tensors or\n    # scalars, and >1d is already in a good format.\n    if rank == 1:\n      logging.warning(\"Weights {} has shape {}, expanding to make it 2d.\".\n                      format(weight_column_name, shape))\n      return (\n          sparse_ops.sparse_reshape(weight_tensor, (-1, 1))\n          if isinstance(weight_tensor, sparse_tensor.SparseTensor) else\n          array_ops.reshape(weight_tensor, (-1, 1)))\n    return weight_tensor\n\n\n# TODO(zakaria): This function is needed for backward compatibility and should\n#   be removed when we migrate to core.\ndef _compute_weighted_loss(loss_unweighted, weight, name=\"loss\"):\n  \"\"\"Returns a tuple of (loss_train, loss_report).\n\n  loss is used for gradient descent while weighted_average_loss is used for\n  summaries to be backward compatible.\n\n  loss is different from the loss reported on the tensorboard as we\n  should respect the example weights when computing the gradient.\n\n    L = sum_{i} w_{i} * l_{i} / B\n\n  where B is the number of examples in the batch, l_{i}, w_{i} are individual\n  losses, and example weight.\n\n  Args:\n    loss_unweighted: Unweighted loss\n    weight: Weight tensor\n    name: Optional name\n\n  Returns:\n    A tuple of losses. First one for training and the second one for reporting.\n  \"\"\"\n  with ops.name_scope(name, values=(loss_unweighted, weight)) as name_scope:\n    if weight is None:\n      loss = math_ops.reduce_mean(loss_unweighted, name=name_scope)\n      return loss, loss\n    weight = weights_broadcast_ops.broadcast_weights(weight, loss_unweighted)\n    with ops.name_scope(None, \"weighted_loss\",\n                        (loss_unweighted, weight)) as name:\n      weighted_loss = math_ops.multiply(loss_unweighted, weight, name=name)\n    weighted_loss_mean = math_ops.reduce_mean(weighted_loss, name=name_scope)\n    weighted_loss_normalized = math_ops.div(\n        math_ops.reduce_sum(weighted_loss),\n        math_ops.to_float(math_ops.reduce_sum(weight)),\n        name=\"weighted_average_loss\")\n\n    return weighted_loss_mean, weighted_loss_normalized\n\n\ndef _wrap_custom_loss_fn(loss_fn):\n  def _wrapper(labels, logits, weights=None):\n    if weights is None:\n      loss = loss_fn(labels, logits)\n    else:\n      loss = loss_fn(labels, logits, weights)\n    return loss, loss\n  return _wrapper\n\n\ndef _check_mode_valid(mode):\n  \"\"\"Raises ValueError if the given mode is invalid.\"\"\"\n  if (mode != model_fn.ModeKeys.TRAIN and mode != model_fn.ModeKeys.INFER and\n      mode != model_fn.ModeKeys.EVAL):\n    raise ValueError(\"mode=%s unrecognized.\" % str(mode))\n\n\ndef _get_arguments(func):\n  \"\"\"Returns a spec of given func.\"\"\"\n  _, func = tf_decorator.unwrap(func)\n  if hasattr(func, \"__code__\"):\n    # Regular function.\n    return tf_inspect.getargspec(func)\n  elif hasattr(func, \"func\"):\n    # Partial function.\n    return _get_arguments(func.func)\n  elif hasattr(func, \"__call__\"):\n    # Callable object.\n    return _get_arguments(func.__call__)\n\n\ndef _verify_loss_fn_args(loss_fn):\n  args = _get_arguments(loss_fn).args\n  for arg_name in [\"labels\", \"logits\", \"weights\"]:\n    if arg_name not in args:\n      raise ValueError(\"Argument %s not found in loss_fn.\" % arg_name)\n\n\ndef _centered_bias(logits_dimension, head_name=None):\n  \"\"\"Returns centered_bias `Variable`.\n\n  Args:\n    logits_dimension: Last dimension of `logits`. Must be >= 1.\n    head_name: Optional name of the head.\n\n  Returns:\n    `Variable` with shape `[logits_dimension]`.\n\n  Raises:\n    ValueError: if `logits_dimension` is invalid.\n  \"\"\"\n  if (logits_dimension is None) or (logits_dimension < 1):\n    raise ValueError(\"Invalid logits_dimension %s.\" % logits_dimension)\n  # Do not create a variable with variable_scope.get_variable, because that may\n  # create a PartitionedVariable, which does not support indexing, so\n  # summary.scalar will not work.\n  centered_bias = variable_scope.variable(\n      name=\"centered_bias_weight\",\n      initial_value=array_ops.zeros(shape=(logits_dimension,)),\n      trainable=True)\n  for dim in range(logits_dimension):\n    if head_name:\n      summary.scalar(\"centered_bias/bias_%d/%s\" % (dim, head_name),\n                     centered_bias[dim])\n    else:\n      summary.scalar(\"centered_bias/bias_%d\" % dim, centered_bias[dim])\n  return centered_bias\n\n\ndef _centered_bias_step(centered_bias, batch_size, labels, loss_fn, weights):\n  \"\"\"Creates and returns training op for centered bias.\"\"\"\n  with ops.name_scope(None, \"centered_bias_step\", (labels,)) as name:\n    logits_dimension = array_ops.shape(centered_bias)[0]\n    logits = array_ops.reshape(\n        array_ops.tile(centered_bias, (batch_size,)),\n        (batch_size, logits_dimension))\n    with ops.name_scope(None, \"centered_bias\", (labels, logits)):\n      centered_bias_loss = math_ops.reduce_mean(\n          loss_fn(labels, logits, weights), name=\"training_loss\")\n  # Learn central bias by an optimizer. 0.1 is a convervative lr for a\n  # single variable.\n  return training.AdagradOptimizer(0.1).minimize(\n      centered_bias_loss, var_list=(centered_bias,), name=name)\n\n\ndef _summary_key(head_name, val):\n  return \"%s/%s\" % (val, head_name) if head_name else val\n\n\ndef _train_op(loss, labels, train_op_fn, centered_bias, batch_size, loss_fn,\n              weights):\n  \"\"\"Returns op for the training step.\"\"\"\n  if centered_bias is not None:\n    centered_bias_step = _centered_bias_step(\n        centered_bias=centered_bias,\n        batch_size=batch_size,\n        labels=labels,\n        loss_fn=loss_fn,\n        weights=weights)\n  else:\n    centered_bias_step = None\n  with ops.name_scope(None, \"train_op\", (loss, labels)):\n    train_op = train_op_fn(loss)\n    if centered_bias_step is not None:\n      train_op = control_flow_ops.group(train_op, centered_bias_step)\n    return train_op\n\n\ndef _sigmoid_cross_entropy_loss(labels, logits, weights=None):\n  with ops.name_scope(None, \"sigmoid_cross_entropy_loss\",\n                      (logits, labels)) as name:\n    # sigmoid_cross_entropy_with_logits requires [batch_size, n_classes] labels.\n    loss = nn.sigmoid_cross_entropy_with_logits(\n        labels=math_ops.to_float(labels), logits=logits, name=name)\n    return _compute_weighted_loss(loss, weights)\n\n\ndef _float_weights_or_none(weights):\n  if weights is None:\n    return None\n  with ops.name_scope(None, \"float_weights\", (weights,)) as name:\n    return math_ops.to_float(weights, name=name)\n\n\ndef _indicator_labels_streaming_mean(labels, weights=None, class_id=None):\n  labels = math_ops.to_float(labels)\n  weights = _float_weights_or_none(weights)\n  if weights is not None:\n    weights = weights_broadcast_ops.broadcast_weights(weights, labels)\n  if class_id is not None:\n    if weights is not None:\n      weights = weights[:, class_id]\n    labels = labels[:, class_id]\n  return metrics_lib.mean(labels, weights)\n\n\ndef _predictions_streaming_mean(predictions,\n                                weights=None,\n                                class_id=None):\n  predictions = math_ops.to_float(predictions)\n  weights = _float_weights_or_none(weights)\n  if weights is not None:\n    weights = weights_broadcast_ops.broadcast_weights(weights, predictions)\n  if class_id is not None:\n    if weights is not None:\n      weights = weights[:, class_id]\n    predictions = predictions[:, class_id]\n  return metrics_lib.mean(predictions, weights)\n\n\n# TODO(ptucker): Add support for SparseTensor labels.\ndef _class_id_labels_to_indicator(labels, num_classes):\n  if (num_classes is None) or (num_classes < 2):\n    raise ValueError(\"Invalid num_classes %s.\" % num_classes)\n  with ops.control_dependencies((_assert_labels_rank(labels),)):\n    labels = array_ops.reshape(labels, (-1,))\n  return array_ops.one_hot(labels, depth=num_classes, axis=-1)\n\n\ndef _class_predictions_streaming_mean(predictions, weights, class_id):\n  return metrics_lib.mean(\n      array_ops.where(\n          math_ops.equal(\n              math_ops.to_int32(class_id), math_ops.to_int32(predictions)),\n          array_ops.ones_like(predictions),\n          array_ops.zeros_like(predictions)),\n      weights=weights)\n\n\ndef _class_labels_streaming_mean(labels, weights, class_id):\n  return metrics_lib.mean(\n      array_ops.where(\n          math_ops.equal(\n              math_ops.to_int32(class_id), math_ops.to_int32(labels)),\n          array_ops.ones_like(labels), array_ops.zeros_like(labels)),\n      weights=weights)\n\n\ndef _streaming_auc(predictions, labels, weights=None, class_id=None,\n                   curve=\"ROC\"):\n  # pylint: disable=missing-docstring\n  predictions = math_ops.to_float(predictions)\n  if labels.dtype.base_dtype != dtypes.bool:\n    logging.warning(\"Casting %s labels to bool.\", labels.dtype)\n    labels = math_ops.cast(labels, dtypes.bool)\n  weights = _float_weights_or_none(weights)\n  if weights is not None:\n    weights = weights_broadcast_ops.broadcast_weights(weights, predictions)\n  if class_id is not None:\n    if weights is not None:\n      weights = weights[:, class_id]\n    predictions = predictions[:, class_id]\n    labels = labels[:, class_id]\n  return metrics_lib.auc(labels, predictions, weights, curve=curve)\n\n\ndef _assert_class_id(class_id, num_classes=None):\n  \"\"\"Average label value for class `class_id`.\"\"\"\n  if (class_id is None) or (class_id < 0):\n    raise ValueError(\"Invalid class_id %s.\" % class_id)\n  if num_classes is not None:\n    if num_classes < 2:\n      raise ValueError(\"Invalid num_classes %s.\" % num_classes)\n    if class_id >= num_classes:\n      raise ValueError(\"Invalid class_id %s.\" % class_id)\n\n\ndef _streaming_accuracy_at_threshold(predictions, labels, weights, threshold):\n  threshold_predictions = math_ops.to_float(\n      math_ops.greater_equal(predictions, threshold))\n  return metrics_lib.accuracy(labels, threshold_predictions, weights)\n\n\ndef _streaming_precision_at_threshold(predictions, labels, weights, threshold):\n  precision_tensor, update_op = metrics_lib.precision_at_thresholds(\n      labels, predictions, (threshold,), _float_weights_or_none(weights))\n  return array_ops.squeeze(precision_tensor), array_ops.squeeze(update_op)\n\n\ndef _streaming_recall_at_threshold(predictions, labels, weights, threshold):\n  precision_tensor, update_op = metrics_lib.recall_at_thresholds(\n      labels, predictions, (threshold,), _float_weights_or_none(weights))\n  return array_ops.squeeze(precision_tensor), array_ops.squeeze(update_op)\n\n\ndef _classification_output_alternatives(head_name, problem_type,\n                                        label_keys=None):\n  \"\"\"Creates a func to generate output alternatives for classification.\n\n  Servo expects classes to be a string tensor, and have the same dimensions\n  as the probabilities tensor. It should contain the labels of the corresponding\n  entries in probabilities. This function creates a new classes tensor that\n  satisfies these conditions and can be exported.\n\n  Args:\n    head_name: Name of the head.\n    problem_type: `ProblemType`\n    label_keys: Optional label keys\n\n  Returns:\n    A function to generate output alternatives.\n  \"\"\"\n  def _create_output_alternatives(predictions):\n    \"\"\"Creates output alternative for the Head.\n\n    Args:\n      predictions: a dict of {tensor_name: Tensor}, where 'tensor_name' is a\n        symbolic name for an output Tensor possibly but not necessarily taken\n        from `PredictionKey`, and 'Tensor' is the corresponding output Tensor\n        itself.\n\n    Returns:\n      `dict` of {submodel_name: (problem_type, {tensor_name: Tensor})}, where\n      'submodel_name' is a submodel identifier that should be consistent across\n      the pipeline (here likely taken from the head_name),\n      'problem_type' is a `ProblemType`,\n      'tensor_name' is a symbolic name for an output Tensor possibly but not\n       necessarily taken from `PredictionKey`, and\n      'Tensor' is the corresponding output Tensor itself.\n\n    Raises:\n      ValueError: if predictions does not have PredictionKey.PROBABILITIES key.\n    \"\"\"\n    probabilities = predictions.get(prediction_key.PredictionKey.PROBABILITIES)\n    if probabilities is None:\n      raise ValueError(\"%s missing in predictions\" %\n                       prediction_key.PredictionKey.PROBABILITIES)\n\n    with ops.name_scope(None, \"_classification_output_alternatives\",\n                        (probabilities,)):\n      batch_size = array_ops.shape(probabilities)[0]\n      if label_keys:\n        classes = array_ops.tile(\n            input=array_ops.expand_dims(input=label_keys, axis=0),\n            multiples=[batch_size, 1],\n            name=\"classes_tensor\")\n      else:\n        n = array_ops.shape(probabilities)[1]\n        classes = array_ops.tile(\n            input=array_ops.expand_dims(input=math_ops.range(n), axis=0),\n            multiples=[batch_size, 1])\n        classes = string_ops.as_string(classes, name=\"classes_tensor\")\n\n    exported_predictions = {\n        prediction_key.PredictionKey.PROBABILITIES: probabilities,\n        prediction_key.PredictionKey.CLASSES: classes}\n    return {head_name: (problem_type, exported_predictions)}\n\n  return _create_output_alternatives\n\n# Aliases\n# TODO(zakaria): Remove these aliases, See b/34751732\n_regression_head = regression_head\n_poisson_regression_head = poisson_regression_head\n_multi_class_head = multi_class_head\n_binary_svm_head = binary_svm_head\n_multi_label_head = multi_label_head\n_multi_head = multi_head\n_Head = Head\n", "framework": "tensorflow"}
{"repo_name": "allenlavoie/tensorflow", "file_path": "tensorflow/contrib/slim/python/slim/learning.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Contains TF-Slim code for training models.\n\nThis script contains various functions for training models. These include\nmanipulating gradients, creating a `train_op` (an operation that computes the\nloss and applies the gradients) and a training loop function. The training loop\nallows the user to pass in the `train_op` and runs the optimization according\nto user-specified arguments. Note that the training loop uses the\ntf.train.Supervisor and its managed_session in its implementation to ensure the\nability of worker processes to recover from failures.\n\n************************************\n* A simple working training script *\n************************************\n\n  # Load data and create the model:\n  images, labels = LoadData(...)\n  predictions = MyModel(images)\n\n  # Define the loss:\n  slim.losses.log_loss(predictions, labels)\n  total_loss = slim.losses.get_total_loss()\n\n  # Define the optimizer:\n  optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n\n  # Create the train_op\n  train_op = slim.learning.create_train_op(total_loss, optimizer)\n\n  # Run training.\n  slim.learning.train(train_op, my_log_dir)\n\n*************************\n* Creating the train_op *\n*************************\n\nIn order to train, TF-Slim's train loop needs a train_op: an `Operation` that\n(a) computes the loss, (b) applies the gradients to update the weights and\n(c) returns the value of the loss. slim.learning.create_train_op creates\nsuch an `Operation`. This function also provides the ability to manipulate\nthe gradients using a few arguments:\n\n  # Create the train_op and clip the gradient norms:\n  train_op = slim.learning.create_train_op(\n      total_loss,\n      optimizer,\n      clip_gradient_norm=4)\n\n  # Create the train_op and scale the gradients by providing a map from variable\n  # name (or variable) to a scaling coefficient:\n  gradient_multipliers = {\n    'conv0/weights': 1.2,\n    'fc8/weights': 3.4,\n  }\n  train_op = slim.learning.create_train_op(\n      total_loss,\n      optimizer,\n      gradient_multipliers=gradient_multipliers)\n\n****************************************************************\n* Performing additional (non-gradient) updates during training *\n****************************************************************\n\nMany networks utilize modules, like BatchNorm, that require performing a series\nof non-gradient updates during training. slim.learning.create_train_op allows\na user to pass in a list of update_ops to call along with the gradient updates.\n\n  train_op = slim.learning.create_train_op(total_loss, optimizer, update_ops)\n\nBy default, slim.learning.create_train_op includes all update ops that are\npart of the `tf.GraphKeys.UPDATE_OPS` collection. Additionally, TF-Slim's\nslim.batch_norm function adds the moving mean and moving variance updates to\nthis collection. Consequently, users who want to use slim.batch_norm will not\nneed to take any additional steps in order to have the moving mean and moving\nvariance updates be computed.\n\nHowever, users with additional, specialized updates can either override the\ndefault update ops or simply add additional update ops to the\n`tf.GraphKeys.UPDATE_OPS` collection:\n\n  # Force TF-Slim NOT to use ANY update_ops:\n  train_op = slim.learning.create_train_op(\n     total_loss,\n     optimizer,\n     update_ops=[])\n\n  # Use an alternative set of update ops:\n  train_op = slim.learning.create_train_op(\n     total_loss,\n     optimizer,\n     update_ops=my_other_update_ops)\n\n  # Use an alternative set of update ops in addition to the default updates:\n  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, my_update0)\n  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, my_update1)\n\n  train_op = slim.learning.create_train_op(\n     total_loss,\n     optimizer)\n\n  # Which is the same as:\n  train_op = slim.learning.create_train_op(\n     total_loss,\n     optimizer,\n     update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n\n******************************************\n* Initializing a model from a checkpoint *\n******************************************\n\nIt is common to want to 'warm-start' a model from a pre-trained checkpoint.\nTF-Slim provides a convenient mechanism for doing so:\n\n  ...\n\n  # Create the train_op\n  train_op = slim.learning.create_train_op(total_loss, optimizer)\n\n  # Create the initial assignment op\n  checkpoint_path = '/path/to/old_model_checkpoint'\n  variables_to_restore = slim.get_model_variables()\n  init_assign_op, init_feed_dict = slim.assign_from_checkpoint(\n      checkpoint_path, variables_to_restore)\n\n  # Create an initial assignment function.\n  def InitAssignFn(sess):\n      sess.run(init_assign_op, init_feed_dict)\n\n  # Run training.\n  slim.learning.train(train_op, my_log_dir, init_fn=InitAssignFn)\n\n***************************************************************************\n* Initializing a model from a checkpoint whose variable names don't match *\n***************************************************************************\n\nAt times, a user may want to initialize a new model with values from a\ncheckpoint whose variable names do not match those of the current model. In this\ncase, one needs to create a mapping from the checkpoint variable names to the\ncurrent model variables. This requires only a small modification of the code\nabove:\n  ...\n  # Creates a model with two variables, var0 and var1\n  predictions = MyModel(images)\n  ...\n\n  # Create the train_op\n  train_op = slim.learning.create_train_op(total_loss, optimizer)\n\n  checkpoint_path = '/path/to/old_model_checkpoint'\n\n  # Create the mapping:\n  variables_to_restore = {\n      'name_var_0_in_checkpoint': slim.get_unique_variable('var0'),\n      'name_var_1_in_checkpoint': slim.get_unique_variable('var1')\n  }\n  init_assign_op, init_feed_dict = slim.assign_from_checkpoint(\n      checkpoint_path, variables_to_restore)\n\n  # Create an initial assignment function.\n  def InitAssignFn(sess):\n      sess.run(init_assign_op, init_feed_dict)\n\n  # Run training.\n  slim.learning.train(train_op, my_log_dir, init_fn=InitAssignFn)\n\n\n*************************************************\n* Fine-Tuning Part of a model from a checkpoint *\n*************************************************\n\nRather than initializing all of the weights of a given model, we sometimes\nonly want to restore some of the weights from a checkpoint. To do this, one\nneed only filter those variables to initialize as follows:\n\n  ...\n\n  # Create the train_op\n  train_op = slim.learning.create_train_op(total_loss, optimizer)\n\n  checkpoint_path = '/path/to/old_model_checkpoint'\n\n  # Specify the variables to restore via a list of inclusion or exclusion\n  # patterns:\n  variables_to_restore = slim.get_variables_to_restore(\n      include=[\"conv\"], exclude=[\"fc8\", \"fc9])\n  # or\n  variables_to_restore = slim.get_variables_to_restore(exclude=[\"conv\"])\n\n  init_assign_op, init_feed_dict = slim.assign_from_checkpoint(\n      checkpoint_path, variables_to_restore)\n\n  # Create an initial assignment function.\n  def InitAssignFn(sess):\n      sess.run(init_assign_op, init_feed_dict)\n\n  # Run training.\n  slim.learning.train(train_op, my_log_dir, init_fn=InitAssignFn)\n\n******************************************************\n* Initializing model variables from values in memory *\n******************************************************\n\nOne may want to initialize the weights of a model from values from an arbitrary\nsource (a text document, matlab file, etc). While this is technically feasible\nusing plain TensorFlow, it also results in the values of your weights being\nstored in the graph. For large models, this becomes prohibitively large. TF-Slim\nallows you to perform this initial assignment without having to store the values\nof the initial model in the graph itself by using placeholders and a feed\ndictionary:\n\n  ...\n\n  # Create the train_op\n  train_op = slim.learning.create_train_op(total_loss, optimizer)\n\n  # Create the mapping from variable names to values:\n  var0_initial_value = ReadFromDisk(...)\n  var1_initial_value = ReadFromDisk(...)\n\n  var_names_to_values = {\n    'var0': var0_initial_value,\n    'var1': var1_initial_value,\n  }\n  init_assign_op, init_feed_dict = slim.assign_from_values(var_names_to_values)\n\n  # Create an initial assignment function.\n  def InitAssignFn(sess):\n      sess.run(init_assign_op, init_feed_dict)\n\n  # Run training.\n  slim.learning.train(train_op, my_log_dir, init_fn=InitAssignFn)\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\n\nfrom tensorflow.contrib.training.python.training import training\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import timeline\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import lookup_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.summary import summary\nfrom tensorflow.python.training import optimizer as tf_optimizer\nfrom tensorflow.python.training import saver as tf_saver\nfrom tensorflow.python.training import supervisor\nfrom tensorflow.python.training import sync_replicas_optimizer\nfrom tensorflow.python.training import training_util\n\n__all__ = [\n    'add_gradients_summaries', 'clip_gradient_norms', 'multiply_gradients',\n    'create_train_op', 'train_step', 'train'\n]\n\n\ndef clip_gradient_norms(gradients_to_variables, max_norm):\n  \"\"\"Clips the gradients by the given value.\n\n  Args:\n    gradients_to_variables: A list of gradient to variable pairs (tuples).\n    max_norm: the maximum norm value.\n\n  Returns:\n    A list of clipped gradient to variable pairs.\n  \"\"\"\n  clipped_grads_and_vars = []\n  for grad, var in gradients_to_variables:\n    if grad is not None:\n      if isinstance(grad, ops.IndexedSlices):\n        tmp = clip_ops.clip_by_norm(grad.values, max_norm)\n        grad = ops.IndexedSlices(tmp, grad.indices, grad.dense_shape)\n      else:\n        grad = clip_ops.clip_by_norm(grad, max_norm)\n    clipped_grads_and_vars.append((grad, var))\n  return clipped_grads_and_vars\n\n\ndef multiply_gradients(grads_and_vars, gradient_multipliers):\n  \"\"\"Multiply specified gradients.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n    gradient_multipliers: A map from either `Variables` or `Variable` op names\n      to the coefficient by which the associated gradient should be scaled.\n\n  Returns:\n    The updated list of gradient to variable pairs.\n\n  Raises:\n    ValueError: If `grads_and_vars` is not a list or if `gradient_multipliers`\n    is empty or None or if `gradient_multipliers` is not a dictionary.\n  \"\"\"\n  if not isinstance(grads_and_vars, list):\n    raise ValueError('`grads_and_vars` must be a list.')\n  if not gradient_multipliers:\n    raise ValueError('`gradient_multipliers` is empty.')\n  if not isinstance(gradient_multipliers, dict):\n    raise ValueError('`gradient_multipliers` must be a dict.')\n\n  multiplied_grads_and_vars = []\n  for grad, var in grads_and_vars:\n    if var in gradient_multipliers or var.op.name in gradient_multipliers:\n      key = var if var in gradient_multipliers else var.op.name\n      if grad is None:\n        raise ValueError('Requested multiple of `None` gradient.')\n\n      multiplier = gradient_multipliers[key]\n      if not isinstance(multiplier, ops.Tensor):\n        multiplier = constant_op.constant(multiplier, dtype=grad.dtype)\n\n      if isinstance(grad, ops.IndexedSlices):\n        tmp = grad.values * multiplier\n        grad = ops.IndexedSlices(tmp, grad.indices, grad.dense_shape)\n      else:\n        grad *= multiplier\n    multiplied_grads_and_vars.append((grad, var))\n  return multiplied_grads_and_vars\n\n\ndef add_gradients_summaries(grads_and_vars):\n  \"\"\"Add summaries to gradients.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n\n  Returns:\n    The list of created summaries.\n  \"\"\"\n  summaries = []\n  for grad, var in grads_and_vars:\n    if grad is not None:\n      if isinstance(grad, ops.IndexedSlices):\n        grad_values = grad.values\n      else:\n        grad_values = grad\n      summaries.append(\n          summary.histogram(var.op.name + '/gradient', grad_values))\n      summaries.append(\n          summary.scalar(var.op.name + '/gradient_norm',\n                         clip_ops.global_norm([grad_values])))\n    else:\n      logging.info('Var %s has no gradient', var.op.name)\n\n  return summaries\n\n\n_USE_GLOBAL_STEP = 0\n\n\ndef create_train_op(total_loss,\n                    optimizer,\n                    global_step=_USE_GLOBAL_STEP,\n                    update_ops=None,\n                    variables_to_train=None,\n                    clip_gradient_norm=0,\n                    summarize_gradients=False,\n                    gate_gradients=tf_optimizer.Optimizer.GATE_OP,\n                    aggregation_method=None,\n                    colocate_gradients_with_ops=False,\n                    gradient_multipliers=None,\n                    check_numerics=True):\n  \"\"\"Creates an `Operation` that evaluates the gradients and returns the loss.\n\n  Args:\n    total_loss: A `Tensor` representing the total loss.\n    optimizer: A tf.Optimizer to use for computing the gradients.\n    global_step: A `Tensor` representing the global step variable. If left as\n      `_USE_GLOBAL_STEP`, then slim.variables.global_step() is used.\n    update_ops: An optional list of updates to execute. If `update_ops` is\n      `None`, then the update ops are set to the contents of the\n      `tf.GraphKeys.UPDATE_OPS` collection. If `update_ops` is not `None`, but\n      it doesn't contain all of the update ops in `tf.GraphKeys.UPDATE_OPS`,\n      a warning will be displayed.\n    variables_to_train: an optional list of variables to train. If None, it will\n      default to all tf.trainable_variables().\n    clip_gradient_norm: If greater than 0 then the gradients would be clipped\n      by it.\n    summarize_gradients: Whether or not add summaries for each gradient.\n    gate_gradients: How to gate the computation of gradients. See tf.Optimizer.\n    aggregation_method: Specifies the method used to combine gradient terms.\n      Valid values are defined in the class `AggregationMethod`.\n    colocate_gradients_with_ops: Whether or not to try colocating the gradients\n      with the ops that generated them.\n    gradient_multipliers: A dictionary of either `Variables` or `Variable` op\n      names to the coefficient by which the associated gradient should be\n      scaled.\n    check_numerics: Whether or not we apply check_numerics.\n\n  Returns:\n    A `Tensor` that when evaluated, computes the gradients and returns the total\n      loss value.\n  \"\"\"\n  def transform_grads_fn(grads):\n    if gradient_multipliers:\n      with ops.name_scope('multiply_grads'):\n        grads = multiply_gradients(grads, gradient_multipliers)\n\n    # Clip gradients.\n    if clip_gradient_norm > 0:\n      with ops.name_scope('clip_grads'):\n        grads = clip_gradient_norms(grads, clip_gradient_norm)\n    return grads\n\n  return training.create_train_op(\n      total_loss=total_loss,\n      optimizer=optimizer,\n      global_step=global_step,\n      update_ops=update_ops,\n      variables_to_train=variables_to_train,\n      transform_grads_fn=transform_grads_fn,\n      summarize_gradients=summarize_gradients,\n      gate_gradients=gate_gradients,\n      aggregation_method=aggregation_method,\n      colocate_gradients_with_ops=colocate_gradients_with_ops,\n      check_numerics=check_numerics)\n\n\ndef _wait_for_step(sess, global_step, step):\n  \"\"\"Wait till the global step has reached at least 'step'.\n\n  Args:\n    sess: A session.\n    global_step: A Tensor.\n    step: Int.  The global step to reach.\n  \"\"\"\n  while True:\n    if training_util.global_step(sess, global_step) >= step:\n      break\n    time.sleep(1.0)\n\n\ndef train_step(sess, train_op, global_step, train_step_kwargs):\n  \"\"\"Function that takes a gradient step and specifies whether to stop.\n\n  Args:\n    sess: The current session.\n    train_op: An `Operation` that evaluates the gradients and returns the\n      total loss.\n    global_step: A `Tensor` representing the global training step.\n    train_step_kwargs: A dictionary of keyword arguments.\n\n  Returns:\n    The total loss and a boolean indicating whether or not to stop training.\n\n  Raises:\n    ValueError: if 'should_trace' is in `train_step_kwargs` but `logdir` is not.\n  \"\"\"\n  start_time = time.time()\n\n  trace_run_options = None\n  run_metadata = None\n  if 'should_trace' in train_step_kwargs:\n    if 'logdir' not in train_step_kwargs:\n      raise ValueError('logdir must be present in train_step_kwargs when '\n                       'should_trace is present')\n    if sess.run(train_step_kwargs['should_trace']):\n      trace_run_options = config_pb2.RunOptions(\n          trace_level=config_pb2.RunOptions.FULL_TRACE)\n      run_metadata = config_pb2.RunMetadata()\n\n  total_loss, np_global_step = sess.run([train_op, global_step],\n                                        options=trace_run_options,\n                                        run_metadata=run_metadata)\n  time_elapsed = time.time() - start_time\n\n  if run_metadata is not None:\n    tl = timeline.Timeline(run_metadata.step_stats)\n    trace = tl.generate_chrome_trace_format()\n    trace_filename = os.path.join(train_step_kwargs['logdir'],\n                                  'tf_trace-%d.json' % np_global_step)\n    logging.info('Writing trace to %s', trace_filename)\n    file_io.write_string_to_file(trace_filename, trace)\n    if 'summary_writer' in train_step_kwargs:\n      train_step_kwargs['summary_writer'].add_run_metadata(run_metadata,\n                                                           'run_metadata-%d' %\n                                                           np_global_step)\n\n  if 'should_log' in train_step_kwargs:\n    if sess.run(train_step_kwargs['should_log']):\n      logging.info('global step %d: loss = %.4f (%.3f sec/step)',\n                   np_global_step, total_loss, time_elapsed)\n\n  # TODO(nsilberman): figure out why we can't put this into sess.run. The\n  # issue right now is that the stop check depends on the global step. The\n  # increment of global step often happens via the train op, which used\n  # created using optimizer.apply_gradients.\n  #\n  # Since running `train_op` causes the global step to be incremented, one\n  # would expected that using a control dependency would allow the\n  # should_stop check to be run in the same session.run call:\n  #\n  #   with ops.control_dependencies([train_op]):\n  #     should_stop_op = ...\n  #\n  # However, this actually seems not to work on certain platforms.\n  if 'should_stop' in train_step_kwargs:\n    should_stop = sess.run(train_step_kwargs['should_stop'])\n  else:\n    should_stop = False\n\n  return total_loss, should_stop\n\n\n_USE_DEFAULT = 0\n\n\ndef train(train_op,\n          logdir,\n          train_step_fn=train_step,\n          train_step_kwargs=_USE_DEFAULT,\n          log_every_n_steps=1,\n          graph=None,\n          master='',\n          is_chief=True,\n          global_step=None,\n          number_of_steps=None,\n          init_op=_USE_DEFAULT,\n          init_feed_dict=None,\n          local_init_op=_USE_DEFAULT,\n          init_fn=None,\n          ready_op=_USE_DEFAULT,\n          summary_op=_USE_DEFAULT,\n          save_summaries_secs=600,\n          summary_writer=_USE_DEFAULT,\n          startup_delay_steps=0,\n          saver=None,\n          save_interval_secs=600,\n          sync_optimizer=None,\n          session_config=None,\n          session_wrapper=None,\n          trace_every_n_steps=None,\n          ignore_live_threads=False):\n  \"\"\"Runs a training loop using a TensorFlow supervisor.\n\n  When the sync_optimizer is supplied, gradient updates are applied\n  synchronously. Otherwise, gradient updates are applied asynchronous.\n\n  Args:\n    train_op: A `Tensor` that, when executed, will apply the gradients and\n      return the loss value.\n    logdir: The directory where training logs are written to. If None, model\n      checkpoints and summaries will not be written.\n    train_step_fn: The function to call in order to execute a single gradient\n      step. The function must have take exactly four arguments: the current\n      session, the `train_op` `Tensor`, a global step `Tensor` and a dictionary.\n    train_step_kwargs: A dictionary which is passed to the `train_step_fn`. By\n      default, two `Boolean`, scalar ops called \"should_stop\" and \"should_log\"\n      are provided.\n    log_every_n_steps: The frequency, in terms of global steps, that the loss\n      and global step and logged.\n    graph: The graph to pass to the supervisor. If no graph is supplied the\n      default graph is used.\n    master: The address of the tensorflow master.\n    is_chief: Specifies whether or not the training is being run by the primary\n      replica during replica training.\n    global_step: The `Tensor` representing the global step. If left as `None`,\n      then slim.variables.get_or_create_global_step() is used.\n    number_of_steps: The max number of gradient steps to take during training,\n      as measured by 'global_step': training will stop if global_step is\n      greater than 'number_of_steps'. If the value is left as None, training\n      proceeds indefinitely.\n    init_op: The initialization operation. If left to its default value, then\n      the session is initialized by calling `tf.global_variables_initializer()`.\n    init_feed_dict: A feed dictionary to use when executing the `init_op`.\n    local_init_op: The local initialization operation. If left to its default\n      value, then the session is initialized by calling\n      `tf.local_variables_initializer()` and `tf.tables_initializer()`.\n    init_fn: An optional callable to be executed after `init_op` is called. The\n      callable must accept one argument, the session being initialized.\n    ready_op: Operation to check if the model is ready to use. If left to its\n      default value, then the session checks for readiness by calling\n      `tf.report_uninitialized_variables()`.\n    summary_op: The summary operation.\n    save_summaries_secs: How often, in seconds, to save summaries.\n    summary_writer: `SummaryWriter` to use.  Can be `None`\n      to indicate that no summaries should be written. If unset, we\n      create a SummaryWriter.\n    startup_delay_steps: The number of steps to wait for before beginning. Note\n      that this must be 0 if a sync_optimizer is supplied.\n    saver: Saver to save checkpoints. If None, a default one will be created\n      and used.\n    save_interval_secs: How often, in seconds, to save the model to `logdir`.\n    sync_optimizer: an instance of tf.train.SyncReplicasOptimizer, or a list of\n      them. If the argument is supplied, gradient updates will be synchronous.\n      If left as `None`, gradient updates will be asynchronous.\n    session_config: An instance of `tf.ConfigProto` that will be used to\n      configure the `Session`. If left as `None`, the default will be used.\n    session_wrapper: A function that takes a `tf.Session` object as the only\n      argument and returns a wrapped session object that has the same methods\n      that the original object has, or `None`. Iff not `None`, the wrapped\n      object will be used for training.\n    trace_every_n_steps: produce and save a `Timeline` in Chrome trace format\n      and add it to the summaries every `trace_every_n_steps`. If None, no trace\n      information will be produced or saved.\n    ignore_live_threads: If `True` ignores threads that remain running after\n      a grace period when stopping the supervisor, instead of raising a\n      RuntimeError.\n\n  Returns:\n    the value of the loss function after training.\n\n  Raises:\n    ValueError: if `train_op` is empty or if `startup_delay_steps` is\n      non-zero when `sync_optimizer` is supplied, if `number_of_steps` is\n      negative, or if `trace_every_n_steps` is not `None` and no `logdir` is\n      provided.\n  \"\"\"\n  if train_op is None:\n    raise ValueError('train_op cannot be None.')\n\n  if logdir is None:\n    if summary_op != _USE_DEFAULT:\n      raise ValueError('Cannot provide summary_op because logdir=None')\n    if saver is not None:\n      raise ValueError('Cannot provide saver because logdir=None')\n    if trace_every_n_steps is not None:\n      raise ValueError('Cannot provide trace_every_n_steps because '\n                       'logdir=None')\n\n  if isinstance(sync_optimizer, sync_replicas_optimizer.SyncReplicasOptimizer):\n    sync_optimizer = [sync_optimizer]\n  if sync_optimizer is not None and startup_delay_steps > 0:\n    raise ValueError(\n        'startup_delay_steps must be zero when sync_optimizer is supplied.')\n\n  if number_of_steps is not None and number_of_steps <= 0:\n    raise ValueError(\n        '`number_of_steps` must be either None or a positive number.')\n\n  graph = graph or ops.get_default_graph()\n  with graph.as_default():\n    if global_step is None:\n      global_step = training_util.get_or_create_global_step()\n    saver = saver or tf_saver.Saver()\n\n    if sync_optimizer is not None:\n      for opt in sync_optimizer:\n        if not isinstance(opt, sync_replicas_optimizer.SyncReplicasOptimizer):\n          raise ValueError(\n              '`sync_optimizer` must be a tf.train.SyncReplicasOptimizer.')\n\n    with ops.name_scope('init_ops'):\n      if init_op == _USE_DEFAULT:\n        init_op = variables.global_variables_initializer()\n\n      if ready_op == _USE_DEFAULT:\n        ready_op = variables.report_uninitialized_variables()\n\n      if local_init_op == _USE_DEFAULT:\n        local_init_op = control_flow_ops.group(\n            variables.local_variables_initializer(),\n            lookup_ops.tables_initializer())\n\n      if sync_optimizer is not None and isinstance(sync_optimizer, list):\n        with ops.control_dependencies([local_init_op] if local_init_op is\n                                      not None else []):\n          if is_chief:\n            local_init_op = control_flow_ops.group(\n                *[opt.chief_init_op for opt in sync_optimizer])\n          else:\n            local_init_op = control_flow_ops.group(\n                *[opt.local_step_init_op for opt in sync_optimizer])\n        ready_for_local_init_op = control_flow_ops.group(\n            *[opt.ready_for_local_init_op for opt in sync_optimizer])\n      else:\n        ready_for_local_init_op = None\n\n    if summary_op == _USE_DEFAULT:\n      summary_op = summary.merge_all()\n\n    if summary_writer == _USE_DEFAULT:\n      summary_writer = supervisor.Supervisor.USE_DEFAULT\n\n    if is_chief and sync_optimizer is not None:\n      # Need to create these BEFORE the supervisor finalizes the graph:\n      init_tokens_op = [opt.get_init_tokens_op() for opt in sync_optimizer]\n      chief_queue_runner = [\n          opt.get_chief_queue_runner() for opt in sync_optimizer]\n\n    if train_step_kwargs == _USE_DEFAULT:\n      with ops.name_scope('train_step'):\n        train_step_kwargs = {}\n\n        if number_of_steps:\n          should_stop_op = math_ops.greater_equal(global_step, number_of_steps)\n        else:\n          should_stop_op = constant_op.constant(False)\n        train_step_kwargs['should_stop'] = should_stop_op\n        if log_every_n_steps > 0:\n          train_step_kwargs['should_log'] = math_ops.equal(\n              math_ops.mod(global_step, log_every_n_steps), 0)\n        if is_chief and trace_every_n_steps is not None:\n          train_step_kwargs['should_trace'] = math_ops.equal(\n              math_ops.mod(global_step, trace_every_n_steps), 0)\n          train_step_kwargs['logdir'] = logdir\n\n  sv = supervisor.Supervisor(\n      graph=graph,\n      is_chief=is_chief,\n      logdir=logdir,\n      init_op=init_op,\n      init_feed_dict=init_feed_dict,\n      local_init_op=local_init_op,\n      ready_for_local_init_op=ready_for_local_init_op,\n      ready_op=ready_op,\n      summary_op=summary_op,\n      summary_writer=summary_writer,\n      global_step=global_step,\n      saver=saver,\n      save_summaries_secs=save_summaries_secs,\n      save_model_secs=save_interval_secs,\n      init_fn=init_fn)\n\n  if summary_writer is not None:\n    train_step_kwargs['summary_writer'] = sv.summary_writer\n\n  total_loss = None\n  should_retry = True\n  while should_retry:\n    try:\n      should_retry = False\n      with sv.managed_session(\n          master, start_standard_services=False, config=session_config) as sess:\n        logging.info('Starting Session.')\n        if session_wrapper is not None:\n          logging.info(\n              'Wrapping session with wrapper function: %s', session_wrapper)\n          sess = session_wrapper(sess)\n        if is_chief:\n          if logdir:\n            sv.start_standard_services(sess)\n        elif startup_delay_steps > 0:\n           # (use sys.maxsize because sys.maxint doesn't exist in Python 3)\n          _wait_for_step(sess, global_step,\n                         min(startup_delay_steps, number_of_steps or\n                             sys.maxsize))\n        threads = sv.start_queue_runners(sess)\n        logging.info('Starting Queues.')\n        if is_chief and sync_optimizer is not None:\n          sv.start_queue_runners(sess, chief_queue_runner)\n          sess.run(init_tokens_op)\n        try:\n          while not sv.should_stop():\n            total_loss, should_stop = train_step_fn(\n                sess, train_op, global_step, train_step_kwargs)\n            if should_stop:\n              logging.info('Stopping Training.')\n              sv.request_stop()\n              break\n        except errors.OutOfRangeError as e:\n          # OutOfRangeError is thrown when epoch limit per\n          # tf.train.limit_epochs is reached.\n          logging.info('Caught OutOfRangeError. Stopping Training. %s', e)\n        if logdir and sv.is_chief:\n          logging.info('Finished training! Saving model to disk.')\n          sv.saver.save(sess, sv.save_path, global_step=sv.global_step)\n          sv.stop(\n              threads,\n              close_summary_writer=True,\n              ignore_live_threads=ignore_live_threads)\n\n    except errors.AbortedError:\n      # Always re-run on AbortedError as it indicates a restart of one of the\n      # distributed tensorflow servers.\n      logging.info('Retrying training!')\n      should_retry = True\n\n  return total_loss\n", "framework": "tensorflow"}
{"repo_name": "benoitsteiner/tensorflow-opencl", "file_path": "tensorflow/contrib/timeseries/examples/lstm.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"A more advanced example, of building an RNN-based time series model.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(\"TkAgg\")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We'd still like to test the TensorFlow-dependent parts of this\n  # example.\n  HAS_MATPLOTLIB = False\n\n_MODULE_PATH = path.dirname(__file__)\n_DATA_FILE = path.join(_MODULE_PATH, \"data/multivariate_periods.csv\")\n\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  \"\"\"A time series model-building example using an RNNCell.\"\"\"\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    \"\"\"Initialize/configure the model object.\n\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n\n    Args:\n      num_units: The number of units in the model's LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    \"\"\"\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we'll be outputting (just a mean here).\n        train_output_names=[\"mean\"],\n        predict_output_names=[\"mean\"],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    \"\"\"Save templates for components, which can then be used repeatedly.\n\n    This method is called every time a new graph is created. It's safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    \"\"\"\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don't have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=\"lstm_cell\",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=\"predict_from_lstm_output\",\n        func_=\n        lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    \"\"\"Return initial state for the time series model.\"\"\"\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    \"\"\"Update model state based on observations.\n\n    Note that we don't do much here aside from computing a loss. In this case\n    it's easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model's state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only \"loss\" will be optimized).\n    \"\"\"\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n        [tf.assert_equal(current_times, state_from_time)]):\n      # Subtract the mean and divide by the variance of the series.  Slightly\n      # more efficient if done for a whole window (using the normalize_features\n      # argument to SequentialTimeSeriesModel).\n      transformed_values = self._scale_data(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[\"loss\"] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won't be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    \"\"\"Advance the RNN state using a previous observation or prediction.\"\"\"\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {\"mean\": self._scale_back_data(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    \"\"\"Advance model state across a gap.\"\"\"\n    # Does not do anything special if we're jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n      self, current_times, current_exogenous_regressors, state):\n    \"\"\"Update model state based on exogenous regressors.\"\"\"\n    raise NotImplementedError(\n        \"Exogenous inputs are not implemented for this example.\")\n\n\ndef train_and_predict(csv_file_name=_DATA_FILE, training_steps=200):\n  \"\"\"Train and predict using a custom time series model.\"\"\"\n  # Construct an Estimator from our LSTM model.\n  estimator = ts_estimators.TimeSeriesRegressor(\n      model=_LSTMModel(num_features=5, num_units=128),\n      optimizer=tf.train.AdamOptimizer(0.001))\n  reader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=4, window_size=32)\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=100)))\n  times = evaluation[\"times\"][0]\n  observed = evaluation[\"observed\"][0, :, :]\n  predicted_mean = numpy.squeeze(numpy.concatenate(\n      [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\n  all_times = numpy.concatenate([times, predictions[\"times\"]], axis=0)\n  return times, observed, all_times, predicted_mean\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        \"Please install matplotlib to generate a plot from this example.\")\n  (observed_times, observations,\n   all_times, predictions) = train_and_predict()\n  pyplot.axvline(99, linestyle=\"dotted\")\n  observed_lines = pyplot.plot(\n      observed_times, observations, label=\"Observed\", color=\"k\")\n  predicted_lines = pyplot.plot(\n      all_times, predictions, label=\"Predicted\", color=\"b\")\n  pyplot.legend(handles=[observed_lines[0], predicted_lines[0]],\n                loc=\"upper left\")\n  pyplot.show()\n\n\nif __name__ == \"__main__\":\n  tf.app.run(main=main)\n", "framework": "tensorflow"}
{"repo_name": "guschmue/tensorflow", "file_path": "tensorflow/contrib/timeseries/examples/lstm.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"A more advanced example, of building an RNN-based time series model.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(\"TkAgg\")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We'd still like to test the TensorFlow-dependent parts of this\n  # example.\n  HAS_MATPLOTLIB = False\n\n_MODULE_PATH = path.dirname(__file__)\n_DATA_FILE = path.join(_MODULE_PATH, \"data/multivariate_periods.csv\")\n\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  \"\"\"A time series model-building example using an RNNCell.\"\"\"\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    \"\"\"Initialize/configure the model object.\n\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n\n    Args:\n      num_units: The number of units in the model's LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    \"\"\"\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we'll be outputting (just a mean here).\n        train_output_names=[\"mean\"],\n        predict_output_names=[\"mean\"],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    \"\"\"Save templates for components, which can then be used repeatedly.\n\n    This method is called every time a new graph is created. It's safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    \"\"\"\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don't have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=\"lstm_cell\",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=\"predict_from_lstm_output\",\n        func_=\n        lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    \"\"\"Return initial state for the time series model.\"\"\"\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    \"\"\"Update model state based on observations.\n\n    Note that we don't do much here aside from computing a loss. In this case\n    it's easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model's state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only \"loss\" will be optimized).\n    \"\"\"\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n        [tf.assert_equal(current_times, state_from_time)]):\n      # Subtract the mean and divide by the variance of the series.  Slightly\n      # more efficient if done for a whole window (using the normalize_features\n      # argument to SequentialTimeSeriesModel).\n      transformed_values = self._scale_data(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[\"loss\"] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won't be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    \"\"\"Advance the RNN state using a previous observation or prediction.\"\"\"\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {\"mean\": self._scale_back_data(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    \"\"\"Advance model state across a gap.\"\"\"\n    # Does not do anything special if we're jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n      self, current_times, current_exogenous_regressors, state):\n    \"\"\"Update model state based on exogenous regressors.\"\"\"\n    raise NotImplementedError(\n        \"Exogenous inputs are not implemented for this example.\")\n\n\ndef train_and_predict(csv_file_name=_DATA_FILE, training_steps=200):\n  \"\"\"Train and predict using a custom time series model.\"\"\"\n  # Construct an Estimator from our LSTM model.\n  estimator = ts_estimators.TimeSeriesRegressor(\n      model=_LSTMModel(num_features=5, num_units=128),\n      optimizer=tf.train.AdamOptimizer(0.001))\n  reader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=4, window_size=32)\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=100)))\n  times = evaluation[\"times\"][0]\n  observed = evaluation[\"observed\"][0, :, :]\n  predicted_mean = numpy.squeeze(numpy.concatenate(\n      [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\n  all_times = numpy.concatenate([times, predictions[\"times\"]], axis=0)\n  return times, observed, all_times, predicted_mean\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        \"Please install matplotlib to generate a plot from this example.\")\n  (observed_times, observations,\n   all_times, predictions) = train_and_predict()\n  pyplot.axvline(99, linestyle=\"dotted\")\n  observed_lines = pyplot.plot(\n      observed_times, observations, label=\"Observed\", color=\"k\")\n  predicted_lines = pyplot.plot(\n      all_times, predictions, label=\"Predicted\", color=\"b\")\n  pyplot.legend(handles=[observed_lines[0], predicted_lines[0]],\n                loc=\"upper left\")\n  pyplot.show()\n\n\nif __name__ == \"__main__\":\n  tf.app.run(main=main)\n", "framework": "tensorflow"}
{"repo_name": "alistairlow/tensorflow", "file_path": "tensorflow/contrib/timeseries/examples/lstm.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"A more advanced example, of building an RNN-based time series model.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(\"TkAgg\")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We'd still like to test the TensorFlow-dependent parts of this\n  # example.\n  HAS_MATPLOTLIB = False\n\n_MODULE_PATH = path.dirname(__file__)\n_DATA_FILE = path.join(_MODULE_PATH, \"data/multivariate_periods.csv\")\n\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  \"\"\"A time series model-building example using an RNNCell.\"\"\"\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    \"\"\"Initialize/configure the model object.\n\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n\n    Args:\n      num_units: The number of units in the model's LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    \"\"\"\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we'll be outputting (just a mean here).\n        train_output_names=[\"mean\"],\n        predict_output_names=[\"mean\"],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    \"\"\"Save templates for components, which can then be used repeatedly.\n\n    This method is called every time a new graph is created. It's safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    \"\"\"\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don't have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=\"lstm_cell\",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=\"predict_from_lstm_output\",\n        func_=\n        lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    \"\"\"Return initial state for the time series model.\"\"\"\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    \"\"\"Update model state based on observations.\n\n    Note that we don't do much here aside from computing a loss. In this case\n    it's easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model's state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only \"loss\" will be optimized).\n    \"\"\"\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n        [tf.assert_equal(current_times, state_from_time)]):\n      # Subtract the mean and divide by the variance of the series.  Slightly\n      # more efficient if done for a whole window (using the normalize_features\n      # argument to SequentialTimeSeriesModel).\n      transformed_values = self._scale_data(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[\"loss\"] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won't be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    \"\"\"Advance the RNN state using a previous observation or prediction.\"\"\"\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {\"mean\": self._scale_back_data(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    \"\"\"Advance model state across a gap.\"\"\"\n    # Does not do anything special if we're jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n      self, current_times, current_exogenous_regressors, state):\n    \"\"\"Update model state based on exogenous regressors.\"\"\"\n    raise NotImplementedError(\n        \"Exogenous inputs are not implemented for this example.\")\n\n\ndef train_and_predict(csv_file_name=_DATA_FILE, training_steps=200):\n  \"\"\"Train and predict using a custom time series model.\"\"\"\n  # Construct an Estimator from our LSTM model.\n  estimator = ts_estimators.TimeSeriesRegressor(\n      model=_LSTMModel(num_features=5, num_units=128),\n      optimizer=tf.train.AdamOptimizer(0.001))\n  reader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=4, window_size=32)\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=100)))\n  times = evaluation[\"times\"][0]\n  observed = evaluation[\"observed\"][0, :, :]\n  predicted_mean = numpy.squeeze(numpy.concatenate(\n      [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\n  all_times = numpy.concatenate([times, predictions[\"times\"]], axis=0)\n  return times, observed, all_times, predicted_mean\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        \"Please install matplotlib to generate a plot from this example.\")\n  (observed_times, observations,\n   all_times, predictions) = train_and_predict()\n  pyplot.axvline(99, linestyle=\"dotted\")\n  observed_lines = pyplot.plot(\n      observed_times, observations, label=\"Observed\", color=\"k\")\n  predicted_lines = pyplot.plot(\n      all_times, predictions, label=\"Predicted\", color=\"b\")\n  pyplot.legend(handles=[observed_lines[0], predicted_lines[0]],\n                loc=\"upper left\")\n  pyplot.show()\n\n\nif __name__ == \"__main__\":\n  tf.app.run(main=main)\n", "framework": "tensorflow"}
{"repo_name": "laszlocsomor/tensorflow", "file_path": "tensorflow/contrib/timeseries/examples/lstm.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"A more advanced example, of building an RNN-based time series model.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(\"TkAgg\")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We'd still like to test the TensorFlow-dependent parts of this\n  # example.\n  HAS_MATPLOTLIB = False\n\n_MODULE_PATH = path.dirname(__file__)\n_DATA_FILE = path.join(_MODULE_PATH, \"data/multivariate_periods.csv\")\n\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  \"\"\"A time series model-building example using an RNNCell.\"\"\"\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    \"\"\"Initialize/configure the model object.\n\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n\n    Args:\n      num_units: The number of units in the model's LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    \"\"\"\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we'll be outputting (just a mean here).\n        train_output_names=[\"mean\"],\n        predict_output_names=[\"mean\"],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    \"\"\"Save templates for components, which can then be used repeatedly.\n\n    This method is called every time a new graph is created. It's safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    \"\"\"\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don't have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=\"lstm_cell\",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=\"predict_from_lstm_output\",\n        func_=\n        lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    \"\"\"Return initial state for the time series model.\"\"\"\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    \"\"\"Update model state based on observations.\n\n    Note that we don't do much here aside from computing a loss. In this case\n    it's easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model's state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only \"loss\" will be optimized).\n    \"\"\"\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n        [tf.assert_equal(current_times, state_from_time)]):\n      # Subtract the mean and divide by the variance of the series.  Slightly\n      # more efficient if done for a whole window (using the normalize_features\n      # argument to SequentialTimeSeriesModel).\n      transformed_values = self._scale_data(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[\"loss\"] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won't be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    \"\"\"Advance the RNN state using a previous observation or prediction.\"\"\"\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {\"mean\": self._scale_back_data(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    \"\"\"Advance model state across a gap.\"\"\"\n    # Does not do anything special if we're jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n      self, current_times, current_exogenous_regressors, state):\n    \"\"\"Update model state based on exogenous regressors.\"\"\"\n    raise NotImplementedError(\n        \"Exogenous inputs are not implemented for this example.\")\n\n\ndef train_and_predict(csv_file_name=_DATA_FILE, training_steps=200):\n  \"\"\"Train and predict using a custom time series model.\"\"\"\n  # Construct an Estimator from our LSTM model.\n  estimator = ts_estimators.TimeSeriesRegressor(\n      model=_LSTMModel(num_features=5, num_units=128),\n      optimizer=tf.train.AdamOptimizer(0.001))\n  reader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=4, window_size=32)\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=100)))\n  times = evaluation[\"times\"][0]\n  observed = evaluation[\"observed\"][0, :, :]\n  predicted_mean = numpy.squeeze(numpy.concatenate(\n      [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\n  all_times = numpy.concatenate([times, predictions[\"times\"]], axis=0)\n  return times, observed, all_times, predicted_mean\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        \"Please install matplotlib to generate a plot from this example.\")\n  (observed_times, observations,\n   all_times, predictions) = train_and_predict()\n  pyplot.axvline(99, linestyle=\"dotted\")\n  observed_lines = pyplot.plot(\n      observed_times, observations, label=\"Observed\", color=\"k\")\n  predicted_lines = pyplot.plot(\n      all_times, predictions, label=\"Predicted\", color=\"b\")\n  pyplot.legend(handles=[observed_lines[0], predicted_lines[0]],\n                loc=\"upper left\")\n  pyplot.show()\n\n\nif __name__ == \"__main__\":\n  tf.app.run(main=main)\n", "framework": "tensorflow"}
{"repo_name": "dyoung418/tensorflow", "file_path": "tensorflow/contrib/timeseries/examples/lstm.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"A more advanced example, of building an RNN-based time series model.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom os import path\n\nimport numpy\nimport tensorflow as tf\n\nfrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimators\nfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_model\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(\"TkAgg\")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We'd still like to test the TensorFlow-dependent parts of this\n  # example.\n  HAS_MATPLOTLIB = False\n\n_MODULE_PATH = path.dirname(__file__)\n_DATA_FILE = path.join(_MODULE_PATH, \"data/multivariate_periods.csv\")\n\n\nclass _LSTMModel(ts_model.SequentialTimeSeriesModel):\n  \"\"\"A time series model-building example using an RNNCell.\"\"\"\n\n  def __init__(self, num_units, num_features, dtype=tf.float32):\n    \"\"\"Initialize/configure the model object.\n\n    Note that we do not start graph building here. Rather, this object is a\n    configurable factory for TensorFlow graphs which are run by an Estimator.\n\n    Args:\n      num_units: The number of units in the model's LSTMCell.\n      num_features: The dimensionality of the time series (features per\n        timestep).\n      dtype: The floating point data type to use.\n    \"\"\"\n    super(_LSTMModel, self).__init__(\n        # Pre-register the metrics we'll be outputting (just a mean here).\n        train_output_names=[\"mean\"],\n        predict_output_names=[\"mean\"],\n        num_features=num_features,\n        dtype=dtype)\n    self._num_units = num_units\n    # Filled in by initialize_graph()\n    self._lstm_cell = None\n    self._lstm_cell_run = None\n    self._predict_from_lstm_output = None\n\n  def initialize_graph(self, input_statistics):\n    \"\"\"Save templates for components, which can then be used repeatedly.\n\n    This method is called every time a new graph is created. It's safe to start\n    adding ops to the current default graph here, but the graph should be\n    constructed from scratch.\n\n    Args:\n      input_statistics: A math_utils.InputStatistics object.\n    \"\"\"\n    super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)\n    self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)\n    # Create templates so we don't have to worry about variable reuse.\n    self._lstm_cell_run = tf.make_template(\n        name_=\"lstm_cell\",\n        func_=self._lstm_cell,\n        create_scope_now_=True)\n    # Transforms LSTM output into mean predictions.\n    self._predict_from_lstm_output = tf.make_template(\n        name_=\"predict_from_lstm_output\",\n        func_=\n        lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features),\n        create_scope_now_=True)\n\n  def get_start_state(self):\n    \"\"\"Return initial state for the time series model.\"\"\"\n    return (\n        # Keeps track of the time associated with this state for error checking.\n        tf.zeros([], dtype=tf.int64),\n        # The previous observation or prediction.\n        tf.zeros([self.num_features], dtype=self.dtype),\n        # The state of the RNNCell (batch dimension removed since this parent\n        # class will broadcast).\n        [tf.squeeze(state_element, axis=0)\n         for state_element\n         in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)])\n\n  def _filtering_step(self, current_times, current_values, state, predictions):\n    \"\"\"Update model state based on observations.\n\n    Note that we don't do much here aside from computing a loss. In this case\n    it's easier to update the RNN state in _prediction_step, since that covers\n    running the RNN both on observations (from this method) and our own\n    predictions. This distinction can be important for probabilistic models,\n    where repeatedly predicting without filtering should lead to low-confidence\n    predictions.\n\n    Args:\n      current_times: A [batch size] integer Tensor.\n      current_values: A [batch size, self.num_features] floating point Tensor\n        with new observations.\n      state: The model's state tuple.\n      predictions: The output of the previous `_prediction_step`.\n    Returns:\n      A tuple of new state and a predictions dictionary updated to include a\n      loss (note that we could also return other measures of goodness of fit,\n      although only \"loss\" will be optimized).\n    \"\"\"\n    state_from_time, prediction, lstm_state = state\n    with tf.control_dependencies(\n        [tf.assert_equal(current_times, state_from_time)]):\n      # Subtract the mean and divide by the variance of the series.  Slightly\n      # more efficient if done for a whole window (using the normalize_features\n      # argument to SequentialTimeSeriesModel).\n      transformed_values = self._scale_data(current_values)\n      # Use mean squared error across features for the loss.\n      predictions[\"loss\"] = tf.reduce_mean(\n          (prediction - transformed_values) ** 2, axis=-1)\n      # Keep track of the new observation in model state. It won't be run\n      # through the LSTM until the next _imputation_step.\n      new_state_tuple = (current_times, transformed_values, lstm_state)\n    return (new_state_tuple, predictions)\n\n  def _prediction_step(self, current_times, state):\n    \"\"\"Advance the RNN state using a previous observation or prediction.\"\"\"\n    _, previous_observation_or_prediction, lstm_state = state\n    lstm_output, new_lstm_state = self._lstm_cell_run(\n        inputs=previous_observation_or_prediction, state=lstm_state)\n    next_prediction = self._predict_from_lstm_output(lstm_output)\n    new_state_tuple = (current_times, next_prediction, new_lstm_state)\n    return new_state_tuple, {\"mean\": self._scale_back_data(next_prediction)}\n\n  def _imputation_step(self, current_times, state):\n    \"\"\"Advance model state across a gap.\"\"\"\n    # Does not do anything special if we're jumping across a gap. More advanced\n    # models, especially probabilistic ones, would want a special case that\n    # depends on the gap size.\n    return state\n\n  def _exogenous_input_step(\n      self, current_times, current_exogenous_regressors, state):\n    \"\"\"Update model state based on exogenous regressors.\"\"\"\n    raise NotImplementedError(\n        \"Exogenous inputs are not implemented for this example.\")\n\n\ndef train_and_predict(csv_file_name=_DATA_FILE, training_steps=200):\n  \"\"\"Train and predict using a custom time series model.\"\"\"\n  # Construct an Estimator from our LSTM model.\n  estimator = ts_estimators.TimeSeriesRegressor(\n      model=_LSTMModel(num_features=5, num_units=128),\n      optimizer=tf.train.AdamOptimizer(0.001))\n  reader = tf.contrib.timeseries.CSVReader(\n      csv_file_name,\n      column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,)\n                    + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5))\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=4, window_size=32)\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=100)))\n  times = evaluation[\"times\"][0]\n  observed = evaluation[\"observed\"][0, :, :]\n  predicted_mean = numpy.squeeze(numpy.concatenate(\n      [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\n  all_times = numpy.concatenate([times, predictions[\"times\"]], axis=0)\n  return times, observed, all_times, predicted_mean\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        \"Please install matplotlib to generate a plot from this example.\")\n  (observed_times, observations,\n   all_times, predictions) = train_and_predict()\n  pyplot.axvline(99, linestyle=\"dotted\")\n  observed_lines = pyplot.plot(\n      observed_times, observations, label=\"Observed\", color=\"k\")\n  predicted_lines = pyplot.plot(\n      all_times, predictions, label=\"Predicted\", color=\"b\")\n  pyplot.legend(handles=[observed_lines[0], predicted_lines[0]],\n                loc=\"upper left\")\n  pyplot.show()\n\n\nif __name__ == \"__main__\":\n  tf.app.run(main=main)\n", "framework": "tensorflow"}
{"repo_name": "Bismarrck/tensorflow", "file_path": "tensorflow/contrib/timeseries/examples/predict.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"An example of training and predicting with a TFTS estimator.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(\"TkAgg\")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We'd still like to test the TensorFlow-dependent parts of this\n  # example, namely train_and_predict.\n  HAS_MATPLOTLIB = False\n\nFLAGS = None\n\n\n_MODULE_PATH = os.path.dirname(__file__)\n_DEFAULT_DATA_FILE = os.path.join(_MODULE_PATH, \"data/period_trend.csv\")\n\n\ndef structural_ensemble_train_and_predict(csv_file_name):\n  # Cycle between 5 latent values over a period of 100. This leads to a very\n  # smooth periodic component (and a small model), which is a good fit for our\n  # example data. Modeling high-frequency periodic variations will require a\n  # higher cycle_num_latent_values.\n  structural = tf.contrib.timeseries.StructuralEnsembleRegressor(\n      periodicities=100, num_features=1, cycle_num_latent_values=5)\n  return train_and_predict(structural, csv_file_name, training_steps=150)\n\n\ndef ar_train_and_predict(csv_file_name):\n  # An autoregressive model, with periodicity handled as a time-based\n  # regression. Note that this requires windows of size 16 (input_window_size +\n  # output_window_size) for training.\n  ar = tf.contrib.timeseries.ARRegressor(\n      periodicities=100, input_window_size=10, output_window_size=6,\n      num_features=1,\n      # Use the (default) normal likelihood loss to adaptively fit the\n      # variance. SQUARED_LOSS overestimates variance when there are trends in\n      # the series.\n      loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)\n  return train_and_predict(ar, csv_file_name, training_steps=600)\n\n\ndef train_and_predict(estimator, csv_file_name, training_steps):\n  \"\"\"A simple example of training and predicting.\"\"\"\n  # Read data in the default \"time,value\" CSV format with no header\n  reader = tf.contrib.timeseries.CSVReader(csv_file_name)\n  # Set up windowing and batching for training\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=16, window_size=16)\n  # Fit model parameters to data\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  # Evaluate on the full dataset sequentially, collecting in-sample predictions\n  # for a qualitative evaluation. Note that this loads the whole dataset into\n  # memory. For quantitative evaluation, use RandomWindowChunker.\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=200)))\n  times = evaluation[\"times\"][0]\n  observed = evaluation[\"observed\"][0, :, 0]\n  mean = np.squeeze(np.concatenate(\n      [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\n  variance = np.squeeze(np.concatenate(\n      [evaluation[\"covariance\"][0], predictions[\"covariance\"]], axis=0))\n  all_times = np.concatenate([times, predictions[\"times\"]], axis=0)\n  upper_limit = mean + np.sqrt(variance)\n  lower_limit = mean - np.sqrt(variance)\n  return times, observed, all_times, mean, upper_limit, lower_limit\n\n\ndef make_plot(name, training_times, observed, all_times, mean,\n              upper_limit, lower_limit):\n  \"\"\"Plot a time series in a new figure.\"\"\"\n  pyplot.figure()\n  pyplot.plot(training_times, observed, \"b\", label=\"training series\")\n  pyplot.plot(all_times, mean, \"r\", label=\"forecast\")\n  pyplot.plot(all_times, upper_limit, \"g\", label=\"forecast upper bound\")\n  pyplot.plot(all_times, lower_limit, \"g\", label=\"forecast lower bound\")\n  pyplot.fill_between(all_times, lower_limit, upper_limit, color=\"grey\",\n                      alpha=\"0.2\")\n  pyplot.axvline(training_times[-1], color=\"k\", linestyle=\"--\")\n  pyplot.xlabel(\"time\")\n  pyplot.ylabel(\"observations\")\n  pyplot.legend(loc=0)\n  pyplot.title(name)\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        \"Please install matplotlib to generate a plot from this example.\")\n  input_filename = FLAGS.input_filename\n  if input_filename is None:\n    input_filename = _DEFAULT_DATA_FILE\n  make_plot(\"Structural ensemble\",\n            *structural_ensemble_train_and_predict(input_filename))\n  make_plot(\"AR\", *ar_train_and_predict(input_filename))\n  pyplot.show()\n\n\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \"--input_filename\",\n      type=str,\n      required=False,\n      help=\"Input csv file (omit to use the data/period_trend.csv).\")\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n", "framework": "tensorflow"}
{"repo_name": "hfp/tensorflow-xsmm", "file_path": "tensorflow/contrib/timeseries/examples/predict.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"An example of training and predicting with a TFTS estimator.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(\"TkAgg\")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We'd still like to test the TensorFlow-dependent parts of this\n  # example, namely train_and_predict.\n  HAS_MATPLOTLIB = False\n\nFLAGS = None\n\n\n_MODULE_PATH = os.path.dirname(__file__)\n_DEFAULT_DATA_FILE = os.path.join(_MODULE_PATH, \"data/period_trend.csv\")\n\n\ndef structural_ensemble_train_and_predict(csv_file_name):\n  # Cycle between 5 latent values over a period of 100. This leads to a very\n  # smooth periodic component (and a small model), which is a good fit for our\n  # example data. Modeling high-frequency periodic variations will require a\n  # higher cycle_num_latent_values.\n  structural = tf.contrib.timeseries.StructuralEnsembleRegressor(\n      periodicities=100, num_features=1, cycle_num_latent_values=5)\n  return train_and_predict(structural, csv_file_name, training_steps=150)\n\n\ndef ar_train_and_predict(csv_file_name):\n  # An autoregressive model, with periodicity handled as a time-based\n  # regression. Note that this requires windows of size 16 (input_window_size +\n  # output_window_size) for training.\n  ar = tf.contrib.timeseries.ARRegressor(\n      periodicities=100, input_window_size=10, output_window_size=6,\n      num_features=1,\n      # Use the (default) normal likelihood loss to adaptively fit the\n      # variance. SQUARED_LOSS overestimates variance when there are trends in\n      # the series.\n      loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)\n  return train_and_predict(ar, csv_file_name, training_steps=600)\n\n\ndef train_and_predict(estimator, csv_file_name, training_steps):\n  \"\"\"A simple example of training and predicting.\"\"\"\n  # Read data in the default \"time,value\" CSV format with no header\n  reader = tf.contrib.timeseries.CSVReader(csv_file_name)\n  # Set up windowing and batching for training\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=16, window_size=16)\n  # Fit model parameters to data\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  # Evaluate on the full dataset sequentially, collecting in-sample predictions\n  # for a qualitative evaluation. Note that this loads the whole dataset into\n  # memory. For quantitative evaluation, use RandomWindowChunker.\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=200)))\n  times = evaluation[\"times\"][0]\n  observed = evaluation[\"observed\"][0, :, 0]\n  mean = np.squeeze(np.concatenate(\n      [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\n  variance = np.squeeze(np.concatenate(\n      [evaluation[\"covariance\"][0], predictions[\"covariance\"]], axis=0))\n  all_times = np.concatenate([times, predictions[\"times\"]], axis=0)\n  upper_limit = mean + np.sqrt(variance)\n  lower_limit = mean - np.sqrt(variance)\n  return times, observed, all_times, mean, upper_limit, lower_limit\n\n\ndef make_plot(name, training_times, observed, all_times, mean,\n              upper_limit, lower_limit):\n  \"\"\"Plot a time series in a new figure.\"\"\"\n  pyplot.figure()\n  pyplot.plot(training_times, observed, \"b\", label=\"training series\")\n  pyplot.plot(all_times, mean, \"r\", label=\"forecast\")\n  pyplot.plot(all_times, upper_limit, \"g\", label=\"forecast upper bound\")\n  pyplot.plot(all_times, lower_limit, \"g\", label=\"forecast lower bound\")\n  pyplot.fill_between(all_times, lower_limit, upper_limit, color=\"grey\",\n                      alpha=\"0.2\")\n  pyplot.axvline(training_times[-1], color=\"k\", linestyle=\"--\")\n  pyplot.xlabel(\"time\")\n  pyplot.ylabel(\"observations\")\n  pyplot.legend(loc=0)\n  pyplot.title(name)\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        \"Please install matplotlib to generate a plot from this example.\")\n  input_filename = FLAGS.input_filename\n  if input_filename is None:\n    input_filename = _DEFAULT_DATA_FILE\n  make_plot(\"Structural ensemble\",\n            *structural_ensemble_train_and_predict(input_filename))\n  make_plot(\"AR\", *ar_train_and_predict(input_filename))\n  pyplot.show()\n\n\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \"--input_filename\",\n      type=str,\n      required=False,\n      help=\"Input csv file (omit to use the data/period_trend.csv).\")\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n", "framework": "tensorflow"}
{"repo_name": "dancingdan/tensorflow", "file_path": "tensorflow/contrib/timeseries/examples/predict.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"An example of training and predicting with a TFTS estimator.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(\"TkAgg\")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We'd still like to test the TensorFlow-dependent parts of this\n  # example, namely train_and_predict.\n  HAS_MATPLOTLIB = False\n\nFLAGS = None\n\n\n_MODULE_PATH = os.path.dirname(__file__)\n_DEFAULT_DATA_FILE = os.path.join(_MODULE_PATH, \"data/period_trend.csv\")\n\n\ndef structural_ensemble_train_and_predict(csv_file_name):\n  # Cycle between 5 latent values over a period of 100. This leads to a very\n  # smooth periodic component (and a small model), which is a good fit for our\n  # example data. Modeling high-frequency periodic variations will require a\n  # higher cycle_num_latent_values.\n  structural = tf.contrib.timeseries.StructuralEnsembleRegressor(\n      periodicities=100, num_features=1, cycle_num_latent_values=5)\n  return train_and_predict(structural, csv_file_name, training_steps=150)\n\n\ndef ar_train_and_predict(csv_file_name):\n  # An autoregressive model, with periodicity handled as a time-based\n  # regression. Note that this requires windows of size 16 (input_window_size +\n  # output_window_size) for training.\n  ar = tf.contrib.timeseries.ARRegressor(\n      periodicities=100, input_window_size=10, output_window_size=6,\n      num_features=1,\n      # Use the (default) normal likelihood loss to adaptively fit the\n      # variance. SQUARED_LOSS overestimates variance when there are trends in\n      # the series.\n      loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)\n  return train_and_predict(ar, csv_file_name, training_steps=600)\n\n\ndef train_and_predict(estimator, csv_file_name, training_steps):\n  \"\"\"A simple example of training and predicting.\"\"\"\n  # Read data in the default \"time,value\" CSV format with no header\n  reader = tf.contrib.timeseries.CSVReader(csv_file_name)\n  # Set up windowing and batching for training\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=16, window_size=16)\n  # Fit model parameters to data\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  # Evaluate on the full dataset sequentially, collecting in-sample predictions\n  # for a qualitative evaluation. Note that this loads the whole dataset into\n  # memory. For quantitative evaluation, use RandomWindowChunker.\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=200)))\n  times = evaluation[\"times\"][0]\n  observed = evaluation[\"observed\"][0, :, 0]\n  mean = np.squeeze(np.concatenate(\n      [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\n  variance = np.squeeze(np.concatenate(\n      [evaluation[\"covariance\"][0], predictions[\"covariance\"]], axis=0))\n  all_times = np.concatenate([times, predictions[\"times\"]], axis=0)\n  upper_limit = mean + np.sqrt(variance)\n  lower_limit = mean - np.sqrt(variance)\n  return times, observed, all_times, mean, upper_limit, lower_limit\n\n\ndef make_plot(name, training_times, observed, all_times, mean,\n              upper_limit, lower_limit):\n  \"\"\"Plot a time series in a new figure.\"\"\"\n  pyplot.figure()\n  pyplot.plot(training_times, observed, \"b\", label=\"training series\")\n  pyplot.plot(all_times, mean, \"r\", label=\"forecast\")\n  pyplot.plot(all_times, upper_limit, \"g\", label=\"forecast upper bound\")\n  pyplot.plot(all_times, lower_limit, \"g\", label=\"forecast lower bound\")\n  pyplot.fill_between(all_times, lower_limit, upper_limit, color=\"grey\",\n                      alpha=\"0.2\")\n  pyplot.axvline(training_times[-1], color=\"k\", linestyle=\"--\")\n  pyplot.xlabel(\"time\")\n  pyplot.ylabel(\"observations\")\n  pyplot.legend(loc=0)\n  pyplot.title(name)\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        \"Please install matplotlib to generate a plot from this example.\")\n  input_filename = FLAGS.input_filename\n  if input_filename is None:\n    input_filename = _DEFAULT_DATA_FILE\n  make_plot(\"Structural ensemble\",\n            *structural_ensemble_train_and_predict(input_filename))\n  make_plot(\"AR\", *ar_train_and_predict(input_filename))\n  pyplot.show()\n\n\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \"--input_filename\",\n      type=str,\n      required=False,\n      help=\"Input csv file (omit to use the data/period_trend.csv).\")\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n", "framework": "tensorflow"}
{"repo_name": "AnishShah/tensorflow", "file_path": "tensorflow/contrib/timeseries/examples/predict.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"An example of training and predicting with a TFTS estimator.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\n\ntry:\n  import matplotlib  # pylint: disable=g-import-not-at-top\n  matplotlib.use(\"TkAgg\")  # Need Tk for interactive plots.\n  from matplotlib import pyplot  # pylint: disable=g-import-not-at-top\n  HAS_MATPLOTLIB = True\nexcept ImportError:\n  # Plotting requires matplotlib, but the unit test running this code may\n  # execute in an environment without it (i.e. matplotlib is not a build\n  # dependency). We'd still like to test the TensorFlow-dependent parts of this\n  # example, namely train_and_predict.\n  HAS_MATPLOTLIB = False\n\nFLAGS = None\n\n\n_MODULE_PATH = os.path.dirname(__file__)\n_DEFAULT_DATA_FILE = os.path.join(_MODULE_PATH, \"data/period_trend.csv\")\n\n\ndef structural_ensemble_train_and_predict(csv_file_name):\n  # Cycle between 5 latent values over a period of 100. This leads to a very\n  # smooth periodic component (and a small model), which is a good fit for our\n  # example data. Modeling high-frequency periodic variations will require a\n  # higher cycle_num_latent_values.\n  structural = tf.contrib.timeseries.StructuralEnsembleRegressor(\n      periodicities=100, num_features=1, cycle_num_latent_values=5)\n  return train_and_predict(structural, csv_file_name, training_steps=150)\n\n\ndef ar_train_and_predict(csv_file_name):\n  # An autoregressive model, with periodicity handled as a time-based\n  # regression. Note that this requires windows of size 16 (input_window_size +\n  # output_window_size) for training.\n  ar = tf.contrib.timeseries.ARRegressor(\n      periodicities=100, input_window_size=10, output_window_size=6,\n      num_features=1,\n      # Use the (default) normal likelihood loss to adaptively fit the\n      # variance. SQUARED_LOSS overestimates variance when there are trends in\n      # the series.\n      loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS)\n  return train_and_predict(ar, csv_file_name, training_steps=600)\n\n\ndef train_and_predict(estimator, csv_file_name, training_steps):\n  \"\"\"A simple example of training and predicting.\"\"\"\n  # Read data in the default \"time,value\" CSV format with no header\n  reader = tf.contrib.timeseries.CSVReader(csv_file_name)\n  # Set up windowing and batching for training\n  train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(\n      reader, batch_size=16, window_size=16)\n  # Fit model parameters to data\n  estimator.train(input_fn=train_input_fn, steps=training_steps)\n  # Evaluate on the full dataset sequentially, collecting in-sample predictions\n  # for a qualitative evaluation. Note that this loads the whole dataset into\n  # memory. For quantitative evaluation, use RandomWindowChunker.\n  evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)\n  evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)\n  # Predict starting after the evaluation\n  (predictions,) = tuple(estimator.predict(\n      input_fn=tf.contrib.timeseries.predict_continuation_input_fn(\n          evaluation, steps=200)))\n  times = evaluation[\"times\"][0]\n  observed = evaluation[\"observed\"][0, :, 0]\n  mean = np.squeeze(np.concatenate(\n      [evaluation[\"mean\"][0], predictions[\"mean\"]], axis=0))\n  variance = np.squeeze(np.concatenate(\n      [evaluation[\"covariance\"][0], predictions[\"covariance\"]], axis=0))\n  all_times = np.concatenate([times, predictions[\"times\"]], axis=0)\n  upper_limit = mean + np.sqrt(variance)\n  lower_limit = mean - np.sqrt(variance)\n  return times, observed, all_times, mean, upper_limit, lower_limit\n\n\ndef make_plot(name, training_times, observed, all_times, mean,\n              upper_limit, lower_limit):\n  \"\"\"Plot a time series in a new figure.\"\"\"\n  pyplot.figure()\n  pyplot.plot(training_times, observed, \"b\", label=\"training series\")\n  pyplot.plot(all_times, mean, \"r\", label=\"forecast\")\n  pyplot.plot(all_times, upper_limit, \"g\", label=\"forecast upper bound\")\n  pyplot.plot(all_times, lower_limit, \"g\", label=\"forecast lower bound\")\n  pyplot.fill_between(all_times, lower_limit, upper_limit, color=\"grey\",\n                      alpha=\"0.2\")\n  pyplot.axvline(training_times[-1], color=\"k\", linestyle=\"--\")\n  pyplot.xlabel(\"time\")\n  pyplot.ylabel(\"observations\")\n  pyplot.legend(loc=0)\n  pyplot.title(name)\n\n\ndef main(unused_argv):\n  if not HAS_MATPLOTLIB:\n    raise ImportError(\n        \"Please install matplotlib to generate a plot from this example.\")\n  input_filename = FLAGS.input_filename\n  if input_filename is None:\n    input_filename = _DEFAULT_DATA_FILE\n  make_plot(\"Structural ensemble\",\n            *structural_ensemble_train_and_predict(input_filename))\n  make_plot(\"AR\", *ar_train_and_predict(input_filename))\n  pyplot.show()\n\n\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \"--input_filename\",\n      type=str,\n      required=False,\n      help=\"Input csv file (omit to use the data/period_trend.csv).\")\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n", "framework": "tensorflow"}
{"repo_name": "mortada/tensorflow", "file_path": "tensorflow/examples/learn/mnist.py", "content": "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\"\"\"This showcases how simple it is to build image classification networks.\n\nIt follows description from this TensorFlow tutorial:\n    https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom sklearn import metrics\nimport tensorflow as tf\n\nlayers = tf.contrib.layers\nlearn = tf.contrib.learn\n\n\ndef max_pool_2x2(tensor_in):\n  return tf.nn.max_pool(\n      tensor_in, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n\ndef conv_model(feature, target, mode):\n  \"\"\"2-layer convolution model.\"\"\"\n  # Convert the target to a one-hot tensor of shape (batch_size, 10) and\n  # with a on-value of 1 for each one-hot vector of length 10.\n  target = tf.one_hot(tf.cast(target, tf.int32), 10, 1, 0)\n\n  # Reshape feature to 4d tensor with 2nd and 3rd dimensions being\n  # image width and height final dimension being the number of color channels.\n  feature = tf.reshape(feature, [-1, 28, 28, 1])\n\n  # First conv layer will compute 32 features for each 5x5 patch\n  with tf.variable_scope('conv_layer1'):\n    h_conv1 = layers.convolution2d(\n        feature, 32, kernel_size=[5, 5], activation_fn=tf.nn.relu)\n    h_pool1 = max_pool_2x2(h_conv1)\n\n  # Second conv layer will compute 64 features for each 5x5 patch.\n  with tf.variable_scope('conv_layer2'):\n    h_conv2 = layers.convolution2d(\n        h_pool1, 64, kernel_size=[5, 5], activation_fn=tf.nn.relu)\n    h_pool2 = max_pool_2x2(h_conv2)\n    # reshape tensor into a batch of vectors\n    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n\n  # Densely connected layer with 1024 neurons.\n  h_fc1 = layers.dropout(\n      layers.fully_connected(\n          h_pool2_flat, 1024, activation_fn=tf.nn.relu),\n      keep_prob=0.5,\n      is_training=mode == tf.contrib.learn.ModeKeys.TRAIN)\n\n  # Compute logits (1 per class) and compute loss.\n  logits = layers.fully_connected(h_fc1, 10, activation_fn=None)\n  loss = tf.losses.softmax_cross_entropy(target, logits)\n\n  # Create a tensor for training op.\n  train_op = layers.optimize_loss(\n      loss,\n      tf.contrib.framework.get_global_step(),\n      optimizer='SGD',\n      learning_rate=0.001)\n\n  return tf.argmax(logits, 1), loss, train_op\n\n\ndef main(unused_args):\n  ### Download and load MNIST dataset.\n  mnist = learn.datasets.load_dataset('mnist')\n\n  ### Linear classifier.\n  feature_columns = learn.infer_real_valued_columns_from_input(\n      mnist.train.images)\n  classifier = learn.LinearClassifier(\n      feature_columns=feature_columns, n_classes=10)\n  classifier.fit(mnist.train.images,\n                 mnist.train.labels.astype(np.int32),\n                 batch_size=100,\n                 steps=1000)\n  score = metrics.accuracy_score(mnist.test.labels,\n                                 list(classifier.predict(mnist.test.images)))\n  print('Accuracy: {0:f}'.format(score))\n\n  ### Convolutional network\n  classifier = learn.Estimator(model_fn=conv_model)\n  classifier.fit(mnist.train.images,\n                 mnist.train.labels,\n                 batch_size=100,\n                 steps=20000)\n  score = metrics.accuracy_score(mnist.test.labels,\n                                 list(classifier.predict(mnist.test.images)))\n  print('Accuracy: {0:f}'.format(score))\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "framework": "tensorflow"}
{"repo_name": "asadziach/tensorflow", "file_path": "tensorflow/examples/learn/mnist.py", "content": "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\"\"\"This showcases how simple it is to build image classification networks.\n\nIt follows description from this TensorFlow tutorial:\n    https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom sklearn import metrics\nimport tensorflow as tf\n\nlayers = tf.contrib.layers\nlearn = tf.contrib.learn\n\n\ndef max_pool_2x2(tensor_in):\n  return tf.nn.max_pool(\n      tensor_in, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n\ndef conv_model(feature, target, mode):\n  \"\"\"2-layer convolution model.\"\"\"\n  # Convert the target to a one-hot tensor of shape (batch_size, 10) and\n  # with a on-value of 1 for each one-hot vector of length 10.\n  target = tf.one_hot(tf.cast(target, tf.int32), 10, 1, 0)\n\n  # Reshape feature to 4d tensor with 2nd and 3rd dimensions being\n  # image width and height final dimension being the number of color channels.\n  feature = tf.reshape(feature, [-1, 28, 28, 1])\n\n  # First conv layer will compute 32 features for each 5x5 patch\n  with tf.variable_scope('conv_layer1'):\n    h_conv1 = layers.convolution2d(\n        feature, 32, kernel_size=[5, 5], activation_fn=tf.nn.relu)\n    h_pool1 = max_pool_2x2(h_conv1)\n\n  # Second conv layer will compute 64 features for each 5x5 patch.\n  with tf.variable_scope('conv_layer2'):\n    h_conv2 = layers.convolution2d(\n        h_pool1, 64, kernel_size=[5, 5], activation_fn=tf.nn.relu)\n    h_pool2 = max_pool_2x2(h_conv2)\n    # reshape tensor into a batch of vectors\n    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n\n  # Densely connected layer with 1024 neurons.\n  h_fc1 = layers.dropout(\n      layers.fully_connected(\n          h_pool2_flat, 1024, activation_fn=tf.nn.relu),\n      keep_prob=0.5,\n      is_training=mode == tf.contrib.learn.ModeKeys.TRAIN)\n\n  # Compute logits (1 per class) and compute loss.\n  logits = layers.fully_connected(h_fc1, 10, activation_fn=None)\n  loss = tf.losses.softmax_cross_entropy(target, logits)\n\n  # Create a tensor for training op.\n  train_op = layers.optimize_loss(\n      loss,\n      tf.contrib.framework.get_global_step(),\n      optimizer='SGD',\n      learning_rate=0.001)\n\n  return tf.argmax(logits, 1), loss, train_op\n\n\ndef main(unused_args):\n  ### Download and load MNIST dataset.\n  mnist = learn.datasets.load_dataset('mnist')\n\n  ### Linear classifier.\n  feature_columns = learn.infer_real_valued_columns_from_input(\n      mnist.train.images)\n  classifier = learn.LinearClassifier(\n      feature_columns=feature_columns, n_classes=10)\n  classifier.fit(mnist.train.images,\n                 mnist.train.labels.astype(np.int32),\n                 batch_size=100,\n                 steps=1000)\n  score = metrics.accuracy_score(mnist.test.labels,\n                                 list(classifier.predict(mnist.test.images)))\n  print('Accuracy: {0:f}'.format(score))\n\n  ### Convolutional network\n  classifier = learn.Estimator(model_fn=conv_model)\n  classifier.fit(mnist.train.images,\n                 mnist.train.labels,\n                 batch_size=100,\n                 steps=20000)\n  score = metrics.accuracy_score(mnist.test.labels,\n                                 list(classifier.predict(mnist.test.images)))\n  print('Accuracy: {0:f}'.format(score))\n\n\nif __name__ == '__main__':\n  tf.app.run()\n", "framework": "tensorflow"}
{"repo_name": "cg31/tensorflow", "file_path": "tensorflow/models/image/mnist/convolutional.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\n\nThis should achieve a test error of 0.7%. Please keep this model as simple and\nlinear as possible, it is meant as a tutorial for simple convolutional models.\nRun with --self_test on the command line to execute a short self-test.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport gzip\nimport os\nimport sys\nimport time\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nSOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\nWORK_DIRECTORY = 'data'\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\nEVAL_BATCH_SIZE = 64\nEVAL_FREQUENCY = 100  # Number of steps between evaluations.\n\n\nFLAGS = None\n\n\ndef data_type():\n  \"\"\"Return the type of the activations, weights, and placeholder variables.\"\"\"\n  if FLAGS.use_fp16:\n    return tf.float16\n  else:\n    return tf.float32\n\n\ndef maybe_download(filename):\n  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n  if not tf.gfile.Exists(WORK_DIRECTORY):\n    tf.gfile.MakeDirs(WORK_DIRECTORY)\n  filepath = os.path.join(WORK_DIRECTORY, filename)\n  if not tf.gfile.Exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    with tf.gfile.GFile(filepath) as f:\n      size = f.size()\n    print('Successfully downloaded', filename, size, 'bytes.')\n  return filepath\n\n\ndef extract_data(filename, num_images):\n  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n\n  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n  \"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n    return data\n\n\ndef extract_labels(filename, num_images):\n  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_images)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n  return labels\n\n\ndef fake_data(num_images):\n  \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\n  data = numpy.ndarray(\n      shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n      dtype=numpy.float32)\n  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\n  for image in xrange(num_images):\n    label = image % 2\n    data[image, :, :, 0] = label - 0.5\n    labels[image] = label\n  return data, labels\n\n\ndef error_rate(predictions, labels):\n  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n  return 100.0 - (\n      100.0 *\n      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n      predictions.shape[0])\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  if FLAGS.self_test:\n    print('Running self-test.')\n    train_data, train_labels = fake_data(256)\n    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n    num_epochs = 1\n  else:\n    # Get the data.\n    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n\n    # Extract it into numpy arrays.\n    train_data = extract_data(train_data_filename, 60000)\n    train_labels = extract_labels(train_labels_filename, 60000)\n    test_data = extract_data(test_data_filename, 10000)\n    test_labels = extract_labels(test_labels_filename, 10000)\n\n    # Generate a validation set.\n    validation_data = train_data[:VALIDATION_SIZE, ...]\n    validation_labels = train_labels[:VALIDATION_SIZE]\n    train_data = train_data[VALIDATION_SIZE:, ...]\n    train_labels = train_labels[VALIDATION_SIZE:]\n    num_epochs = NUM_EPOCHS\n  train_size = train_labels.shape[0]\n\n  # This is where training samples and labels are fed to the graph.\n  # These placeholder nodes will be fed a batch of training data at each\n  # training step using the {feed_dict} argument to the Run() call below.\n  train_data_node = tf.placeholder(\n      data_type(),\n      shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n  train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n  eval_data = tf.placeholder(\n      data_type(),\n      shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n\n  # The variables below hold all the trainable weights. They are passed an\n  # initial value which will be assigned when we call:\n  # {tf.initialize_all_variables().run()}\n  conv1_weights = tf.Variable(\n      tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                          stddev=0.1,\n                          seed=SEED, dtype=data_type()))\n  conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n  conv2_weights = tf.Variable(tf.truncated_normal(\n      [5, 5, 32, 64], stddev=0.1,\n      seed=SEED, dtype=data_type()))\n  conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n  fc1_weights = tf.Variable(  # fully connected, depth 512.\n      tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n                          stddev=0.1,\n                          seed=SEED,\n                          dtype=data_type()))\n  fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n  fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n                                                stddev=0.1,\n                                                seed=SEED,\n                                                dtype=data_type()))\n  fc2_biases = tf.Variable(tf.constant(\n      0.1, shape=[NUM_LABELS], dtype=data_type()))\n\n  # We will replicate the model structure for the training subgraph, as well\n  # as the evaluation subgraphs, while sharing the trainable parameters.\n  def model(data, train=False):\n    \"\"\"The Model definition.\"\"\"\n    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n    # the same size as the input). Note that {strides} is a 4D array whose\n    # shape matches the data layout: [image index, y, x, depth].\n    conv = tf.nn.conv2d(data,\n                        conv1_weights,\n                        strides=[1, 1, 1, 1],\n                        padding='SAME')\n    # Bias and rectified linear non-linearity.\n    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n    # Max pooling. The kernel size spec {ksize} also follows the layout of\n    # the data. Here we have a pooling window of 2, and a stride of 2.\n    pool = tf.nn.max_pool(relu,\n                          ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1],\n                          padding='SAME')\n    conv = tf.nn.conv2d(pool,\n                        conv2_weights,\n                        strides=[1, 1, 1, 1],\n                        padding='SAME')\n    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n    pool = tf.nn.max_pool(relu,\n                          ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1],\n                          padding='SAME')\n    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n    # fully connected layers.\n    pool_shape = pool.get_shape().as_list()\n    reshape = tf.reshape(\n        pool,\n        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n    # Fully connected layer. Note that the '+' operation automatically\n    # broadcasts the biases.\n    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n    # Add a 50% dropout during training only. Dropout also scales\n    # activations such that no rescaling is needed at evaluation time.\n    if train:\n      hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n    return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n  # Training computation: logits + cross-entropy loss.\n  logits = model(train_data_node, True)\n  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits, train_labels_node))\n\n  # L2 regularization for the fully connected parameters.\n  regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n  # Add the regularization term to the loss.\n  loss += 5e-4 * regularizers\n\n  # Optimizer: set up a variable that's incremented once per batch and\n  # controls the learning rate decay.\n  batch = tf.Variable(0, dtype=data_type())\n  # Decay once per epoch, using an exponential schedule starting at 0.01.\n  learning_rate = tf.train.exponential_decay(\n      0.01,                # Base learning rate.\n      batch * BATCH_SIZE,  # Current index into the dataset.\n      train_size,          # Decay step.\n      0.95,                # Decay rate.\n      staircase=True)\n  # Use simple momentum for the optimization.\n  optimizer = tf.train.MomentumOptimizer(learning_rate,\n                                         0.9).minimize(loss,\n                                                       global_step=batch)\n\n  # Predictions for the current training minibatch.\n  train_prediction = tf.nn.softmax(logits)\n\n  # Predictions for the test and validation, which we'll compute less often.\n  eval_prediction = tf.nn.softmax(model(eval_data))\n\n  # Small utility function to evaluate a dataset by feeding batches of data to\n  # {eval_data} and pulling the results from {eval_predictions}.\n  # Saves memory and enables this to run on smaller GPUs.\n  def eval_in_batches(data, sess):\n    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n    size = data.shape[0]\n    if size < EVAL_BATCH_SIZE:\n      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n      end = begin + EVAL_BATCH_SIZE\n      if end <= size:\n        predictions[begin:end, :] = sess.run(\n            eval_prediction,\n            feed_dict={eval_data: data[begin:end, ...]})\n      else:\n        batch_predictions = sess.run(\n            eval_prediction,\n            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n        predictions[begin:, :] = batch_predictions[begin - size:, :]\n    return predictions\n\n  # Create a local session to run the training.\n  start_time = time.time()\n  with tf.Session() as sess:\n    # Run all the initializers to prepare the trainable parameters.\n    tf.initialize_all_variables().run()\n    print('Initialized!')\n    # Loop through training steps.\n    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n      # Compute the offset of the current minibatch in the data.\n      # Note that we could use better randomization across epochs.\n      offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n      batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n      batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n      # This dictionary maps the batch data (as a numpy array) to the\n      # node in the graph it should be fed to.\n      feed_dict = {train_data_node: batch_data,\n                   train_labels_node: batch_labels}\n      # Run the graph and fetch some of the nodes.\n      _, l, lr, predictions = sess.run(\n          [optimizer, loss, learning_rate, train_prediction],\n          feed_dict=feed_dict)\n      if step % EVAL_FREQUENCY == 0:\n        elapsed_time = time.time() - start_time\n        start_time = time.time()\n        print('Step %d (epoch %.2f), %.1f ms' %\n              (step, float(step) * BATCH_SIZE / train_size,\n               1000 * elapsed_time / EVAL_FREQUENCY))\n        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n        print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n        print('Validation error: %.1f%%' % error_rate(\n            eval_in_batches(validation_data, sess), validation_labels))\n        sys.stdout.flush()\n    # Finally print the result!\n    test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n    print('Test error: %.1f%%' % test_error)\n    if FLAGS.self_test:\n      print('test_error', test_error)\n      assert test_error == 0.0, 'expected 0.0 test_error, got %.2f' % (\n          test_error,)\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '--use_fp16',\n      default=False,\n      help='Use half floats instead of full floats if True.',\n      action='store_true'\n  )\n  parser.add_argument(\n      '--self_test',\n      default=False,\n      action='store_true',\n      help='True if running a self test.'\n  )\n  FLAGS = parser.parse_args()\n\n  tf.app.run()\n", "framework": "tensorflow"}
{"repo_name": "gautam1858/tensorflow", "file_path": "tensorflow/python/keras/backend_test.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Keras backend.\"\"\"\n\nimport gc\nimport warnings\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport scipy.sparse\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.eager.context import get_config\nfrom tensorflow.python.framework import config\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.keras import activations\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras import combinations\nfrom tensorflow.python.keras.engine import input_layer\nfrom tensorflow.python.keras.layers import advanced_activations\nfrom tensorflow.python.keras.utils import tf_inspect\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.platform import test\n\n\ndef compare_single_input_op_to_numpy(keras_op,\n                                     np_op,\n                                     input_shape,\n                                     dtype='float32',\n                                     negative_values=True,\n                                     keras_args=None,\n                                     keras_kwargs=None,\n                                     np_args=None,\n                                     np_kwargs=None):\n  keras_args = keras_args or []\n  keras_kwargs = keras_kwargs or {}\n  np_args = np_args or []\n  np_kwargs = np_kwargs or {}\n  inputs = 2. * np.random.random(input_shape)\n  if negative_values:\n    inputs -= 1.\n  keras_output = keras_op(\n      backend.variable(inputs, dtype=dtype), *keras_args, **keras_kwargs)\n  keras_output = backend.eval(keras_output)\n  np_output = np_op(inputs.astype(dtype), *np_args, **np_kwargs)\n  try:\n    np.testing.assert_allclose(keras_output, np_output, atol=1e-4)\n  except AssertionError:\n    raise AssertionError('Test for op `' + str(keras_op.__name__) + '` failed; '\n                         'Expected ' + str(np_output) + ' but got ' +\n                         str(keras_output))\n\n\ndef compare_two_inputs_op_to_numpy(keras_op,\n                                   np_op,\n                                   input_shape_a,\n                                   input_shape_b,\n                                   dtype='float32',\n                                   keras_args=None,\n                                   keras_kwargs=None,\n                                   np_args=None,\n                                   np_kwargs=None):\n  keras_args = keras_args or []\n  keras_kwargs = keras_kwargs or {}\n  np_args = np_args or []\n  np_kwargs = np_kwargs or {}\n  input_a = np.random.random(input_shape_a)\n  input_b = np.random.random(input_shape_b)\n  keras_output = keras_op(\n      backend.variable(input_a, dtype=dtype),\n      backend.variable(input_b, dtype=dtype), *keras_args, **keras_kwargs)\n  keras_output = backend.eval(keras_output)\n  np_output = np_op(\n      input_a.astype(dtype), input_b.astype(dtype), *np_args, **np_kwargs)\n  try:\n    np.testing.assert_allclose(keras_output, np_output, atol=1e-4)\n  except AssertionError:\n    raise AssertionError('Test for op `' + str(keras_op.__name__) + '` failed; '\n                         'Expected ' + str(np_output) + ' but got ' +\n                         str(keras_output))\n\n\nclass BackendResetTest(test.TestCase, parameterized.TestCase):\n\n  def test_new_config(self):\n    # User defined jit setting\n    config.set_optimizer_jit(False)\n    sess = backend.get_session()\n    default_config = get_config()\n    self.assertEqual(\n        sess._config.graph_options.optimizer_options.global_jit_level,\n        default_config.graph_options.optimizer_options.global_jit_level)\n    backend.clear_session()\n\n    # New session has the same jit setting\n    sess = backend.get_session()\n    default_config = get_config()\n    self.assertEqual(\n        sess._config.graph_options.optimizer_options.global_jit_level,\n        default_config.graph_options.optimizer_options.global_jit_level)\n    backend.clear_session()\n\n    # Change respected\n    config.set_optimizer_jit(True)\n    sess = backend.get_session()\n    default_config = get_config()\n    self.assertEqual(\n        sess._config.graph_options.optimizer_options.global_jit_level,\n        default_config.graph_options.optimizer_options.global_jit_level)\n    backend.clear_session()\n\n  # We can't use the normal parameterized decorator because the test session\n  # will block graph clearing.\n  @parameterized.named_parameters(('_v1', context.graph_mode),\n                                  ('_v2', context.eager_mode))\n  def test_new_graph(self, test_context):\n    with test_context():\n      g_old = backend.get_graph()\n      backend.clear_session()\n      g = backend.get_graph()\n\n      assert g_old is not g\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass BackendUtilsTest(test.TestCase):\n\n  def test_backend(self):\n    self.assertEqual(backend.backend(), 'tensorflow')\n\n  def test_get_reset_uids(self):\n    self.assertEqual(backend.get_uid('foo'), 1)\n    self.assertEqual(backend.get_uid('foo'), 2)\n\n    backend.reset_uids()\n    self.assertEqual(backend.get_uid('foo'), 1)\n\n  def test_learning_phase(self):\n    with self.cached_session():\n      with self.assertRaises(ValueError):\n        backend.set_learning_phase(2)\n\n  def test_learning_phase_name(self):\n    with backend.name_scope('test_scope'):\n      # Test that outer name scopes do not affect the learning phase's name.\n      lp = backend.symbolic_learning_phase()\n    self.assertEqual(lp.name, 'keras_learning_phase:0')\n\n  def test_learning_phase_scope(self):\n    initial_learning_phase = backend.learning_phase()\n    with backend.learning_phase_scope(1):\n      self.assertEqual(backend.learning_phase(), 1)\n    self.assertEqual(backend.learning_phase(), initial_learning_phase)\n    with backend.learning_phase_scope(0):\n      self.assertEqual(backend.learning_phase(), 0)\n    self.assertEqual(backend.learning_phase(), initial_learning_phase)\n    with self.assertRaises(ValueError):\n      with backend.learning_phase_scope(None):\n        pass\n    self.assertEqual(backend.learning_phase(), initial_learning_phase)\n\n    new_learning_phase = 0\n    backend.set_learning_phase(new_learning_phase)\n    self.assertEqual(backend.learning_phase(), new_learning_phase)\n    with backend.learning_phase_scope(1):\n      self.assertEqual(backend.learning_phase(), 1)\n    self.assertEqual(backend.learning_phase(), new_learning_phase)\n\n  def test_learning_phase_scope_in_graph(self):\n    initial_learning_phase_outside_graph = backend.learning_phase()\n    with backend.get_graph().as_default():\n      initial_learning_phase_in_graph = backend.learning_phase()\n\n    self.assertEqual(backend.learning_phase(),\n                     initial_learning_phase_outside_graph)\n    with backend.learning_phase_scope(1):\n      self.assertEqual(backend.learning_phase(), 1)\n    self.assertEqual(backend.learning_phase(),\n                     initial_learning_phase_outside_graph)\n\n    with backend.get_graph().as_default():\n      self.assertIs(backend.learning_phase(), initial_learning_phase_in_graph)\n\n    self.assertEqual(backend.learning_phase(),\n                     initial_learning_phase_outside_graph)\n\n  def test_int_shape(self):\n    x = backend.ones(shape=(3, 4))\n    self.assertEqual(backend.int_shape(x), (3, 4))\n\n    if not context.executing_eagerly():\n      x = backend.placeholder(shape=(None, 4))\n      self.assertEqual(backend.int_shape(x), (None, 4))\n\n  def test_in_train_phase(self):\n    y1 = backend.variable(1)\n    y2 = backend.variable(2)\n    if context.executing_eagerly():\n      with backend.learning_phase_scope(0):\n        y_val_test = backend.in_train_phase(y1, y2).numpy()\n      with backend.learning_phase_scope(1):\n        y_val_train = backend.in_train_phase(y1, y2).numpy()\n    else:\n      y = backend.in_train_phase(y1, y2)\n      f = backend.function([backend.learning_phase()], [y])\n      y_val_test = f([0])[0]\n      y_val_train = f([1])[0]\n    self.assertAllClose(y_val_test, 2)\n    self.assertAllClose(y_val_train, 1)\n\n  def test_is_keras_tensor(self):\n    x = backend.variable(1)\n    self.assertEqual(backend.is_keras_tensor(x), False)\n    x = input_layer.Input(shape=(1,))\n    self.assertEqual(backend.is_keras_tensor(x), True)\n    x = input_layer.Input(shape=(None,), ragged=True)\n    self.assertEqual(backend.is_keras_tensor(x), True)\n    x = input_layer.Input(shape=(None, None), sparse=True)\n    self.assertEqual(backend.is_keras_tensor(x), True)\n    with self.assertRaises(ValueError):\n      backend.is_keras_tensor(0)\n\n  def test_stop_gradient(self):\n    x = backend.variable(1)\n    y = backend.stop_gradient(x)\n    if not context.executing_eagerly():\n      self.assertEqual(y.op.name[:12], 'StopGradient')\n\n    xs = [backend.variable(1) for _ in range(3)]\n    ys = backend.stop_gradient(xs)\n    if not context.executing_eagerly():\n      for y in ys:\n        self.assertEqual(y.op.name[:12], 'StopGradient')\n\n  def test_placeholder(self):\n    x = backend.placeholder(shape=(3, 4))\n    self.assertEqual(x.shape.as_list(), [3, 4])\n    x = backend.placeholder(shape=(3, 4), sparse=True)\n    self.assertEqual(x.shape.as_list(), [3, 4])\n\n  def test_is_placeholder(self):\n    x = backend.placeholder(shape=(1,))\n    self.assertEqual(backend.is_placeholder(x), True)\n    x = backend.variable(1)\n    self.assertEqual(backend.is_placeholder(x), False)\n\n  def test_print_tensor(self):\n    # Unfortunately it seems impossible to use `mock` (or any other method)\n    # to capture stdout when used inside a graph or graph function, thus\n    # we cannot test correctness.\n    # The message gets correctly printed in practice.\n    x = backend.placeholder(shape=())\n    y = backend.print_tensor(x, 'eager=%s' % context.executing_eagerly())\n    f = backend.function(x, y)\n    f(0)\n\n  def test_cast_to_floatx(self):\n    x = backend.variable(1, dtype='float64')\n    x = backend.cast_to_floatx(x)\n    self.assertEqual(x.dtype.name, 'float32')\n    x = backend.cast_to_floatx(2)\n    self.assertEqual(x.dtype.name, 'float32')\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass BackendVariableTest(test.TestCase):\n\n  def test_zeros(self):\n    x = backend.zeros((3, 4))\n    val = backend.eval(x)\n    self.assertAllClose(val, np.zeros((3, 4)))\n\n  def test_ones(self):\n    x = backend.ones((3, 4))\n    val = backend.eval(x)\n    self.assertAllClose(val, np.ones((3, 4)))\n\n  def test_eye(self):\n    x = backend.eye(4)\n    val = backend.eval(x)\n    self.assertAllClose(val, np.eye(4))\n\n  def test_zeros_like(self):\n    x = backend.zeros((3, 4))\n    y = backend.zeros_like(x)\n    val = backend.eval(y)\n    self.assertAllClose(val, np.zeros((3, 4)))\n\n  def test_ones_like(self):\n    x = backend.zeros((3, 4))\n    y = backend.ones_like(x)\n    val = backend.eval(y)\n    self.assertAllClose(val, np.ones((3, 4)))\n\n  def test_random_uniform_variable(self):\n    x = backend.random_uniform_variable((30, 20), low=1, high=2, seed=0)\n    val = backend.eval(x)\n    self.assertAllClose(val.mean(), 1.5, atol=1e-1)\n    self.assertAllClose(val.max(), 2., atol=1e-1)\n    self.assertAllClose(val.min(), 1., atol=1e-1)\n\n  def test_random_normal_variable(self):\n    x = backend.random_normal_variable((30, 20), 1., 0.5, seed=0)\n    val = backend.eval(x)\n    self.assertAllClose(val.mean(), 1., atol=1e-1)\n    self.assertAllClose(val.std(), 0.5, atol=1e-1)\n\n  def test_count_params(self):\n    x = backend.zeros((4, 5))\n    val = backend.count_params(x)\n    self.assertAllClose(val, 20)\n\n  def test_constant(self):\n    ref_val = np.random.random((3, 4)).astype('float32')\n    x = backend.constant(ref_val)\n    val = backend.eval(x)\n    self.assertAllClose(val, ref_val)\n\n  def test_sparse_variable(self):\n    val = scipy.sparse.eye(10)\n    x = backend.variable(val)\n    self.assertTrue(isinstance(x, sparse_tensor.SparseTensor))\n\n    y = backend.to_dense(x)\n    self.assertFalse(backend.is_sparse(y))\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass BackendLinearAlgebraTest(test.TestCase, parameterized.TestCase):\n\n  def test_dot(self):\n    x = backend.ones(shape=(2, 3))\n    y = backend.ones(shape=(3, 4))\n    xy = backend.dot(x, y)\n    self.assertEqual(xy.shape.as_list(), [2, 4])\n\n    x = backend.ones(shape=(32, 28, 3))\n    y = backend.ones(shape=(3, 4))\n    xy = backend.dot(x, y)\n    self.assertEqual(xy.shape.as_list(), [32, 28, 4])\n\n  @parameterized.parameters(\n      [(2, 3, 4, 5), (2, 5, 6, 7), (2, 3, 4, 6, 7), (3, 1)],\n      [(2, 20, 1), (2, 30, 20), (2, 1, 30), (1, 2)],\n      [(4, 2, 3), (4, 5, 3), (4, 2, 5), (2, 2)],\n      [(4, 2), (4, 2, 3), (4, 3), (1, 1)],\n      [(4, 2), (4, 2, 3), (4, 3), 1],\n      [(4, 2, 3), (4, 3), (4, 2), (2, 1)],\n  )\n  def test_batch_dot(self, x_shape, y_shape, output_shape, axes):\n    x_val = np.random.random(x_shape)\n    y_val = np.random.random(y_shape)\n    x = backend.variable(x_val)\n    y = backend.variable(y_val)\n    xy = backend.batch_dot(x, y, axes=axes)\n    self.assertEqual(tuple(xy.shape.as_list()), output_shape)\n    xy_val = backend.eval(xy)\n    ref_val = self._reference_batch_dot(x_val, y_val, axes)\n    self.assertAllClose(xy_val, ref_val, atol=1e-5)\n\n  def _reference_batch_dot(self, x, y, axes):\n    if isinstance(axes, int):\n      axes = [axes, axes]\n    elif isinstance(axes, tuple):\n      axes = list(axes)\n    if axes is None:\n      if y.ndim == 2:\n        axes = [x.ndim - 1, y.ndim - 1]\n      else:\n        axes = [x.ndim - 1, y.ndim - 2]\n    if axes[0] < 0:\n      axes[0] += x.ndim\n    if axes[1] < 0:\n      axes[1] += y.ndim\n    result = []\n    axes = [axes[0] - 1, axes[1] - 1]\n    for xi, yi in zip(x, y):\n      result.append(np.tensordot(xi, yi, axes))\n    result = np.array(result)\n    if result.ndim == 1:\n      result = np.expand_dims(result, -1)\n    return result\n\n  def test_reduction_ops(self):\n    ops_to_test = [\n        (backend.max, np.max),\n        (backend.min, np.min),\n        (backend.sum, np.sum),\n        (backend.prod, np.prod),\n        (backend.var, np.var),\n        (backend.std, np.std),\n        (backend.mean, np.mean),\n        (backend.argmin, np.argmin),\n        (backend.argmax, np.argmax),\n    ]\n    for keras_op, np_op in ops_to_test:\n      compare_single_input_op_to_numpy(\n          keras_op,\n          np_op,\n          input_shape=(4, 7, 5),\n          keras_kwargs={'axis': 1},\n          np_kwargs={'axis': 1})\n      compare_single_input_op_to_numpy(\n          keras_op,\n          np_op,\n          input_shape=(4, 7, 5),\n          keras_kwargs={'axis': -1},\n          np_kwargs={'axis': -1})\n      if 'keepdims' in tf_inspect.getargspec(keras_op).args:\n        compare_single_input_op_to_numpy(\n            keras_op,\n            np_op,\n            input_shape=(4, 7, 5),\n            keras_kwargs={\n                'axis': 1,\n                'keepdims': True\n            },\n            np_kwargs={\n                'axis': 1,\n                'keepdims': True\n            })\n\n  def test_elementwise_ops(self):\n    ops_to_test = [\n        (backend.square, np.square),\n        (backend.abs, np.abs),\n        (backend.round, np.round),\n        (backend.sign, np.sign),\n        (backend.sin, np.sin),\n        (backend.cos, np.cos),\n        (backend.exp, np.exp),\n    ]\n    for keras_op, np_op in ops_to_test:\n      compare_single_input_op_to_numpy(keras_op, np_op, input_shape=(4, 7))\n\n    ops_to_test = [\n        (backend.sqrt, np.sqrt),\n        (backend.log, np.log),\n    ]\n    for keras_op, np_op in ops_to_test:\n      compare_single_input_op_to_numpy(\n          keras_op, np_op, input_shape=(4, 7), negative_values=False)\n\n    compare_single_input_op_to_numpy(\n        backend.clip,\n        np.clip,\n        input_shape=(6, 4),\n        keras_kwargs={\n            'min_value': 0.1,\n            'max_value': 2.4\n        },\n        np_kwargs={\n            'a_min': 0.1,\n            'a_max': 1.4\n        })\n\n    compare_single_input_op_to_numpy(\n        backend.pow, np.power, input_shape=(6, 4), keras_args=[3], np_args=[3])\n\n  def test_two_tensor_ops(self):\n    ops_to_test = [\n        (backend.equal, np.equal),\n        (backend.not_equal, np.not_equal),\n        (backend.greater, np.greater),\n        (backend.greater_equal, np.greater_equal),\n        (backend.less, np.less),\n        (backend.less_equal, np.less_equal),\n        (backend.maximum, np.maximum),\n        (backend.minimum, np.minimum),\n    ]\n    for keras_op, np_op in ops_to_test:\n      compare_two_inputs_op_to_numpy(\n          keras_op, np_op, input_shape_a=(4, 7), input_shape_b=(4, 7))\n\n  def test_relu(self):\n    x = ops.convert_to_tensor_v2_with_dispatch([[-4, 0], [2, 7]], 'float32')\n\n    # standard relu\n    relu_op = backend.relu(x)\n    self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 7]])\n\n    # alpha (leaky relu used)\n    relu_op = backend.relu(x, alpha=0.5)\n    if not context.executing_eagerly():\n      self.assertTrue('LeakyRelu' in relu_op.name)\n    self.assertAllClose(backend.eval(relu_op), [[-2, 0], [2, 7]])\n\n    # max_value < some elements\n    relu_op = backend.relu(x, max_value=5)\n    self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 5]])\n\n    # nn.relu6 used\n    relu_op = backend.relu(x, max_value=6)\n    if not context.executing_eagerly():\n      self.assertTrue('Relu6' in relu_op.name)  # uses tf.nn.relu6\n    self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 6]])\n\n    # max value > 6\n    relu_op = backend.relu(x, max_value=10)\n    self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 7]])\n\n    # max value is float\n    relu_op = backend.relu(x, max_value=4.3)\n    self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 4.3]])\n\n    # max value == 0\n    relu_op = backend.relu(x, max_value=0)\n    self.assertAllClose(backend.eval(relu_op), [[0, 0], [0, 0]])\n\n    # alpha and max_value\n    relu_op = backend.relu(x, alpha=0.25, max_value=3)\n    self.assertAllClose(backend.eval(relu_op), [[-1, 0], [2, 3]])\n\n    # threshold\n    relu_op = backend.relu(x, threshold=3)\n    self.assertAllClose(backend.eval(relu_op), [[0, 0], [0, 7]])\n\n    # threshold is float\n    relu_op = backend.relu(x, threshold=1.5)\n    self.assertAllClose(backend.eval(relu_op), [[0, 0], [2, 7]])\n\n    # threshold is negative\n    relu_op = backend.relu(x, threshold=-5)\n    self.assertAllClose(backend.eval(relu_op), [[-4, 0], [2, 7]])\n\n    # threshold and max_value\n    relu_op = backend.relu(x, threshold=3, max_value=5)\n    self.assertAllClose(backend.eval(relu_op), [[0, 0], [0, 5]])\n\n    # threshold and alpha\n    relu_op = backend.relu(x, alpha=0.25, threshold=4)\n    self.assertAllClose(backend.eval(relu_op), [[-2, -1], [-0.5, 7]])\n\n    # threshold, alpha, and max_value\n    relu_op = backend.relu(x, alpha=0.25, threshold=4, max_value=5)\n    self.assertAllClose(backend.eval(relu_op), [[-2, -1], [-0.5, 5]])\n\n    # Test case for GitHub issue 35430, with integer dtype\n    x = input_layer.Input(shape=(), name='x', dtype='int64')\n    _ = advanced_activations.ReLU(max_value=100, dtype='int64')(x)\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass BackendShapeOpsTest(test.TestCase):\n\n  def test_reshape(self):\n    compare_single_input_op_to_numpy(\n        backend.reshape,\n        np.reshape,\n        input_shape=(4, 7),\n        keras_args=[(2, 14)],\n        np_args=[(2, 14)])\n\n  def test_concatenate(self):\n    a = backend.variable(np.ones((1, 2, 3)))\n    b = backend.variable(np.ones((1, 2, 2)))\n    y = backend.concatenate([a, b], axis=-1)\n    self.assertEqual(y.shape.as_list(), [1, 2, 5])\n\n  def test_permute_dimensions(self):\n    compare_single_input_op_to_numpy(\n        backend.permute_dimensions,\n        np.transpose,\n        input_shape=(4, 7),\n        keras_args=[(1, 0)],\n        np_args=[(1, 0)])\n\n  def test_resize_images(self):\n    height_factor = 2\n    width_factor = 2\n    data_format = 'channels_last'\n    x = backend.variable(np.ones((1, 2, 2, 3)))\n    y = backend.resize_images(x, height_factor, width_factor, data_format)\n    self.assertEqual(y.shape.as_list(), [1, 4, 4, 3])\n\n    data_format = 'channels_first'\n    x = backend.variable(np.ones((1, 3, 2, 2)))\n    y = backend.resize_images(x, height_factor, width_factor, data_format)\n    self.assertEqual(y.shape.as_list(), [1, 3, 4, 4])\n\n    # Use with a dynamic axis:\n    if not context.executing_eagerly():\n      x = backend.placeholder(shape=(1, 3, None, None))\n      y = backend.resize_images(x, height_factor, width_factor, data_format)\n      self.assertEqual(y.shape.as_list(), [1, 3, None, None])\n\n    # Invalid use:\n    with self.assertRaises(ValueError):\n      backend.resize_images(\n          x, height_factor, width_factor, data_format='unknown')\n\n  def test_resize_volumes(self):\n    height_factor = 2\n    width_factor = 2\n    depth_factor = 2\n    data_format = 'channels_last'\n    x = backend.variable(np.ones((1, 2, 2, 2, 3)))\n    y = backend.resize_volumes(x, depth_factor, height_factor, width_factor,\n                               data_format)\n    self.assertEqual(y.shape.as_list(), [1, 4, 4, 4, 3])\n\n    data_format = 'channels_first'\n    x = backend.variable(np.ones((1, 3, 2, 2, 2)))\n    y = backend.resize_volumes(x, depth_factor, height_factor, width_factor,\n                               data_format)\n    self.assertEqual(y.shape.as_list(), [1, 3, 4, 4, 4])\n\n    # Invalid use:\n    with self.assertRaises(ValueError):\n      backend.resize_volumes(\n          x, depth_factor, height_factor, width_factor, data_format='unknown')\n\n  def test_repeat_elements(self):\n    x = backend.variable(np.ones((1, 3, 2)))\n    y = backend.repeat_elements(x, 3, axis=1)\n    self.assertEqual(y.shape.as_list(), [1, 9, 2])\n\n    # Use with a dynamic axis:\n    if not context.executing_eagerly():\n      x = backend.placeholder(shape=(2, None, 2))\n      y = backend.repeat_elements(x, 3, axis=1)\n      self.assertEqual(y.shape.as_list(), [2, None, 2])\n\n  def test_repeat(self):\n    x = backend.variable(np.ones((1, 3)))\n    y = backend.repeat(x, 2)\n    self.assertEqual(y.shape.as_list(), [1, 2, 3])\n\n  def test_flatten(self):\n    compare_single_input_op_to_numpy(\n        backend.flatten,\n        np.reshape,\n        input_shape=(4, 7, 6),\n        np_args=[(4 * 7 * 6,)])\n\n  def test_batch_flatten(self):\n    compare_single_input_op_to_numpy(\n        backend.batch_flatten,\n        np.reshape,\n        input_shape=(4, 7, 6),\n        np_args=[(4, 7 * 6)])\n\n  def test_temporal_padding(self):\n\n    def ref_op(x, padding):\n      shape = list(x.shape)\n      shape[1] += padding[0] + padding[1]\n      y = np.zeros(tuple(shape))\n      y[:, padding[0]:-padding[1], :] = x\n      return y\n\n    compare_single_input_op_to_numpy(\n        backend.temporal_padding,\n        ref_op,\n        input_shape=(4, 7, 6),\n        keras_args=[(2, 3)],\n        np_args=[(2, 3)])\n\n  def test_spatial_2d_padding(self):\n\n    def ref_op(x, padding, data_format='channels_last'):\n      shape = list(x.shape)\n      if data_format == 'channels_last':\n        shape[1] += padding[0][0] + padding[0][1]\n        shape[2] += padding[1][0] + padding[1][1]\n        y = np.zeros(tuple(shape))\n        y[:, padding[0][0]:-padding[0][1], padding[1][0]:-padding[1][1], :] = x\n      else:\n        shape[2] += padding[0][0] + padding[0][1]\n        shape[3] += padding[1][0] + padding[1][1]\n        y = np.zeros(tuple(shape))\n        y[:, :, padding[0][0]:-padding[0][1], padding[1][0]:-padding[1][1]] = x\n      return y\n\n    compare_single_input_op_to_numpy(\n        backend.spatial_2d_padding,\n        ref_op,\n        input_shape=(2, 3, 2, 3),\n        keras_args=[((2, 3), (1, 2))],\n        keras_kwargs={'data_format': 'channels_last'},\n        np_args=[((2, 3), (1, 2))],\n        np_kwargs={'data_format': 'channels_last'})\n    compare_single_input_op_to_numpy(\n        backend.spatial_2d_padding,\n        ref_op,\n        input_shape=(2, 3, 2, 3),\n        keras_args=[((2, 3), (1, 2))],\n        keras_kwargs={'data_format': 'channels_first'},\n        np_args=[((2, 3), (1, 2))],\n        np_kwargs={'data_format': 'channels_first'})\n\n  def test_spatial_3d_padding(self):\n\n    def ref_op(x, padding, data_format='channels_last'):\n      shape = list(x.shape)\n      if data_format == 'channels_last':\n        shape[1] += padding[0][0] + padding[0][1]\n        shape[2] += padding[1][0] + padding[1][1]\n        shape[3] += padding[2][0] + padding[2][1]\n        y = np.zeros(tuple(shape))\n        y[:, padding[0][0]:-padding[0][1], padding[1][0]:-padding[1][1],\n          padding[2][0]:-padding[2][1], :] = x\n      else:\n        shape[2] += padding[0][0] + padding[0][1]\n        shape[3] += padding[1][0] + padding[1][1]\n        shape[4] += padding[2][0] + padding[2][1]\n        y = np.zeros(tuple(shape))\n        y[:, :, padding[0][0]:-padding[0][1], padding[1][0]:-padding[1][1],\n          padding[2][0]:-padding[2][1]] = x\n      return y\n\n    compare_single_input_op_to_numpy(\n        backend.spatial_3d_padding,\n        ref_op,\n        input_shape=(2, 3, 2, 3, 2),\n        keras_args=[((2, 3), (1, 2), (2, 3))],\n        keras_kwargs={'data_format': 'channels_last'},\n        np_args=[((2, 3), (1, 2), (2, 3))],\n        np_kwargs={'data_format': 'channels_last'})\n    compare_single_input_op_to_numpy(\n        backend.spatial_3d_padding,\n        ref_op,\n        input_shape=(2, 3, 2, 3, 2),\n        keras_args=[((2, 3), (1, 2), (2, 3))],\n        keras_kwargs={'data_format': 'channels_first'},\n        np_args=[((2, 3), (1, 2), (2, 3))],\n        np_kwargs={'data_format': 'channels_first'})\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass BackendNNOpsTest(test.TestCase, parameterized.TestCase):\n\n  def test_bias_add(self):\n    keras_op = backend.bias_add\n    np_op = np.add\n    compare_two_inputs_op_to_numpy(\n        keras_op, np_op, input_shape_a=(4, 7), input_shape_b=(7,))\n    compare_two_inputs_op_to_numpy(\n        keras_op, np_op, input_shape_a=(4, 3, 7), input_shape_b=(7,))\n    compare_two_inputs_op_to_numpy(\n        keras_op, np_op, input_shape_a=(4, 3, 5, 7), input_shape_b=(7,))\n    compare_two_inputs_op_to_numpy(\n        keras_op, np_op, input_shape_a=(4, 3, 5, 2, 7), input_shape_b=(7,))\n\n    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n      x = backend.variable((3, 4))\n      b = backend.variable((3, 4))\n      backend.bias_add(x, b)\n    with self.assertRaises(ValueError):\n      x = backend.variable((3, 4))\n      b = backend.variable((4,))\n      backend.bias_add(x, b, data_format='unknown')\n\n  def test_bias_add_channels_first(self):\n\n    def keras_op(x, b):\n      return backend.bias_add(x, b, data_format='channels_first')\n\n    def np_op(x, b):\n      if x.ndim == 3:\n        b = b.reshape((1, b.shape[0], 1))\n      if x.ndim == 4:\n        b = b.reshape((1, b.shape[0], 1, 1))\n      return x + b\n\n    compare_two_inputs_op_to_numpy(\n        keras_op, np_op, input_shape_a=(4, 3, 7), input_shape_b=(3,))\n    compare_two_inputs_op_to_numpy(\n        keras_op, np_op, input_shape_a=(4, 3, 5, 7), input_shape_b=(3,))\n\n  def test_pool2d(self):\n    val = np.random.random((10, 3, 10, 10))\n    x = backend.variable(val)\n    y = backend.pool2d(\n        x, (2, 2),\n        strides=(1, 1),\n        padding='valid',\n        data_format='channels_first',\n        pool_mode='max')\n    self.assertEqual(y.shape.as_list(), [10, 3, 9, 9])\n\n    y = backend.pool2d(\n        x, (2, 2),\n        strides=(1, 1),\n        padding='valid',\n        data_format='channels_first',\n        pool_mode='avg')\n    self.assertEqual(y.shape.as_list(), [10, 3, 9, 9])\n\n    val = np.random.random((10, 10, 10, 3))\n    x = backend.variable(val)\n    y = backend.pool2d(\n        x, (2, 2), strides=(1, 1), padding='valid', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 9, 9, 3])\n\n    val = np.random.random((10, 10, 10, 3))\n    x = backend.variable(val)\n    y = backend.pool2d(\n        x, (2, 2), strides=(1, 1), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 10, 10, 3])\n\n    val = np.random.random((10, 10, 10, 3))\n    x = backend.variable(val)\n    y = backend.pool2d(\n        x, (2, 2), strides=(2, 2), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 5, 5, 3])\n\n    with self.assertRaises(ValueError):\n      y = backend.pool2d(\n          x, (2, 2),\n          strides=(2, 2),\n          padding='other',\n          data_format='channels_last')\n    with self.assertRaises(ValueError):\n      y = backend.pool2d(x, (2, 2), strides=(2, 2), data_format='other')\n    with self.assertRaises(ValueError):\n      y = backend.pool2d(x, (2, 2, 2), strides=(2, 2))\n    with self.assertRaises(ValueError):\n      y = backend.pool2d(x, (2, 2), strides=(2, 2, 2))\n    with self.assertRaises(ValueError):\n      y = backend.pool2d(x, (2, 2), strides=(2, 2), pool_mode='other')\n\n  def test_pool3d(self):\n    val = np.random.random((10, 3, 10, 10, 10))\n    x = backend.variable(val)\n    y = backend.pool3d(\n        x, (2, 2, 2),\n        strides=(1, 1, 1),\n        padding='valid',\n        data_format='channels_first',\n        pool_mode='max')\n    self.assertEqual(y.shape.as_list(), [10, 3, 9, 9, 9])\n\n    y = backend.pool3d(\n        x, (2, 2, 2),\n        strides=(1, 1, 1),\n        padding='valid',\n        data_format='channels_first',\n        pool_mode='avg')\n    self.assertEqual(y.shape.as_list(), [10, 3, 9, 9, 9])\n\n    val = np.random.random((10, 10, 10, 10, 3))\n    x = backend.variable(val)\n    y = backend.pool3d(\n        x, (2, 2, 2),\n        strides=(1, 1, 1),\n        padding='valid',\n        data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 9, 9, 9, 3])\n\n    val = np.random.random((10, 10, 10, 10, 3))\n    x = backend.variable(val)\n    y = backend.pool3d(\n        x, (2, 2, 2),\n        strides=(1, 1, 1),\n        padding='same',\n        data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 10, 10, 10, 3])\n\n    val = np.random.random((10, 10, 10, 10, 3))\n    x = backend.variable(val)\n    y = backend.pool3d(\n        x, (2, 2, 2),\n        strides=(2, 2, 2),\n        padding='same',\n        data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 5, 5, 5, 3])\n\n  def test_conv1d(self):\n    val = np.random.random((10, 4, 10))\n    x = backend.variable(val)\n    kernel_val = np.random.random((3, 4, 5))\n    k = backend.variable(kernel_val)\n    y = backend.conv1d(\n        x, k, strides=(1,), padding='valid', data_format='channels_first')\n    self.assertEqual(y.shape.as_list(), [10, 5, 8])\n\n    val = np.random.random((10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv1d(\n        x, k, strides=(1,), padding='valid', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 8, 5])\n\n    val = np.random.random((10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv1d(\n        x, k, strides=(1,), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 10, 5])\n\n    val = np.random.random((10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv1d(\n        x, k, strides=(2,), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 5, 5])\n\n  def test_local_conv_channels_dim(self):\n    filters = 3\n    batch_size = 2\n\n    for input_shape in [(3, 5), (2, 3, 5), (2, 5, 3, 4)]:\n      channels_in = input_shape[0]\n      input_spatial_shape = input_shape[1:]\n      dim = len(input_spatial_shape)\n\n      inputs = np.random.normal(0, 1, (batch_size,) + input_shape)\n      inputs_cf = backend.variable(inputs)\n\n      for kernel_size in [1, 2]:\n        for stride in [1, 2]:\n          kernel_sizes = (kernel_size,) * dim\n          strides = (stride,) * dim\n\n          output_shape = tuple([\n              (i - kernel_size + stride) // stride for i in input_spatial_shape\n          ])\n\n          kernel_shape = (np.prod(output_shape),\n                          np.prod(kernel_sizes) * channels_in, filters)\n\n          kernel = np.random.normal(\n              0, 1,\n              output_shape + (channels_in, np.prod(kernel_sizes), filters))\n\n          kernel_cf = np.reshape(kernel, kernel_shape)\n          kernel_cf = backend.variable(kernel_cf)\n\n          conv_cf = backend.local_conv(inputs_cf, kernel_cf, kernel_sizes,\n                                       strides, output_shape, 'channels_first')\n\n          inputs_cl = np.transpose(inputs,\n                                   [0, 2] + list(range(3, dim + 2)) + [1])\n          inputs_cl = backend.variable(inputs_cl)\n\n          kernel_cl = np.reshape(\n              np.transpose(kernel,\n                           list(range(dim)) + [dim + 1, dim, dim + 2]),\n              kernel_shape)\n          kernel_cl = backend.variable(kernel_cl)\n\n          conv_cl = backend.local_conv(inputs_cl, kernel_cl, kernel_sizes,\n                                       strides, output_shape, 'channels_last')\n\n          conv_cf = backend.eval(conv_cf)\n          conv_cl = backend.eval(conv_cl)\n\n          self.assertAllCloseAccordingToType(\n              conv_cf,\n              np.transpose(conv_cl, [0, dim + 1] + list(range(1, dim + 1))),\n              atol=1e-5)\n\n  @parameterized.named_parameters(\n      ('local_conv1d', (5, 6), (3,), (1,), (3,)),\n      ('local_conv2d', (4, 5, 6), (3, 3), (1, 1), (2, 3)))\n  def test_local_conv_1d_and_2d(self, input_shape, kernel_sizes, strides,\n                                output_shape):\n    filters = 3\n    batch_size = 2\n\n    inputs = np.random.normal(0, 1, (batch_size,) + input_shape)\n    inputs = backend.variable(inputs)\n\n    kernel = np.random.normal(0, 1,\n                              (np.prod(output_shape), np.prod(kernel_sizes) *\n                               input_shape[-1], filters))\n    kernel = backend.variable(kernel)\n\n    local_conv = backend.local_conv(inputs, kernel, kernel_sizes, strides,\n                                    output_shape, 'channels_last')\n    if len(output_shape) == 1:\n      local_conv_dim = backend.local_conv1d(inputs, kernel, kernel_sizes,\n                                            strides, 'channels_last')\n    else:\n      local_conv_dim = backend.local_conv2d(inputs, kernel, kernel_sizes,\n                                            strides, output_shape,\n                                            'channels_last')\n\n    local_conv = backend.eval(local_conv)\n    local_conv_dim = backend.eval(local_conv_dim)\n\n    self.assertAllCloseAccordingToType(local_conv, local_conv_dim)\n\n  def test_conv2d(self):\n    kernel_val = np.random.random((3, 3, 4, 5))\n    k = backend.variable(kernel_val)\n\n    # Test channels_first\n    val = np.random.random((10, 4, 10, 10))\n    x = backend.variable(val)\n    y = backend.conv2d(x, k, padding='valid', data_format='channels_first')\n    self.assertEqual(y.shape.as_list(), [10, 5, 8, 8])\n\n    # Test channels_last\n    val = np.random.random((10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv2d(\n        x, k, strides=(1, 1), padding='valid', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 8, 8, 5])\n\n    # Test same padding\n    val = np.random.random((10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv2d(x, k, padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 10, 10, 5])\n\n    # Test dilation_rate\n    val = np.random.random((10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv2d(\n        x, k, dilation_rate=(2, 2), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 10, 10, 5])\n\n    # Test strides\n    val = np.random.random((10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv2d(\n        x, k, strides=(2, 2), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 5, 5, 5])\n\n    # Test invalid arguments\n    with self.assertRaises(ValueError):\n      y = backend.conv2d(\n          x, k, (2, 2), padding='other', data_format='channels_last')\n    with self.assertRaises(ValueError):\n      y = backend.conv2d(x, k, (2, 2), data_format='other')\n    with self.assertRaises(ValueError):\n      y = backend.conv2d(x, k, (2, 2, 2))\n\n  def test_conv2d_transpose(self):\n    input_size = (7, 8)\n    kernel_size = (3, 3)\n    input_depth = 6\n    filters = 6\n    batch_size = 2\n\n    kernel_val = np.random.random(kernel_size + (input_depth, filters))\n    k = backend.variable(kernel_val)\n\n    # Test channels_first\n    input_val = np.random.random((batch_size, input_depth) + input_size)\n    x = backend.variable(input_val)\n    y = backend.conv2d_transpose(\n        x,\n        k, (batch_size, filters) + input_size,\n        padding='same',\n        data_format='channels_first')\n    self.assertEqual(\n        tuple(y.shape.as_list()), (batch_size, filters) + input_size)\n\n    # Test channels_last\n    input_val = np.random.random((batch_size,) + input_size + (input_depth,))\n    x = backend.variable(input_val)\n    y = backend.conv2d_transpose(\n        x,\n        k, (batch_size,) + input_size + (filters,),\n        padding='same',\n        data_format='channels_last')\n    self.assertEqual(\n        tuple(y.shape.as_list()), (batch_size,) + input_size + (filters,))\n\n    # Test dilation_rate\n    y = backend.conv2d_transpose(\n        x,\n        k, (batch_size,) + input_size + (filters,),\n        padding='same',\n        data_format='channels_last',\n        dilation_rate=(2, 2))\n    self.assertEqual(\n        tuple(y.shape.as_list()), (batch_size,) + input_size + (filters,))\n\n    # Test batch size of None in output_shape\n    y = backend.conv2d_transpose(\n        x,\n        k, (None,) + input_size + (filters,),\n        padding='same',\n        data_format='channels_last')\n    self.assertEqual(\n        tuple(y.shape.as_list()), (batch_size,) + input_size + (filters,))\n\n    # Test invalid values\n    with self.assertRaises(ValueError):\n      y = backend.conv2d_transpose(\n          x, k, (2, 2, 8, 9), padding='other', data_format='channels_last')\n    with self.assertRaises(ValueError):\n      y = backend.conv2d_transpose(x, k, (2, 2, 8, 9), data_format='other')\n\n  def test_separable_conv2d(self):\n    val = np.random.random((10, 4, 10, 10))\n    x = backend.variable(val)\n    depthwise_kernel_val = np.random.random((3, 3, 4, 1))\n    pointwise_kernel_val = np.random.random((1, 1, 4, 5))\n    dk = backend.variable(depthwise_kernel_val)\n    pk = backend.variable(pointwise_kernel_val)\n    y = backend.separable_conv2d(\n        x, dk, pk, padding='valid', data_format='channels_first')\n    self.assertEqual(y.shape.as_list(), [10, 5, 8, 8])\n\n    val = np.random.random((10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.separable_conv2d(\n        x, dk, pk, strides=(1, 1), padding='valid', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 8, 8, 5])\n\n    val = np.random.random((10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.separable_conv2d(\n        x, dk, pk, strides=(1, 1), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 10, 10, 5])\n\n    val = np.random.random((10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.separable_conv2d(\n        x, dk, pk, strides=(2, 2), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 5, 5, 5])\n    with self.assertRaises(ValueError):\n      y = backend.separable_conv2d(\n          x, dk, pk, (2, 2), padding='other', data_format='channels_last')\n    with self.assertRaises(ValueError):\n      y = backend.separable_conv2d(x, dk, pk, (2, 2), data_format='other')\n    with self.assertRaises(ValueError):\n      y = backend.separable_conv2d(x, dk, pk, (2, 2, 2))\n\n  def test_conv3d(self):\n    val = np.random.random((10, 4, 10, 10, 10))\n    x = backend.variable(val)\n    kernel_val = np.random.random((3, 3, 3, 4, 5))\n    k = backend.variable(kernel_val)\n    y = backend.conv3d(x, k, padding='valid', data_format='channels_first')\n    self.assertEqual(y.shape.as_list(), [10, 5, 8, 8, 8])\n\n    val = np.random.random((10, 10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv3d(\n        x, k, strides=(1, 1, 1), padding='valid', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 8, 8, 8, 5])\n\n    val = np.random.random((10, 10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv3d(\n        x, k, strides=(1, 1, 1), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 10, 10, 10, 5])\n\n    val = np.random.random((10, 10, 10, 10, 4))\n    x = backend.variable(val)\n    y = backend.conv3d(\n        x, k, strides=(2, 2, 2), padding='same', data_format='channels_last')\n    self.assertEqual(y.shape.as_list(), [10, 5, 5, 5, 5])\n    with self.assertRaises(ValueError):\n      y = backend.conv3d(\n          x, k, (2, 2, 2), padding='other', data_format='channels_last')\n    with self.assertRaises(ValueError):\n      y = backend.conv3d(x, k, (2, 2, 2), data_format='other')\n    with self.assertRaises(ValueError):\n      y = backend.conv3d(x, k, (2, 2))\n\n  def test_rnn(self):\n    # implement a simple RNN\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n\n    input_val = np.random.random(\n        (num_samples, timesteps, input_dim)).astype(np.float32)\n    init_state_val = np.random.random(\n        (num_samples, output_dim)).astype(np.float32)\n    w_i_val = np.random.random((input_dim, output_dim)).astype(np.float32)\n    w_o_val = np.random.random((output_dim, output_dim)).astype(np.float32)\n    np_mask = np.random.randint(2, size=(num_samples, timesteps))\n\n    def rnn_step_fn():\n      w_i = backend.variable(w_i_val)\n      w_o = backend.variable(w_o_val)\n\n      def step_function(x, states):\n        assert len(states) == 1\n        prev_output = states[0]\n        output = backend.dot(x, w_i) + backend.dot(prev_output, w_o)\n        return output, [output]\n\n      return step_function\n\n    # test default setup\n    last_output_list = [[], [], [], [], [], []]\n    outputs_list = [[], [], [], [], [], []]\n    state_list = [[], [], [], [], [], []]\n\n    rnn_fn = rnn_step_fn()\n    inputs = backend.variable(input_val)\n    initial_states = [backend.variable(init_state_val)]\n    mask = backend.variable(np_mask)\n\n    kwargs_list = [\n        {\n            'go_backwards': False,\n            'mask': None\n        },\n        {\n            'go_backwards': False,\n            'mask': None,\n            'unroll': True\n        },\n        {\n            'go_backwards': True,\n            'mask': None\n        },\n        {\n            'go_backwards': True,\n            'mask': None,\n            'unroll': True\n        },\n        {\n            'go_backwards': False,\n            'mask': mask\n        },\n        {\n            'go_backwards': False,\n            'mask': mask,\n            'unroll': True\n        },\n    ]\n    for i, kwargs in enumerate(kwargs_list):\n      last_output, outputs, new_states = backend.rnn(rnn_fn, inputs,\n                                                     initial_states, **kwargs)\n      # check static shape inference\n      self.assertEqual(last_output.shape.as_list(), [num_samples, output_dim])\n      self.assertEqual(outputs.shape.as_list(),\n                       [num_samples, timesteps, output_dim])\n      for state in new_states:\n        self.assertEqual(state.shape.as_list(), [num_samples, output_dim])\n\n      last_output_list[i].append(backend.eval(last_output))\n      outputs_list[i].append(backend.eval(outputs))\n      self.assertLen(new_states, 1)\n      state_list[i].append(backend.eval(new_states[0]))\n\n      def assert_list_pairwise(z_list, atol=1e-05):\n        for (z1, z2) in zip(z_list[1:], z_list[:-1]):\n          self.assertAllClose(z1, z2, atol=atol)\n\n      assert_list_pairwise(last_output_list[0], atol=1e-04)\n      assert_list_pairwise(outputs_list[0], atol=1e-04)\n      assert_list_pairwise(state_list[0], atol=1e-04)\n      assert_list_pairwise(last_output_list[2], atol=1e-04)\n      assert_list_pairwise(outputs_list[2], atol=1e-04)\n      assert_list_pairwise(state_list[2], atol=1e-04)\n\n      for l, u_l in zip(last_output_list[0], last_output_list[1]):\n        self.assertAllClose(l, u_l, atol=1e-04)\n\n      for o, u_o in zip(outputs_list[0], outputs_list[1]):\n        self.assertAllClose(o, u_o, atol=1e-04)\n\n      for s, u_s in zip(state_list[0], state_list[1]):\n        self.assertAllClose(s, u_s, atol=1e-04)\n\n      for b_l, b_u_l in zip(last_output_list[2], last_output_list[3]):\n        self.assertAllClose(b_l, b_u_l, atol=1e-04)\n\n      for b_o, b_u_o in zip(outputs_list[2], outputs_list[3]):\n        self.assertAllClose(b_o, b_u_o, atol=1e-04)\n\n      for b_s, b_u_s in zip(state_list[2], state_list[3]):\n        self.assertAllClose(b_s, b_u_s, atol=1e-04)\n\n  def test_rnn_additional_states(self):\n    # implement a simple RNN\n    num_samples = 4\n    input_dim = 5\n    output_dim = 3\n    timesteps = 6\n\n    input_val = np.random.random(\n        (num_samples, timesteps, input_dim)).astype(np.float32)\n    init_state_val = np.random.random(\n        (num_samples, output_dim)).astype(np.float32)\n    w_i_val = np.random.random((input_dim, output_dim)).astype(np.float32)\n    w_o_val = np.random.random((output_dim, output_dim)).astype(np.float32)\n    np_mask = np.random.randint(2, size=(num_samples, timesteps))\n\n    def rnn_step_fn():\n      w_i = backend.variable(w_i_val)\n      w_o = backend.variable(w_o_val)\n\n      def step_function(x, states):\n        assert len(states) == 2\n        prev_output = states[0]\n        output = backend.dot(x, w_i) + backend.dot(prev_output, w_o)\n        return output, [output, backend.concatenate([output, output], axis=-1)]\n\n      return step_function\n\n    # test default setup\n    last_output_list = [[], [], [], [], [], []]\n    outputs_list = [[], [], [], [], [], []]\n    state_list = [[], [], [], [], [], []]\n    additional_state_list = [[], [], [], [], [], []]\n\n    rnn_fn = rnn_step_fn()\n    inputs = backend.variable(input_val)\n    initial_states = [\n        backend.variable(init_state_val),\n        ops.convert_to_tensor_v2_with_dispatch(\n            np.concatenate([init_state_val, init_state_val], axis=-1))\n    ]\n    mask = backend.variable(np_mask)\n\n    kwargs_list = [\n        {\n            'go_backwards': False,\n            'mask': None\n        },\n        {\n            'go_backwards': False,\n            'mask': None,\n            'unroll': True\n        },\n        {\n            'go_backwards': True,\n            'mask': None\n        },\n        {\n            'go_backwards': True,\n            'mask': None,\n            'unroll': True\n        },\n        {\n            'go_backwards': False,\n            'mask': mask\n        },\n        {\n            'go_backwards': False,\n            'mask': mask,\n            'unroll': True\n        },\n    ]\n    for i, kwargs in enumerate(kwargs_list):\n      last_output, outputs, new_states = backend.rnn(rnn_fn, inputs,\n                                                     initial_states, **kwargs)\n      # check static shape inference\n      self.assertEqual(last_output.shape.as_list(), [num_samples, output_dim])\n      self.assertEqual(outputs.shape.as_list(),\n                       [num_samples, timesteps, output_dim])\n      # for state in new_states:\n      #   self.assertEqual(state.shape.as_list(),\n      #                     [num_samples, output_dim])\n      self.assertEqual(new_states[0].shape.as_list(), [num_samples, output_dim])\n      self.assertEqual(new_states[1].shape.as_list(),\n                       [num_samples, 2 * output_dim])\n\n      last_output_list[i].append(backend.eval(last_output))\n      outputs_list[i].append(backend.eval(outputs))\n      self.assertLen(new_states, 2)\n      state_list[i].append(backend.eval(new_states[0]))\n      additional_state_list[i].append(backend.eval(new_states[1]))\n\n      def assert_list_pairwise(z_list, atol=1e-05):\n        for (z1, z2) in zip(z_list[1:], z_list[:-1]):\n          self.assertAllClose(z1, z2, atol=atol)\n\n      assert_list_pairwise(last_output_list[0], atol=1e-04)\n      assert_list_pairwise(outputs_list[0], atol=1e-04)\n      assert_list_pairwise(state_list[0], atol=1e-04)\n      assert_list_pairwise(additional_state_list[0], atol=1e-04)\n      assert_list_pairwise(last_output_list[2], atol=1e-04)\n      assert_list_pairwise(outputs_list[2], atol=1e-04)\n      assert_list_pairwise(state_list[2], atol=1e-04)\n      assert_list_pairwise(additional_state_list[2], atol=1e-04)\n\n      for l, u_l in zip(last_output_list[0], last_output_list[1]):\n        self.assertAllClose(l, u_l, atol=1e-04)\n\n      for o, u_o in zip(outputs_list[0], outputs_list[1]):\n        self.assertAllClose(o, u_o, atol=1e-04)\n\n      for s, u_s in zip(state_list[0], state_list[1]):\n        self.assertAllClose(s, u_s, atol=1e-04)\n\n      for s, u_s in zip(additional_state_list[0], additional_state_list[1]):\n        self.assertAllClose(s, u_s, atol=1e-04)\n\n      for b_l, b_u_l in zip(last_output_list[2], last_output_list[3]):\n        self.assertAllClose(b_l, b_u_l, atol=1e-04)\n\n      for b_o, b_u_o in zip(outputs_list[2], outputs_list[3]):\n        self.assertAllClose(b_o, b_u_o, atol=1e-04)\n\n      for b_s, b_u_s in zip(state_list[2], state_list[3]):\n        self.assertAllClose(b_s, b_u_s, atol=1e-04)\n\n      for s, u_s in zip(additional_state_list[2], additional_state_list[3]):\n        self.assertAllClose(s, u_s, atol=1e-04)\n\n  def test_rnn_output_and_state_masking_independent(self):\n    num_samples = 2\n    num_timesteps = 4\n    state_and_io_size = 2\n    mask_last_num_timesteps = 2  # for second sample only\n\n    # a step function that just outputs inputs,\n    # but increments states +1 per timestep\n    def step_function(inputs, states):\n      return inputs, [s + 1 for s in states]\n\n    inputs_vals = np.random.random(\n        (num_samples, num_timesteps, state_and_io_size))\n    initial_state_vals = np.random.random((num_samples, state_and_io_size))\n    # masking of two last timesteps for second sample only\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[1, -mask_last_num_timesteps:] = 0\n\n    # outputs expected to be same as inputs for the first sample\n    expected_outputs = inputs_vals.copy()\n    # but for the second sample all outputs in masked region should be the same\n    # as last output before masked region\n    expected_outputs[1, -mask_last_num_timesteps:] = \\\n        expected_outputs[1, -(mask_last_num_timesteps + 1)]\n\n    expected_last_state = initial_state_vals.copy()\n    # first state should be incremented for every timestep (no masking)\n    expected_last_state[0] += num_timesteps\n    # second state should not be incremented for last two timesteps\n    expected_last_state[1] += (num_timesteps - mask_last_num_timesteps)\n\n    # verify same expected output for `unroll=true/false`\n    inputs = backend.variable(inputs_vals)\n    initial_states = [backend.variable(initial_state_vals)]\n    mask = backend.variable(mask_vals)\n    for unroll in [True, False]:\n      _, outputs, last_states = backend.rnn(\n          step_function,\n          inputs,\n          initial_states,\n          mask=mask,\n          unroll=unroll,\n          input_length=num_timesteps if unroll else None)\n\n      self.assertAllClose(backend.eval(outputs), expected_outputs)\n      self.assertAllClose(backend.eval(last_states[0]), expected_last_state)\n\n  def test_rnn_output_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n    num_features = 5\n\n    def step_function(inputs, states):\n      outputs = backend.tile(backend.expand_dims(inputs), [1, 1, 2])\n      return outputs, [backend.identity(s) for s in states]\n      # Note: cannot just return states (which can be a problem) ->\n      # tensorflow/python/ops/resource_variable_ops.py\", line 824, in set_shape\n      # NotImplementedError: ResourceVariable does not implement set_shape()\n\n    inputs_vals = np.random.random((num_samples, num_timesteps, num_features))\n    initial_state_vals = np.random.random((num_samples, 6))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[-1, -1] = 0  # final timestep masked for last sample\n\n    expected_outputs = np.repeat(inputs_vals[..., None], repeats=2, axis=-1)\n    # for the last sample, the final timestep (in masked region) should be the\n    # same as the second to final output (before masked region)\n    expected_outputs[-1, -1] = expected_outputs[-1, -2]\n\n    inputs = backend.variable(inputs_vals)\n    initial_states = [backend.variable(initial_state_vals)]\n    mask = backend.variable(mask_vals)\n    for unroll in [True, False]:\n      _, outputs, _ = backend.rnn(\n          step_function,\n          inputs,\n          initial_states,\n          mask=mask,\n          unroll=unroll,\n          input_length=num_timesteps if unroll else None)\n\n      self.assertAllClose(backend.eval(outputs), expected_outputs)\n\n  def test_rnn_state_num_dim_larger_than_2_masking(self):\n    num_samples = 3\n    num_timesteps = 4\n\n    def step_function(inputs, states):\n      return inputs, [s + 1 for s in states]\n\n    inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n    initial_state_vals = np.random.random((num_samples, 6, 7))\n    mask_vals = np.ones((num_samples, num_timesteps))\n    mask_vals[0, -2:] = 0  # final two timesteps masked for first sample\n\n    expected_last_state = initial_state_vals.copy()\n    expected_last_state[0] += (num_timesteps - 2)\n    expected_last_state[1:] += num_timesteps\n\n    inputs = backend.variable(inputs_vals)\n    initial_states = [backend.variable(initial_state_vals)]\n    mask = backend.variable(mask_vals)\n    for unroll in [True, False]:\n      _, _, last_states = backend.rnn(\n          step_function,\n          inputs,\n          initial_states,\n          mask=mask,\n          unroll=unroll,\n          input_length=num_timesteps if unroll else None)\n\n      self.assertAllClose(backend.eval(last_states[0]), expected_last_state)\n\n  def test_batch_normalization(self):\n    g_val = np.random.random((3,))\n    b_val = np.random.random((3,))\n    gamma = backend.variable(g_val)\n    beta = backend.variable(b_val)\n\n    # 3D NHC case\n    val = np.random.random((10, 5, 3))\n    x = backend.variable(val)\n    mean, var = nn.moments(x, (0, 1), None, None, False)\n    normed = backend.batch_normalization(\n        x, mean, var, beta, gamma, axis=-1, epsilon=1e-3)\n    self.assertEqual(normed.shape.as_list(), [10, 5, 3])\n\n    # 4D NHWC case\n    val = np.random.random((10, 5, 5, 3))\n    x = backend.variable(val)\n    mean, var = nn.moments(x, (0, 1, 2), None, None, False)\n    normed = backend.batch_normalization(\n        x, mean, var, beta, gamma, axis=-1, epsilon=1e-3)\n    self.assertEqual(normed.shape.as_list(), [10, 5, 5, 3])\n\n    # 4D NCHW case\n    if not context.executing_eagerly():\n      # Eager CPU kernel for NCHW does not exist.\n      val = np.random.random((10, 3, 5, 5))\n      x = backend.variable(val)\n      mean, var = nn.moments(x, (0, 2, 3), None, None, False)\n      normed = backend.batch_normalization(\n          x, mean, var, beta, gamma, axis=1, epsilon=1e-3)\n      self.assertEqual(normed.shape.as_list(), [10, 3, 5, 5])\n\n  def test_normalize_batch_in_training(self):\n    val = np.random.random((10, 3, 10, 10))\n    x = backend.variable(val)\n    reduction_axes = (0, 2, 3)\n\n    g_val = np.random.random((3,))\n    b_val = np.random.random((3,))\n    gamma = backend.variable(g_val)\n    beta = backend.variable(b_val)\n    normed, mean, var = backend.normalize_batch_in_training(\n        x, gamma, beta, reduction_axes, epsilon=1e-3)\n    self.assertEqual(normed.shape.as_list(), [10, 3, 10, 10])\n    self.assertEqual(mean.shape.as_list(), [\n        3,\n    ])\n    self.assertEqual(var.shape.as_list(), [\n        3,\n    ])\n\n    # case: gamma=None\n    gamma = None\n    normed, mean, var = backend.normalize_batch_in_training(\n        x, gamma, beta, reduction_axes, epsilon=1e-3)\n    self.assertEqual(normed.shape.as_list(), [10, 3, 10, 10])\n    self.assertEqual(mean.shape.as_list(), [\n        3,\n    ])\n    self.assertEqual(var.shape.as_list(), [\n        3,\n    ])\n\n    # case: beta=None\n    beta = None\n    normed, mean, var = backend.normalize_batch_in_training(\n        x, gamma, beta, reduction_axes, epsilon=1e-3)\n    self.assertEqual(normed.shape.as_list(), [10, 3, 10, 10])\n    self.assertEqual(mean.shape.as_list(), [\n        3,\n    ])\n    self.assertEqual(var.shape.as_list(), [\n        3,\n    ])\n\n  def test_dropout(self):\n    inputs = array_ops.ones((200, 200))\n    outputs = backend.dropout(inputs, 0.2)\n    outputs_val = backend.eval(outputs)\n    self.assertEqual(np.min(outputs_val), 0)\n    self.assertAllClose(np.count_nonzero(outputs_val), 32000, atol=1000)\n    # Test noise shape\n    outputs = backend.dropout(inputs, 0.2, noise_shape=(200, 1))\n    outputs_val = backend.eval(outputs)\n    self.assertAllClose(outputs_val[2, :], outputs_val[3, :], atol=1e-5)\n\n\nclass BackendCrossEntropyLossesTest(test.TestCase, parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_binary_crossentropy_with_sigmoid(self):\n    t = backend.constant([[0, 1, 0]])\n    logits = backend.constant([[8., 1., 1.]])\n    p = backend.sigmoid(logits)\n    p = array_ops.identity(array_ops.identity(p))\n    result = self.evaluate(backend.binary_crossentropy(t, p))\n    self.assertArrayNear(result[0], [8., 0.313, 1.313], 1e-3)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_categorical_crossentropy_loss(self):\n    t = backend.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n\n    p = backend.constant([[.9, .05, .05], [.05, .89, .06], [.05, .01, .94]])\n    result = backend.categorical_crossentropy(t, p)\n    self.assertArrayNear(self.evaluate(result), [.105, .116, .062], 1e-3)\n\n    p = backend.constant([[.9, .05, .05], [.05, .89, .01], [.05, .06, .94]])\n    result = backend.categorical_crossentropy(t, p, axis=0)\n    self.assertArrayNear(self.evaluate(result), [.105, .116, .062], 1e-3)\n\n    p = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n    result = backend.categorical_crossentropy(t, p, from_logits=True),\n    self.assertArrayNear(self.evaluate(result)[0], [.002, 0, .17], 1e-3)\n\n    p = backend.constant([[8., 0., 2.], [1., 9., 3.], [1., 1., 5.]])\n    result = backend.categorical_crossentropy(t, p, from_logits=True, axis=0),\n    self.assertArrayNear(self.evaluate(result)[0], [.002, 0, .17], 1e-3)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_categorical_crossentropy_loss_with_unknown_rank_tensor(self):\n    t = backend.placeholder()\n    p = backend.placeholder()\n    o = backend.categorical_crossentropy(t, p)\n\n    t_val = ops.convert_to_tensor_v2_with_dispatch([[1., 0., 0.], [0., 1., 0.],\n                                                    [0., 0., 1.]])\n    p_val = ops.convert_to_tensor_v2_with_dispatch([[.9, .05, .05],\n                                                    [.05, .89, .06],\n                                                    [.05, .01, .94]])\n    f = backend.function([t, p], o)\n\n    result = f([t_val, p_val])\n    self.assertArrayNear(result, [.105, .116, .062], 1e-3)\n\n    # With axis set\n    o = backend.categorical_crossentropy(t, p, axis=0)\n    f = backend.function([t, p], o)\n\n    result = f([t_val, p_val])\n    self.assertArrayNear(result, [.105, .065, .111], 1e-3)\n\n    # from logits\n    p_val = ops.convert_to_tensor_v2_with_dispatch([[8., 1., 1.], [0., 9., 1.],\n                                                    [2., 3., 5.]])\n    o = backend.categorical_crossentropy(t, p, from_logits=True)\n    f = backend.function([t, p], o)\n\n    result = f([t_val, p_val])\n    self.assertArrayNear(result, [.002, 0, .17], 1e-3)\n\n    # from logits and axis set\n    o = backend.categorical_crossentropy(t, p, from_logits=True, axis=0)\n    f = backend.function([t, p], o)\n\n    result = f([t_val, p_val])\n    self.assertArrayNear(result, [.002, .003, .036], 1e-3)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_categorical_crossentropy_with_softmax(self):\n    t = backend.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    logits = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n    p = backend.softmax(logits)\n    p = array_ops.identity(array_ops.identity(p))\n    result = self.evaluate(backend.categorical_crossentropy(t, p))\n    self.assertArrayNear(result, [0.002, 0.0005, 0.17], 1e-3)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_sparse_categorical_crossentropy_loss(self):\n    t = backend.constant([0, 1, 2])\n\n    p = backend.constant([[.9, .05, .05], [.05, .89, .06], [.05, .01, .94]])\n    result = backend.sparse_categorical_crossentropy(t, p)\n    self.assertArrayNear(self.evaluate(result), [.105, .116, .062], 1e-3)\n\n    p = backend.constant([[.9, .05, .05], [.05, .89, .01], [.05, .06, .94]])\n    result = backend.sparse_categorical_crossentropy(t, p, axis=0)\n    self.assertArrayNear(self.evaluate(result), [.105, .116, .062], 1e-3)\n\n    p = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n    result = backend.sparse_categorical_crossentropy(t, p, from_logits=True),\n    self.assertArrayNear(self.evaluate(result)[0], [.002, 0, .17], 1e-3)\n\n    p = backend.constant([[8., 0., 2.], [1., 9., 3.], [1., 1., 5.]])\n    result = backend.sparse_categorical_crossentropy(\n        t, p, from_logits=True, axis=0),\n    self.assertArrayNear(self.evaluate(result)[0], [.002, 0, .17], 1e-3)\n\n  @combinations.generate(combinations.combine(mode=['graph']))\n  def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(self):\n    # This test only runs in graph because the TF op layer is not supported yet\n    # for sparse ops.\n    t = backend.placeholder()\n    p = backend.placeholder()\n    o = backend.sparse_categorical_crossentropy(t, p)\n\n    t_val = ops.convert_to_tensor_v2_with_dispatch([0, 1, 2])\n    p_val = ops.convert_to_tensor_v2_with_dispatch([[.9, .05, .05],\n                                                    [.05, .89, .06],\n                                                    [.05, .01, .94]])\n    f = backend.function([t, p], o)\n\n    result = f([t_val, p_val])\n    self.assertArrayNear(result, [.105, .116, .062], 1e-3)\n\n    # With axis set\n    with self.assertRaisesRegex(\n        ValueError,\n        'Cannot compute sparse categorical crossentropy with `axis=0`'):\n      o = backend.sparse_categorical_crossentropy(t, p, axis=0)\n      f = backend.function([t, p], o)\n\n      _ = f([t_val, p_val])\n\n    # from logits\n    p_val = ops.convert_to_tensor_v2_with_dispatch([[8., 1., 1.], [0., 9., 1.],\n                                                    [2., 3., 5.]])\n    o = backend.sparse_categorical_crossentropy(t, p, from_logits=True)\n    f = backend.function([t, p], o)\n\n    result = f([t_val, p_val])\n    self.assertArrayNear(result, [.002, 0, .17], 1e-3)\n\n    # from logits and axis set\n    with self.assertRaisesRegex(\n        ValueError,\n        'Cannot compute sparse categorical crossentropy with `axis=0`'):\n      o = backend.sparse_categorical_crossentropy(\n          t, p, from_logits=True, axis=0)\n      f = backend.function([t, p], o)\n\n      _ = f([t_val, p_val])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_sparse_categorical_crossentropy_with_softmax(self):\n    t = backend.constant([0, 1, 2])\n    logits = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n    p = backend.softmax(logits)\n    p = array_ops.identity(array_ops.identity(p))\n    result = self.evaluate(backend.sparse_categorical_crossentropy(t, p))\n    self.assertArrayNear(result, [0.002, 0.0005, 0.17], 1e-3)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_binary_crossentropy_from_logits_no_warnings(self):\n    t = backend.constant([[0, 1, 0]])\n    logits = backend.constant([[8., 1., 1.]])\n    with warnings.catch_warnings(record=True) as w:\n      self.evaluate(backend.binary_crossentropy(t, logits, from_logits=True))\n      self.assertEmpty(w)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_binary_crossentropy_from_logits_with_sigmoid(self):\n    t = backend.constant([[0, 1, 0]])\n    logits = backend.constant([[8., 1., 1.]])\n    p = activations.sigmoid(logits)\n    with warnings.catch_warnings(record=True) as w:\n      self.evaluate(backend.binary_crossentropy(t, p, from_logits=True))\n      self.assertLen(w, 1)\n      self.assertIn('received `from_logits=True`', str(w[0].message))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_categorical_crossentropy_from_logits_with_softmax(self):\n    t = backend.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    logits = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n    p = activations.softmax(logits)\n    with warnings.catch_warnings(record=True) as w:\n      self.evaluate(backend.categorical_crossentropy(t, p, from_logits=True))\n      self.assertLen(w, 1)\n      self.assertIn('received `from_logits=True`', str(w[0].message))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_sparse_categorical_crossentropy_from_logits_with_softmax(self):\n    t = backend.constant([0, 1, 2])\n    logits = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n    p = activations.softmax(logits)\n    with warnings.catch_warnings(record=True) as w:\n      self.evaluate(\n          backend.sparse_categorical_crossentropy(t, p, from_logits=True))\n      self.assertLen(w, 1)\n      self.assertIn('received `from_logits=True`', str(w[0].message))\n\n\n@test_util.with_control_flow_v2\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass TestCTC(test.TestCase):\n\n  def test_ctc_decode(self):\n    depth = 6\n    seq_len_0 = 5\n    input_prob_matrix_0 = np.asarray(\n        [\n            [0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908],\n            [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517],\n            [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763],\n            [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655],\n            [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878],\n            # Random entry added in at time=5\n            [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]\n        ],\n        dtype=np.float32)\n\n    # len max_time_steps array of batch_size x depth matrices\n    inputs = (\n        [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)\n        ] +  # Pad to max_time_steps = 8\n        2 * [np.zeros((1, depth), dtype=np.float32)])\n\n    inputs = backend.variable(np.asarray(inputs).transpose((1, 0, 2)))\n\n    # batch_size length vector of sequence_lengths\n    input_length = backend.variable(np.array([seq_len_0], dtype=np.int32))\n    # batch_size length vector of negative log probabilities\n    log_prob_truth = np.array(\n        [\n            -3.5821197,  # output beam 0\n            -3.777835  # output beam 1\n        ],\n        np.float32)[np.newaxis, :]\n\n    decode_truth = [\n        np.array([1, 0, -1, -1, -1, -1, -1]),\n        np.array([0, 1, 0, -1, -1, -1, -1])\n    ]\n    beam_width = 2\n    top_paths = 2\n\n    decode_pred_tf, log_prob_pred_tf = backend.ctc_decode(\n        inputs,\n        input_length,\n        greedy=False,\n        beam_width=beam_width,\n        top_paths=top_paths)\n\n    self.assertEqual(len(decode_pred_tf), top_paths)\n    log_prob_pred = backend.eval(log_prob_pred_tf)\n    for i in range(top_paths):\n      self.assertTrue(\n          np.alltrue(decode_truth[i] == backend.eval(decode_pred_tf[i])))\n    self.assertAllClose(log_prob_truth, log_prob_pred)\n\n  def test_ctc_batch_cost(self):\n    with self.cached_session():\n      label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n      input_lens = np.expand_dims(np.asarray([5, 5]), 1)  # number of timesteps\n      loss_log_probs = [3.34211, 5.42262]\n\n      # dimensions are batch x time x categories\n      labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n      inputs = np.asarray(\n          [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n            [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n            [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n            [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n            [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]],\n           [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508],\n            [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549],\n            [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456],\n            [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345],\n            [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]],\n          dtype=np.float32)\n\n      labels = backend.variable(labels, dtype='int32')\n      inputs = backend.variable(inputs, dtype='float32')\n      input_lens = backend.variable(input_lens, dtype='int32')\n      label_lens = backend.variable(label_lens, dtype='int32')\n      res = backend.eval(\n          backend.ctc_batch_cost(labels, inputs, input_lens, label_lens))\n      self.assertAllClose(res[:, 0], loss_log_probs, atol=1e-05)\n\n      # test when batch_size = 1, that is, one sample only\n      ref = [3.34211]\n      input_lens = np.expand_dims(np.asarray([5]), 1)\n      label_lens = np.expand_dims(np.asarray([5]), 1)\n\n      labels = np.asarray([[0, 1, 2, 1, 0]])\n      inputs = np.asarray(\n          [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n            [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n            [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n            [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n            [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]\n          ],\n          dtype=np.float32)\n\n      k_labels = backend.variable(labels, dtype='int32')\n      k_inputs = backend.variable(inputs, dtype='float32')\n      k_input_lens = backend.variable(input_lens, dtype='int32')\n      k_label_lens = backend.variable(label_lens, dtype='int32')\n      res = backend.eval(\n          backend.ctc_batch_cost(k_labels, k_inputs, k_input_lens,\n                                 k_label_lens))\n      self.assertAllClose(res[:, 0], ref, atol=1e-05)\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass TestRandomOps(test.TestCase):\n\n  def test_random_normal(self):\n    np.random.seed(123)\n    x = backend.random_normal((500, 500))\n    val = backend.eval(x)\n    self.assertAllClose(np.mean(val), 0., atol=0.01)\n    self.assertAllClose(np.std(val), 1., atol=0.01)\n\n  def test_random_uniform(self):\n    np.random.seed(123)\n    x = backend.random_uniform((500, 500))\n    val = backend.eval(x)\n    self.assertAllClose(np.mean(val), 0.5, atol=0.01)\n    self.assertAllClose(np.max(val), 1., atol=0.01)\n    self.assertAllClose(np.min(val), 0., atol=0.01)\n\n  def test_random_binomial(self):\n    np.random.seed(123)\n    x = backend.random_binomial((500, 500), p=0.5)\n    self.assertAllClose(np.mean(backend.eval(x)), 0.5, atol=0.01)\n\n  def test_truncated_normal(self):\n    np.random.seed(123)\n    x = backend.truncated_normal((500, 500), mean=0.0, stddev=1.0)\n    x = backend.truncated_normal((1000, 1000), mean=0.0, stddev=1.0)\n    y = backend.eval(x)\n    self.assertAllClose(np.mean(y), 0., atol=0.01)\n    self.assertAllClose(np.std(y), 0.88, atol=0.01)\n    self.assertAllClose(np.max(y), 2., atol=0.01)\n    self.assertAllClose(np.min(y), -2., atol=0.01)\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass FunctionTest(test.TestCase):\n\n  def test_function_basics(self):\n    if context.executing_eagerly():\n      self.skipTest('eager backend.function does not support updates')\n    x1 = backend.placeholder(shape=(), dtype='float32')\n    x2 = backend.placeholder(shape=(), dtype='int32')\n    v = backend.variable(10.)\n\n    y1 = x1 + backend.cast(x2, 'float32') + v\n    y2 = x1 * backend.cast(x2, 'float32')\n\n    with ops.control_dependencies([y1]):\n      u = backend.update(v, x1)\n\n    f = backend.function([x1, x2], [y1, y2], updates=[u])\n    output_values = f([2, 3])\n    self.assertEqual(output_values, [15., 6.])\n    self.assertEqual(backend.eval(v), 2.)\n\n  def test_function_dict_outputs(self):\n    x_ph = backend.placeholder(shape=(), name='x')\n    y_ph = backend.placeholder(shape=(), name='y')\n    outputs = {'x*y': y_ph * x_ph, 'x*x': x_ph * x_ph}\n\n    f = backend.function(inputs=[x_ph, y_ph], outputs=outputs)\n    x, y = 2., 5.\n    results = f([x, y])\n\n    self.assertEqual(results['x*y'], 10.)\n    self.assertEqual(results['x*x'], 4)\n\n  def test_function_dict_inputs(self):\n    placeholders = {\n        'x': backend.placeholder(shape=()),\n        'y': backend.placeholder(shape=())\n    }\n    outputs = [placeholders['x'] * placeholders['y']]\n\n    f = backend.function(inputs=placeholders, outputs=outputs)\n    results = f({'x': 2., 'y': 3.})\n    self.assertEqual(results[0], 6.)\n\n  def test_function_single_input_output(self):\n    x_ph = backend.placeholder(shape=(), name='x')\n    output = x_ph * x_ph\n    f = backend.function(x_ph, output)\n    result = f(2.)\n    self.assertEqual(result, 4.)\n\n  def test_tuple_updates(self):\n    if context.executing_eagerly():\n      self.skipTest('eager backend.function does not support updates')\n\n    x_ph = backend.placeholder(ndim=2)\n    v = backend.variable(np.ones((4, 2)))\n    output = x_ph**2 + v\n    new_v = v + x_ph\n    f = backend.function(x_ph, output, updates=[(v, new_v)])\n    input_val = np.random.random((4, 2))\n    result = f(input_val)\n    self.assertAllClose(result, input_val**2 + 1)\n    self.assertAllClose(backend.get_value(v), np.ones((4, 2)) + input_val)\n\n\nclass BackendGraphTests(test.TestCase, parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph']))\n  def test_function_placeholder_with_default(self):\n    with backend.get_graph().as_default():\n      x1 = array_ops.placeholder_with_default(\n          np.array(2., dtype='float32'), shape=())\n      x2 = array_ops.placeholder_with_default(\n          np.array(3, dtype='int32'), shape=())\n    y1 = x1 + backend.cast(x2, 'float32')\n    y2 = x1 * backend.cast(x2, 'float32')\n    f = backend.function([x1, x2], [y1, y2])\n    output_values = f([4, 5])\n    self.assertEqual(output_values, [9., 20.])\n    output_values = f([None, None])\n    self.assertEqual(output_values, [5., 6.])\n\n  def test_function_tf_feed_symbols(self):\n    # Test Keras backend functions with TF tensor inputs.\n    with ops.Graph().as_default(), self.cached_session():\n      # Test feeding a resource variable to `function`.\n      x1 = backend.placeholder(shape=())\n      x2 = backend.placeholder(shape=())\n      lr = backend.learning_phase()  # Include a placeholder_with_default.\n\n      y1 = backend.variable(10.)\n      y2 = 3\n\n      f = backend.function(\n          inputs=[x1, x2, lr],\n          outputs=[x1 + 1, backend.in_train_phase(x2 + 2, x2 - 1)])\n      outs = f([y1, y2, None])  # Use default learning_phase value.\n      self.assertEqual(outs, [11., 2.])\n      outs = f([y1, y2, 1])  # Set learning phase value.\n      self.assertEqual(outs, [11., 5.])\n\n      # Test triggering a callable refresh by changing the input.\n      y3 = backend.constant(20.)  # Test with tensor\n      outs = f([y3, y2, None])\n      self.assertEqual(outs, [21., 2.])\n\n      y4 = 4  # Test with non-symbol\n      outs = f([y4, y2, None])\n      self.assertEqual(outs, [5., 2.])\n\n      # Test with a different dtype\n      y5 = backend.constant(10., dtype='float64')\n      outs = f([y5, y2, None])\n      self.assertEqual(outs, [11., 2.])\n\n  def test_function_tf_fetches(self):\n    # Additional operations can be passed to tf.compat.v1.Session().run() via\n    # its `fetches` arguments. In contrast to `updates` argument of\n    # backend.function() these do not have control dependency on `outputs`\n    # so they can run in parallel. Also they should not contribute to output of\n    # backend.function().\n    with ops.Graph().as_default(), self.cached_session():\n      x = backend.variable(0.)\n      y = backend.variable(0.)\n      x_placeholder = backend.placeholder(shape=())\n      y_placeholder = backend.placeholder(shape=())\n\n      f = backend.function(\n          inputs=[x_placeholder, y_placeholder],\n          outputs=[x_placeholder + y_placeholder],\n          updates=[(x, x_placeholder + 1.)],\n          fetches=[backend.update(y, 5.)])\n      output = f([10., 20.])\n      self.assertEqual(output, [30.])\n      self.assertEqual(backend.get_session().run(fetches=[x, y]), [11., 5.])\n\n  def test_function_tf_feed_dict(self):\n    # Additional substitutions can be passed to `tf.compat.v1.Session().run()`\n    # via its `feed_dict` arguments. Note that the feed_dict is passed once in\n    # the constructor but we can modify the values in the dictionary. Through\n    # this feed_dict we can provide additional substitutions besides Keras\n    # inputs.\n    with ops.Graph().as_default(), self.cached_session():\n      x = backend.variable(0.)\n      y = backend.variable(0.)\n      x_placeholder = backend.placeholder(shape=())\n      y_placeholder = backend.placeholder(shape=())\n\n      feed_dict = {y_placeholder: 3.}\n      fetches = [backend.update(y, y_placeholder * 10.)]\n      f = backend.function(\n          inputs=[x_placeholder],\n          outputs=[x_placeholder + 1.],\n          updates=[(x, x_placeholder + 10.)],\n          feed_dict=feed_dict,\n          fetches=fetches)\n      output = f([10.])\n      self.assertEqual(output, [11.])\n      self.assertEqual(backend.get_session().run(fetches=[x, y]), [20., 30.])\n\n      # updated value in feed_dict will be modified within the K.function()\n      feed_dict[y_placeholder] = 4.\n      output = f([20.])\n      self.assertEqual(output, [21.])\n      self.assertEqual(backend.get_session().run(fetches=[x, y]), [30., 40.])\n\n  def test_function_tf_run_options_with_run_metadata(self):\n    with ops.Graph().as_default(), self.cached_session():\n      x_placeholder = backend.placeholder(shape=())\n      y_placeholder = backend.placeholder(shape=())\n\n      run_options = config_pb2.RunOptions(output_partition_graphs=True)\n      run_metadata = config_pb2.RunMetadata()\n      # enable run_options.\n      f = backend.function(\n          inputs=[x_placeholder, y_placeholder],\n          outputs=[x_placeholder + y_placeholder],\n          options=run_options,\n          run_metadata=run_metadata)\n      output = f([10., 20.])\n      self.assertEqual(output, [30.])\n      self.assertNotEmpty(run_metadata.partition_graphs)\n      # disable run_options.\n      f1 = backend.function(\n          inputs=[x_placeholder, y_placeholder],\n          outputs=[x_placeholder + y_placeholder],\n          run_metadata=run_metadata)\n      output1 = f1([10., 20.])\n      self.assertEqual(output1, [30.])\n      self.assertEmpty(run_metadata.partition_graphs)\n\n  def test_function_fetch_callbacks(self):\n\n    class CallbackStub(object):\n\n      def __init__(self):\n        self.times_called = 0\n        self.callback_result = 0\n\n      def _fetch_callback(self, result):\n        self.times_called += 1\n        self.callback_result = result\n\n    with ops.Graph().as_default(), self.cached_session():\n      callback = CallbackStub()\n      x_placeholder = backend.placeholder(shape=())\n      y_placeholder = backend.placeholder(shape=())\n\n      callback_op = x_placeholder * y_placeholder\n\n      f = backend.function(\n          inputs=[x_placeholder, y_placeholder],\n          outputs=[x_placeholder + y_placeholder])\n      f.fetches.append(callback_op)\n      f.fetch_callbacks[callback_op] = callback._fetch_callback\n\n      _ = f([10., 20.])\n\n      self.assertEqual(callback.times_called, 1)\n      self.assertEqual(callback.callback_result, 200)\n\n  def test_get_session_different_graphs(self):\n    with ops.Graph().as_default():\n      x = backend.constant(1)\n      session = backend.get_session()\n      self.assertIs(session, backend.get_session((x,)))\n      self.assertIs(session, backend.get_session())\n    with ops.Graph().as_default():\n      self.assertIs(session, backend.get_session((x,)))\n      self.assertIsNot(session, backend.get_session())\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass ControlOpsTests(test.TestCase):\n\n  def test_function_switch_basics(self):\n    x = array_ops.constant(2.0)\n    y = array_ops.constant(3.0)\n\n    def xpowy():\n      return backend.pow(x, y)\n\n    def ypowx():\n      return backend.pow(y, x)\n\n    tensor = backend.switch(backend.less(x, y), xpowy, ypowx)\n    self.assertEqual(backend.eval(tensor), [8.0])\n\n    tensor = backend.switch(backend.greater(x, y), xpowy, ypowx)\n    self.assertEqual(backend.eval(tensor), [9.0])\n\n  def test_unequal_rank(self):\n    x = ops.convert_to_tensor_v2_with_dispatch(\n        np.array([[1, 2, 3], [4, 5, 6]]), dtype='float32')\n    y = ops.convert_to_tensor_v2_with_dispatch(\n        np.array([1, 2, 3]), dtype='float32')\n\n    def true_func():\n      return x\n\n    def false_func():\n      return y\n\n    with self.assertRaisesRegex(ValueError,\n                                'Rank of `condition` should be less than'):\n      backend.switch(backend.equal(x, x), false_func, true_func)\n\n\nclass ContextValueCacheTest(test.TestCase):\n\n  def test_cache(self):\n    cache = backend.ContextValueCache(list)\n    graph1 = ops.Graph()\n    graph2 = ops.Graph()\n\n    cache[graph1].append(1)\n    with graph1.as_default():\n      cache[None].append(2)\n\n    with graph2.as_default():\n      cache[None].append(3)\n    cache[graph2].append(4)\n\n    self.assertAllEqual(cache[graph1], [1, 2])\n    self.assertAllEqual(cache[graph2], [3, 4])\n\n    with context.eager_mode():\n      cache[None].append(5)\n      cache[None].append(6)\n      self.assertAllEqual(cache[None], [5, 6])\n\n    self.assertLen(cache, 3)\n\n    del graph1\n    gc.collect()\n    self.assertLen(cache, 2)\n\n  def test_cache_in_parent_graph(self):\n    cache = backend.ContextValueCache(int)\n    cache.setdefault(None, backend.constant(5))\n\n    with ops.Graph().as_default() as g:\n      # g is not a child graph of the default test context, so the recursive\n      # lookup will create a new default value.\n      self.assertAllEqual(cache[g], 0)\n\n    @def_function.function\n    def fn():\n      # The function graph is a child of the default test context, so\n      # __getitem__ will return the previously saved value.\n      return cache[ops.get_default_graph()]\n\n    self.assertEqual(self.evaluate(fn()), 5)\n\n\nif __name__ == '__main__':\n  test.main()\n", "framework": "tensorflow"}
{"repo_name": "gautam1858/tensorflow", "file_path": "tensorflow/python/keras/engine/functional_test.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#,============================================================================\n\"\"\"Tests for layer graphs construction & handling.\"\"\"\n\nimport warnings\n\nimport numpy as np\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.keras import backend\nfrom tensorflow.python.keras import combinations\nfrom tensorflow.python.keras import initializers\nfrom tensorflow.python.keras import keras_parameterized\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import losses\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras import testing_utils\nfrom tensorflow.python.keras.engine import base_layer\nfrom tensorflow.python.keras.engine import functional\nfrom tensorflow.python.keras.engine import input_layer as input_layer_lib\nfrom tensorflow.python.keras.engine import sequential\nfrom tensorflow.python.keras.engine import training as training_lib\nfrom tensorflow.python.keras.utils import layer_utils\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.training.tracking.util import Checkpoint\n\n\nclass NetworkConstructionTest(keras_parameterized.TestCase):\n\n  def test_default_model_name(self):\n    inputs = input_layer_lib.Input(shape=(1,))\n    outputs = layers.Dense(1, activation='relu')(inputs)\n    model = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model.name, 'model')\n\n    model_2 = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model_2.name, 'model_1')\n\n    model_3 = training_lib.Model(inputs=inputs, outputs=outputs)\n    self.assertEqual(model_3.name, 'model_2')\n\n  def test_get_updates(self):\n\n    class MyLayer(layers.Layer):\n\n      def build(self, input_shape):\n        self.a = self.add_variable('a',\n                                   (1, 1),\n                                   'float32',\n                                   trainable=False)\n        self.b = self.add_variable('b',\n                                   (1, 1),\n                                   'float32',\n                                   trainable=False)\n        self.add_update(state_ops.assign_add(self.a, [[1.]],\n                                             name='unconditional_update'))\n        self.built = True\n\n      def call(self, inputs):\n        self.add_update(state_ops.assign_add(self.b, inputs,\n                                             name='conditional_update'),\n                        inputs=True)\n        return inputs + 1\n\n    with ops.Graph().as_default():\n      x1 = input_layer_lib.Input(shape=(1,))\n      layer = MyLayer()\n      _ = layer(x1)\n\n      self.assertEqual(len(layer.updates), 2)\n\n      x2 = input_layer_lib.Input(shape=(1,))\n      y2 = layer(x2)\n\n      self.assertEqual(len(layer.updates), 3)\n\n      network = functional.Functional(x2, y2)\n      self.assertEqual(len(network.updates), 3)\n\n      x3 = input_layer_lib.Input(shape=(1,))\n      _ = layer(x3)\n      self.assertEqual(len(network.updates), 4)\n\n      x4 = input_layer_lib.Input(shape=(1,))\n      _ = network(x4)\n      self.assertEqual(len(network.updates), 5)\n\n      network.add_update(state_ops.assign_add(layer.a, [[1]]))\n      self.assertEqual(len(network.updates), 6)\n\n      network.add_update(state_ops.assign_add(layer.b, x4), inputs=True)\n      self.assertEqual(len(network.updates), 7)\n\n  def test_get_layer(self):\n    # create a simple network\n    x = input_layer_lib.Input(shape=(32,))\n    dense_a = layers.Dense(4, name='dense_a')\n    dense_b = layers.Dense(2, name='dense_b')\n    y = dense_b(dense_a(x))\n    network = functional.Functional(x, y, name='dense_network')\n\n    # test various get_layer by index\n    self.assertEqual(network.get_layer(index=1), dense_a)\n\n    # test invalid get_layer by index\n    with self.assertRaisesRegex(\n        ValueError, 'Was asked to retrieve layer at index ' + str(3) +\n        ' but model only has ' + str(len(network.layers)) + ' layers.'):\n      network.get_layer(index=3)\n\n    # test that only one between name and index is requested\n    with self.assertRaisesRegex(ValueError,\n                                'Provide only a layer name or a layer index'):\n      network.get_layer(index=1, name='dense_b')\n\n    # test that a name or an index must be provided\n    with self.assertRaisesRegex(ValueError,\n                                'Provide either a layer name or layer index.'):\n      network.get_layer()\n\n    # test various get_layer by name\n    self.assertEqual(network.get_layer(name='dense_a'), dense_a)\n\n    # test invalid get_layer by name\n    with self.assertRaisesRegex(ValueError, 'No such layer: dense_c.'):\n      network.get_layer(name='dense_c')\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributes(self):\n    # test layer attributes / methods related to cross-layer connectivity.\n    a = input_layer_lib.Input(shape=(32,), name='input_a')\n    b = input_layer_lib.Input(shape=(32,), name='input_b')\n\n    # test input, output, input_shape, output_shape\n    test_layer = layers.Dense(16, name='test_layer')\n    a_test = test_layer(a)\n    self.assertIs(test_layer.input, a)\n    self.assertIs(test_layer.output, a_test)\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, (None, 16))\n\n    # test `get_*_at` methods\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    self.assertIs(dense.get_input_at(0), a)\n    self.assertIs(dense.get_input_at(1), b)\n    self.assertIs(dense.get_output_at(0), a_2)\n    self.assertIs(dense.get_output_at(1), b_2)\n    self.assertEqual(dense.get_input_shape_at(0), (None, 32))\n    self.assertEqual(dense.get_input_shape_at(1), (None, 32))\n    self.assertEqual(dense.get_output_shape_at(0), (None, 16))\n    self.assertEqual(dense.get_output_shape_at(1), (None, 16))\n\n    # Test invalid value for attribute retrieval.\n    with self.assertRaises(ValueError):\n      dense.get_input_at(2)\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.input\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.output\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.output_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      _ = new_dense.input_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      a = input_layer_lib.Input(shape=(3, 32))\n      a = input_layer_lib.Input(shape=(5, 32))\n      a_2 = dense(a)\n      b_2 = dense(b)\n      _ = new_dense.input_shape\n    with self.assertRaises(AttributeError):\n      new_dense = layers.Dense(16)\n      a = input_layer_lib.Input(shape=(3, 32))\n      a = input_layer_lib.Input(shape=(5, 32))\n      a_2 = dense(a)\n      b_2 = dense(b)\n      _ = new_dense.output_shape\n\n  def _assertAllIs(self, a, b):\n    self.assertTrue(all(x is y for x, y in zip(a, b)))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributesMultiOutputLayer(self):\n\n    class PowersLayer(layers.Layer):\n\n      def call(self, inputs):\n        return [inputs**2, inputs**3]\n\n    x = input_layer_lib.Input(shape=(32,))\n    test_layer = PowersLayer()\n    p1, p2 = test_layer(x)  # pylint: disable=not-callable\n\n    self.assertIs(test_layer.input, x)\n    self._assertAllIs(test_layer.output, [p1, p2])\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, [(None, 32), (None, 32)])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTopologicalAttributesMultiInputLayer(self):\n\n    class AddLayer(layers.Layer):\n\n      def call(self, inputs):\n        assert len(inputs) == 2\n        return inputs[0] + inputs[1]\n\n    a = input_layer_lib.Input(shape=(32,))\n    b = input_layer_lib.Input(shape=(32,))\n    test_layer = AddLayer()\n    y = test_layer([a, b])  # pylint: disable=not-callable\n\n    self._assertAllIs(test_layer.input, [a, b])\n    self.assertIs(test_layer.output, y)\n    self.assertEqual(test_layer.input_shape, [(None, 32), (None, 32)])\n    self.assertEqual(test_layer.output_shape, (None, 32))\n\n  def testBasicNetwork(self):\n    with ops.Graph().as_default():\n      # minimum viable network\n      x = input_layer_lib.Input(shape=(32,))\n      dense = layers.Dense(2)\n      y = dense(x)\n      network = functional.Functional(x, y, name='dense_network')\n\n      # test basic attributes\n      self.assertEqual(network.name, 'dense_network')\n      self.assertEqual(len(network.layers), 2)  # InputLayer + Dense\n      self.assertEqual(network.layers[1], dense)\n      self._assertAllIs(network.weights, dense.weights)\n      self._assertAllIs(network.trainable_weights, dense.trainable_weights)\n      self._assertAllIs(network.non_trainable_weights,\n                        dense.non_trainable_weights)\n\n      # test callability on Input\n      x_2 = input_layer_lib.Input(shape=(32,))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 2])\n\n      # test callability on regular tensor\n      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 2])\n\n      # test network `trainable` attribute\n      network.trainable = False\n      self._assertAllIs(network.weights, dense.weights)\n      self.assertEqual(network.trainable_weights, [])\n      self._assertAllIs(network.non_trainable_weights,\n                        dense.trainable_weights + dense.non_trainable_weights)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_trainable_weights(self):\n    a = layers.Input(shape=(2,))\n    b = layers.Dense(1)(a)\n    model = training_lib.Model(a, b)\n\n    weights = model.weights\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    model.trainable = True\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.layers[1].trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    # sequential model\n    model = sequential.Sequential()\n    model.add(layers.Dense(1, input_dim=2))\n    weights = model.weights\n\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n    model.trainable = True\n    self._assertAllIs(model.trainable_weights, weights)\n    self.assertListEqual(model.non_trainable_weights, [])\n\n    model.layers[0].trainable = False\n    self.assertListEqual(model.trainable_weights, [])\n    self._assertAllIs(model.non_trainable_weights, weights)\n\n  def test_layer_call_arguments(self):\n    with ops.Graph().as_default():\n      # Test the ability to pass and serialize arguments to `call`.\n      inp = layers.Input(shape=(2,))\n      x = layers.Dense(3)(inp)\n      x = layers.Dropout(0.5)(x, training=True)\n      model = training_lib.Model(inp, x)\n      # Would be `dropout/cond/Merge` by default\n      self.assertIn('dropout', model.output.op.name)\n\n      # Test that argument is kept when applying the model\n      inp2 = layers.Input(shape=(2,))\n      out2 = model(inp2)\n      self.assertIn('dropout', out2.op.name)\n\n      # Test that argument is kept after loading a model\n      config = model.get_config()\n      model = training_lib.Model.from_config(config)\n      self.assertIn('dropout', model.output.op.name)\n\n  def test_node_construction(self):\n    # test basics\n    a = layers.Input(shape=(32,), name='input_a')\n    b = layers.Input(shape=(32,), name='input_b')\n\n    with self.assertRaises(ValueError):\n      _ = layers.Input(shape=(32,), batch_shape=(10, 32))\n    with self.assertRaises(ValueError):\n      _ = layers.Input(shape=(32,), unknown_kwarg=None)\n\n    self.assertListEqual(a.shape.as_list(), [None, 32])\n    a_layer, a_node_index, a_tensor_index = a._keras_history\n    b_layer, _, _ = b._keras_history\n    self.assertEqual(len(a_layer._inbound_nodes), 1)\n    self.assertEqual(a_tensor_index, 0)\n    node = a_layer._inbound_nodes[a_node_index]\n    self.assertEqual(node.outbound_layer, a_layer)\n\n    self.assertListEqual(node.inbound_layers, [])\n    self.assertListEqual(node.input_tensors, [a])\n    self.assertListEqual(node.input_shapes, [(None, 32)])\n    self.assertListEqual(node.output_tensors, [a])\n    self.assertListEqual(node.output_shapes, [(None, 32)])\n\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    self.assertEqual(len(dense._inbound_nodes), 2)\n    self.assertEqual(len(dense._outbound_nodes), 0)\n    self.assertEqual(dense._inbound_nodes[0].inbound_layers, a_layer)\n    self.assertEqual(dense._inbound_nodes[0].outbound_layer, dense)\n    self.assertEqual(dense._inbound_nodes[1].inbound_layers, b_layer)\n    self.assertEqual(dense._inbound_nodes[1].outbound_layer, dense)\n    self.assertIs(dense._inbound_nodes[0].input_tensors, a)\n    self.assertIs(dense._inbound_nodes[1].input_tensors, b)\n\n    # test layer properties\n    test_layer = layers.Dense(16, name='test_layer')\n    a_test = test_layer(a)\n    self.assertListEqual(test_layer.kernel.shape.as_list(), [32, 16])\n    self.assertIs(test_layer.input, a)\n    self.assertIs(test_layer.output, a_test)\n    self.assertEqual(test_layer.input_shape, (None, 32))\n    self.assertEqual(test_layer.output_shape, (None, 16))\n\n    self.assertIs(dense.get_input_at(0), a)\n    self.assertIs(dense.get_input_at(1), b)\n    self.assertIs(dense.get_output_at(0), a_2)\n    self.assertIs(dense.get_output_at(1), b_2)\n    self.assertEqual(dense.get_input_shape_at(0), (None, 32))\n    self.assertEqual(dense.get_input_shape_at(1), (None, 32))\n    self.assertEqual(dense.get_output_shape_at(0), (None, 16))\n    self.assertEqual(dense.get_output_shape_at(1), (None, 16))\n    self.assertEqual(dense.get_input_mask_at(0), None)\n    self.assertEqual(dense.get_input_mask_at(1), None)\n    self.assertEqual(dense.get_output_mask_at(0), None)\n    self.assertEqual(dense.get_output_mask_at(1), None)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_multi_input_layer(self):\n    with self.cached_session():\n      # test multi-input layer\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      self.assertListEqual(merged.shape.as_list(), [None, 16 * 2])\n      merge_layer, merge_node_index, merge_tensor_index = merged._keras_history\n\n      self.assertEqual(merge_node_index, 0)\n      self.assertEqual(merge_tensor_index, 0)\n\n      self.assertEqual(len(merge_layer._inbound_nodes), 1)\n      self.assertEqual(len(merge_layer._outbound_nodes), 0)\n\n      self.assertEqual(len(merge_layer._inbound_nodes[0].input_tensors), 2)\n      self.assertEqual(len(merge_layer._inbound_nodes[0].inbound_layers), 2)\n\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n      self.assertEqual(len(model.layers), 6)\n      output_shapes = model.compute_output_shape([(None, 32), (None, 32)])\n      self.assertListEqual(output_shapes[0].as_list(), [None, 64])\n      self.assertListEqual(output_shapes[1].as_list(), [None, 5])\n      self.assertListEqual(\n          model.compute_mask([a, b], [None, None]), [None, None])\n\n      # we don't check names of first 2 layers (inputs) because\n      # ordering of same-level layers is not fixed\n      self.assertListEqual([l.name for l in model.layers][2:],\n                           ['dense_1', 'merge', 'dense_2', 'dense_3'])\n      self.assertListEqual([l.name for l in model._input_layers],\n                           ['input_a', 'input_b'])\n      self.assertListEqual([l.name for l in model._output_layers],\n                           ['dense_2', 'dense_3'])\n\n      # actually run model\n      fn = backend.function(model.inputs, model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])\n\n      # test get_source_inputs\n      self._assertAllIs(layer_utils.get_source_inputs(c), [a, b])\n\n      # serialization / deserialization\n      json_config = model.to_json()\n      recreated_model = models.model_from_json(json_config)\n      recreated_model.compile('rmsprop', 'mse')\n\n      self.assertListEqual([l.name for l in recreated_model.layers][2:],\n                           ['dense_1', 'merge', 'dense_2', 'dense_3'])\n      self.assertListEqual([l.name for l in recreated_model._input_layers],\n                           ['input_a', 'input_b'])\n      self.assertListEqual([l.name for l in recreated_model._output_layers],\n                           ['dense_2', 'dense_3'])\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 64), (10, 5)])\n\n  def test_multi_output_layer_output_names(self):\n    inp = layers.Input(name='inp', shape=(None,), dtype=dtypes.float32)\n\n    class _MultiOutput(layers.Layer):\n\n      def call(self, x):\n        return x + 1., x + 2.\n\n    out = _MultiOutput(name='out')(inp)\n    model = training_lib.Model(inp, out)\n    self.assertEqual(['out', 'out_1'], model.output_names)\n    self.assertAllClose([2., 3.], model(1.))\n\n  def test_recursion(self):\n    with ops.Graph().as_default(), self.cached_session():\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      e = layers.Input(shape=(32,), name='input_e')\n      f = layers.Input(shape=(32,), name='input_f')\n      self.assertEqual(len(model.inputs), 2)\n      g, h = model([e, f])\n      self.assertEqual(len(model.inputs), 2)\n      self.assertEqual(g.name, 'model/dense_2/BiasAdd:0')\n\n      self.assertListEqual(g.shape.as_list(), c.shape.as_list())\n      self.assertListEqual(h.shape.as_list(), d.shape.as_list())\n\n      # test separate manipulation of different layer outputs\n      i = layers.Dense(7, name='dense_4')(h)\n\n      final_model = training_lib.Model(\n          inputs=[e, f], outputs=[i, g], name='final')\n      self.assertEqual(len(final_model.inputs), 2)\n      self.assertEqual(len(final_model.outputs), 2)\n      self.assertEqual(len(final_model.layers), 4)\n\n      # we don't check names of first 2 layers (inputs) because\n      # ordering of same-level layers is not fixed\n      self.assertListEqual([layer.name for layer in final_model.layers][2:],\n                           ['model', 'dense_4'])\n      self.assertListEqual(\n          model.compute_mask([e, f], [None, None]), [None, None])\n      self.assertListEqual(\n          final_model.compute_output_shape([(10, 32), (10, 32)]), [(10, 7),\n                                                                   (10, 64)])\n\n      # run recursive model\n      fn = backend.function(final_model.inputs, final_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])\n\n      # test serialization\n      model_config = final_model.get_config()\n      recreated_model = models.Model.from_config(model_config)\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      input_a_np = np.random.random((10, 32))\n      input_b_np = np.random.random((10, 32))\n      fn_outputs = fn([input_a_np, input_b_np])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_multi_input_multi_output_recursion(self):\n    with self.cached_session():\n      # test multi-input multi-output\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      j = layers.Input(shape=(32,), name='input_j')\n      k = layers.Input(shape=(32,), name='input_k')\n      _, n = model([j, k])\n\n      o = layers.Input(shape=(32,), name='input_o')\n      p = layers.Input(shape=(32,), name='input_p')\n      q, _ = model([o, p])\n\n      self.assertListEqual(n.shape.as_list(), [None, 5])\n      self.assertListEqual(q.shape.as_list(), [None, 64])\n      s = layers.concatenate([n, q], name='merge_nq')\n      self.assertListEqual(s.shape.as_list(), [None, 64 + 5])\n\n      # test with single output as 1-elem list\n      multi_io_model = training_lib.Model([j, k, o, p], [s])\n\n      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      # test with single output as tensor\n      multi_io_model = training_lib.Model([j, k, o, p], s)\n\n      fn = backend.function(multi_io_model.inputs, multi_io_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      # note that the output of the function will still be a 1-elem list\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      # test serialization\n      model_config = multi_io_model.get_config()\n      recreated_model = models.Model.from_config(model_config)\n\n      fn = backend.function(recreated_model.inputs, recreated_model.outputs)\n      fn_outputs = fn([\n          np.random.random((10, 32)), np.random.random((10, 32)),\n          np.random.random((10, 32)), np.random.random((10, 32))\n      ])\n      # note that the output of the function will still be a 1-elem list\n      self.assertListEqual([x.shape for x in fn_outputs], [(10, 69)])\n\n      config = model.get_config()\n      models.Model.from_config(config)\n\n      model.summary()\n      json_str = model.to_json()\n      models.model_from_json(json_str)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_invalid_graphs(self):\n    a = layers.Input(shape=(32,), name='input_a')\n    b = layers.Input(shape=(32,), name='input_b')\n\n    dense = layers.Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n    merged = layers.concatenate([a_2, b_2], name='merge')\n    c = layers.Dense(64, name='dense_2')(merged)\n    d = layers.Dense(5, name='dense_3')(c)\n\n    model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n    # input is not an Input tensor\n    j = layers.Input(shape=(32,), name='input_j')\n    j = layers.Dense(32)(j)\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k], [m, n])\n\n    # disconnected graph\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j], [m, n])\n\n    # redundant outputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    training_lib.Model([j, k], [m, n, n])\n\n    # redundant inputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k, j], [m, n])\n\n    # i have not idea what I'm doing: garbage as inputs/outputs\n    j = layers.Input(shape=(32,), name='input_j')\n    k = layers.Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with self.assertRaises(Exception):\n      training_lib.Model([j, k], [m, n, 0])\n\n  def test_raw_tf_compatibility(self):\n    with ops.Graph().as_default():\n      # test calling layers/models on TF tensors\n      a = layers.Input(shape=(32,), name='input_a')\n      b = layers.Input(shape=(32,), name='input_b')\n\n      dense = layers.Dense(16, name='dense_1')\n      a_2 = dense(a)\n      b_2 = dense(b)\n      merged = layers.concatenate([a_2, b_2], name='merge')\n      c = layers.Dense(64, name='dense_2')(merged)\n      d = layers.Dense(5, name='dense_3')(c)\n\n      model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n\n      j = layers.Input(shape=(32,), name='input_j')\n      k = layers.Input(shape=(32,), name='input_k')\n      self.assertEqual(len(model.inputs), 2)\n      m, n = model([j, k])\n      self.assertEqual(len(model.inputs), 2)\n      tf_model = training_lib.Model([j, k], [m, n])\n\n      j_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))\n      k_tf = array_ops.placeholder(dtype=dtypes.float32, shape=(None, 32))\n      m_tf, n_tf = tf_model([j_tf, k_tf])\n      self.assertListEqual(m_tf.shape.as_list(), [None, 64])\n      self.assertListEqual(n_tf.shape.as_list(), [None, 5])\n\n      # test merge\n      layers.concatenate([j_tf, k_tf], axis=1)\n      layers.add([j_tf, k_tf])\n\n      # test tensor input\n      x = array_ops.placeholder(shape=(None, 2), dtype=dtypes.float32)\n      layers.InputLayer(input_tensor=x)\n\n      x = layers.Input(tensor=x)\n      layers.Dense(2)(x)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_basic_masking(self):\n    a = layers.Input(shape=(10, 32), name='input_a')\n    b = layers.Masking()(a)\n    model = training_lib.Model(a, b)\n    self.assertEqual(model.output_mask.shape.as_list(), [None, 10])\n\n  def testMaskingSingleInput(self):\n\n    class MaskedLayer(layers.Layer):\n\n      def call(self, inputs, mask=None):\n        if mask is not None:\n          return inputs * mask\n        return inputs\n\n      def compute_mask(self, inputs, mask=None):\n        return array_ops.ones_like(inputs)\n\n    if context.executing_eagerly():\n      a = constant_op.constant([2] * 32)\n      mask = constant_op.constant([0, 1] * 16)\n      a._keras_mask = mask\n      b = MaskedLayer().apply(a)\n      self.assertTrue(hasattr(b, '_keras_mask'))\n      self.assertAllEqual(\n          self.evaluate(array_ops.ones_like(mask)),\n          self.evaluate(getattr(b, '_keras_mask')))\n      self.assertAllEqual(self.evaluate(a * mask), self.evaluate(b))\n    else:\n      x = input_layer_lib.Input(shape=(32,))\n      y = MaskedLayer()(x)  # pylint: disable=not-callable\n      network = functional.Functional(x, y)\n\n      # test callability on Input\n      x_2 = input_layer_lib.Input(shape=(32,))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 32])\n\n      # test callability on regular tensor\n      x_2 = array_ops.placeholder(dtype='float32', shape=(None, 32))\n      y_2 = network(x_2)\n      self.assertEqual(y_2.shape.as_list(), [None, 32])\n\n  def test_activity_regularization_with_model_composition(self):\n\n    def reg(x):\n      return math_ops.reduce_sum(x)\n\n    net_a_input = input_layer_lib.Input((2,))\n    net_a = net_a_input\n    net_a = layers.Dense(\n        2, kernel_initializer='ones', use_bias=False, activity_regularizer=reg)(\n            net_a)\n    model_a = training_lib.Model([net_a_input], [net_a])\n\n    net_b_input = input_layer_lib.Input((2,))\n    net_b = model_a(net_b_input)\n    model_b = training_lib.Model([net_b_input], [net_b])\n\n    model_b.compile(optimizer='sgd', loss=None)\n    x = np.ones((1, 2))\n    loss = model_b.evaluate(x)\n    self.assertEqual(loss, 4.)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_layer_sharing_at_heterogenous_depth(self):\n    x_val = np.random.random((10, 5))\n\n    x = input_layer_lib.Input(shape=(5,))\n    a = layers.Dense(5, name='A')\n    b = layers.Dense(5, name='B')\n    output = a(b(a(b(x))))\n    m = training_lib.Model(x, output)\n    m.run_eagerly = testing_utils.should_run_eagerly()\n\n    output_val = m.predict(x_val)\n\n    config = m.get_config()\n    weights = m.get_weights()\n\n    m2 = models.Model.from_config(config)\n    m2.set_weights(weights)\n\n    output_val_2 = m2.predict(x_val)\n    self.assertAllClose(output_val, output_val_2, atol=1e-6)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_layer_sharing_at_heterogenous_depth_with_concat(self):\n    input_shape = (16, 9, 3)\n    input_layer = input_layer_lib.Input(shape=input_shape)\n\n    a = layers.Dense(3, name='dense_A')\n    b = layers.Dense(3, name='dense_B')\n    c = layers.Dense(3, name='dense_C')\n\n    x1 = b(a(input_layer))\n    x2 = a(c(input_layer))\n    output = layers.concatenate([x1, x2])\n\n    m = training_lib.Model(inputs=input_layer, outputs=output)\n    m.run_eagerly = testing_utils.should_run_eagerly()\n\n    x_val = np.random.random((10, 16, 9, 3))\n    output_val = m.predict(x_val)\n\n    config = m.get_config()\n    weights = m.get_weights()\n\n    m2 = models.Model.from_config(config)\n    m2.set_weights(weights)\n\n    output_val_2 = m2.predict(x_val)\n    self.assertAllClose(output_val, output_val_2, atol=1e-6)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_explicit_training_argument(self):\n    a = layers.Input(shape=(2,))\n    b = layers.Dropout(0.5)(a)\n    base_model = training_lib.Model(a, b)\n\n    a = layers.Input(shape=(2,))\n    b = base_model(a, training=False)\n    model = training_lib.Model(a, b)\n\n    x = np.ones((100, 2))\n    y = np.ones((100, 2))\n    model.compile(\n        optimizer='sgd',\n        loss='mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    loss = model.train_on_batch(x, y)\n    self.assertEqual(loss, 0)  # In inference mode, output is equal to input.\n\n    a = layers.Input(shape=(2,))\n    b = base_model(a, training=True)\n    model = training_lib.Model(a, b)\n    preds = model.predict(x)\n    self.assertEqual(np.min(preds), 0.)  # At least one unit was dropped.\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_mask_derived_from_keras_layer(self):\n    inputs = input_layer_lib.Input((5, 10))\n    mask = input_layer_lib.Input((5,))\n    outputs = layers.RNN(layers.LSTMCell(100))(inputs, mask=mask)\n    model = training_lib.Model([inputs, mask], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # All data is masked, returned values are 0's.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.ones((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # Data is not masked, returned values are random.\n    self.assertGreater(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(model.get_config())\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # All data is masked, returned values are 0's.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    history = model.fit(\n        x=[np.ones((10, 5, 10)), np.ones((10, 5))],\n        y=np.zeros((10, 100)),\n        batch_size=2)\n    # Data is not masked, returned values are random.\n    self.assertGreater(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_arg_derived_from_keras_layer(self):\n\n    class MyAdd(layers.Layer):\n\n      def call(self, x1, x2):\n        return x1 + x2\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    outputs = MyAdd()(input1, input2)\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check serialization.\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MyAdd': MyAdd})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations(mode='eager'),)\n  def test_only_some_in_first_arg_derived_from_keras_layer_keras_tensors(self):\n    # This functionality is unsupported in v1 graphs\n\n    class MyAddAll(layers.Layer):\n\n      def call(self, inputs):\n        x = inputs[0]\n        for inp in inputs[1:]:\n          if inp is not None:\n            x = x + inp\n        return x\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    layer = MyAddAll()\n    outputs = layer([0.0, input1, None, input2, None])\n    model = training_lib.Model([input1, input2], outputs)\n    self.assertIn(layer, model.layers)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check serialization.\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MyAddAll': MyAddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(\n      combinations.times(\n          combinations.keras_mode_combinations(),\n          combinations.combine(share_already_used_layer=[True, False])))\n  def test_call_kwarg_derived_from_keras_layer(self, share_already_used_layer):\n\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        if x2 is not None:\n          return x1 + x2\n        return x1\n\n    class IdentityLayer(layers.Layer):\n\n      def call(self, x):\n        return x\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    identity_layer = IdentityLayer()\n\n    if share_already_used_layer:\n      # We have had model serialization/deserialization break in the past:\n      # when a layer was previously used to construct other functional models\n      # and had a non-empty list of inbound nodes before being used to define\n      # the model being serialized/deserialized.\n      # (The serialization/deserialization was not correctly adjusting\n      # the node_index serialization/deserialization).\n      # So, we explicitly test this case.\n      training_lib.Model([input1], identity_layer(input1))\n\n    outputs = MaybeAdd()(input1, x2=identity_layer(input2))\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(),\n        custom_objects={\n            'MaybeAdd': MaybeAdd,\n            'IdentityLayer': IdentityLayer\n        })\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_kwarg_dtype_serialization(self):\n\n    class Double(layers.Layer):\n\n      def call(self, x1, dtype=None):\n        return math_ops.cast(x1 + x1, dtype=dtype)\n\n    input1 = input_layer_lib.Input(10)\n    outputs = Double()(input1, dtype=dtypes.float16)\n    model = training_lib.Model([input1], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check the output dtype\n    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'Double': Double})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    # Check the output dtype\n    self.assertEqual(model(array_ops.ones((3, 10))).dtype, dtypes.float16)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_kwarg_nonserializable(self):\n\n    class Double(layers.Layer):\n\n      def call(self, x1, kwarg=None):\n        return x1 + x1\n\n    class NonSerializable(object):\n\n      def __init__(self, foo=None):\n        self.foo = foo\n\n    input1 = input_layer_lib.Input(10)\n    outputs = Double()(input1, kwarg=NonSerializable())\n    model = training_lib.Model([input1], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[3 * np.ones((10, 10))],\n        y=6 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that input was correctly doubled.\n    self.assertEqual(history.history['loss'][0], 0.0)\n    with self.assertRaisesRegex(\n        TypeError, 'Layer double was passed non-JSON-serializable arguments.'):\n      model.get_config()\n\n  @combinations.generate(\n      combinations.times(\n          combinations.keras_mode_combinations(),\n          combinations.combine(share_already_used_layer=[True, False])))\n  def test_call_kwarg_derived_from_keras_layer_and_first_arg_is_constant(\n      self, share_already_used_layer):\n\n    class IdentityLayer(layers.Layer):\n\n      def call(self, x):\n        return x\n\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        if x2 is not None:\n          return x1 + x2\n        return x1\n\n    input2 = input_layer_lib.Input(10)\n    identity_layer = IdentityLayer()\n    if share_already_used_layer:\n      # We have had model serialization/deserialization break in the past:\n      # when a layer was previously used to construct other functional models\n      # and had a non-empty list of inbound nodes before being used to define\n      # the model being serialized/deserialized.\n      # (The serialization/deserialization was not correctly adjusting\n      # the node_index serialization/deserialization).\n      # So, we explicitly test this case.\n      training_lib.Model([input2], identity_layer(input2))\n\n    outputs = MaybeAdd()(3., x2=identity_layer(input2))\n    model = training_lib.Model([input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=7 * np.ones((10, 10)),\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(),\n        custom_objects={\n            'MaybeAdd': MaybeAdd,\n            'IdentityLayer': IdentityLayer\n        })\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=7 * np.ones((10, 10)),\n        y=10 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_composite_call_kwarg_derived_from_keras_layer(self):\n\n    # Create a test layer that accepts composite tensor inputs.\n    class MaybeAdd(layers.Layer):\n\n      def call(self, x1, x2=None):\n        # We need to convert this to a tensor for loss calculations -\n        # losses don't play nicely with ragged tensors yet.\n        if x2 is not None:\n          return (x1 + x2).to_tensor(default_value=0)\n        return x1.to_tensor(default_value=0)\n\n    input1 = input_layer_lib.Input((None,), ragged=True)\n    input2 = input_layer_lib.Input((None,), ragged=True)\n    outputs = MaybeAdd()(input1, x2=input2)\n    model = training_lib.Model([input1, input2], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    input_data = [\n        ragged_factory_ops.constant([[3.0, 3.0], [3.0, 3.0], [3.0]]),\n        ragged_factory_ops.constant([[7.0, 7.0], [7.0, 7.0], [7.0]])\n    ]\n    expected_data = np.array([[10.0, 10.0], [10.0, 10.0], [10.0, 0.0]])\n\n    history = model.fit(x=input_data, y=expected_data)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'MaybeAdd': MaybeAdd})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(x=input_data, y=expected_data)\n    # Check that second input was correctly added to first.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations(mode='eager'))\n  def test_call_some_not_all_nested_in_first_arg_derived_from_keras_layer(self):\n    # This functionality is unsupported in v1 graphs\n\n    class AddAll(layers.Layer):\n\n      def call(self, x1_x2, x3):\n        x1, x2 = x1_x2\n        out = x1 + x2\n        if x3 is not None:\n          for t in x3.values():\n            out += t\n        return out\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    input3 = input_layer_lib.Input(10)\n\n    layer = AddAll()\n    outputs = layer(\n        [input1, 4 * array_ops.ones((1, 10))],\n        x3={\n            'a': input2,\n            'b': input3,\n            'c': 5 * array_ops.ones((1, 10))\n        })\n    model = training_lib.Model([input1, input2, input3], outputs)\n    self.assertIn(layer, model.layers)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'AddAll': AddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_call_nested_arg_derived_from_keras_layer(self):\n\n    class AddAll(layers.Layer):\n\n      def call(self, x1, x2, x3=None):\n        out = x1 + x2\n        if x3 is not None:\n          for t in x3.values():\n            out += t\n        return out\n\n    input1 = input_layer_lib.Input(10)\n    input2 = input_layer_lib.Input(10)\n    input3 = input_layer_lib.Input(10)\n    outputs = AddAll()(\n        input1,\n        4 * array_ops.ones((1, 10)),\n        x3={\n            'a': input2,\n            'b': input3,\n            'c': 5 * array_ops.ones((1, 10))\n        })\n    model = training_lib.Model([input1, input2, input3], outputs)\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n    model = training_lib.Model.from_config(\n        model.get_config(), custom_objects={'AddAll': AddAll})\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    history = model.fit(\n        x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n        y=15 * np.ones((10, 10)),\n        batch_size=2)\n    # Check that all inputs were correctly added.\n    self.assertEqual(history.history['loss'][0], 0.0)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_multi_output_model_with_none_masking(self):\n    def func(x):\n      return [x * 0.2, x * 0.3]\n\n    def output_shape(input_shape):\n      return [input_shape, input_shape]\n\n    i = layers.Input(shape=(3, 2, 1))\n    o = layers.Lambda(function=func, output_shape=output_shape)(i)\n\n    self.assertEqual(backend.int_shape(o[0]), (None, 3, 2, 1))\n    self.assertEqual(backend.int_shape(o[1]), (None, 3, 2, 1))\n\n    o = layers.add(o)\n    model = training_lib.Model(i, o)\n    model.run_eagerly = testing_utils.should_run_eagerly()\n\n    i2 = layers.Input(shape=(3, 2, 1))\n    o2 = model(i2)\n    model2 = training_lib.Model(i2, o2)\n    model2.run_eagerly = testing_utils.should_run_eagerly()\n\n    x = np.random.random((4, 3, 2, 1))\n    out = model2.predict(x)\n    assert out.shape == (4, 3, 2, 1)\n    self.assertAllClose(out, x * 0.2 + x * 0.3, atol=1e-4)\n\n  @combinations.generate(combinations.keras_mode_combinations())\n  def test_constant_initializer_with_numpy(self):\n    initializer = initializers.Constant(np.ones((3, 2)))\n    model = sequential.Sequential()\n    model.add(layers.Dense(2, input_shape=(3,), kernel_initializer=initializer))\n    model.add(layers.Dense(3))\n    model.compile(\n        loss='mse',\n        optimizer='sgd',\n        metrics=['acc'],\n        run_eagerly=testing_utils.should_run_eagerly())\n\n    json_str = model.to_json()\n    models.model_from_json(json_str)\n\n  def test_subclassed_error_if_init_not_called(self):\n\n    class MyNetwork(training_lib.Model):\n\n      def __init__(self):\n        self._foo = [layers.Dense(10), layers.Dense(10)]\n\n    with self.assertRaisesRegex(RuntimeError, 'forgot to call'):\n      MyNetwork()\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_int_input_shape(self):\n    inputs = input_layer_lib.Input(10)\n    self.assertEqual([None, 10], inputs.shape.as_list())\n\n    inputs_with_batch = input_layer_lib.Input(batch_size=20, shape=5)\n    self.assertEqual([20, 5], inputs_with_batch.shape.as_list())\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_model_initialization(self):\n    # Functional model\n    inputs = input_layer_lib.Input(shape=(32,))\n    outputs = layers.Dense(4)(inputs)\n\n    with self.assertRaisesRegex(TypeError,\n                                'Keyword argument not understood'):\n      model = training_lib.Model(\n          inputs, outputs, name='m', trainable=False, dtype='int64')\n    with self.assertRaisesRegex(TypeError,\n                                'Keyword argument not understood'):\n      model = training_lib.Model(\n          inputs, outputs, name='m', trainable=False, dynamic=False)\n\n    model = training_lib.Model(inputs, outputs, name='m', trainable=False)\n    self.assertEqual('m', model.name)\n    self.assertFalse(model.trainable)\n    self.assertFalse(model.dynamic)\n\n    class SubclassModel(training_lib.Model):\n      pass\n    # Subclassed model\n    model = SubclassModel(\n        name='subclassed', trainable=True, dtype='int64', dynamic=True)\n    self.assertEqual('subclassed', model.name)\n    self.assertTrue(model.dynamic)\n    self.assertTrue(model.trainable)\n    w = model.add_weight('w', [], initializer=initializers.Constant(1))\n    self.assertEqual(dtypes.int64, w.dtype)\n\n  def test_disconnected_inputs(self):\n    input_tensor1 = input_layer_lib.Input(shape=[200], name='a')\n    input_tensor2 = input_layer_lib.Input(shape=[10], name='b')\n    output_tensor1 = layers.Dense(units=10)(input_tensor1)\n\n    net = functional.Functional(\n        inputs=[input_tensor1, input_tensor2], outputs=[output_tensor1])\n    net2 = functional.Functional.from_config(net.get_config())\n    self.assertLen(net2.inputs, 2)\n    self.assertEqual('a', net2.layers[0].name)\n    self.assertEqual('b', net2.layers[1].name)\n\n  @combinations.generate(combinations.keras_model_type_combinations())\n  def test_dependency_tracking(self):\n    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n    model.trackable = Checkpoint()\n    self.assertIn('trackable', model._unconditional_dependency_names)\n    self.assertEqual(model.trackable, model._lookup_dependency('trackable'))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_model_construction_in_tf_function(self):\n\n    d = {'model': None}\n\n    @def_function.function\n    def fn(x):\n      if d['model'] is None:\n        # Check that Functional can be built in a `tf.function`.\n        inputs = input_layer_lib.Input(10)\n        outputs = layers.Dense(1)(inputs)\n        model = functional.Functional(inputs, outputs)\n        d['model'] = model\n      else:\n        model = d['model']\n\n      return model(x)\n\n    x = array_ops.ones((10, 10))\n    y = fn(x)\n    self.assertEqual(y.shape.as_list(), [10, 1])\n\n\nclass DeferredModeTest(keras_parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testSimpleNetworkBuilding(self):\n    inputs = input_layer_lib.Input(shape=(32,))\n    if context.executing_eagerly():\n      self.assertEqual(inputs.dtype.name, 'float32')\n      self.assertEqual(inputs.shape.as_list(), [None, 32])\n\n    x = layers.Dense(2)(inputs)\n    if context.executing_eagerly():\n      self.assertEqual(x.dtype.name, 'float32')\n      self.assertEqual(x.shape.as_list(), [None, 2])\n\n    outputs = layers.Dense(4)(x)\n    network = functional.Functional(inputs, outputs)\n    self.assertIsInstance(network, functional.Functional)\n\n    if context.executing_eagerly():\n      # It should be possible to call such a network on EagerTensors.\n      inputs = constant_op.constant(\n          np.random.random((10, 32)).astype('float32'))\n      outputs = network(inputs)\n      self.assertEqual(outputs.shape.as_list(), [10, 4])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testMultiIONetworkBuilding(self):\n    input_a = input_layer_lib.Input(shape=(32,))\n    input_b = input_layer_lib.Input(shape=(16,))\n    a = layers.Dense(16)(input_a)\n\n    class AddLayer(layers.Layer):\n\n      def call(self, inputs):\n        return inputs[0] + inputs[1]\n\n    c = AddLayer()([a, input_b])  # pylint: disable=not-callable\n    c = layers.Dense(2)(c)\n\n    network = functional.Functional([input_a, input_b], [a, c])\n    if context.executing_eagerly():\n      a_val = constant_op.constant(\n          np.random.random((10, 32)).astype('float32'))\n      b_val = constant_op.constant(\n          np.random.random((10, 16)).astype('float32'))\n      outputs = network([a_val, b_val])\n      self.assertEqual(len(outputs), 2)\n      self.assertEqual(outputs[0].shape.as_list(), [10, 16])\n      self.assertEqual(outputs[1].shape.as_list(), [10, 2])\n\n\nclass DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n\n  def _testShapeInference(self, model, input_shape, expected_output_shape):\n    input_value = np.random.random(input_shape)\n    output_value = model.predict(input_value)\n    self.assertEqual(output_value.shape, expected_output_shape)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testSingleInputCase(self):\n\n    class LayerWithOneInput(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs):\n        return backend.dot(inputs, self.w)\n\n    inputs = input_layer_lib.Input(shape=(3,))\n    layer = LayerWithOneInput()\n\n    if context.executing_eagerly():\n      self.assertEqual(\n          layer.compute_output_shape((None, 3)).as_list(), [None, 4])\n      # As a side-effect, compute_output_shape builds the layer.\n      self.assertTrue(layer.built)\n      # We can still query the layer's compute_output_shape with compatible\n      # input shapes.\n      self.assertEqual(\n          layer.compute_output_shape((6, 3)).as_list(), [6, 4])\n\n    outputs = layer(inputs)\n    model = training_lib.Model(inputs, outputs)\n    self._testShapeInference(model, (2, 3), (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testMultiInputOutputCase(self):\n\n    class MultiInputOutputLayer(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs):\n        a = backend.dot(inputs[0], self.w)\n        b = a + inputs[1]\n        return [a, b]\n\n    input_a = input_layer_lib.Input(shape=(3,))\n    input_b = input_layer_lib.Input(shape=(4,))\n    output_a, output_b = MultiInputOutputLayer()([input_a, input_b])\n    model = training_lib.Model([input_a, input_b], [output_a, output_b])\n    output_a_val, output_b_val = model.predict(\n        [np.random.random((2, 3)), np.random.random((2, 4))])\n    self.assertEqual(output_a_val.shape, (2, 4))\n    self.assertEqual(output_b_val.shape, (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testTrainingArgument(self):\n\n    class LayerWithTrainingArg(layers.Layer):\n\n      def build(self, input_shape):\n        self.w = array_ops.ones(shape=(3, 4))\n\n      def call(self, inputs, training):\n        return backend.dot(inputs, self.w)\n\n    inputs = input_layer_lib.Input(shape=(3,))\n    outputs = LayerWithTrainingArg()(inputs, training=False)\n    model = training_lib.Model(inputs, outputs)\n    self._testShapeInference(model, (2, 3), (2, 4))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShape(self):\n\n    class Model(training_lib.Model):\n\n      def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n        self.pool = layers.GlobalAveragePooling2D()\n        self.fc = layers.Dense(3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.fc(x)\n        return x\n\n    model = Model()\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShapeWithCompoundModel(self):\n\n    class BasicBlock(training_lib.Model):\n\n      def __init__(self):\n        super(BasicBlock, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n        self.pool = layers.GlobalAveragePooling2D()\n        self.dense = layers.Dense(3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n        x = self.dense(x)\n        return x\n\n    class CompoundModel(training_lib.Model):\n\n      def __init__(self):\n        super(CompoundModel, self).__init__()\n        self.block = BasicBlock()\n\n      def call(self, x):\n        x = self.block(x)  # pylint: disable=not-callable\n        return x\n\n    model = CompoundModel()\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)  # pylint: disable=not-callable\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def testNoneInShapeWithFunctionalAPI(self):\n\n    class BasicBlock(training_lib.Model):\n      # Inheriting from layers.Layer since we are calling this layer\n      # inside a model created using functional API.\n\n      def __init__(self):\n        super(BasicBlock, self).__init__()\n        self.conv1 = layers.Conv2D(8, 3)\n\n      def call(self, x):\n        x = self.conv1(x)\n        return x\n\n    input_layer = layers.Input(shape=(None, None, 1))\n    x = BasicBlock()(input_layer)\n    x = layers.GlobalAveragePooling2D()(x)\n    output_layer = layers.Dense(3)(x)\n\n    model = training_lib.Model(inputs=input_layer, outputs=output_layer)\n\n    model.build(tensor_shape.TensorShape((None, None, None, 1)))\n    self.assertTrue(model.built, 'Model should be built')\n    self.assertTrue(model.weights,\n                    'Model should have its weights created as it '\n                    'has been built')\n    sample_input = array_ops.ones((1, 10, 10, 1))\n    output = model(sample_input)\n    self.assertEqual(output.shape, (1, 3))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_external_keras_serialization_compat_input_layers(self):\n    inputs = input_layer_lib.Input(shape=(10,))\n    outputs = layers.Dense(1)(inputs)\n    model = training_lib.Model(inputs, outputs)\n    config = model.get_config()\n    # Checks that single inputs and outputs are still saved as 1-element lists.\n    # Saving as 1-element lists or not is equivalent in TF Keras, but only the\n    # 1-element list format is supported in TF.js and keras-team/Keras.\n    self.assertLen(config['input_layers'], 1)\n    self.assertLen(config['output_layers'], 1)\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_external_keras_serialization_compat_inbound_nodes(self):\n    # Check single Tensor input.\n    inputs = input_layer_lib.Input(shape=(10,), name='in')\n    outputs = layers.Dense(1)(inputs)\n    model = training_lib.Model(inputs, outputs)\n    config = model.get_config()\n    self.assertEqual(config['layers'][1]['inbound_nodes'], [[['in', 0, 0, {}]]])\n\n    # Check multiple Tensor input.\n    inputs1 = input_layer_lib.Input(shape=(10,), name='in1')\n    inputs2 = input_layer_lib.Input(shape=(10,), name='in2')\n    outputs = layers.Add()([inputs1, inputs2])\n    model = training_lib.Model([inputs1, inputs2], outputs)\n    config = model.get_config()\n    self.assertEqual(config['layers'][2]['inbound_nodes'],\n                     [[['in1', 0, 0, {}], ['in2', 0, 0, {}]]])\n\n  @combinations.generate(combinations.combine(mode=['eager']))\n  def test_dict_inputs_tensors(self):\n    # Note that this test is running with v2 eager only, since the v1\n    # will behave differently wrt to dict input for training.\n    inputs = {\n        'sentence2': input_layer_lib.Input(\n            shape=(), name='a', dtype=dtypes.string),\n        'sentence1': input_layer_lib.Input(\n            shape=(), name='b', dtype=dtypes.string),\n    }\n    strlen = layers.Lambda(string_ops.string_length_v2)\n    diff = layers.Subtract()(\n        [strlen(inputs['sentence1']), strlen(inputs['sentence2'])])\n    diff = math_ops.cast(diff, dtypes.float32)\n    model = training_lib.Model(inputs, diff)\n\n    extra_keys = {\n        'sentence1': constant_op.constant(['brown fox', 'lazy dog']),\n        'sentence2': constant_op.constant(['owl', 'cheeky cat']),\n        'label': constant_op.constant([0, 1]),\n    }\n\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model(extra_keys)\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    model.compile('sgd', 'mse')\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model.fit(extra_keys, y=constant_op.constant([0, 1]), steps_per_epoch=1)\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    with warnings.catch_warnings(record=True) as w:\n      warnings.simplefilter('always')\n      model.evaluate(extra_keys, constant_op.constant([0, 1]))\n      self.assertIn('ignored by the model', str(w[-1].message))\n\n    # Make sure the model inputs are sorted with the dict keys.\n    self.assertEqual(model.inputs[0]._keras_history.layer.name, 'b')\n    self.assertEqual(model.inputs[1]._keras_history.layer.name, 'a')\n\n\nclass GraphUtilsTest(test.TestCase):\n\n  def testGetReachableFromInputs(self):\n\n    with ops.Graph().as_default(), self.cached_session():\n      pl_1 = array_ops.placeholder(shape=None, dtype='float32')\n      pl_2 = array_ops.placeholder(shape=None, dtype='float32')\n      pl_3 = array_ops.placeholder(shape=None, dtype='float32')\n      x_1 = pl_1 + pl_2\n      x_2 = pl_2 * 2\n      x_3 = pl_3 + 1\n      x_4 = x_1 + x_2\n      x_5 = x_3 * pl_1\n\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_1]),\n          {pl_1, x_1, x_4, x_5, x_1.op, x_4.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_1, pl_2]),\n          {pl_1, pl_2, x_1, x_2, x_4, x_5, x_1.op, x_2.op, x_4.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([pl_3]),\n          {pl_3, x_3, x_5, x_3.op, x_5.op})\n      self.assertEqual(\n          tf_utils.get_reachable_from_inputs([x_3]), {x_3, x_5, x_5.op})\n\n\nclass NestedNetworkTest(keras_parameterized.TestCase):\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_inputs_network(self):\n    inputs = {\n        'x1': input_layer_lib.Input(shape=(1,)),\n        'x2': input_layer_lib.Input(shape=(1,))\n    }\n    outputs = layers.Add()([inputs['x1'], inputs['x2']])\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    result_tensor = network({\n        'x1': array_ops.ones((1, 1), 'float32'),\n        'x2': array_ops.ones((1, 1), 'float32')\n    })\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result, [[2.]])\n\n    # TODO(b/122726584): Investigate why concrete batch is flaky in some builds.\n    output_shape = network.compute_output_shape({\n        'x1': (None, 1),\n        'x2': (None, 1)\n    })\n    self.assertListEqual(output_shape.as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_outputs_network(self):\n    inputs = input_layer_lib.Input(shape=(1,))\n    outputs = {\n        'x+x': layers.Add()([inputs, inputs]),\n        'x*x': layers.Multiply()([inputs, inputs])\n    }\n\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    result_tensor = network(array_ops.ones((1, 1), 'float32'))\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result['x+x'], [[2.]])\n    self.assertAllEqual(result['x*x'], [[1.]])\n\n    output_shape = network.compute_output_shape((None, 1))\n    self.assertListEqual(output_shape['x+x'].as_list(), [None, 1])\n    self.assertListEqual(output_shape['x*x'].as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_network_inside_network(self):\n    inner_inputs = {\n        'x1': input_layer_lib.Input(shape=(1,)),\n        'x2': input_layer_lib.Input(shape=(1,))\n    }\n    inner_outputs = {\n        'x1+x2': layers.Add()([inner_inputs['x1'], inner_inputs['x2']]),\n        'x1*x2': layers.Multiply()([inner_inputs['x1'], inner_inputs['x2']])\n    }\n    inner_network = functional.Functional(\n        inner_inputs, inner_outputs)\n\n    inputs = [\n        input_layer_lib.Input(shape=(1,)),\n        input_layer_lib.Input(shape=(1,))\n    ]\n    middle = inner_network({'x1': inputs[0], 'x2': inputs[1]})\n    outputs = layers.Add()([middle['x1+x2'], middle['x1*x2']])\n    network = functional.Functional(inputs, outputs)\n\n    network = functional.Functional.from_config(network.get_config())\n\n    # Computes: `(x1+x2) + (x1*x2)`\n    result_tensor = network(\n        [array_ops.ones((1, 1), 'float32'),\n         array_ops.ones((1, 1), 'float32')])\n    result = self.evaluate(result_tensor)\n    self.assertAllEqual(result, [[3.]])\n\n    output_shape = network.compute_output_shape([(None, 1), (None, 1)])\n    self.assertListEqual(output_shape.as_list(), [None, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_dict_mapping_input(self):\n\n    class ReturnFirst(layers.Layer):\n\n      def call(self, inputs):\n        b, _ = inputs\n        return b\n\n    # Checks that inputs are put in same order as the\n    # Model was constructed with.\n    b = input_layer_lib.Input(shape=(10,), name='b')\n    a = input_layer_lib.Input(shape=(10,), name='a')\n    outputs = ReturnFirst()([b, a])\n\n    b_val = array_ops.ones((10, 10))\n    a_val = array_ops.zeros((10, 10))\n\n    model = training_lib.Model([b, a], outputs)\n    res = model({'a': a_val, 'b': b_val})\n    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))\n\n    reversed_model = training_lib.Model([a, b], outputs)\n    res = reversed_model({'a': a_val, 'b': b_val})\n    self.assertAllClose(self.evaluate(res), self.evaluate(b_val))\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_dict_mapping_single_input(self):\n    b = input_layer_lib.Input(shape=(1,), name='b')\n    outputs = b * 2\n    model = training_lib.Model(b, outputs)\n\n    b_val = array_ops.ones((1, 1))\n    extra_val = array_ops.ones((1, 10))\n\n    inputs = {'a': extra_val, 'b': b_val}\n    res = model(inputs)\n\n    # Check that 'b' was used and 'a' was ignored.\n    self.assertEqual(res.shape.as_list(), [1, 1])\n\n  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n  def test_nested_dict_mapping(self):\n    a = input_layer_lib.Input(shape=(1,), dtype='int32', name='a')\n    b = input_layer_lib.Input(shape=(1,), dtype='int32', name='b')\n    c = input_layer_lib.Input(shape=(1,), dtype='int32', name='c')\n    d = input_layer_lib.Input(shape=(1,), dtype='int32', name='d')\n    inputs = {'a': (a, b), 'c': (c, d)}\n    outputs = 1000 * a + 100 * b + 10 * c + d\n    model = training_lib.Model(inputs, outputs)\n\n    a_val = array_ops.ones((1, 1), dtype='int32')\n    b_val = 2 * array_ops.ones((1, 1), dtype='int32')\n    c_val = 3 * array_ops.ones((1, 1), dtype='int32')\n    d_val = 4 * array_ops.ones((1, 1), dtype='int32')\n\n    inputs_val = {'a': (a_val, b_val), 'c': (c_val, d_val)}\n    res = model(inputs_val)\n\n    # Check that inputs were flattened in the correct order.\n    self.assertFalse(model._enable_dict_to_input_mapping)\n    self.assertEqual(self.evaluate(res), [1234])\n\n\n@combinations.generate(combinations.keras_mode_combinations())\nclass AddLossTest(keras_parameterized.TestCase):\n\n  def test_add_loss_outside_call_only_loss(self):\n    inputs = input_layer_lib.Input((10,))\n    mid = layers.Dense(10)(inputs)\n    outputs = layers.Dense(1)(mid)\n    model = training_lib.Model(inputs, outputs)\n    model.add_loss(math_ops.reduce_mean(outputs))\n    self.assertLen(model.losses, 1)\n\n    initial_weights = model.get_weights()\n\n    x = np.ones((10, 10))\n    model.compile(\n        'sgd',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model.fit(x, batch_size=2, epochs=1)\n\n    model2 = model.from_config(model.get_config())\n    model2.compile(\n        'sgd',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model2.set_weights(initial_weights)\n    model2.fit(x, batch_size=2, epochs=1)\n\n    # The TFOpLayer and the AddLoss layer are serialized.\n    self.assertLen(model2.layers, 5)\n    self.assertAllClose(model.get_weights(), model2.get_weights())\n\n  def test_add_loss_outside_call_multiple_losses(self):\n    inputs = input_layer_lib.Input((10,))\n    x1 = layers.Dense(10)(inputs)\n    x2 = layers.Dense(10)(x1)\n    outputs = layers.Dense(1)(x2)\n    model = training_lib.Model(inputs, outputs)\n    model.add_loss(math_ops.reduce_sum(x1 * x2))\n    model.add_loss(math_ops.reduce_mean(outputs))\n    self.assertLen(model.losses, 2)\n\n    initial_weights = model.get_weights()\n\n    x, y = np.ones((10, 10)), np.ones((10, 1))\n    model.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model.fit(x, y, batch_size=2, epochs=1)\n\n    model2 = model.from_config(model.get_config())\n    model2.compile(\n        'sgd',\n        'mse',\n        run_eagerly=testing_utils.should_run_eagerly())\n    model2.set_weights(initial_weights)\n    model2.fit(x, y, batch_size=2, epochs=1)\n\n    self.assertAllClose(model.get_weights(), model2.get_weights())\n\n  def test_add_loss_crossentropy_backtracking(self):\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((1,))\n    outputs = layers.Dense(1, activation='sigmoid')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.binary_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.random((2, 1))\n    model.fit([x, y])\n\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((2,))\n    outputs = layers.Dense(2, activation='softmax')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.categorical_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.random((2, 2))\n    model.fit([x, y])\n\n    inputs = input_layer_lib.Input((2,))\n    labels = input_layer_lib.Input((1,), dtype='int32')\n    outputs = layers.Dense(2, activation='softmax')(inputs)\n    model = functional.Functional([inputs, labels], outputs)\n    model.add_loss(losses.sparse_categorical_crossentropy(labels, outputs))\n    model.compile('adam')\n    x = np.random.random((2, 2))\n    y = np.random.randint(0, 2, size=(2, 1))\n    model.fit([x, y])\n\n\n@combinations.generate(combinations.keras_mode_combinations())\nclass WeightAccessTest(keras_parameterized.TestCase):\n\n  def test_functional_model(self):\n    inputs = input_layer_lib.Input((10,))\n    x1 = layers.Dense(10)(inputs)\n    x2 = layers.Dense(10)(x1)\n    outputs = layers.Dense(1)(x2)\n    model = training_lib.Model(inputs, outputs)\n\n    self.assertEqual(len(model.weights), 6)\n\n  def test_sequential_model_with_input_shape(self):\n    x1 = layers.Dense(10, input_shape=(10,))\n    x2 = layers.Dense(10)\n    x3 = layers.Dense(1)\n    model = sequential.Sequential([x1, x2, x3])\n\n    self.assertEqual(len(model.weights), 6)\n\n  def test_sequential_model_without_input_shape(self):\n    x1 = layers.Dense(10)\n    x2 = layers.Dense(10)\n    x3 = layers.Dense(1)\n    model = sequential.Sequential([x1, x2, x3])\n\n    with self.assertRaisesRegex(\n        ValueError, 'Weights for model .* have not yet been created'):\n      _ = model.weights\n\n  def test_subclass_model_with_build_method(self):\n\n    class SubclassModel(models.Model):\n\n      def build(self, input_shape):\n        self.w = self.add_weight(shape=input_shape[-1], initializer='ones')\n\n      def call(self, inputs):\n        return inputs * self.w\n\n    model = SubclassModel()\n\n    with self.assertRaisesRegex(\n        ValueError, 'Weights for model .* have not yet been created'):\n      _ = model.weights\n\n    model(input_layer_lib.Input((10,)))\n    self.assertEqual(len(model.weights), 1)\n\n  def test_subclass_model_without_build_method(self):\n\n    class SubclassModel(models.Model):\n\n      def __init__(self):\n        super(SubclassModel, self).__init__()\n        self.w = self.add_weight(shape=(), initializer='ones')\n\n      def call(self, inputs):\n        return inputs * self.w\n\n    model = SubclassModel()\n    self.assertEqual(len(model.weights), 1)\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass DTypeTest(keras_parameterized.TestCase):\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_graph_network_dtype(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    network = functional.Functional(inputs, outputs)\n    self.assertEqual(network.dtype, 'float32')\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_subclassed_network_dtype(self):\n\n    class IdentityNetwork(training_lib.Model):\n\n      def call(self, inputs):\n        return inputs\n\n    network = IdentityNetwork()\n    self.assertEqual(network.dtype, 'float32')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float32')\n\n    network = IdentityNetwork(dtype='float16')\n    self.assertEqual(network.dtype, 'float16')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float16')\n\n    network = IdentityNetwork(autocast=False)\n    self.assertEqual(network.dtype, 'float32')\n    self.assertEqual(network(array_ops.constant(1, 'float64')).dtype, 'float64')\n\n\nclass AttrTrackingLayer(base_layer.Layer):\n  \"\"\"Count how many times `dynamic` and `stateful` are called.\n\n  These counts are used to test that the attribute cache behaves as expected.\n  \"\"\"\n  def __init__(self, *args, **kwargs):\n    self.stateful_count = 0\n    self.dynamic_count = 0\n    super(AttrTrackingLayer, self).__init__(*args, **kwargs)\n\n  @base_layer.Layer.stateful.getter\n  def stateful(self):\n    self.stateful_count += 1\n    return super(AttrTrackingLayer, self).stateful\n\n  @property\n  def dynamic(self):\n    self.dynamic_count += 1\n    return super(AttrTrackingLayer, self).dynamic\n\n\n@combinations.generate(combinations.combine(mode=['graph', 'eager']))\nclass CacheCorrectnessTest(keras_parameterized.TestCase):\n\n  def layer_and_network_test(self):\n    # Top level layer\n    network = functional.Functional()\n\n    layer_0 = AttrTrackingLayer()\n\n    sub_network = functional.Functional()\n    layer_1 = AttrTrackingLayer(dynamic=True)\n    layer_2 = AttrTrackingLayer()\n    sub_network.sub_layers = [layer_1, layer_2]\n\n    network.sub_layer = layer_0\n\n    for _ in range(2):\n      self.assertEqual(network.dynamic, False)\n      self.assertEqual(network.stateful, False)\n\n      # The second pass should be a cache hit.\n      self.assertEqual(layer_0.dynamic_count, 1)\n      self.assertEqual(layer_0.stateful_count, 1)\n\n    # Mutations of the sub-layer should force recalculation of the network's\n    # stateful attribute. (mutations bubble up.)\n    layer_0.stateful = True\n    self.assertEqual(network.stateful, True)\n    self.assertEqual(layer_0.stateful_count, 2)\n\n    layer_0.stateful = False\n    self.assertEqual(network.stateful, False)\n    self.assertEqual(layer_0.stateful_count, 3)\n\n    # But changing stateful should not affect dynamic.\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(layer_0.dynamic_count, 1)\n\n    network.sub_network = sub_network\n\n    # Adding to the topology should invalidate the cache and reflect in the top\n    # level network.\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 2)\n    self.assertEqual(layer_1.dynamic_count, 1)\n\n    # Still dynamic, but we need to recompute.\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 3)\n    self.assertEqual(layer_1.dynamic_count, 2)\n\n    # Now that we've removed the dynamic layer deep in the layer hierarchy, we\n    # need to make sure that that bubbles up through all the levels.\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(layer_0.dynamic_count, 4)\n    self.assertEqual(layer_1.dynamic_count, 2)\n\n    # Now check with a tracked dict.\n    sub_network.sub_layers = {\n        \"layer_1\": layer_1,\n        \"layer_2\": layer_2,\n    }\n\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 5)\n    self.assertEqual(layer_1.dynamic_count, 3)\n\n    # In-place assignment should still invalidate the cache.\n    sub_network.sub_layers[\"layer_1\"] = layer_1\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(layer_0.dynamic_count, 6)\n    self.assertEqual(layer_1.dynamic_count, 4)\n\n    sub_network.sub_layers[\"layer_1\"] = None\n    for _ in range(2):\n      self.assertEqual(network.dynamic, False)\n      self.assertEqual(layer_0.dynamic_count, 7)\n      self.assertEqual(layer_1.dynamic_count, 4)\n\n    layer_3 = AttrTrackingLayer()\n    layer_3.stateful = True\n\n    sub_network.sub_layers = None\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(network.stateful, False)\n\n    # Test duplicate layers.\n    sub_network.sub_layers = [layer_1, layer_1, layer_1, layer_3]\n    self.assertEqual(network.dynamic, True)\n    self.assertEqual(network.stateful, True)\n\n    for _ in range(3):\n      sub_network.sub_layers.pop()\n      self.assertEqual(network.dynamic, True)\n      self.assertEqual(network.stateful, False)\n\n    sub_network.sub_layers.pop()\n    self.assertEqual(network.dynamic, False)\n    self.assertEqual(network.stateful, False)\n\n  def test_compute_output_shape_cache(self):\n    # See https://github.com/tensorflow/tensorflow/issues/32029.\n    x = input_layer_lib.Input(shape=(None, 32))\n    dense = layers.Dense(2)\n    y = dense(x)\n    network = functional.Functional(x, y, name='dense_network')\n\n    for i in range(999, 1024):\n      self.assertEqual(network.compute_output_shape((1, i, 32)), (1, i, 2))\n\n  def test_2d_inputs_squeezed_to_1d(self):\n    input_1d = input_layer_lib.Input(shape=())\n    outputs = input_1d * 2.\n    net = functional.Functional(input_1d, outputs)\n\n    x = np.ones((10, 1))\n    y = net(x)\n    self.assertEqual(y.shape.rank, 1)\n\n  def test_1d_inputs_expanded_to_2d(self):\n    input_1d = input_layer_lib.Input(shape=(1,))\n    outputs = input_1d * 2.\n    net = functional.Functional(input_1d, outputs)\n\n    x = np.ones((10,))\n    y = net(x)\n    self.assertEqual(y.shape.rank, 2)\n\n  def test_training_passed_during_construction(self):\n\n    def _call(inputs, training):\n      if training is None:\n        return inputs * -1.0\n      elif training:\n        return inputs\n      else:\n        return inputs * 0.0\n\n    class MyLayer(base_layer.Layer):\n\n      def call(self, inputs, training=True):\n        return _call(inputs, training)\n\n    my_layer = MyLayer()\n    x = np.ones((1, 10))\n\n    # Hard-coded `true` value passed during construction is respected.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=True)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, True))\n    self.assertAllEqual(network(x), _call(x, True))\n\n    # Hard-coded `false` value passed during construction is respected.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=False)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, False))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    self.assertAllEqual(network(x), _call(x, False))\n\n    if context.executing_eagerly():\n      # In v2, construction still works when no `training` is specified\n      # When no value passed during construction, it uses the local default.\n      inputs = input_layer_lib.Input(10)\n      outputs = my_layer(inputs)\n      network = functional.Functional(inputs, outputs)\n      self.assertAllEqual(network(x, training=True), _call(x, True))\n      self.assertAllEqual(network(x, training=False), _call(x, False))\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n\n    # `None` value passed positionally during construction is ignored at runtime\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, None)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    if context.executing_eagerly():\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n    else:\n      # in v1 training would have defaulted to using the `None` inside the layer\n      # if training is not passed at runtime\n      self.assertAllEqual(network(x), _call(x, None))\n\n    # `None` value passed as kwarg during construction is ignored at runtime.\n    inputs = input_layer_lib.Input(10)\n    outputs = my_layer(inputs, training=None)\n    network = functional.Functional(inputs, outputs)\n    self.assertAllEqual(network(x, training=True), _call(x, True))\n    self.assertAllEqual(network(x, training=False), _call(x, False))\n    if context.executing_eagerly():\n      self.assertAllEqual(network(x), _call(x, True))  # Use local default\n    else:\n      # in v1 training would have defaulted to using the `None` inside the layer\n      # if training is not passed at runtime\n      self.assertAllEqual(network(x), _call(x, None))\n\n\nclass InputsOutputsErrorTest(keras_parameterized.TestCase):\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_input_error(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    with self.assertRaisesRegex(\n        TypeError, \"('Keyword argument not understood:', 'input')\"):\n      models.Model(input=inputs, outputs=outputs)\n\n  @testing_utils.enable_v2_dtype_behavior\n  def test_output_error(self):\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    with self.assertRaisesRegex(\n        TypeError, \"('Keyword argument not understood:', 'output')\"):\n      models.Model(inputs=inputs, output=outputs)\n\n  def test_input_spec(self):\n    if not context.executing_eagerly():\n      return\n    inputs = input_layer_lib.Input((10,))\n    outputs = layers.Dense(10)(inputs)\n    model = models.Model(inputs, outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model(np.zeros((3, 11)))\n\n  def test_input_spec_list_of_inputs(self):\n    if not context.executing_eagerly():\n      return\n    input_1 = input_layer_lib.Input((10,), name='1')\n    input_2 = input_layer_lib.Input((5,), name='2')\n    x = layers.Concatenate()([input_1, input_2])\n    outputs = layers.Dense(10)(x)\n    model = models.Model([input_1, input_2], outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'.*expects 2 input.*'):\n      model(np.zeros((3, 10)))\n    with self.assertRaisesRegex(\n        ValueError, r'.*expects 2 input.*'):\n      model([np.zeros((3, 10)), np.zeros((3, 5)), np.zeros((3, 10))])\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model([np.zeros((3, 10)), np.zeros((3, 6))])\n\n    # Test passing data via dict keyed by input name\n    with self.assertRaisesRegex(\n        ValueError, r'Missing data for input.*'):\n      model({'1': np.zeros((3, 10))})\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})\n\n  def test_input_spec_dict(self):\n    if not context.executing_eagerly():\n      return\n    input_1 = input_layer_lib.Input((10,))\n    input_2 = input_layer_lib.Input((5,))\n    x = layers.Concatenate()([input_1, input_2])\n    outputs = layers.Dense(10)(x)\n    model = models.Model({'1': input_1, '2': input_2}, outputs)\n    with self.assertRaisesRegex(\n        ValueError, r'Missing data for input.*'):\n      model({'1': np.zeros((3, 10))})\n    with self.assertRaisesRegex(\n        ValueError, r'.*expected shape=.*'):\n      model({'1': np.zeros((3, 10)), '2': np.zeros((3, 6))})\n\n\nclass FunctionalSubclassModel(training_lib.Model):\n\n  def __init__(self, *args, **kwargs):\n    self.foo = {'foo': 'bar'}  # Make sure users can assign dict attributes\n    my_input = input_layer_lib.Input(shape=(16,))\n    dense = layers.Dense(32, activation='relu')\n    output = dense(my_input)\n    outputs = {'output': output}\n    super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)\n\n\nclass MixinClass(object):\n\n  def __init__(self, foo, **kwargs):\n    self._foo = foo\n    super().__init__(**kwargs)\n\n  def get_foo(self):\n    return self._foo\n\n\nclass SubclassedModel(training_lib.Model):\n\n  def __init__(self, bar, **kwargs):\n    self._bar = bar\n    super().__init__(**kwargs)\n\n  def get_bar(self):\n    return self._bar\n\n\nclass MultipleInheritanceModelTest(keras_parameterized.TestCase):\n\n  def testFunctionalSubclass(self):\n    m = FunctionalSubclassModel()\n    # Some smoke test for the weights and output shape of the model\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n\n  def testFunctionalSubclassPreMixin(self):\n    class MixedFunctionalSubclassModel(MixinClass, FunctionalSubclassModel):\n      pass\n\n    m = MixedFunctionalSubclassModel(foo='123')\n    self.assertTrue(m._is_graph_network)\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n    self.assertEqual(m.get_foo(), '123')\n\n  def testFunctionalSubclassPostMixin(self):\n    # Make sure the the mixin class is also init correct when the order changed.\n\n    class MixedFunctionalSubclassModel(FunctionalSubclassModel, MixinClass):\n      pass\n\n    m = MixedFunctionalSubclassModel(foo='123')\n    self.assertTrue(m._is_graph_network)\n    self.assertLen(m.weights, 2)\n    self.assertEqual(m.outputs[0].shape.as_list(), [None, 32])\n    self.assertEqual(m.get_foo(), '123')\n\n  def testSubclassModelPreMixin(self):\n    class MixedSubclassModel(MixinClass, SubclassedModel):\n      pass\n\n    m = MixedSubclassModel(foo='123', bar='456')\n    self.assertFalse(m._is_graph_network)\n    self.assertEqual(m.get_foo(), '123')\n    self.assertEqual(m.get_bar(), '456')\n\n\nif __name__ == '__main__':\n  test.main()\n", "framework": "tensorflow"}
{"repo_name": "dancingdan/tensorflow", "file_path": "tensorflow/python/keras/preprocessing/sequence.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Utilities for preprocessing sequence data.\n\"\"\"\n# pylint: disable=invalid-name\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom keras_preprocessing import sequence\n\nfrom tensorflow.python.keras import utils\nfrom tensorflow.python.util.tf_export import tf_export\n\npad_sequences = sequence.pad_sequences\nmake_sampling_table = sequence.make_sampling_table\nskipgrams = sequence.skipgrams\n# TODO(fchollet): consider making `_remove_long_seq` public.\n_remove_long_seq = sequence._remove_long_seq  # pylint: disable=protected-access\n\n\n@tf_export('keras.preprocessing.sequence.TimeseriesGenerator')\nclass TimeseriesGenerator(sequence.TimeseriesGenerator, utils.Sequence):\n  \"\"\"Utility class for generating batches of temporal data.\n  This class takes in a sequence of data-points gathered at\n  equal intervals, along with time series parameters such as\n  stride, length of history, etc., to produce batches for\n  training/validation.\n  # Arguments\n      data: Indexable generator (such as list or Numpy array)\n          containing consecutive data points (timesteps).\n          The data should be at 2D, and axis 0 is expected\n          to be the time dimension.\n      targets: Targets corresponding to timesteps in `data`.\n          It should have same length as `data`.\n      length: Length of the output sequences (in number of timesteps).\n      sampling_rate: Period between successive individual timesteps\n          within sequences. For rate `r`, timesteps\n          `data[i]`, `data[i-r]`, ... `data[i - length]`\n          are used for create a sample sequence.\n      stride: Period between successive output sequences.\n          For stride `s`, consecutive output samples would\n          be centered around `data[i]`, `data[i+s]`, `data[i+2*s]`, etc.\n      start_index: Data points earlier than `start_index` will not be used\n          in the output sequences. This is useful to reserve part of the\n          data for test or validation.\n      end_index: Data points later than `end_index` will not be used\n          in the output sequences. This is useful to reserve part of the\n          data for test or validation.\n      shuffle: Whether to shuffle output samples,\n          or instead draw them in chronological order.\n      reverse: Boolean: if `true`, timesteps in each output sample will be\n          in reverse chronological order.\n      batch_size: Number of timeseries samples in each batch\n          (except maybe the last one).\n  # Returns\n      A [Sequence](/utils/#sequence) instance.\n  # Examples\n  ```python\n  from keras.preprocessing.sequence import TimeseriesGenerator\n  import numpy as np\n  data = np.array([[i] for i in range(50)])\n  targets = np.array([[i] for i in range(50)])\n  data_gen = TimeseriesGenerator(data, targets,\n                                 length=10, sampling_rate=2,\n                                 batch_size=2)\n  assert len(data_gen) == 20\n  batch_0 = data_gen[0]\n  x, y = batch_0\n  assert np.array_equal(x,\n                        np.array([[[0], [2], [4], [6], [8]],\n                                  [[1], [3], [5], [7], [9]]]))\n  assert np.array_equal(y,\n                        np.array([[10], [11]]))\n  ```\n  \"\"\"\n  pass\n\n\ntf_export('keras.preprocessing.sequence.pad_sequences')(pad_sequences)\ntf_export(\n    'keras.preprocessing.sequence.make_sampling_table')(make_sampling_table)\ntf_export('keras.preprocessing.sequence.skipgrams')(skipgrams)\n", "framework": "tensorflow"}
{"repo_name": "AnishShah/tensorflow", "file_path": "tensorflow/python/keras/preprocessing/sequence.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Utilities for preprocessing sequence data.\n\"\"\"\n# pylint: disable=invalid-name\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom keras_preprocessing import sequence\n\nfrom tensorflow.python.keras import utils\nfrom tensorflow.python.util.tf_export import tf_export\n\npad_sequences = sequence.pad_sequences\nmake_sampling_table = sequence.make_sampling_table\nskipgrams = sequence.skipgrams\n# TODO(fchollet): consider making `_remove_long_seq` public.\n_remove_long_seq = sequence._remove_long_seq  # pylint: disable=protected-access\n\n\n@tf_export('keras.preprocessing.sequence.TimeseriesGenerator')\nclass TimeseriesGenerator(sequence.TimeseriesGenerator, utils.Sequence):\n  \"\"\"Utility class for generating batches of temporal data.\n  This class takes in a sequence of data-points gathered at\n  equal intervals, along with time series parameters such as\n  stride, length of history, etc., to produce batches for\n  training/validation.\n  # Arguments\n      data: Indexable generator (such as list or Numpy array)\n          containing consecutive data points (timesteps).\n          The data should be at 2D, and axis 0 is expected\n          to be the time dimension.\n      targets: Targets corresponding to timesteps in `data`.\n          It should have same length as `data`.\n      length: Length of the output sequences (in number of timesteps).\n      sampling_rate: Period between successive individual timesteps\n          within sequences. For rate `r`, timesteps\n          `data[i]`, `data[i-r]`, ... `data[i - length]`\n          are used for create a sample sequence.\n      stride: Period between successive output sequences.\n          For stride `s`, consecutive output samples would\n          be centered around `data[i]`, `data[i+s]`, `data[i+2*s]`, etc.\n      start_index: Data points earlier than `start_index` will not be used\n          in the output sequences. This is useful to reserve part of the\n          data for test or validation.\n      end_index: Data points later than `end_index` will not be used\n          in the output sequences. This is useful to reserve part of the\n          data for test or validation.\n      shuffle: Whether to shuffle output samples,\n          or instead draw them in chronological order.\n      reverse: Boolean: if `true`, timesteps in each output sample will be\n          in reverse chronological order.\n      batch_size: Number of timeseries samples in each batch\n          (except maybe the last one).\n  # Returns\n      A [Sequence](/utils/#sequence) instance.\n  # Examples\n  ```python\n  from keras.preprocessing.sequence import TimeseriesGenerator\n  import numpy as np\n  data = np.array([[i] for i in range(50)])\n  targets = np.array([[i] for i in range(50)])\n  data_gen = TimeseriesGenerator(data, targets,\n                                 length=10, sampling_rate=2,\n                                 batch_size=2)\n  assert len(data_gen) == 20\n  batch_0 = data_gen[0]\n  x, y = batch_0\n  assert np.array_equal(x,\n                        np.array([[[0], [2], [4], [6], [8]],\n                                  [[1], [3], [5], [7], [9]]]))\n  assert np.array_equal(y,\n                        np.array([[10], [11]]))\n  ```\n  \"\"\"\n  pass\n\n\ntf_export('keras.preprocessing.sequence.pad_sequences')(pad_sequences)\ntf_export(\n    'keras.preprocessing.sequence.make_sampling_table')(make_sampling_table)\ntf_export('keras.preprocessing.sequence.skipgrams')(skipgrams)\n", "framework": "tensorflow"}
{"repo_name": "aselle/tensorflow", "file_path": "tensorflow/python/layers/core.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n# pylint: disable=unused-import,g-bad-import-order\n\"\"\"Contains the core layers: Dense, Dropout.\n\nAlso contains their functional aliases.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport numpy as np\n\nfrom tensorflow.python.keras import layers as keras_layers\nfrom tensorflow.python.layers import base\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n@tf_export('layers.Dense')\nclass Dense(keras_layers.Dense, base.Layer):\n  \"\"\"Densely-connected layer class.\n\n  This layer implements the operation:\n  `outputs = activation(inputs * kernel + bias)`\n  Where `activation` is the activation function passed as the `activation`\n  argument (if not `None`), `kernel` is a weights matrix created by the layer,\n  and `bias` is a bias vector created by the layer\n  (only if `use_bias` is `True`).\n\n  Arguments:\n    units: Integer or Long, dimensionality of the output space.\n    activation: Activation function (callable). Set it to None to maintain a\n      linear activation.\n    use_bias: Boolean, whether the layer uses a bias.\n    kernel_initializer: Initializer function for the weight matrix.\n      If `None` (default), weights are initialized using the default\n      initializer used by `tf.get_variable`.\n    bias_initializer: Initializer function for the bias.\n    kernel_regularizer: Regularizer function for the weight matrix.\n    bias_regularizer: Regularizer function for the bias.\n    activity_regularizer: Regularizer function for the output.\n    kernel_constraint: An optional projection function to be applied to the\n        kernel after being updated by an `Optimizer` (e.g. used to implement\n        norm constraints or value constraints for layer weights). The function\n        must take as input the unprojected variable and must return the\n        projected variable (which must have the same shape). Constraints are\n        not safe to use when doing asynchronous distributed training.\n    bias_constraint: An optional projection function to be applied to the\n        bias after being updated by an `Optimizer`.\n    trainable: Boolean, if `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    name: String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require reuse=True in such cases.\n    reuse: Boolean, whether to reuse the weights of a previous layer\n      by the same name.\n\n  Properties:\n    units: Python integer, dimensionality of the output space.\n    activation: Activation function (callable).\n    use_bias: Boolean, whether the layer uses a bias.\n    kernel_initializer: Initializer instance (or name) for the kernel matrix.\n    bias_initializer: Initializer instance (or name) for the bias.\n    kernel_regularizer: Regularizer instance for the kernel matrix (callable)\n    bias_regularizer: Regularizer instance for the bias (callable).\n    activity_regularizer: Regularizer instance for the output (callable)\n    kernel_constraint: Constraint function for the kernel matrix.\n    bias_constraint: Constraint function for the bias.\n    kernel: Weight matrix (TensorFlow variable or tensor).\n    bias: Bias vector, if applicable (TensorFlow variable or tensor).\n  \"\"\"\n\n  def __init__(self, units,\n               activation=None,\n               use_bias=True,\n               kernel_initializer=None,\n               bias_initializer=init_ops.zeros_initializer(),\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               trainable=True,\n               name=None,\n               **kwargs):\n    super(Dense, self).__init__(units=units,\n                                activation=activation,\n                                use_bias=use_bias,\n                                kernel_initializer=kernel_initializer,\n                                bias_initializer=bias_initializer,\n                                kernel_regularizer=kernel_regularizer,\n                                bias_regularizer=bias_regularizer,\n                                activity_regularizer=activity_regularizer,\n                                kernel_constraint=kernel_constraint,\n                                bias_constraint=bias_constraint,\n                                trainable=trainable,\n                                name=name,\n                                **kwargs)\n\n\n@tf_export('layers.dense')\ndef dense(\n    inputs, units,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=None,\n    bias_initializer=init_ops.zeros_initializer(),\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    trainable=True,\n    name=None,\n    reuse=None):\n  \"\"\"Functional interface for the densely-connected layer.\n\n  This layer implements the operation:\n  `outputs = activation(inputs.kernel + bias)`\n  Where `activation` is the activation function passed as the `activation`\n  argument (if not `None`), `kernel` is a weights matrix created by the layer,\n  and `bias` is a bias vector created by the layer\n  (only if `use_bias` is `True`).\n\n  Arguments:\n    inputs: Tensor input.\n    units: Integer or Long, dimensionality of the output space.\n    activation: Activation function (callable). Set it to None to maintain a\n      linear activation.\n    use_bias: Boolean, whether the layer uses a bias.\n    kernel_initializer: Initializer function for the weight matrix.\n      If `None` (default), weights are initialized using the default\n      initializer used by `tf.get_variable`.\n    bias_initializer: Initializer function for the bias.\n    kernel_regularizer: Regularizer function for the weight matrix.\n    bias_regularizer: Regularizer function for the bias.\n    activity_regularizer: Regularizer function for the output.\n    kernel_constraint: An optional projection function to be applied to the\n        kernel after being updated by an `Optimizer` (e.g. used to implement\n        norm constraints or value constraints for layer weights). The function\n        must take as input the unprojected variable and must return the\n        projected variable (which must have the same shape). Constraints are\n        not safe to use when doing asynchronous distributed training.\n    bias_constraint: An optional projection function to be applied to the\n        bias after being updated by an `Optimizer`.\n    trainable: Boolean, if `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    name: String, the name of the layer.\n    reuse: Boolean, whether to reuse the weights of a previous layer\n      by the same name.\n\n  Returns:\n    Output tensor the same shape as `inputs` except the last dimension is of\n    size `units`.\n\n  Raises:\n    ValueError: if eager execution is enabled.\n  \"\"\"\n  layer = Dense(units,\n                activation=activation,\n                use_bias=use_bias,\n                kernel_initializer=kernel_initializer,\n                bias_initializer=bias_initializer,\n                kernel_regularizer=kernel_regularizer,\n                bias_regularizer=bias_regularizer,\n                activity_regularizer=activity_regularizer,\n                kernel_constraint=kernel_constraint,\n                bias_constraint=bias_constraint,\n                trainable=trainable,\n                name=name,\n                _scope=name,\n                _reuse=reuse)\n  return layer.apply(inputs)\n\n\n@tf_export('layers.Dropout')\nclass Dropout(keras_layers.Dropout, base.Layer):\n  \"\"\"Applies Dropout to the input.\n\n  Dropout consists in randomly setting a fraction `rate` of input units to 0\n  at each update during training time, which helps prevent overfitting.\n  The units that are kept are scaled by `1 / (1 - rate)`, so that their\n  sum is unchanged at training time and inference time.\n\n  Arguments:\n    rate: The dropout rate, between 0 and 1. E.g. `rate=0.1` would drop out\n      10% of input units.\n    noise_shape: 1D tensor of type `int32` representing the shape of the\n      binary dropout mask that will be multiplied with the input.\n      For instance, if your inputs have shape\n      `(batch_size, timesteps, features)`, and you want the dropout mask\n      to be the same for all timesteps, you can use\n      `noise_shape=[batch_size, 1, features]`.\n    seed: A Python integer. Used to create random seeds. See\n      @{tf.set_random_seed}.\n      for behavior.\n    name: The name of the layer (string).\n  \"\"\"\n\n  def __init__(self, rate=0.5,\n               noise_shape=None,\n               seed=None,\n               name=None,\n               **kwargs):\n    super(Dropout, self).__init__(rate=rate,\n                                  noise_shape=noise_shape,\n                                  seed=seed,\n                                  name=name,\n                                  **kwargs)\n\n  def call(self, inputs, training=False):\n    return super(Dropout, self).call(inputs, training=training)\n\n\n@tf_export('layers.dropout')\ndef dropout(inputs,\n            rate=0.5,\n            noise_shape=None,\n            seed=None,\n            training=False,\n            name=None):\n  \"\"\"Applies Dropout to the input.\n\n  Dropout consists in randomly setting a fraction `rate` of input units to 0\n  at each update during training time, which helps prevent overfitting.\n  The units that are kept are scaled by `1 / (1 - rate)`, so that their\n  sum is unchanged at training time and inference time.\n\n  Arguments:\n    inputs: Tensor input.\n    rate: The dropout rate, between 0 and 1. E.g. \"rate=0.1\" would drop out\n      10% of input units.\n    noise_shape: 1D tensor of type `int32` representing the shape of the\n      binary dropout mask that will be multiplied with the input.\n      For instance, if your inputs have shape\n      `(batch_size, timesteps, features)`, and you want the dropout mask\n      to be the same for all timesteps, you can use\n      `noise_shape=[batch_size, 1, features]`.\n    seed: A Python integer. Used to create random seeds. See\n      @{tf.set_random_seed}\n      for behavior.\n    training: Either a Python boolean, or a TensorFlow boolean scalar tensor\n      (e.g. a placeholder). Whether to return the output in training mode\n      (apply dropout) or in inference mode (return the input untouched).\n    name: The name of the layer (string).\n\n  Returns:\n    Output tensor.\n\n  Raises:\n    ValueError: if eager execution is enabled.\n  \"\"\"\n  layer = Dropout(rate, noise_shape=noise_shape, seed=seed, name=name)\n  return layer.apply(inputs, training=training)\n\n\n@tf_export('layers.Flatten')\nclass Flatten(keras_layers.Flatten, base.Layer):\n  \"\"\"Flattens an input tensor while preserving the batch axis (axis 0).\n\n  Examples:\n\n  ```\n    x = tf.placeholder(shape=(None, 4, 4), dtype='float32')\n    y = Flatten()(x)\n    # now `y` has shape `(None, 16)`\n\n    x = tf.placeholder(shape=(None, 3, None), dtype='float32')\n    y = Flatten()(x)\n    # now `y` has shape `(None, None)`\n  ```\n  \"\"\"\n  pass\n\n\n@tf_export('layers.flatten')\ndef flatten(inputs, name=None):\n  \"\"\"Flattens an input tensor while preserving the batch axis (axis 0).\n\n  Arguments:\n    inputs: Tensor input.\n    name: The name of the layer (string).\n\n  Returns:\n    Reshaped tensor.\n\n  Examples:\n\n  ```\n    x = tf.placeholder(shape=(None, 4, 4), dtype='float32')\n    y = flatten(x)\n    # now `y` has shape `(None, 16)`\n\n    x = tf.placeholder(shape=(None, 3, None), dtype='float32')\n    y = flatten(x)\n    # now `y` has shape `(None, None)`\n  ```\n  \"\"\"\n  layer = Flatten(name=name)\n  return layer.apply(inputs)\n\n\n# Aliases\n\nFullyConnected = Dense\nfully_connected = dense\n", "framework": "tensorflow"}
{"repo_name": "caisq/tensorflow", "file_path": "tensorflow/python/layers/core.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n# pylint: disable=unused-import,g-bad-import-order\n\"\"\"Contains the core layers: Dense, Dropout.\n\nAlso contains their functional aliases.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport numpy as np\n\nfrom tensorflow.python.keras import layers as keras_layers\nfrom tensorflow.python.layers import base\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n@tf_export('layers.Dense')\nclass Dense(keras_layers.Dense, base.Layer):\n  \"\"\"Densely-connected layer class.\n\n  This layer implements the operation:\n  `outputs = activation(inputs * kernel + bias)`\n  Where `activation` is the activation function passed as the `activation`\n  argument (if not `None`), `kernel` is a weights matrix created by the layer,\n  and `bias` is a bias vector created by the layer\n  (only if `use_bias` is `True`).\n\n  Arguments:\n    units: Integer or Long, dimensionality of the output space.\n    activation: Activation function (callable). Set it to None to maintain a\n      linear activation.\n    use_bias: Boolean, whether the layer uses a bias.\n    kernel_initializer: Initializer function for the weight matrix.\n      If `None` (default), weights are initialized using the default\n      initializer used by `tf.get_variable`.\n    bias_initializer: Initializer function for the bias.\n    kernel_regularizer: Regularizer function for the weight matrix.\n    bias_regularizer: Regularizer function for the bias.\n    activity_regularizer: Regularizer function for the output.\n    kernel_constraint: An optional projection function to be applied to the\n        kernel after being updated by an `Optimizer` (e.g. used to implement\n        norm constraints or value constraints for layer weights). The function\n        must take as input the unprojected variable and must return the\n        projected variable (which must have the same shape). Constraints are\n        not safe to use when doing asynchronous distributed training.\n    bias_constraint: An optional projection function to be applied to the\n        bias after being updated by an `Optimizer`.\n    trainable: Boolean, if `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    name: String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require reuse=True in such cases.\n    reuse: Boolean, whether to reuse the weights of a previous layer\n      by the same name.\n\n  Properties:\n    units: Python integer, dimensionality of the output space.\n    activation: Activation function (callable).\n    use_bias: Boolean, whether the layer uses a bias.\n    kernel_initializer: Initializer instance (or name) for the kernel matrix.\n    bias_initializer: Initializer instance (or name) for the bias.\n    kernel_regularizer: Regularizer instance for the kernel matrix (callable)\n    bias_regularizer: Regularizer instance for the bias (callable).\n    activity_regularizer: Regularizer instance for the output (callable)\n    kernel_constraint: Constraint function for the kernel matrix.\n    bias_constraint: Constraint function for the bias.\n    kernel: Weight matrix (TensorFlow variable or tensor).\n    bias: Bias vector, if applicable (TensorFlow variable or tensor).\n  \"\"\"\n\n  def __init__(self, units,\n               activation=None,\n               use_bias=True,\n               kernel_initializer=None,\n               bias_initializer=init_ops.zeros_initializer(),\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               trainable=True,\n               name=None,\n               **kwargs):\n    super(Dense, self).__init__(units=units,\n                                activation=activation,\n                                use_bias=use_bias,\n                                kernel_initializer=kernel_initializer,\n                                bias_initializer=bias_initializer,\n                                kernel_regularizer=kernel_regularizer,\n                                bias_regularizer=bias_regularizer,\n                                activity_regularizer=activity_regularizer,\n                                kernel_constraint=kernel_constraint,\n                                bias_constraint=bias_constraint,\n                                trainable=trainable,\n                                name=name,\n                                **kwargs)\n\n\n@tf_export('layers.dense')\ndef dense(\n    inputs, units,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=None,\n    bias_initializer=init_ops.zeros_initializer(),\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    trainable=True,\n    name=None,\n    reuse=None):\n  \"\"\"Functional interface for the densely-connected layer.\n\n  This layer implements the operation:\n  `outputs = activation(inputs.kernel + bias)`\n  Where `activation` is the activation function passed as the `activation`\n  argument (if not `None`), `kernel` is a weights matrix created by the layer,\n  and `bias` is a bias vector created by the layer\n  (only if `use_bias` is `True`).\n\n  Arguments:\n    inputs: Tensor input.\n    units: Integer or Long, dimensionality of the output space.\n    activation: Activation function (callable). Set it to None to maintain a\n      linear activation.\n    use_bias: Boolean, whether the layer uses a bias.\n    kernel_initializer: Initializer function for the weight matrix.\n      If `None` (default), weights are initialized using the default\n      initializer used by `tf.get_variable`.\n    bias_initializer: Initializer function for the bias.\n    kernel_regularizer: Regularizer function for the weight matrix.\n    bias_regularizer: Regularizer function for the bias.\n    activity_regularizer: Regularizer function for the output.\n    kernel_constraint: An optional projection function to be applied to the\n        kernel after being updated by an `Optimizer` (e.g. used to implement\n        norm constraints or value constraints for layer weights). The function\n        must take as input the unprojected variable and must return the\n        projected variable (which must have the same shape). Constraints are\n        not safe to use when doing asynchronous distributed training.\n    bias_constraint: An optional projection function to be applied to the\n        bias after being updated by an `Optimizer`.\n    trainable: Boolean, if `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    name: String, the name of the layer.\n    reuse: Boolean, whether to reuse the weights of a previous layer\n      by the same name.\n\n  Returns:\n    Output tensor the same shape as `inputs` except the last dimension is of\n    size `units`.\n\n  Raises:\n    ValueError: if eager execution is enabled.\n  \"\"\"\n  layer = Dense(units,\n                activation=activation,\n                use_bias=use_bias,\n                kernel_initializer=kernel_initializer,\n                bias_initializer=bias_initializer,\n                kernel_regularizer=kernel_regularizer,\n                bias_regularizer=bias_regularizer,\n                activity_regularizer=activity_regularizer,\n                kernel_constraint=kernel_constraint,\n                bias_constraint=bias_constraint,\n                trainable=trainable,\n                name=name,\n                _scope=name,\n                _reuse=reuse)\n  return layer.apply(inputs)\n\n\n@tf_export('layers.Dropout')\nclass Dropout(keras_layers.Dropout, base.Layer):\n  \"\"\"Applies Dropout to the input.\n\n  Dropout consists in randomly setting a fraction `rate` of input units to 0\n  at each update during training time, which helps prevent overfitting.\n  The units that are kept are scaled by `1 / (1 - rate)`, so that their\n  sum is unchanged at training time and inference time.\n\n  Arguments:\n    rate: The dropout rate, between 0 and 1. E.g. `rate=0.1` would drop out\n      10% of input units.\n    noise_shape: 1D tensor of type `int32` representing the shape of the\n      binary dropout mask that will be multiplied with the input.\n      For instance, if your inputs have shape\n      `(batch_size, timesteps, features)`, and you want the dropout mask\n      to be the same for all timesteps, you can use\n      `noise_shape=[batch_size, 1, features]`.\n    seed: A Python integer. Used to create random seeds. See\n      @{tf.set_random_seed}.\n      for behavior.\n    name: The name of the layer (string).\n  \"\"\"\n\n  def __init__(self, rate=0.5,\n               noise_shape=None,\n               seed=None,\n               name=None,\n               **kwargs):\n    super(Dropout, self).__init__(rate=rate,\n                                  noise_shape=noise_shape,\n                                  seed=seed,\n                                  name=name,\n                                  **kwargs)\n\n  def call(self, inputs, training=False):\n    return super(Dropout, self).call(inputs, training=training)\n\n\n@tf_export('layers.dropout')\ndef dropout(inputs,\n            rate=0.5,\n            noise_shape=None,\n            seed=None,\n            training=False,\n            name=None):\n  \"\"\"Applies Dropout to the input.\n\n  Dropout consists in randomly setting a fraction `rate` of input units to 0\n  at each update during training time, which helps prevent overfitting.\n  The units that are kept are scaled by `1 / (1 - rate)`, so that their\n  sum is unchanged at training time and inference time.\n\n  Arguments:\n    inputs: Tensor input.\n    rate: The dropout rate, between 0 and 1. E.g. \"rate=0.1\" would drop out\n      10% of input units.\n    noise_shape: 1D tensor of type `int32` representing the shape of the\n      binary dropout mask that will be multiplied with the input.\n      For instance, if your inputs have shape\n      `(batch_size, timesteps, features)`, and you want the dropout mask\n      to be the same for all timesteps, you can use\n      `noise_shape=[batch_size, 1, features]`.\n    seed: A Python integer. Used to create random seeds. See\n      @{tf.set_random_seed}\n      for behavior.\n    training: Either a Python boolean, or a TensorFlow boolean scalar tensor\n      (e.g. a placeholder). Whether to return the output in training mode\n      (apply dropout) or in inference mode (return the input untouched).\n    name: The name of the layer (string).\n\n  Returns:\n    Output tensor.\n\n  Raises:\n    ValueError: if eager execution is enabled.\n  \"\"\"\n  layer = Dropout(rate, noise_shape=noise_shape, seed=seed, name=name)\n  return layer.apply(inputs, training=training)\n\n\n@tf_export('layers.Flatten')\nclass Flatten(keras_layers.Flatten, base.Layer):\n  \"\"\"Flattens an input tensor while preserving the batch axis (axis 0).\n\n  Examples:\n\n  ```\n    x = tf.placeholder(shape=(None, 4, 4), dtype='float32')\n    y = Flatten()(x)\n    # now `y` has shape `(None, 16)`\n\n    x = tf.placeholder(shape=(None, 3, None), dtype='float32')\n    y = Flatten()(x)\n    # now `y` has shape `(None, None)`\n  ```\n  \"\"\"\n  pass\n\n\n@tf_export('layers.flatten')\ndef flatten(inputs, name=None):\n  \"\"\"Flattens an input tensor while preserving the batch axis (axis 0).\n\n  Arguments:\n    inputs: Tensor input.\n    name: The name of the layer (string).\n\n  Returns:\n    Reshaped tensor.\n\n  Examples:\n\n  ```\n    x = tf.placeholder(shape=(None, 4, 4), dtype='float32')\n    y = flatten(x)\n    # now `y` has shape `(None, 16)`\n\n    x = tf.placeholder(shape=(None, 3, None), dtype='float32')\n    y = flatten(x)\n    # now `y` has shape `(None, None)`\n  ```\n  \"\"\"\n  layer = Flatten(name=name)\n  return layer.apply(inputs)\n\n\n# Aliases\n\nFullyConnected = Dense\nfully_connected = dense\n", "framework": "tensorflow"}
{"repo_name": "caisq/tensorflow", "file_path": "tensorflow/python/ops/rnn.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"RNN helpers for TensorFlow models.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n# pylint: disable=protected-access\n_concat = rnn_cell_impl._concat\n# pylint: enable=protected-access\n\n\ndef _transpose_batch_time(x):\n  \"\"\"Transposes the batch and time dimensions of a Tensor.\n\n  If the input tensor has rank < 2 it returns the original tensor. Retains as\n  much of the static shape information as possible.\n\n  Args:\n    x: A Tensor.\n\n  Returns:\n    x transposed along the first two dimensions.\n  \"\"\"\n  x_static_shape = x.get_shape()\n  if x_static_shape.ndims is not None and x_static_shape.ndims < 2:\n    return x\n\n  x_rank = array_ops.rank(x)\n  x_t = array_ops.transpose(\n      x, array_ops.concat(\n          ([1, 0], math_ops.range(2, x_rank)), axis=0))\n  x_t.set_shape(\n      tensor_shape.TensorShape([\n          x_static_shape[1].value, x_static_shape[0].value\n      ]).concatenate(x_static_shape[2:]))\n  return x_t\n\n\ndef _best_effort_input_batch_size(flat_input):\n  \"\"\"Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape\n      `[max_time, batch_size, ...]`.\n    All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  \"\"\"\n  for input_ in flat_input:\n    shape = input_.shape\n    if shape.ndims is None:\n      continue\n    if shape.ndims < 2:\n      raise ValueError(\n          \"Expected input tensor %s to have rank at least 2\" % input_)\n    batch_size = shape[1].value\n    if batch_size is not None:\n      return batch_size\n  # Fallback to the dynamic batch size of the first input.\n  return array_ops.shape(flat_input[0])[1]\n\n\ndef _infer_state_dtype(explicit_dtype, state):\n  \"\"\"Infer the dtype of an RNN state.\n\n  Args:\n    explicit_dtype: explicitly declared dtype or None.\n    state: RNN's hidden state. Must be a Tensor or a nested iterable containing\n      Tensors.\n\n  Returns:\n    dtype: inferred dtype of hidden state.\n\n  Raises:\n    ValueError: if `state` has heterogeneous dtypes or is empty.\n  \"\"\"\n  if explicit_dtype is not None:\n    return explicit_dtype\n  elif nest.is_sequence(state):\n    inferred_dtypes = [element.dtype for element in nest.flatten(state)]\n    if not inferred_dtypes:\n      raise ValueError(\"Unable to infer dtype from empty state.\")\n    all_same = all([x == inferred_dtypes[0] for x in inferred_dtypes])\n    if not all_same:\n      raise ValueError(\n          \"State has tensors of different inferred_dtypes. Unable to infer a \"\n          \"single representative dtype.\")\n    return inferred_dtypes[0]\n  else:\n    return state.dtype\n\n\ndef _maybe_tensor_shape_from_tensor(shape):\n  if isinstance(shape, ops.Tensor):\n    return tensor_shape.as_shape(tensor_util.constant_value(shape))\n  else:\n    return shape\n\n\ndef _should_cache():\n  \"\"\"Returns True if a default caching device should be set, otherwise False.\"\"\"\n  if context.executing_eagerly():\n    return False\n  # Don't set a caching device when running in a loop, since it is possible that\n  # train steps could be wrapped in a tf.while_loop. In that scenario caching\n  # prevents forward computations in loop iterations from re-reading the\n  # updated weights.\n  ctxt = ops.get_default_graph()._get_control_flow_context()  # pylint: disable=protected-access\n  return control_flow_util.GetContainingWhileContext(ctxt) is None\n\n\n# pylint: disable=unused-argument\ndef _rnn_step(\n    time, sequence_length, min_sequence_length, max_sequence_length,\n    zero_output, state, call_cell, state_size, skip_conditionals=False):\n  \"\"\"Calculate one step of a dynamic RNN minibatch.\n\n  Returns an (output, state) pair conditioned on `sequence_length`.\n  When skip_conditionals=False, the pseudocode is something like:\n\n  if t >= max_sequence_length:\n    return (zero_output, state)\n  if t < min_sequence_length:\n    return call_cell()\n\n  # Selectively output zeros or output, old state or new state depending\n  # on whether we've finished calculating each row.\n  new_output, new_state = call_cell()\n  final_output = np.vstack([\n    zero_output if time >= sequence_length[r] else new_output_r\n    for r, new_output_r in enumerate(new_output)\n  ])\n  final_state = np.vstack([\n    state[r] if time >= sequence_length[r] else new_state_r\n    for r, new_state_r in enumerate(new_state)\n  ])\n  return (final_output, final_state)\n\n  Args:\n    time: int32 `Tensor` scalar.\n    sequence_length: int32 `Tensor` vector of size [batch_size].\n    min_sequence_length: int32 `Tensor` scalar, min of sequence_length.\n    max_sequence_length: int32 `Tensor` scalar, max of sequence_length.\n    zero_output: `Tensor` vector of shape [output_size].\n    state: Either a single `Tensor` matrix of shape `[batch_size, state_size]`,\n      or a list/tuple of such tensors.\n    call_cell: lambda returning tuple of (new_output, new_state) where\n      new_output is a `Tensor` matrix of shape `[batch_size, output_size]`.\n      new_state is a `Tensor` matrix of shape `[batch_size, state_size]`.\n    state_size: The `cell.state_size` associated with the state.\n    skip_conditionals: Python bool, whether to skip using the conditional\n      calculations.  This is useful for `dynamic_rnn`, where the input tensor\n      matches `max_sequence_length`, and using conditionals just slows\n      everything down.\n\n  Returns:\n    A tuple of (`final_output`, `final_state`) as given by the pseudocode above:\n      final_output is a `Tensor` matrix of shape [batch_size, output_size]\n      final_state is either a single `Tensor` matrix, or a tuple of such\n        matrices (matching length and shapes of input `state`).\n\n  Raises:\n    ValueError: If the cell returns a state tuple whose length does not match\n      that returned by `state_size`.\n  \"\"\"\n\n  # Convert state to a list for ease of use\n  flat_state = nest.flatten(state)\n  flat_zero_output = nest.flatten(zero_output)\n\n  # Vector describing which batch entries are finished.\n  copy_cond = time >= sequence_length\n\n  def _copy_one_through(output, new_output):\n    # TensorArray and scalar get passed through.\n    if isinstance(output, tensor_array_ops.TensorArray):\n      return new_output\n    if output.shape.ndims == 0:\n      return new_output\n    # Otherwise propagate the old or the new value.\n    with ops.colocate_with(new_output):\n      return array_ops.where(copy_cond, output, new_output)\n\n  def _copy_some_through(flat_new_output, flat_new_state):\n    # Use broadcasting select to determine which values should get\n    # the previous state & zero output, and which values should get\n    # a calculated state & output.\n    flat_new_output = [\n        _copy_one_through(zero_output, new_output)\n        for zero_output, new_output in zip(flat_zero_output, flat_new_output)]\n    flat_new_state = [\n        _copy_one_through(state, new_state)\n        for state, new_state in zip(flat_state, flat_new_state)]\n    return flat_new_output + flat_new_state\n\n  def _maybe_copy_some_through():\n    \"\"\"Run RNN step.  Pass through either no or some past state.\"\"\"\n    new_output, new_state = call_cell()\n\n    nest.assert_same_structure(state, new_state)\n\n    flat_new_state = nest.flatten(new_state)\n    flat_new_output = nest.flatten(new_output)\n    return control_flow_ops.cond(\n        # if t < min_seq_len: calculate and return everything\n        time < min_sequence_length, lambda: flat_new_output + flat_new_state,\n        # else copy some of it through\n        lambda: _copy_some_through(flat_new_output, flat_new_state))\n\n  # TODO(ebrevdo): skipping these conditionals may cause a slowdown,\n  # but benefits from removing cond() and its gradient.  We should\n  # profile with and without this switch here.\n  if skip_conditionals:\n    # Instead of using conditionals, perform the selective copy at all time\n    # steps.  This is faster when max_seq_len is equal to the number of unrolls\n    # (which is typical for dynamic_rnn).\n    new_output, new_state = call_cell()\n    nest.assert_same_structure(state, new_state)\n    new_state = nest.flatten(new_state)\n    new_output = nest.flatten(new_output)\n    final_output_and_state = _copy_some_through(new_output, new_state)\n  else:\n    empty_update = lambda: flat_zero_output + flat_state\n    final_output_and_state = control_flow_ops.cond(\n        # if t >= max_seq_len: copy all state through, output zeros\n        time >= max_sequence_length, empty_update,\n        # otherwise calculation is required: copy some or all of it through\n        _maybe_copy_some_through)\n\n  if len(final_output_and_state) != len(flat_zero_output) + len(flat_state):\n    raise ValueError(\"Internal error: state and output were not concatenated \"\n                     \"correctly.\")\n  final_output = final_output_and_state[:len(flat_zero_output)]\n  final_state = final_output_and_state[len(flat_zero_output):]\n\n  for output, flat_output in zip(final_output, flat_zero_output):\n    output.set_shape(flat_output.get_shape())\n  for substate, flat_substate in zip(final_state, flat_state):\n    if not isinstance(substate, tensor_array_ops.TensorArray):\n      substate.set_shape(flat_substate.get_shape())\n\n  final_output = nest.pack_sequence_as(\n      structure=zero_output, flat_sequence=final_output)\n  final_state = nest.pack_sequence_as(\n      structure=state, flat_sequence=final_state)\n\n  return final_output, final_state\n\n\ndef _reverse_seq(input_seq, lengths):\n  \"\"\"Reverse a list of Tensors up to specified lengths.\n\n  Args:\n    input_seq: Sequence of seq_len tensors of dimension (batch_size, n_features)\n               or nested tuples of tensors.\n    lengths:   A `Tensor` of dimension batch_size, containing lengths for each\n               sequence in the batch. If \"None\" is specified, simply reverses\n               the list.\n\n  Returns:\n    time-reversed sequence\n  \"\"\"\n  if lengths is None:\n    return list(reversed(input_seq))\n\n  flat_input_seq = tuple(nest.flatten(input_) for input_ in input_seq)\n\n  flat_results = [[] for _ in range(len(input_seq))]\n  for sequence in zip(*flat_input_seq):\n    input_shape = tensor_shape.unknown_shape(\n        ndims=sequence[0].get_shape().ndims)\n    for input_ in sequence:\n      input_shape.merge_with(input_.get_shape())\n      input_.set_shape(input_shape)\n\n    # Join into (time, batch_size, depth)\n    s_joined = array_ops.stack(sequence)\n\n    # Reverse along dimension 0\n    s_reversed = array_ops.reverse_sequence(s_joined, lengths, 0, 1)\n    # Split again into list\n    result = array_ops.unstack(s_reversed)\n    for r, flat_result in zip(result, flat_results):\n      r.set_shape(input_shape)\n      flat_result.append(r)\n\n  results = [nest.pack_sequence_as(structure=input_, flat_sequence=flat_result)\n             for input_, flat_result in zip(input_seq, flat_results)]\n  return results\n\n\n@tf_export(\"nn.bidirectional_dynamic_rnn\")\ndef bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length=None,\n                              initial_state_fw=None, initial_state_bw=None,\n                              dtype=None, parallel_iterations=None,\n                              swap_memory=False, time_major=False, scope=None):\n  \"\"\"Creates a dynamic version of bidirectional recurrent neural network.\n\n  Takes input and builds independent forward and backward RNNs. The input_size\n  of forward and backward cell must match. The initial state for both directions\n  is zero by default (but can be set optionally) and no intermediate states are\n  ever returned -- the network is fully unrolled for the given (passed in)\n  length(s) of the sequence(s) or completely unrolled if length(s) is not\n  given.\n\n  Args:\n    cell_fw: An instance of RNNCell, to be used for forward direction.\n    cell_bw: An instance of RNNCell, to be used for backward direction.\n    inputs: The RNN inputs.\n      If time_major == False (default), this must be a tensor of shape:\n        `[batch_size, max_time, ...]`, or a nested tuple of such elements.\n      If time_major == True, this must be a tensor of shape:\n        `[max_time, batch_size, ...]`, or a nested tuple of such elements.\n    sequence_length: (optional) An int32/int64 vector, size `[batch_size]`,\n      containing the actual lengths for each of the sequences in the batch.\n      If not provided, all batch entries are assumed to be full sequences; and\n      time reversal is applied from time `0` to `max_time` for each sequence.\n    initial_state_fw: (optional) An initial state for the forward RNN.\n      This must be a tensor of appropriate type and shape\n      `[batch_size, cell_fw.state_size]`.\n      If `cell_fw.state_size` is a tuple, this should be a tuple of\n      tensors having shapes `[batch_size, s] for s in cell_fw.state_size`.\n    initial_state_bw: (optional) Same as for `initial_state_fw`, but using\n      the corresponding properties of `cell_bw`.\n    dtype: (optional) The data type for the initial states and expected output.\n      Required if initial_states are not provided or RNN states have a\n      heterogeneous dtype.\n    parallel_iterations: (Default: 32).  The number of iterations to run in\n      parallel.  Those operations which do not have any temporal dependency\n      and can be run in parallel, will be.  This parameter trades off\n      time for space.  Values >> 1 use more memory but take less time,\n      while smaller values use less memory but computations take longer.\n    swap_memory: Transparently swap the tensors produced in forward inference\n      but needed for back prop from GPU to CPU.  This allows training RNNs\n      which would typically not fit on a single GPU, with very minimal (or no)\n      performance penalty.\n    time_major: The shape format of the `inputs` and `outputs` Tensors.\n      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n      Using `time_major = True` is a bit more efficient because it avoids\n      transposes at the beginning and end of the RNN calculation.  However,\n      most TensorFlow data is batch-major, so by default this function\n      accepts input and emits output in batch-major form.\n    scope: VariableScope for the created subgraph; defaults to\n      \"bidirectional_rnn\"\n\n  Returns:\n    A tuple (outputs, output_states) where:\n      outputs: A tuple (output_fw, output_bw) containing the forward and\n        the backward rnn output `Tensor`.\n        If time_major == False (default),\n          output_fw will be a `Tensor` shaped:\n          `[batch_size, max_time, cell_fw.output_size]`\n          and output_bw will be a `Tensor` shaped:\n          `[batch_size, max_time, cell_bw.output_size]`.\n        If time_major == True,\n          output_fw will be a `Tensor` shaped:\n          `[max_time, batch_size, cell_fw.output_size]`\n          and output_bw will be a `Tensor` shaped:\n          `[max_time, batch_size, cell_bw.output_size]`.\n        It returns a tuple instead of a single concatenated `Tensor`, unlike\n        in the `bidirectional_rnn`. If the concatenated one is preferred,\n        the forward and backward outputs can be concatenated as\n        `tf.concat(outputs, 2)`.\n      output_states: A tuple (output_state_fw, output_state_bw) containing\n        the forward and the backward final states of bidirectional rnn.\n\n  Raises:\n    TypeError: If `cell_fw` or `cell_bw` is not an instance of `RNNCell`.\n  \"\"\"\n  rnn_cell_impl.assert_like_rnncell(\"cell_fw\", cell_fw)\n  rnn_cell_impl.assert_like_rnncell(\"cell_bw\", cell_bw)\n\n  with vs.variable_scope(scope or \"bidirectional_rnn\"):\n    # Forward direction\n    with vs.variable_scope(\"fw\") as fw_scope:\n      output_fw, output_state_fw = dynamic_rnn(\n          cell=cell_fw, inputs=inputs, sequence_length=sequence_length,\n          initial_state=initial_state_fw, dtype=dtype,\n          parallel_iterations=parallel_iterations, swap_memory=swap_memory,\n          time_major=time_major, scope=fw_scope)\n\n    # Backward direction\n    if not time_major:\n      time_dim = 1\n      batch_dim = 0\n    else:\n      time_dim = 0\n      batch_dim = 1\n\n    def _reverse(input_, seq_lengths, seq_dim, batch_dim):\n      if seq_lengths is not None:\n        return array_ops.reverse_sequence(\n            input=input_, seq_lengths=seq_lengths,\n            seq_dim=seq_dim, batch_dim=batch_dim)\n      else:\n        return array_ops.reverse(input_, axis=[seq_dim])\n\n    with vs.variable_scope(\"bw\") as bw_scope:\n      inputs_reverse = _reverse(\n          inputs, seq_lengths=sequence_length,\n          seq_dim=time_dim, batch_dim=batch_dim)\n      tmp, output_state_bw = dynamic_rnn(\n          cell=cell_bw, inputs=inputs_reverse, sequence_length=sequence_length,\n          initial_state=initial_state_bw, dtype=dtype,\n          parallel_iterations=parallel_iterations, swap_memory=swap_memory,\n          time_major=time_major, scope=bw_scope)\n\n  output_bw = _reverse(\n      tmp, seq_lengths=sequence_length,\n      seq_dim=time_dim, batch_dim=batch_dim)\n\n  outputs = (output_fw, output_bw)\n  output_states = (output_state_fw, output_state_bw)\n\n  return (outputs, output_states)\n\n\n@tf_export(\"nn.dynamic_rnn\")\ndef dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None,\n                dtype=None, parallel_iterations=None, swap_memory=False,\n                time_major=False, scope=None):\n  \"\"\"Creates a recurrent neural network specified by RNNCell `cell`.\n\n  Performs fully dynamic unrolling of `inputs`.\n\n  Example:\n\n  ```python\n  # create a BasicRNNCell\n  rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n\n  # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n\n  # defining initial state\n  initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n\n  # 'state' is a tensor of shape [batch_size, cell_state_size]\n  outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,\n                                     initial_state=initial_state,\n                                     dtype=tf.float32)\n  ```\n\n  ```python\n  # create 2 LSTMCells\n  rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [128, 256]]\n\n  # create a RNN cell composed sequentially of a number of RNNCells\n  multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n\n  # 'outputs' is a tensor of shape [batch_size, max_time, 256]\n  # 'state' is a N-tuple where N is the number of LSTMCells containing a\n  # tf.contrib.rnn.LSTMStateTuple for each cell\n  outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n                                     inputs=data,\n                                     dtype=tf.float32)\n  ```\n\n\n  Args:\n    cell: An instance of RNNCell.\n    inputs: The RNN inputs.\n      If `time_major == False` (default), this must be a `Tensor` of shape:\n        `[batch_size, max_time, ...]`, or a nested tuple of such\n        elements.\n      If `time_major == True`, this must be a `Tensor` of shape:\n        `[max_time, batch_size, ...]`, or a nested tuple of such\n        elements.\n      This may also be a (possibly nested) tuple of Tensors satisfying\n      this property.  The first two dimensions must match across all the inputs,\n      but otherwise the ranks and other shape components may differ.\n      In this case, input to `cell` at each time-step will replicate the\n      structure of these tuples, except for the time dimension (from which the\n      time is taken).\n      The input to `cell` at each time step will be a `Tensor` or (possibly\n      nested) tuple of Tensors each with dimensions `[batch_size, ...]`.\n    sequence_length: (optional) An int32/int64 vector sized `[batch_size]`.\n      Used to copy-through state and zero-out outputs when past a batch\n      element's sequence length.  So it's more for performance than correctness.\n    initial_state: (optional) An initial state for the RNN.\n      If `cell.state_size` is an integer, this must be\n      a `Tensor` of appropriate type and shape `[batch_size, cell.state_size]`.\n      If `cell.state_size` is a tuple, this should be a tuple of\n      tensors having shapes `[batch_size, s] for s in cell.state_size`.\n    dtype: (optional) The data type for the initial state and expected output.\n      Required if initial_state is not provided or RNN state has a heterogeneous\n      dtype.\n    parallel_iterations: (Default: 32).  The number of iterations to run in\n      parallel.  Those operations which do not have any temporal dependency\n      and can be run in parallel, will be.  This parameter trades off\n      time for space.  Values >> 1 use more memory but take less time,\n      while smaller values use less memory but computations take longer.\n    swap_memory: Transparently swap the tensors produced in forward inference\n      but needed for back prop from GPU to CPU.  This allows training RNNs\n      which would typically not fit on a single GPU, with very minimal (or no)\n      performance penalty.\n    time_major: The shape format of the `inputs` and `outputs` Tensors.\n      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n      Using `time_major = True` is a bit more efficient because it avoids\n      transposes at the beginning and end of the RNN calculation.  However,\n      most TensorFlow data is batch-major, so by default this function\n      accepts input and emits output in batch-major form.\n    scope: VariableScope for the created subgraph; defaults to \"rnn\".\n\n  Returns:\n    A pair (outputs, state) where:\n\n    outputs: The RNN output `Tensor`.\n\n      If time_major == False (default), this will be a `Tensor` shaped:\n        `[batch_size, max_time, cell.output_size]`.\n\n      If time_major == True, this will be a `Tensor` shaped:\n        `[max_time, batch_size, cell.output_size]`.\n\n      Note, if `cell.output_size` is a (possibly nested) tuple of integers\n      or `TensorShape` objects, then `outputs` will be a tuple having the\n      same structure as `cell.output_size`, containing Tensors having shapes\n      corresponding to the shape data in `cell.output_size`.\n\n    state: The final state.  If `cell.state_size` is an int, this\n      will be shaped `[batch_size, cell.state_size]`.  If it is a\n      `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n      If it is a (possibly nested) tuple of ints or `TensorShape`, this will\n      be a tuple having the corresponding shapes. If cells are `LSTMCells`\n      `state` will be a tuple containing a `LSTMStateTuple` for each cell.\n\n  Raises:\n    TypeError: If `cell` is not an instance of RNNCell.\n    ValueError: If inputs is None or an empty list.\n  \"\"\"\n  rnn_cell_impl.assert_like_rnncell(\"cell\", cell)\n\n  with vs.variable_scope(scope or \"rnn\") as varscope:\n    # Create a new scope in which the caching device is either\n    # determined by the parent scope, or is set to place the cached\n    # Variable using the same placement as for the rest of the RNN.\n    if _should_cache():\n      if varscope.caching_device is None:\n        varscope.set_caching_device(lambda op: op.device)\n\n    # By default, time_major==False and inputs are batch-major: shaped\n    #   [batch, time, depth]\n    # For internal calculations, we transpose to [time, batch, depth]\n    flat_input = nest.flatten(inputs)\n\n    if not time_major:\n      # (B,T,D) => (T,B,D)\n      flat_input = [ops.convert_to_tensor(input_) for input_ in flat_input]\n      flat_input = tuple(_transpose_batch_time(input_) for input_ in flat_input)\n\n    parallel_iterations = parallel_iterations or 32\n    if sequence_length is not None:\n      sequence_length = math_ops.to_int32(sequence_length)\n      if sequence_length.get_shape().ndims not in (None, 1):\n        raise ValueError(\n            \"sequence_length must be a vector of length batch_size, \"\n            \"but saw shape: %s\" % sequence_length.get_shape())\n      sequence_length = array_ops.identity(  # Just to find it in the graph.\n          sequence_length, name=\"sequence_length\")\n\n    batch_size = _best_effort_input_batch_size(flat_input)\n\n    if initial_state is not None:\n      state = initial_state\n    else:\n      if not dtype:\n        raise ValueError(\"If there is no initial_state, you must give a dtype.\")\n      state = cell.zero_state(batch_size, dtype)\n\n    def _assert_has_shape(x, shape):\n      x_shape = array_ops.shape(x)\n      packed_shape = array_ops.stack(shape)\n      return control_flow_ops.Assert(\n          math_ops.reduce_all(math_ops.equal(x_shape, packed_shape)),\n          [\"Expected shape for Tensor %s is \" % x.name,\n           packed_shape, \" but saw shape: \", x_shape])\n\n    if not context.executing_eagerly() and sequence_length is not None:\n      # Perform some shape validation\n      with ops.control_dependencies(\n          [_assert_has_shape(sequence_length, [batch_size])]):\n        sequence_length = array_ops.identity(\n            sequence_length, name=\"CheckSeqLen\")\n\n    inputs = nest.pack_sequence_as(structure=inputs, flat_sequence=flat_input)\n\n    (outputs, final_state) = _dynamic_rnn_loop(\n        cell,\n        inputs,\n        state,\n        parallel_iterations=parallel_iterations,\n        swap_memory=swap_memory,\n        sequence_length=sequence_length,\n        dtype=dtype)\n\n    # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n    # If we are performing batch-major calculations, transpose output back\n    # to shape [batch, time, depth]\n    if not time_major:\n      # (T,B,D) => (B,T,D)\n      outputs = nest.map_structure(_transpose_batch_time, outputs)\n\n    return (outputs, final_state)\n\n\ndef _dynamic_rnn_loop(cell,\n                      inputs,\n                      initial_state,\n                      parallel_iterations,\n                      swap_memory,\n                      sequence_length=None,\n                      dtype=None):\n  \"\"\"Internal implementation of Dynamic RNN.\n\n  Args:\n    cell: An instance of RNNCell.\n    inputs: A `Tensor` of shape [time, batch_size, input_size], or a nested\n      tuple of such elements.\n    initial_state: A `Tensor` of shape `[batch_size, state_size]`, or if\n      `cell.state_size` is a tuple, then this should be a tuple of\n      tensors having shapes `[batch_size, s] for s in cell.state_size`.\n    parallel_iterations: Positive Python int.\n    swap_memory: A Python boolean\n    sequence_length: (optional) An `int32` `Tensor` of shape [batch_size].\n    dtype: (optional) Expected dtype of output. If not specified, inferred from\n      initial_state.\n\n  Returns:\n    Tuple `(final_outputs, final_state)`.\n    final_outputs:\n      A `Tensor` of shape `[time, batch_size, cell.output_size]`.  If\n      `cell.output_size` is a (possibly nested) tuple of ints or `TensorShape`\n      objects, then this returns a (possibly nested) tuple of Tensors matching\n      the corresponding shapes.\n    final_state:\n      A `Tensor`, or possibly nested tuple of Tensors, matching in length\n      and shapes to `initial_state`.\n\n  Raises:\n    ValueError: If the input depth cannot be inferred via shape inference\n      from the inputs.\n  \"\"\"\n  state = initial_state\n  assert isinstance(parallel_iterations, int), \"parallel_iterations must be int\"\n\n  state_size = cell.state_size\n\n  flat_input = nest.flatten(inputs)\n  flat_output_size = nest.flatten(cell.output_size)\n\n  # Construct an initial output\n  input_shape = array_ops.shape(flat_input[0])\n  time_steps = input_shape[0]\n  batch_size = _best_effort_input_batch_size(flat_input)\n\n  inputs_got_shape = tuple(input_.get_shape().with_rank_at_least(3)\n                           for input_ in flat_input)\n\n  const_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]\n\n  for shape in inputs_got_shape:\n    if not shape[2:].is_fully_defined():\n      raise ValueError(\n          \"Input size (depth of inputs) must be accessible via shape inference,\"\n          \" but saw value None.\")\n    got_time_steps = shape[0].value\n    got_batch_size = shape[1].value\n    if const_time_steps != got_time_steps:\n      raise ValueError(\n          \"Time steps is not the same for all the elements in the input in a \"\n          \"batch.\")\n    if const_batch_size != got_batch_size:\n      raise ValueError(\n          \"Batch_size is not the same for all the elements in the input.\")\n\n  # Prepare dynamic conditional copying of state & output\n  def _create_zero_arrays(size):\n    size = _concat(batch_size, size)\n    return array_ops.zeros(\n        array_ops.stack(size), _infer_state_dtype(dtype, state))\n\n  flat_zero_output = tuple(_create_zero_arrays(output)\n                           for output in flat_output_size)\n  zero_output = nest.pack_sequence_as(structure=cell.output_size,\n                                      flat_sequence=flat_zero_output)\n\n  if sequence_length is not None:\n    min_sequence_length = math_ops.reduce_min(sequence_length)\n    max_sequence_length = math_ops.reduce_max(sequence_length)\n  else:\n    max_sequence_length = time_steps\n\n  time = array_ops.constant(0, dtype=dtypes.int32, name=\"time\")\n\n  with ops.name_scope(\"dynamic_rnn\") as scope:\n    base_name = scope\n\n  def _create_ta(name, element_shape, dtype):\n    return tensor_array_ops.TensorArray(dtype=dtype,\n                                        size=time_steps,\n                                        element_shape=element_shape,\n                                        tensor_array_name=base_name + name)\n\n  in_graph_mode = not context.executing_eagerly()\n  if in_graph_mode:\n    output_ta = tuple(\n        _create_ta(\n            \"output_%d\" % i,\n            element_shape=(tensor_shape.TensorShape([const_batch_size])\n                           .concatenate(\n                               _maybe_tensor_shape_from_tensor(out_size))),\n            dtype=_infer_state_dtype(dtype, state))\n        for i, out_size in enumerate(flat_output_size))\n    input_ta = tuple(\n        _create_ta(\n            \"input_%d\" % i,\n            element_shape=flat_input_i.shape[1:],\n            dtype=flat_input_i.dtype)\n        for i, flat_input_i in enumerate(flat_input))\n    input_ta = tuple(ta.unstack(input_)\n                     for ta, input_ in zip(input_ta, flat_input))\n  else:\n    output_ta = tuple([0 for _ in range(time_steps.numpy())]\n                      for i in range(len(flat_output_size)))\n    input_ta = flat_input\n\n  def _time_step(time, output_ta_t, state):\n    \"\"\"Take a time step of the dynamic RNN.\n\n    Args:\n      time: int32 scalar Tensor.\n      output_ta_t: List of `TensorArray`s that represent the output.\n      state: nested tuple of vector tensors that represent the state.\n\n    Returns:\n      The tuple (time + 1, output_ta_t with updated flow, new_state).\n    \"\"\"\n\n    if in_graph_mode:\n      input_t = tuple(ta.read(time) for ta in input_ta)\n      # Restore some shape information\n      for input_, shape in zip(input_t, inputs_got_shape):\n        input_.set_shape(shape[1:])\n    else:\n      input_t = tuple(ta[time.numpy()] for ta in input_ta)\n\n    input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n    call_cell = lambda: cell(input_t, state)\n\n    if sequence_length is not None:\n      (output, new_state) = _rnn_step(\n          time=time,\n          sequence_length=sequence_length,\n          min_sequence_length=min_sequence_length,\n          max_sequence_length=max_sequence_length,\n          zero_output=zero_output,\n          state=state,\n          call_cell=call_cell,\n          state_size=state_size,\n          skip_conditionals=True)\n    else:\n      (output, new_state) = call_cell()\n\n    # Pack state if using state tuples\n    output = nest.flatten(output)\n\n    if in_graph_mode:\n      output_ta_t = tuple(\n          ta.write(time, out) for ta, out in zip(output_ta_t, output))\n    else:\n      for ta, out in zip(output_ta_t, output):\n        ta[time.numpy()] = out\n\n    return (time + 1, output_ta_t, new_state)\n\n  if in_graph_mode:\n    # Make sure that we run at least 1 step, if necessary, to ensure\n    # the TensorArrays pick up the dynamic shape.\n    loop_bound = math_ops.minimum(\n        time_steps, math_ops.maximum(1, max_sequence_length))\n  else:\n    # Using max_sequence_length isn't currently supported in the Eager branch.\n    loop_bound = time_steps\n\n  _, output_final_ta, final_state = control_flow_ops.while_loop(\n      cond=lambda time, *_: time < loop_bound,\n      body=_time_step,\n      loop_vars=(time, output_ta, state),\n      parallel_iterations=parallel_iterations,\n      maximum_iterations=time_steps,\n      swap_memory=swap_memory)\n\n  # Unpack final output if not using output tuples.\n  if in_graph_mode:\n    final_outputs = tuple(ta.stack() for ta in output_final_ta)\n    # Restore some shape information\n    for output, output_size in zip(final_outputs, flat_output_size):\n      shape = _concat(\n          [const_time_steps, const_batch_size], output_size, static=True)\n      output.set_shape(shape)\n  else:\n    final_outputs = output_final_ta\n\n  final_outputs = nest.pack_sequence_as(\n      structure=cell.output_size, flat_sequence=final_outputs)\n  if not in_graph_mode:\n    final_outputs = nest.map_structure_up_to(\n        cell.output_size, lambda x: array_ops.stack(x, axis=0), final_outputs)\n\n  return (final_outputs, final_state)\n\n\n@tf_export(\"nn.raw_rnn\")\ndef raw_rnn(cell, loop_fn,\n            parallel_iterations=None, swap_memory=False, scope=None):\n  \"\"\"Creates an `RNN` specified by RNNCell `cell` and loop function `loop_fn`.\n\n  **NOTE: This method is still in testing, and the API may change.**\n\n  This function is a more primitive version of `dynamic_rnn` that provides\n  more direct access to the inputs each iteration.  It also provides more\n  control over when to start and finish reading the sequence, and\n  what to emit for the output.\n\n  For example, it can be used to implement the dynamic decoder of a seq2seq\n  model.\n\n  Instead of working with `Tensor` objects, most operations work with\n  `TensorArray` objects directly.\n\n  The operation of `raw_rnn`, in pseudo-code, is basically the following:\n\n  ```python\n  time = tf.constant(0, dtype=tf.int32)\n  (finished, next_input, initial_state, emit_structure, loop_state) = loop_fn(\n      time=time, cell_output=None, cell_state=None, loop_state=None)\n  emit_ta = TensorArray(dynamic_size=True, dtype=initial_state.dtype)\n  state = initial_state\n  while not all(finished):\n    (output, cell_state) = cell(next_input, state)\n    (next_finished, next_input, next_state, emit, loop_state) = loop_fn(\n        time=time + 1, cell_output=output, cell_state=cell_state,\n        loop_state=loop_state)\n    # Emit zeros and copy forward state for minibatch entries that are finished.\n    state = tf.where(finished, state, next_state)\n    emit = tf.where(finished, tf.zeros_like(emit_structure), emit)\n    emit_ta = emit_ta.write(time, emit)\n    # If any new minibatch entries are marked as finished, mark these.\n    finished = tf.logical_or(finished, next_finished)\n    time += 1\n  return (emit_ta, state, loop_state)\n  ```\n\n  with the additional properties that output and state may be (possibly nested)\n  tuples, as determined by `cell.output_size` and `cell.state_size`, and\n  as a result the final `state` and `emit_ta` may themselves be tuples.\n\n  A simple implementation of `dynamic_rnn` via `raw_rnn` looks like this:\n\n  ```python\n  inputs = tf.placeholder(shape=(max_time, batch_size, input_depth),\n                          dtype=tf.float32)\n  sequence_length = tf.placeholder(shape=(batch_size,), dtype=tf.int32)\n  inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_time)\n  inputs_ta = inputs_ta.unstack(inputs)\n\n  cell = tf.contrib.rnn.LSTMCell(num_units)\n\n  def loop_fn(time, cell_output, cell_state, loop_state):\n    emit_output = cell_output  # == None for time == 0\n    if cell_output is None:  # time == 0\n      next_cell_state = cell.zero_state(batch_size, tf.float32)\n    else:\n      next_cell_state = cell_state\n    elements_finished = (time >= sequence_length)\n    finished = tf.reduce_all(elements_finished)\n    next_input = tf.cond(\n        finished,\n        lambda: tf.zeros([batch_size, input_depth], dtype=tf.float32),\n        lambda: inputs_ta.read(time))\n    next_loop_state = None\n    return (elements_finished, next_input, next_cell_state,\n            emit_output, next_loop_state)\n\n  outputs_ta, final_state, _ = raw_rnn(cell, loop_fn)\n  outputs = outputs_ta.stack()\n  ```\n\n  Args:\n    cell: An instance of RNNCell.\n    loop_fn: A callable that takes inputs\n      `(time, cell_output, cell_state, loop_state)`\n      and returns the tuple\n      `(finished, next_input, next_cell_state, emit_output, next_loop_state)`.\n      Here `time` is an int32 scalar `Tensor`, `cell_output` is a\n      `Tensor` or (possibly nested) tuple of tensors as determined by\n      `cell.output_size`, and `cell_state` is a `Tensor`\n      or (possibly nested) tuple of tensors, as determined by the `loop_fn`\n      on its first call (and should match `cell.state_size`).\n      The outputs are: `finished`, a boolean `Tensor` of\n      shape `[batch_size]`, `next_input`: the next input to feed to `cell`,\n      `next_cell_state`: the next state to feed to `cell`,\n      and `emit_output`: the output to store for this iteration.\n\n      Note that `emit_output` should be a `Tensor` or (possibly nested)\n      tuple of tensors which is aggregated in the `emit_ta` inside the\n      `while_loop`. For the first call to `loop_fn`, the `emit_output`\n      corresponds to the `emit_structure` which is then used to determine the\n      size of the `zero_tensor` for the `emit_ta` (defaults to\n      `cell.output_size`). For the subsequent calls to the `loop_fn`, the\n      `emit_output` corresponds to the actual output tensor\n      that is to be aggregated in the `emit_ta`. The parameter `cell_state`\n      and output `next_cell_state` may be either a single or (possibly nested)\n      tuple of tensors.  The parameter `loop_state` and\n      output `next_loop_state` may be either a single or (possibly nested) tuple\n      of `Tensor` and `TensorArray` objects.  This last parameter\n      may be ignored by `loop_fn` and the return value may be `None`.  If it\n      is not `None`, then the `loop_state` will be propagated through the RNN\n      loop, for use purely by `loop_fn` to keep track of its own state.\n      The `next_loop_state` parameter returned may be `None`.\n\n      The first call to `loop_fn` will be `time = 0`, `cell_output = None`,\n      `cell_state = None`, and `loop_state = None`.  For this call:\n      The `next_cell_state` value should be the value with which to initialize\n      the cell's state.  It may be a final state from a previous RNN or it\n      may be the output of `cell.zero_state()`.  It should be a\n      (possibly nested) tuple structure of tensors.\n      If `cell.state_size` is an integer, this must be\n      a `Tensor` of appropriate type and shape `[batch_size, cell.state_size]`.\n      If `cell.state_size` is a `TensorShape`, this must be a `Tensor` of\n      appropriate type and shape `[batch_size] + cell.state_size`.\n      If `cell.state_size` is a (possibly nested) tuple of ints or\n      `TensorShape`, this will be a tuple having the corresponding shapes.\n      The `emit_output` value may be either `None` or a (possibly nested)\n      tuple structure of tensors, e.g.,\n      `(tf.zeros(shape_0, dtype=dtype_0), tf.zeros(shape_1, dtype=dtype_1))`.\n      If this first `emit_output` return value is `None`,\n      then the `emit_ta` result of `raw_rnn` will have the same structure and\n      dtypes as `cell.output_size`.  Otherwise `emit_ta` will have the same\n      structure, shapes (prepended with a `batch_size` dimension), and dtypes\n      as `emit_output`.  The actual values returned for `emit_output` at this\n      initializing call are ignored.  Note, this emit structure must be\n      consistent across all time steps.\n\n    parallel_iterations: (Default: 32).  The number of iterations to run in\n      parallel.  Those operations which do not have any temporal dependency\n      and can be run in parallel, will be.  This parameter trades off\n      time for space.  Values >> 1 use more memory but take less time,\n      while smaller values use less memory but computations take longer.\n    swap_memory: Transparently swap the tensors produced in forward inference\n      but needed for back prop from GPU to CPU.  This allows training RNNs\n      which would typically not fit on a single GPU, with very minimal (or no)\n      performance penalty.\n    scope: VariableScope for the created subgraph; defaults to \"rnn\".\n\n  Returns:\n    A tuple `(emit_ta, final_state, final_loop_state)` where:\n\n    `emit_ta`: The RNN output `TensorArray`.\n       If `loop_fn` returns a (possibly nested) set of Tensors for\n       `emit_output` during initialization, (inputs `time = 0`,\n       `cell_output = None`, and `loop_state = None`), then `emit_ta` will\n       have the same structure, dtypes, and shapes as `emit_output` instead.\n       If `loop_fn` returns `emit_output = None` during this call,\n       the structure of `cell.output_size` is used:\n       If `cell.output_size` is a (possibly nested) tuple of integers\n       or `TensorShape` objects, then `emit_ta` will be a tuple having the\n       same structure as `cell.output_size`, containing TensorArrays whose\n       elements' shapes correspond to the shape data in `cell.output_size`.\n\n    `final_state`: The final cell state.  If `cell.state_size` is an int, this\n      will be shaped `[batch_size, cell.state_size]`.  If it is a\n      `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n      If it is a (possibly nested) tuple of ints or `TensorShape`, this will\n      be a tuple having the corresponding shapes.\n\n    `final_loop_state`: The final loop state as returned by `loop_fn`.\n\n  Raises:\n    TypeError: If `cell` is not an instance of RNNCell, or `loop_fn` is not\n      a `callable`.\n  \"\"\"\n  rnn_cell_impl.assert_like_rnncell(\"cell\", cell)\n\n  if not callable(loop_fn):\n    raise TypeError(\"loop_fn must be a callable\")\n\n  parallel_iterations = parallel_iterations or 32\n\n  # Create a new scope in which the caching device is either\n  # determined by the parent scope, or is set to place the cached\n  # Variable using the same placement as for the rest of the RNN.\n  with vs.variable_scope(scope or \"rnn\") as varscope:\n    if _should_cache():\n      if varscope.caching_device is None:\n        varscope.set_caching_device(lambda op: op.device)\n\n    time = constant_op.constant(0, dtype=dtypes.int32)\n    (elements_finished, next_input, initial_state, emit_structure,\n     init_loop_state) = loop_fn(\n         time, None, None, None)  # time, cell_output, cell_state, loop_state\n    flat_input = nest.flatten(next_input)\n\n    # Need a surrogate loop state for the while_loop if none is available.\n    loop_state = (init_loop_state if init_loop_state is not None\n                  else constant_op.constant(0, dtype=dtypes.int32))\n\n    input_shape = [input_.get_shape() for input_ in flat_input]\n    static_batch_size = input_shape[0][0]\n\n    for input_shape_i in input_shape:\n      # Static verification that batch sizes all match\n      static_batch_size.merge_with(input_shape_i[0])\n\n    batch_size = static_batch_size.value\n    const_batch_size = batch_size\n    if batch_size is None:\n      batch_size = array_ops.shape(flat_input[0])[0]\n\n    nest.assert_same_structure(initial_state, cell.state_size)\n    state = initial_state\n    flat_state = nest.flatten(state)\n    flat_state = [ops.convert_to_tensor(s) for s in flat_state]\n    state = nest.pack_sequence_as(structure=state,\n                                  flat_sequence=flat_state)\n\n    if emit_structure is not None:\n      flat_emit_structure = nest.flatten(emit_structure)\n      flat_emit_size = [emit.shape if emit.shape.is_fully_defined() else\n                        array_ops.shape(emit) for emit in flat_emit_structure]\n      flat_emit_dtypes = [emit.dtype for emit in flat_emit_structure]\n    else:\n      emit_structure = cell.output_size\n      flat_emit_size = nest.flatten(emit_structure)\n      flat_emit_dtypes = [flat_state[0].dtype] * len(flat_emit_size)\n\n    flat_emit_ta = [\n        tensor_array_ops.TensorArray(\n            dtype=dtype_i,\n            dynamic_size=True,\n            element_shape=(tensor_shape.TensorShape([const_batch_size])\n                           .concatenate(\n                               _maybe_tensor_shape_from_tensor(size_i))),\n            size=0,\n            name=\"rnn_output_%d\" % i)\n        for i, (dtype_i, size_i)\n        in enumerate(zip(flat_emit_dtypes, flat_emit_size))]\n    emit_ta = nest.pack_sequence_as(structure=emit_structure,\n                                    flat_sequence=flat_emit_ta)\n    flat_zero_emit = [\n        array_ops.zeros(_concat(batch_size, size_i), dtype_i)\n        for size_i, dtype_i in zip(flat_emit_size, flat_emit_dtypes)]\n    zero_emit = nest.pack_sequence_as(structure=emit_structure,\n                                      flat_sequence=flat_zero_emit)\n\n    def condition(unused_time, elements_finished, *_):\n      return math_ops.logical_not(math_ops.reduce_all(elements_finished))\n\n    def body(time, elements_finished, current_input,\n             emit_ta, state, loop_state):\n      \"\"\"Internal while loop body for raw_rnn.\n\n      Args:\n        time: time scalar.\n        elements_finished: batch-size vector.\n        current_input: possibly nested tuple of input tensors.\n        emit_ta: possibly nested tuple of output TensorArrays.\n        state: possibly nested tuple of state tensors.\n        loop_state: possibly nested tuple of loop state tensors.\n\n      Returns:\n        Tuple having the same size as Args but with updated values.\n      \"\"\"\n      (next_output, cell_state) = cell(current_input, state)\n\n      nest.assert_same_structure(state, cell_state)\n      nest.assert_same_structure(cell.output_size, next_output)\n\n      next_time = time + 1\n      (next_finished, next_input, next_state, emit_output,\n       next_loop_state) = loop_fn(\n           next_time, next_output, cell_state, loop_state)\n\n      nest.assert_same_structure(state, next_state)\n      nest.assert_same_structure(current_input, next_input)\n      nest.assert_same_structure(emit_ta, emit_output)\n\n      # If loop_fn returns None for next_loop_state, just reuse the\n      # previous one.\n      loop_state = loop_state if next_loop_state is None else next_loop_state\n\n      def _copy_some_through(current, candidate):\n        \"\"\"Copy some tensors through via array_ops.where.\"\"\"\n        def copy_fn(cur_i, cand_i):\n          # TensorArray and scalar get passed through.\n          if isinstance(cur_i, tensor_array_ops.TensorArray):\n            return cand_i\n          if cur_i.shape.ndims == 0:\n            return cand_i\n          # Otherwise propagate the old or the new value.\n          with ops.colocate_with(cand_i):\n            return array_ops.where(elements_finished, cur_i, cand_i)\n        return nest.map_structure(copy_fn, current, candidate)\n\n      emit_output = _copy_some_through(zero_emit, emit_output)\n      next_state = _copy_some_through(state, next_state)\n\n      emit_ta = nest.map_structure(\n          lambda ta, emit: ta.write(time, emit), emit_ta, emit_output)\n\n      elements_finished = math_ops.logical_or(elements_finished, next_finished)\n\n      return (next_time, elements_finished, next_input,\n              emit_ta, next_state, loop_state)\n\n    returned = control_flow_ops.while_loop(\n        condition, body, loop_vars=[\n            time, elements_finished, next_input,\n            emit_ta, state, loop_state],\n        parallel_iterations=parallel_iterations,\n        swap_memory=swap_memory)\n\n    (emit_ta, final_state, final_loop_state) = returned[-3:]\n\n    if init_loop_state is None:\n      final_loop_state = None\n\n    return (emit_ta, final_state, final_loop_state)\n\n\n@tf_export(\"nn.static_rnn\")\ndef static_rnn(cell,\n               inputs,\n               initial_state=None,\n               dtype=None,\n               sequence_length=None,\n               scope=None):\n  \"\"\"Creates a recurrent neural network specified by RNNCell `cell`.\n\n  The simplest form of RNN network generated is:\n\n  ```python\n    state = cell.zero_state(...)\n    outputs = []\n    for input_ in inputs:\n      output, state = cell(input_, state)\n      outputs.append(output)\n    return (outputs, state)\n  ```\n  However, a few other options are available:\n\n  An initial state can be provided.\n  If the sequence_length vector is provided, dynamic calculation is performed.\n  This method of calculation does not compute the RNN steps past the maximum\n  sequence length of the minibatch (thus saving computational time),\n  and properly propagates the state at an example's sequence length\n  to the final state output.\n\n  The dynamic calculation performed is, at time `t` for batch row `b`,\n\n  ```python\n    (output, state)(b, t) =\n      (t >= sequence_length(b))\n        ? (zeros(cell.output_size), states(b, sequence_length(b) - 1))\n        : cell(input(b, t), state(b, t - 1))\n  ```\n\n  Args:\n    cell: An instance of RNNCell.\n    inputs: A length T list of inputs, each a `Tensor` of shape\n      `[batch_size, input_size]`, or a nested tuple of such elements.\n    initial_state: (optional) An initial state for the RNN.\n      If `cell.state_size` is an integer, this must be\n      a `Tensor` of appropriate type and shape `[batch_size, cell.state_size]`.\n      If `cell.state_size` is a tuple, this should be a tuple of\n      tensors having shapes `[batch_size, s] for s in cell.state_size`.\n    dtype: (optional) The data type for the initial state and expected output.\n      Required if initial_state is not provided or RNN state has a heterogeneous\n      dtype.\n    sequence_length: Specifies the length of each sequence in inputs.\n      An int32 or int64 vector (tensor) size `[batch_size]`, values in `[0, T)`.\n    scope: VariableScope for the created subgraph; defaults to \"rnn\".\n\n  Returns:\n    A pair (outputs, state) where:\n\n    - outputs is a length T list of outputs (one for each input), or a nested\n      tuple of such elements.\n    - state is the final state\n\n  Raises:\n    TypeError: If `cell` is not an instance of RNNCell.\n    ValueError: If `inputs` is `None` or an empty list, or if the input depth\n      (column size) cannot be inferred from inputs via shape inference.\n  \"\"\"\n  rnn_cell_impl.assert_like_rnncell(\"cell\", cell)\n  if not nest.is_sequence(inputs):\n    raise TypeError(\"inputs must be a sequence\")\n  if not inputs:\n    raise ValueError(\"inputs must not be empty\")\n\n  outputs = []\n  # Create a new scope in which the caching device is either\n  # determined by the parent scope, or is set to place the cached\n  # Variable using the same placement as for the rest of the RNN.\n  with vs.variable_scope(scope or \"rnn\") as varscope:\n    if _should_cache():\n      if varscope.caching_device is None:\n        varscope.set_caching_device(lambda op: op.device)\n\n    # Obtain the first sequence of the input\n    first_input = inputs\n    while nest.is_sequence(first_input):\n      first_input = first_input[0]\n\n    # Temporarily avoid EmbeddingWrapper and seq2seq badness\n    # TODO(lukaszkaiser): remove EmbeddingWrapper\n    if first_input.get_shape().ndims != 1:\n\n      input_shape = first_input.get_shape().with_rank_at_least(2)\n      fixed_batch_size = input_shape[0]\n\n      flat_inputs = nest.flatten(inputs)\n      for flat_input in flat_inputs:\n        input_shape = flat_input.get_shape().with_rank_at_least(2)\n        batch_size, input_size = input_shape[0], input_shape[1:]\n        fixed_batch_size.merge_with(batch_size)\n        for i, size in enumerate(input_size):\n          if size.value is None:\n            raise ValueError(\n                \"Input size (dimension %d of inputs) must be accessible via \"\n                \"shape inference, but saw value None.\" % i)\n    else:\n      fixed_batch_size = first_input.get_shape().with_rank_at_least(1)[0]\n\n    if fixed_batch_size.value:\n      batch_size = fixed_batch_size.value\n    else:\n      batch_size = array_ops.shape(first_input)[0]\n    if initial_state is not None:\n      state = initial_state\n    else:\n      if not dtype:\n        raise ValueError(\"If no initial_state is provided, \"\n                         \"dtype must be specified\")\n      state = cell.zero_state(batch_size, dtype)\n\n    if sequence_length is not None:  # Prepare variables\n      sequence_length = ops.convert_to_tensor(\n          sequence_length, name=\"sequence_length\")\n      if sequence_length.get_shape().ndims not in (None, 1):\n        raise ValueError(\n            \"sequence_length must be a vector of length batch_size\")\n\n      def _create_zero_output(output_size):\n        # convert int to TensorShape if necessary\n        size = _concat(batch_size, output_size)\n        output = array_ops.zeros(\n            array_ops.stack(size), _infer_state_dtype(dtype, state))\n        shape = _concat(fixed_batch_size.value, output_size, static=True)\n        output.set_shape(tensor_shape.TensorShape(shape))\n        return output\n\n      output_size = cell.output_size\n      flat_output_size = nest.flatten(output_size)\n      flat_zero_output = tuple(\n          _create_zero_output(size) for size in flat_output_size)\n      zero_output = nest.pack_sequence_as(\n          structure=output_size, flat_sequence=flat_zero_output)\n\n      sequence_length = math_ops.to_int32(sequence_length)\n      min_sequence_length = math_ops.reduce_min(sequence_length)\n      max_sequence_length = math_ops.reduce_max(sequence_length)\n\n    for time, input_ in enumerate(inputs):\n      if time > 0:\n        varscope.reuse_variables()\n      # pylint: disable=cell-var-from-loop\n      call_cell = lambda: cell(input_, state)\n      # pylint: enable=cell-var-from-loop\n      if sequence_length is not None:\n        (output, state) = _rnn_step(\n            time=time,\n            sequence_length=sequence_length,\n            min_sequence_length=min_sequence_length,\n            max_sequence_length=max_sequence_length,\n            zero_output=zero_output,\n            state=state,\n            call_cell=call_cell,\n            state_size=cell.state_size)\n      else:\n        (output, state) = call_cell()\n\n      outputs.append(output)\n\n    return (outputs, state)\n\n\n@tf_export(\"nn.static_state_saving_rnn\")\ndef static_state_saving_rnn(cell,\n                            inputs,\n                            state_saver,\n                            state_name,\n                            sequence_length=None,\n                            scope=None):\n  \"\"\"RNN that accepts a state saver for time-truncated RNN calculation.\n\n  Args:\n    cell: An instance of `RNNCell`.\n    inputs: A length T list of inputs, each a `Tensor` of shape\n      `[batch_size, input_size]`.\n    state_saver: A state saver object with methods `state` and `save_state`.\n    state_name: Python string or tuple of strings.  The name to use with the\n      state_saver. If the cell returns tuples of states (i.e.,\n      `cell.state_size` is a tuple) then `state_name` should be a tuple of\n      strings having the same length as `cell.state_size`.  Otherwise it should\n      be a single string.\n    sequence_length: (optional) An int32/int64 vector size [batch_size].\n      See the documentation for rnn() for more details about sequence_length.\n    scope: VariableScope for the created subgraph; defaults to \"rnn\".\n\n  Returns:\n    A pair (outputs, state) where:\n      outputs is a length T list of outputs (one for each input)\n      states is the final state\n\n  Raises:\n    TypeError: If `cell` is not an instance of RNNCell.\n    ValueError: If `inputs` is `None` or an empty list, or if the arity and\n     type of `state_name` does not match that of `cell.state_size`.\n  \"\"\"\n  state_size = cell.state_size\n  state_is_tuple = nest.is_sequence(state_size)\n  state_name_tuple = nest.is_sequence(state_name)\n\n  if state_is_tuple != state_name_tuple:\n    raise ValueError(\"state_name should be the same type as cell.state_size.  \"\n                     \"state_name: %s, cell.state_size: %s\" % (str(state_name),\n                                                              str(state_size)))\n\n  if state_is_tuple:\n    state_name_flat = nest.flatten(state_name)\n    state_size_flat = nest.flatten(state_size)\n\n    if len(state_name_flat) != len(state_size_flat):\n      raise ValueError(\"#elems(state_name) != #elems(state_size): %d vs. %d\" %\n                       (len(state_name_flat), len(state_size_flat)))\n\n    initial_state = nest.pack_sequence_as(\n        structure=state_size,\n        flat_sequence=[state_saver.state(s) for s in state_name_flat])\n  else:\n    initial_state = state_saver.state(state_name)\n\n  (outputs, state) = static_rnn(\n      cell,\n      inputs,\n      initial_state=initial_state,\n      sequence_length=sequence_length,\n      scope=scope)\n\n  if state_is_tuple:\n    flat_state = nest.flatten(state)\n    state_name = nest.flatten(state_name)\n    save_state = [\n        state_saver.save_state(name, substate)\n        for name, substate in zip(state_name, flat_state)\n    ]\n  else:\n    save_state = [state_saver.save_state(state_name, state)]\n\n  with ops.control_dependencies(save_state):\n    last_output = outputs[-1]\n    flat_last_output = nest.flatten(last_output)\n    flat_last_output = [\n        array_ops.identity(output) for output in flat_last_output\n    ]\n    outputs[-1] = nest.pack_sequence_as(\n        structure=last_output, flat_sequence=flat_last_output)\n\n    if state_is_tuple:\n      state = nest.pack_sequence_as(\n          structure=state,\n          flat_sequence=[array_ops.identity(s) for s in flat_state])\n    else:\n      state = array_ops.identity(state)\n\n  return (outputs, state)\n\n\n@tf_export(\"nn.static_bidirectional_rnn\")\ndef static_bidirectional_rnn(cell_fw,\n                             cell_bw,\n                             inputs,\n                             initial_state_fw=None,\n                             initial_state_bw=None,\n                             dtype=None,\n                             sequence_length=None,\n                             scope=None):\n  \"\"\"Creates a bidirectional recurrent neural network.\n\n  Similar to the unidirectional case above (rnn) but takes input and builds\n  independent forward and backward RNNs with the final forward and backward\n  outputs depth-concatenated, such that the output will have the format\n  [time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of\n  forward and backward cell must match. The initial state for both directions\n  is zero by default (but can be set optionally) and no intermediate states are\n  ever returned -- the network is fully unrolled for the given (passed in)\n  length(s) of the sequence(s) or completely unrolled if length(s) is not given.\n\n  Args:\n    cell_fw: An instance of RNNCell, to be used for forward direction.\n    cell_bw: An instance of RNNCell, to be used for backward direction.\n    inputs: A length T list of inputs, each a tensor of shape\n      [batch_size, input_size], or a nested tuple of such elements.\n    initial_state_fw: (optional) An initial state for the forward RNN.\n      This must be a tensor of appropriate type and shape\n      `[batch_size, cell_fw.state_size]`.\n      If `cell_fw.state_size` is a tuple, this should be a tuple of\n      tensors having shapes `[batch_size, s] for s in cell_fw.state_size`.\n    initial_state_bw: (optional) Same as for `initial_state_fw`, but using\n      the corresponding properties of `cell_bw`.\n    dtype: (optional) The data type for the initial state.  Required if\n      either of the initial states are not provided.\n    sequence_length: (optional) An int32/int64 vector, size `[batch_size]`,\n      containing the actual lengths for each of the sequences.\n    scope: VariableScope for the created subgraph; defaults to\n      \"bidirectional_rnn\"\n\n  Returns:\n    A tuple (outputs, output_state_fw, output_state_bw) where:\n      outputs is a length `T` list of outputs (one for each input), which\n        are depth-concatenated forward and backward outputs.\n      output_state_fw is the final state of the forward rnn.\n      output_state_bw is the final state of the backward rnn.\n\n  Raises:\n    TypeError: If `cell_fw` or `cell_bw` is not an instance of `RNNCell`.\n    ValueError: If inputs is None or an empty list.\n  \"\"\"\n  rnn_cell_impl.assert_like_rnncell(\"cell_fw\", cell_fw)\n  rnn_cell_impl.assert_like_rnncell(\"cell_bw\", cell_bw)\n  if not nest.is_sequence(inputs):\n    raise TypeError(\"inputs must be a sequence\")\n  if not inputs:\n    raise ValueError(\"inputs must not be empty\")\n\n  with vs.variable_scope(scope or \"bidirectional_rnn\"):\n    # Forward direction\n    with vs.variable_scope(\"fw\") as fw_scope:\n      output_fw, output_state_fw = static_rnn(\n          cell_fw,\n          inputs,\n          initial_state_fw,\n          dtype,\n          sequence_length,\n          scope=fw_scope)\n\n    # Backward direction\n    with vs.variable_scope(\"bw\") as bw_scope:\n      reversed_inputs = _reverse_seq(inputs, sequence_length)\n      tmp, output_state_bw = static_rnn(\n          cell_bw,\n          reversed_inputs,\n          initial_state_bw,\n          dtype,\n          sequence_length,\n          scope=bw_scope)\n\n  output_bw = _reverse_seq(tmp, sequence_length)\n  # Concat each of the forward/backward outputs\n  flat_output_fw = nest.flatten(output_fw)\n  flat_output_bw = nest.flatten(output_bw)\n\n  flat_outputs = tuple(\n      array_ops.concat([fw, bw], 1)\n      for fw, bw in zip(flat_output_fw, flat_output_bw))\n\n  outputs = nest.pack_sequence_as(\n      structure=output_fw, flat_sequence=flat_outputs)\n\n  return (outputs, output_state_fw, output_state_bw)\n", "framework": "tensorflow"}
{"repo_name": "yongtang/tensorflow", "file_path": "tensorflow/python/training/ftrl.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Ftrl-proximal for TensorFlow.\"\"\"\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import training_ops\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n@tf_export(v1=[\"train.FtrlOptimizer\"])\nclass FtrlOptimizer(optimizer.Optimizer):\n  \"\"\"Optimizer that implements the FTRL algorithm.\n\n  This version has support for both online L2 (McMahan et al., 2013) and\n  shrinkage-type L2, which is the addition of an L2 penalty\n  to the loss function.\n\n  References:\n    Ad-click prediction:\n      [McMahan et al., 2013](https://dl.acm.org/citation.cfm?id=2488200)\n      ([pdf](https://dl.acm.org/ft_gateway.cfm?id=2488200&ftid=1388399&dwn=1&CFID=32233078&CFTOKEN=d60fe57a294c056a-CB75C374-F915-E7A6-1573FBBC7BF7D526))\n  \"\"\"\n\n  def __init__(self,\n               learning_rate,\n               learning_rate_power=-0.5,\n               initial_accumulator_value=0.1,\n               l1_regularization_strength=0.0,\n               l2_regularization_strength=0.0,\n               use_locking=False,\n               name=\"Ftrl\",\n               accum_name=None,\n               linear_name=None,\n               l2_shrinkage_regularization_strength=0.0,\n               beta=None):\n    r\"\"\"Construct a new FTRL optimizer.\n\n    Args:\n      learning_rate: A float value or a constant float `Tensor`.\n      learning_rate_power: A float value, must be less or equal to zero.\n        Controls how the learning rate decreases during training. Use zero for\n        a fixed learning rate. See section 3.1 in (McMahan et al., 2013).\n      initial_accumulator_value: The starting value for accumulators.\n        Only zero or positive values are allowed.\n      l1_regularization_strength: A float value, must be greater than or\n        equal to zero.\n      l2_regularization_strength: A float value, must be greater than or\n        equal to zero.\n      use_locking: If `True` use locks for update operations.\n      name: Optional name prefix for the operations created when applying\n        gradients.  Defaults to \"Ftrl\".\n      accum_name: The suffix for the variable that keeps the gradient squared\n        accumulator.  If not present, defaults to name.\n      linear_name: The suffix for the variable that keeps the linear gradient\n        accumulator.  If not present, defaults to name + \"_1\".\n      l2_shrinkage_regularization_strength: A float value, must be greater than\n        or equal to zero. This differs from L2 above in that the L2 above is a\n        stabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\n        The FTRL formulation can be written as:\n        w_{t+1} = argmin_w(\\hat{g}_{1:t}w + L1*||w||_1 + L2*||w||_2^2), where\n        \\hat{g} = g + (2*L2_shrinkage*w), and g is the gradient of the loss\n        function w.r.t. the weights w.\n        Specifically, in the absence of L1 regularization, it is equivalent to\n        the following update rule:\n        w_{t+1} = w_t - lr_t / (beta + 2*L2*lr_t) * g_t -\n                  2*L2_shrinkage*lr_t / (beta + 2*L2*lr_t) * w_t\n        where lr_t is the learning rate at t.\n        When input is sparse shrinkage will only happen on the active weights.\n      beta: A float value; corresponds to the beta parameter in the paper.\n\n    Raises:\n      ValueError: If one of the arguments is invalid.\n\n    References:\n      Ad-click prediction:\n        [McMahan et al., 2013](https://dl.acm.org/citation.cfm?id=2488200)\n        ([pdf](https://dl.acm.org/ft_gateway.cfm?id=2488200&ftid=1388399&dwn=1&CFID=32233078&CFTOKEN=d60fe57a294c056a-CB75C374-F915-E7A6-1573FBBC7BF7D526))\n    \"\"\"\n    super(FtrlOptimizer, self).__init__(use_locking, name)\n\n    if initial_accumulator_value < 0.0:\n      raise ValueError(\n          \"initial_accumulator_value %f needs to be positive or zero\" %\n          initial_accumulator_value)\n    if learning_rate_power > 0.0:\n      raise ValueError(\"learning_rate_power %f needs to be negative or zero\" %\n                       learning_rate_power)\n    if l1_regularization_strength < 0.0:\n      raise ValueError(\n          \"l1_regularization_strength %f needs to be positive or zero\" %\n          l1_regularization_strength)\n    if l2_regularization_strength < 0.0:\n      raise ValueError(\n          \"l2_regularization_strength %f needs to be positive or zero\" %\n          l2_regularization_strength)\n    if l2_shrinkage_regularization_strength < 0.0:\n      raise ValueError(\n          \"l2_shrinkage_regularization_strength %f needs to be positive\"\n          \" or zero\" % l2_shrinkage_regularization_strength)\n\n    self._learning_rate = learning_rate\n    self._learning_rate_power = learning_rate_power\n    self._initial_accumulator_value = initial_accumulator_value\n    self._l1_regularization_strength = l1_regularization_strength\n    self._l2_regularization_strength = l2_regularization_strength\n    self._beta = (0.0 if beta is None else beta)\n    self._l2_shrinkage_regularization_strength = (\n        l2_shrinkage_regularization_strength)\n    self._learning_rate_tensor = None\n    self._learning_rate_power_tensor = None\n    self._l1_regularization_strength_tensor = None\n    self._adjusted_l2_regularization_strength_tensor = None\n    self._l2_shrinkage_regularization_strength_tensor = None\n    self._accum_name = accum_name\n    self._linear_name = linear_name\n\n  def _create_slots(self, var_list):\n    # Create the \"accum\" and \"linear\" slots.\n    def _accum_initializer(shape, dtype=dtypes.float32, partition_info=None):\n      del partition_info\n      return array_ops.ones(\n          shape=shape, dtype=dtype) * self._initial_accumulator_value\n    for v in var_list:\n      self._get_or_make_slot_with_initializer(\n          v, _accum_initializer, v.shape, v.dtype, \"accum\",\n          self._accum_name or self._name)\n      self._zeros_slot(v, \"linear\", self._linear_name or self._name)\n\n  def _prepare(self):\n    self._learning_rate_tensor = ops.convert_to_tensor(\n        self._learning_rate, name=\"learning_rate\")\n    self._l1_regularization_strength_tensor = ops.convert_to_tensor(\n        self._l1_regularization_strength, name=\"l1_regularization_strength\")\n    # L2 regularization strength with beta added in so that the underlying\n    # TensorFlow ops do not need to include that parameter.\n    self._adjusted_l2_regularization_strength_tensor = ops.convert_to_tensor(\n        self._l2_regularization_strength + self._beta /\n        (2. * math_ops.maximum(self._learning_rate, 1e-36)),\n        name=\"adjusted_l2_regularization_strength\")\n    assert self._adjusted_l2_regularization_strength_tensor is not None\n    self._beta_tensor = ops.convert_to_tensor(self._beta, name=\"beta\")\n    self._l2_shrinkage_regularization_strength_tensor = ops.convert_to_tensor(\n        self._l2_shrinkage_regularization_strength,\n        name=\"l2_shrinkage_regularization_strength\")\n    self._learning_rate_power_tensor = ops.convert_to_tensor(\n        self._learning_rate_power, name=\"learning_rate_power\")\n\n  def _apply_dense(self, grad, var):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.apply_ftrl(\n          var,\n          accum,\n          linear,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.apply_ftrl_v2(\n          var,\n          accum,\n          linear,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n\n  def _resource_apply_dense(self, grad, var):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.resource_apply_ftrl(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.resource_apply_ftrl_v2(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n\n  def _apply_sparse(self, grad, var):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.sparse_apply_ftrl(\n          var,\n          accum,\n          linear,\n          grad.values,\n          grad.indices,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.sparse_apply_ftrl_v2(\n          var,\n          accum,\n          linear,\n          grad.values,\n          grad.indices,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        grad.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n\n  def _resource_apply_sparse(self, grad, var, indices):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.resource_sparse_apply_ftrl(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          indices,\n          math_ops.cast(self._learning_rate_tensor, grad.dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor, grad.dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        grad.dtype),\n          math_ops.cast(self._learning_rate_power_tensor, grad.dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.resource_sparse_apply_ftrl_v2(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          indices,\n          math_ops.cast(self._learning_rate_tensor, grad.dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor, grad.dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        grad.dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        grad.dtype),\n          math_ops.cast(self._learning_rate_power_tensor, grad.dtype),\n          use_locking=self._use_locking)\n", "framework": "tensorflow"}
{"repo_name": "Intel-tensorflow/tensorflow", "file_path": "tensorflow/python/training/ftrl.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Ftrl-proximal for TensorFlow.\"\"\"\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import training_ops\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n@tf_export(v1=[\"train.FtrlOptimizer\"])\nclass FtrlOptimizer(optimizer.Optimizer):\n  \"\"\"Optimizer that implements the FTRL algorithm.\n\n  This version has support for both online L2 (McMahan et al., 2013) and\n  shrinkage-type L2, which is the addition of an L2 penalty\n  to the loss function.\n\n  References:\n    Ad-click prediction:\n      [McMahan et al., 2013](https://dl.acm.org/citation.cfm?id=2488200)\n      ([pdf](https://dl.acm.org/ft_gateway.cfm?id=2488200&ftid=1388399&dwn=1&CFID=32233078&CFTOKEN=d60fe57a294c056a-CB75C374-F915-E7A6-1573FBBC7BF7D526))\n  \"\"\"\n\n  def __init__(self,\n               learning_rate,\n               learning_rate_power=-0.5,\n               initial_accumulator_value=0.1,\n               l1_regularization_strength=0.0,\n               l2_regularization_strength=0.0,\n               use_locking=False,\n               name=\"Ftrl\",\n               accum_name=None,\n               linear_name=None,\n               l2_shrinkage_regularization_strength=0.0,\n               beta=None):\n    r\"\"\"Construct a new FTRL optimizer.\n\n    Args:\n      learning_rate: A float value or a constant float `Tensor`.\n      learning_rate_power: A float value, must be less or equal to zero.\n        Controls how the learning rate decreases during training. Use zero for\n        a fixed learning rate. See section 3.1 in (McMahan et al., 2013).\n      initial_accumulator_value: The starting value for accumulators.\n        Only zero or positive values are allowed.\n      l1_regularization_strength: A float value, must be greater than or\n        equal to zero.\n      l2_regularization_strength: A float value, must be greater than or\n        equal to zero.\n      use_locking: If `True` use locks for update operations.\n      name: Optional name prefix for the operations created when applying\n        gradients.  Defaults to \"Ftrl\".\n      accum_name: The suffix for the variable that keeps the gradient squared\n        accumulator.  If not present, defaults to name.\n      linear_name: The suffix for the variable that keeps the linear gradient\n        accumulator.  If not present, defaults to name + \"_1\".\n      l2_shrinkage_regularization_strength: A float value, must be greater than\n        or equal to zero. This differs from L2 above in that the L2 above is a\n        stabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\n        The FTRL formulation can be written as:\n        w_{t+1} = argmin_w(\\hat{g}_{1:t}w + L1*||w||_1 + L2*||w||_2^2), where\n        \\hat{g} = g + (2*L2_shrinkage*w), and g is the gradient of the loss\n        function w.r.t. the weights w.\n        Specifically, in the absence of L1 regularization, it is equivalent to\n        the following update rule:\n        w_{t+1} = w_t - lr_t / (beta + 2*L2*lr_t) * g_t -\n                  2*L2_shrinkage*lr_t / (beta + 2*L2*lr_t) * w_t\n        where lr_t is the learning rate at t.\n        When input is sparse shrinkage will only happen on the active weights.\n      beta: A float value; corresponds to the beta parameter in the paper.\n\n    Raises:\n      ValueError: If one of the arguments is invalid.\n\n    References:\n      Ad-click prediction:\n        [McMahan et al., 2013](https://dl.acm.org/citation.cfm?id=2488200)\n        ([pdf](https://dl.acm.org/ft_gateway.cfm?id=2488200&ftid=1388399&dwn=1&CFID=32233078&CFTOKEN=d60fe57a294c056a-CB75C374-F915-E7A6-1573FBBC7BF7D526))\n    \"\"\"\n    super(FtrlOptimizer, self).__init__(use_locking, name)\n\n    if initial_accumulator_value < 0.0:\n      raise ValueError(\n          \"initial_accumulator_value %f needs to be positive or zero\" %\n          initial_accumulator_value)\n    if learning_rate_power > 0.0:\n      raise ValueError(\"learning_rate_power %f needs to be negative or zero\" %\n                       learning_rate_power)\n    if l1_regularization_strength < 0.0:\n      raise ValueError(\n          \"l1_regularization_strength %f needs to be positive or zero\" %\n          l1_regularization_strength)\n    if l2_regularization_strength < 0.0:\n      raise ValueError(\n          \"l2_regularization_strength %f needs to be positive or zero\" %\n          l2_regularization_strength)\n    if l2_shrinkage_regularization_strength < 0.0:\n      raise ValueError(\n          \"l2_shrinkage_regularization_strength %f needs to be positive\"\n          \" or zero\" % l2_shrinkage_regularization_strength)\n\n    self._learning_rate = learning_rate\n    self._learning_rate_power = learning_rate_power\n    self._initial_accumulator_value = initial_accumulator_value\n    self._l1_regularization_strength = l1_regularization_strength\n    self._l2_regularization_strength = l2_regularization_strength\n    self._beta = (0.0 if beta is None else beta)\n    self._l2_shrinkage_regularization_strength = (\n        l2_shrinkage_regularization_strength)\n    self._learning_rate_tensor = None\n    self._learning_rate_power_tensor = None\n    self._l1_regularization_strength_tensor = None\n    self._adjusted_l2_regularization_strength_tensor = None\n    self._l2_shrinkage_regularization_strength_tensor = None\n    self._accum_name = accum_name\n    self._linear_name = linear_name\n\n  def _create_slots(self, var_list):\n    # Create the \"accum\" and \"linear\" slots.\n    def _accum_initializer(shape, dtype=dtypes.float32, partition_info=None):\n      del partition_info\n      return array_ops.ones(\n          shape=shape, dtype=dtype) * self._initial_accumulator_value\n    for v in var_list:\n      self._get_or_make_slot_with_initializer(\n          v, _accum_initializer, v.shape, v.dtype, \"accum\",\n          self._accum_name or self._name)\n      self._zeros_slot(v, \"linear\", self._linear_name or self._name)\n\n  def _prepare(self):\n    self._learning_rate_tensor = ops.convert_to_tensor(\n        self._learning_rate, name=\"learning_rate\")\n    self._l1_regularization_strength_tensor = ops.convert_to_tensor(\n        self._l1_regularization_strength, name=\"l1_regularization_strength\")\n    # L2 regularization strength with beta added in so that the underlying\n    # TensorFlow ops do not need to include that parameter.\n    self._adjusted_l2_regularization_strength_tensor = ops.convert_to_tensor(\n        self._l2_regularization_strength + self._beta /\n        (2. * math_ops.maximum(self._learning_rate, 1e-36)),\n        name=\"adjusted_l2_regularization_strength\")\n    assert self._adjusted_l2_regularization_strength_tensor is not None\n    self._beta_tensor = ops.convert_to_tensor(self._beta, name=\"beta\")\n    self._l2_shrinkage_regularization_strength_tensor = ops.convert_to_tensor(\n        self._l2_shrinkage_regularization_strength,\n        name=\"l2_shrinkage_regularization_strength\")\n    self._learning_rate_power_tensor = ops.convert_to_tensor(\n        self._learning_rate_power, name=\"learning_rate_power\")\n\n  def _apply_dense(self, grad, var):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.apply_ftrl(\n          var,\n          accum,\n          linear,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.apply_ftrl_v2(\n          var,\n          accum,\n          linear,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n\n  def _resource_apply_dense(self, grad, var):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.resource_apply_ftrl(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.resource_apply_ftrl_v2(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n\n  def _apply_sparse(self, grad, var):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.sparse_apply_ftrl(\n          var,\n          accum,\n          linear,\n          grad.values,\n          grad.indices,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.sparse_apply_ftrl_v2(\n          var,\n          accum,\n          linear,\n          grad.values,\n          grad.indices,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        grad.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n\n  def _resource_apply_sparse(self, grad, var, indices):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.resource_sparse_apply_ftrl(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          indices,\n          math_ops.cast(self._learning_rate_tensor, grad.dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor, grad.dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        grad.dtype),\n          math_ops.cast(self._learning_rate_power_tensor, grad.dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.resource_sparse_apply_ftrl_v2(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          indices,\n          math_ops.cast(self._learning_rate_tensor, grad.dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor, grad.dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        grad.dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        grad.dtype),\n          math_ops.cast(self._learning_rate_power_tensor, grad.dtype),\n          use_locking=self._use_locking)\n", "framework": "tensorflow"}
{"repo_name": "gautam1858/tensorflow", "file_path": "tensorflow/python/training/ftrl.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Ftrl-proximal for TensorFlow.\"\"\"\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import training_ops\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n@tf_export(v1=[\"train.FtrlOptimizer\"])\nclass FtrlOptimizer(optimizer.Optimizer):\n  \"\"\"Optimizer that implements the FTRL algorithm.\n\n  This version has support for both online L2 (McMahan et al., 2013) and\n  shrinkage-type L2, which is the addition of an L2 penalty\n  to the loss function.\n\n  References:\n    Ad-click prediction:\n      [McMahan et al., 2013](https://dl.acm.org/citation.cfm?id=2488200)\n      ([pdf](https://dl.acm.org/ft_gateway.cfm?id=2488200&ftid=1388399&dwn=1&CFID=32233078&CFTOKEN=d60fe57a294c056a-CB75C374-F915-E7A6-1573FBBC7BF7D526))\n  \"\"\"\n\n  def __init__(self,\n               learning_rate,\n               learning_rate_power=-0.5,\n               initial_accumulator_value=0.1,\n               l1_regularization_strength=0.0,\n               l2_regularization_strength=0.0,\n               use_locking=False,\n               name=\"Ftrl\",\n               accum_name=None,\n               linear_name=None,\n               l2_shrinkage_regularization_strength=0.0,\n               beta=None):\n    r\"\"\"Construct a new FTRL optimizer.\n\n    Args:\n      learning_rate: A float value or a constant float `Tensor`.\n      learning_rate_power: A float value, must be less or equal to zero.\n        Controls how the learning rate decreases during training. Use zero for\n        a fixed learning rate. See section 3.1 in (McMahan et al., 2013).\n      initial_accumulator_value: The starting value for accumulators.\n        Only zero or positive values are allowed.\n      l1_regularization_strength: A float value, must be greater than or\n        equal to zero.\n      l2_regularization_strength: A float value, must be greater than or\n        equal to zero.\n      use_locking: If `True` use locks for update operations.\n      name: Optional name prefix for the operations created when applying\n        gradients.  Defaults to \"Ftrl\".\n      accum_name: The suffix for the variable that keeps the gradient squared\n        accumulator.  If not present, defaults to name.\n      linear_name: The suffix for the variable that keeps the linear gradient\n        accumulator.  If not present, defaults to name + \"_1\".\n      l2_shrinkage_regularization_strength: A float value, must be greater than\n        or equal to zero. This differs from L2 above in that the L2 above is a\n        stabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\n        The FTRL formulation can be written as:\n        w_{t+1} = argmin_w(\\hat{g}_{1:t}w + L1*||w||_1 + L2*||w||_2^2), where\n        \\hat{g} = g + (2*L2_shrinkage*w), and g is the gradient of the loss\n        function w.r.t. the weights w.\n        Specifically, in the absence of L1 regularization, it is equivalent to\n        the following update rule:\n        w_{t+1} = w_t - lr_t / (beta + 2*L2*lr_t) * g_t -\n                  2*L2_shrinkage*lr_t / (beta + 2*L2*lr_t) * w_t\n        where lr_t is the learning rate at t.\n        When input is sparse shrinkage will only happen on the active weights.\n      beta: A float value; corresponds to the beta parameter in the paper.\n\n    Raises:\n      ValueError: If one of the arguments is invalid.\n\n    References:\n      Ad-click prediction:\n        [McMahan et al., 2013](https://dl.acm.org/citation.cfm?id=2488200)\n        ([pdf](https://dl.acm.org/ft_gateway.cfm?id=2488200&ftid=1388399&dwn=1&CFID=32233078&CFTOKEN=d60fe57a294c056a-CB75C374-F915-E7A6-1573FBBC7BF7D526))\n    \"\"\"\n    super(FtrlOptimizer, self).__init__(use_locking, name)\n\n    if initial_accumulator_value < 0.0:\n      raise ValueError(\n          \"initial_accumulator_value %f needs to be positive or zero\" %\n          initial_accumulator_value)\n    if learning_rate_power > 0.0:\n      raise ValueError(\"learning_rate_power %f needs to be negative or zero\" %\n                       learning_rate_power)\n    if l1_regularization_strength < 0.0:\n      raise ValueError(\n          \"l1_regularization_strength %f needs to be positive or zero\" %\n          l1_regularization_strength)\n    if l2_regularization_strength < 0.0:\n      raise ValueError(\n          \"l2_regularization_strength %f needs to be positive or zero\" %\n          l2_regularization_strength)\n    if l2_shrinkage_regularization_strength < 0.0:\n      raise ValueError(\n          \"l2_shrinkage_regularization_strength %f needs to be positive\"\n          \" or zero\" % l2_shrinkage_regularization_strength)\n\n    self._learning_rate = learning_rate\n    self._learning_rate_power = learning_rate_power\n    self._initial_accumulator_value = initial_accumulator_value\n    self._l1_regularization_strength = l1_regularization_strength\n    self._l2_regularization_strength = l2_regularization_strength\n    self._beta = (0.0 if beta is None else beta)\n    self._l2_shrinkage_regularization_strength = (\n        l2_shrinkage_regularization_strength)\n    self._learning_rate_tensor = None\n    self._learning_rate_power_tensor = None\n    self._l1_regularization_strength_tensor = None\n    self._adjusted_l2_regularization_strength_tensor = None\n    self._l2_shrinkage_regularization_strength_tensor = None\n    self._accum_name = accum_name\n    self._linear_name = linear_name\n\n  def _create_slots(self, var_list):\n    # Create the \"accum\" and \"linear\" slots.\n    def _accum_initializer(shape, dtype=dtypes.float32, partition_info=None):\n      del partition_info\n      return array_ops.ones(\n          shape=shape, dtype=dtype) * self._initial_accumulator_value\n    for v in var_list:\n      self._get_or_make_slot_with_initializer(\n          v, _accum_initializer, v.shape, v.dtype, \"accum\",\n          self._accum_name or self._name)\n      self._zeros_slot(v, \"linear\", self._linear_name or self._name)\n\n  def _prepare(self):\n    self._learning_rate_tensor = ops.convert_to_tensor(\n        self._learning_rate, name=\"learning_rate\")\n    self._l1_regularization_strength_tensor = ops.convert_to_tensor(\n        self._l1_regularization_strength, name=\"l1_regularization_strength\")\n    # L2 regularization strength with beta added in so that the underlying\n    # TensorFlow ops do not need to include that parameter.\n    self._adjusted_l2_regularization_strength_tensor = ops.convert_to_tensor(\n        self._l2_regularization_strength + self._beta /\n        (2. * math_ops.maximum(self._learning_rate, 1e-36)),\n        name=\"adjusted_l2_regularization_strength\")\n    assert self._adjusted_l2_regularization_strength_tensor is not None\n    self._beta_tensor = ops.convert_to_tensor(self._beta, name=\"beta\")\n    self._l2_shrinkage_regularization_strength_tensor = ops.convert_to_tensor(\n        self._l2_shrinkage_regularization_strength,\n        name=\"l2_shrinkage_regularization_strength\")\n    self._learning_rate_power_tensor = ops.convert_to_tensor(\n        self._learning_rate_power, name=\"learning_rate_power\")\n\n  def _apply_dense(self, grad, var):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.apply_ftrl(\n          var,\n          accum,\n          linear,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.apply_ftrl_v2(\n          var,\n          accum,\n          linear,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n\n  def _resource_apply_dense(self, grad, var):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.resource_apply_ftrl(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.resource_apply_ftrl_v2(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n\n  def _apply_sparse(self, grad, var):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.sparse_apply_ftrl(\n          var,\n          accum,\n          linear,\n          grad.values,\n          grad.indices,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.sparse_apply_ftrl_v2(\n          var,\n          accum,\n          linear,\n          grad.values,\n          grad.indices,\n          math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        var.dtype.base_dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        grad.dtype.base_dtype),\n          math_ops.cast(self._learning_rate_power_tensor, var.dtype.base_dtype),\n          use_locking=self._use_locking)\n\n  def _resource_apply_sparse(self, grad, var, indices):\n    accum = self.get_slot(var, \"accum\")\n    linear = self.get_slot(var, \"linear\")\n    if self._l2_shrinkage_regularization_strength <= 0.0:\n      return training_ops.resource_sparse_apply_ftrl(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          indices,\n          math_ops.cast(self._learning_rate_tensor, grad.dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor, grad.dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        grad.dtype),\n          math_ops.cast(self._learning_rate_power_tensor, grad.dtype),\n          use_locking=self._use_locking)\n    else:\n      return training_ops.resource_sparse_apply_ftrl_v2(\n          var.handle,\n          accum.handle,\n          linear.handle,\n          grad,\n          indices,\n          math_ops.cast(self._learning_rate_tensor, grad.dtype),\n          math_ops.cast(self._l1_regularization_strength_tensor, grad.dtype),\n          math_ops.cast(self._adjusted_l2_regularization_strength_tensor,\n                        grad.dtype),\n          math_ops.cast(self._l2_shrinkage_regularization_strength_tensor,\n                        grad.dtype),\n          math_ops.cast(self._learning_rate_power_tensor, grad.dtype),\n          use_locking=self._use_locking)\n", "framework": "tensorflow"}
{"repo_name": "gautam1858/tensorflow", "file_path": "tensorflow/python/training/input.py", "content": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Input pipeline.\n\nPlease see the [reading data\nhow-to](https://tensorflow.org/api_guides/python/reading_data)\nfor context.\n\"\"\"\n\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.layers import utils\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import io_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.summary import summary\nfrom tensorflow.python.training import queue_runner\nfrom tensorflow.python.util import deprecation\nfrom tensorflow.python.util.compat import collections_abc\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n# pylint: disable=protected-access\n_store_sparse = sparse_ops._add_sparse_to_tensors_map\n_store_many_sparse = sparse_ops._add_many_sparse_to_tensors_map\n_restore_sparse = sparse_ops._take_many_sparse_from_tensors_map\n# pylint: enable=protected-access\n\n\n@tf_export(\n    \"io.match_filenames_once\",\n    v1=[\"io.match_filenames_once\", \"train.match_filenames_once\"])\n@deprecation.deprecated_endpoints(\"train.match_filenames_once\")\ndef match_filenames_once(pattern, name=None):\n  \"\"\"Save the list of files matching pattern, so it is only computed once.\n\n  NOTE: The order of the files returned is deterministic.\n\n  Args:\n    pattern: A file pattern (glob), or 1D tensor of file patterns.\n    name: A name for the operations (optional).\n\n  Returns:\n    A variable that is initialized to the list of files matching the pattern(s).\n  \"\"\"\n  with ops.name_scope(name, \"matching_filenames\", [pattern]) as name:\n    return vs.variable(\n        name=name, initial_value=io_ops.matching_files(pattern),\n        trainable=False, validate_shape=False,\n        collections=[ops.GraphKeys.LOCAL_VARIABLES])\n\n\n@tf_export(v1=[\"train.limit_epochs\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\")\ndef limit_epochs(tensor, num_epochs=None, name=None):\n  \"\"\"Returns tensor `num_epochs` times and then raises an `OutOfRange` error.\n\n  Note: creates local counter `epochs`. Use `local_variables_initializer()` to\n  initialize local variables.\n\n  Args:\n    tensor: Any `Tensor`.\n    num_epochs: A positive integer (optional).  If specified, limits the number\n      of steps the output tensor may be evaluated.\n    name: A name for the operations (optional).\n\n  Returns:\n    tensor or `OutOfRange`.\n\n  Raises:\n    ValueError: if `num_epochs` is invalid.\n  \"\"\"\n  if num_epochs is None:\n    return tensor\n  if num_epochs <= 0:\n    raise ValueError(\"num_epochs must be > 0 not %d.\" % num_epochs)\n  with ops.name_scope(name, \"limit_epochs\", [tensor]) as name:\n    zero64 = constant_op.constant(0, dtype=dtypes.int64)\n    epochs = vs.variable(\n        zero64, name=\"epochs\", trainable=False,\n        collections=[ops.GraphKeys.LOCAL_VARIABLES])\n    counter = epochs.count_up_to(num_epochs)\n    with ops.control_dependencies([counter]):\n      return array_ops.identity(tensor, name=name)\n\n\n@tf_export(v1=[\"train.input_producer\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.from_tensor_slices(input_tensor).shuffle\"\n    \"(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If \"\n    \"`shuffle=False`, omit the `.shuffle(...)`.\")\ndef input_producer(input_tensor,\n                   element_shape=None,\n                   num_epochs=None,\n                   shuffle=True,\n                   seed=None,\n                   capacity=32,\n                   shared_name=None,\n                   summary_name=None,\n                   name=None,\n                   cancel_op=None):\n  \"\"\"Output the rows of `input_tensor` to a queue for an input pipeline.\n\n  Note: if `num_epochs` is not `None`, this function creates local counter\n  `epochs`. Use `local_variables_initializer()` to initialize local variables.\n\n  Args:\n    input_tensor: A tensor with the rows to produce. Must be at least\n      one-dimensional. Must either have a fully-defined shape, or\n      `element_shape` must be defined.\n    element_shape: (Optional.) A `TensorShape` representing the shape of a\n      row of `input_tensor`, if it cannot be inferred.\n    num_epochs: (Optional.) An integer. If specified `input_producer` produces\n      each row of `input_tensor` `num_epochs` times before generating an\n      `OutOfRange` error. If not specified, `input_producer` can cycle through\n      the rows of `input_tensor` an unlimited number of times.\n    shuffle: (Optional.) A boolean. If true, the rows are randomly shuffled\n      within each epoch.\n    seed: (Optional.) An integer. The seed to use if `shuffle` is true.\n    capacity: (Optional.) The capacity of the queue to be used for buffering\n      the input.\n    shared_name: (Optional.) If set, this queue will be shared under the given\n      name across multiple sessions.\n    summary_name: (Optional.) If set, a scalar summary for the current queue\n      size will be generated, using this name as part of the tag.\n    name: (Optional.) A name for queue.\n    cancel_op: (Optional.) Cancel op for the queue\n\n  Returns:\n    A queue with the output rows.  A `QueueRunner` for the queue is\n    added to the current `QUEUE_RUNNER` collection of the current\n    graph.\n\n  Raises:\n    ValueError: If the shape of the input cannot be inferred from the arguments.\n    RuntimeError: If called with eager execution enabled.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  if context.executing_eagerly():\n    raise RuntimeError(\n        \"Input pipelines based on Queues are not supported when eager execution\"\n        \" is enabled. Please use tf.data to ingest data into your model\"\n        \" instead.\")\n  with ops.name_scope(name, \"input_producer\", [input_tensor]):\n    input_tensor = ops.convert_to_tensor(input_tensor, name=\"input_tensor\")\n    element_shape = input_tensor.shape[1:].merge_with(element_shape)\n    if not element_shape.is_fully_defined():\n      raise ValueError(\"Either `input_tensor` must have a fully defined shape \"\n                       \"or `element_shape` must be specified\")\n\n    if shuffle:\n      input_tensor = random_ops.random_shuffle(input_tensor, seed=seed)\n\n    input_tensor = limit_epochs(input_tensor, num_epochs)\n\n    q = data_flow_ops.FIFOQueue(capacity=capacity,\n                                dtypes=[input_tensor.dtype.base_dtype],\n                                shapes=[element_shape],\n                                shared_name=shared_name, name=name)\n    enq = q.enqueue_many([input_tensor])\n    queue_runner.add_queue_runner(\n        queue_runner.QueueRunner(\n            q, [enq], cancel_op=cancel_op))\n    if summary_name is not None:\n      summary.scalar(summary_name,\n                     math_ops.cast(q.size(), dtypes.float32) * (1. / capacity))\n    return q\n\n\n@tf_export(v1=[\"train.string_input_producer\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.from_tensor_slices(string_tensor).shuffle\"\n    \"(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If \"\n    \"`shuffle=False`, omit the `.shuffle(...)`.\")\ndef string_input_producer(string_tensor,\n                          num_epochs=None,\n                          shuffle=True,\n                          seed=None,\n                          capacity=32,\n                          shared_name=None,\n                          name=None,\n                          cancel_op=None):\n  \"\"\"Output strings (e.g. filenames) to a queue for an input pipeline.\n\n  Note: if `num_epochs` is not `None`, this function creates local counter\n  `epochs`. Use `local_variables_initializer()` to initialize local variables.\n\n  Args:\n    string_tensor: A 1-D string tensor with the strings to produce.\n    num_epochs: An integer (optional). If specified, `string_input_producer`\n      produces each string from `string_tensor` `num_epochs` times before\n      generating an `OutOfRange` error. If not specified,\n      `string_input_producer` can cycle through the strings in `string_tensor`\n      an unlimited number of times.\n    shuffle: Boolean. If true, the strings are randomly shuffled within each\n      epoch.\n    seed: An integer (optional). Seed used if shuffle == True.\n    capacity: An integer. Sets the queue capacity.\n    shared_name: (optional). If set, this queue will be shared under the given\n      name across multiple sessions. All sessions open to the device which has\n      this queue will be able to access it via the shared_name. Using this in\n      a distributed setting means each name will only be seen by one of the\n      sessions which has access to this operation.\n    name: A name for the operations (optional).\n    cancel_op: Cancel op for the queue (optional).\n\n  Returns:\n    A queue with the output strings.  A `QueueRunner` for the Queue\n    is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n\n  Raises:\n    ValueError: If the string_tensor is a null Python list.  At runtime,\n    will fail with an assertion if string_tensor becomes a null tensor.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  not_null_err = \"string_input_producer requires a non-null input tensor\"\n  if not isinstance(string_tensor, ops.Tensor) and not string_tensor:\n    raise ValueError(not_null_err)\n\n  with ops.name_scope(name, \"input_producer\", [string_tensor]) as name:\n    string_tensor = ops.convert_to_tensor(string_tensor, dtype=dtypes.string)\n    with ops.control_dependencies([\n        control_flow_ops.Assert(\n            math_ops.greater(array_ops.size(string_tensor), 0),\n            [not_null_err])]):\n      string_tensor = array_ops.identity(string_tensor)\n    return input_producer(\n        input_tensor=string_tensor,\n        element_shape=[],\n        num_epochs=num_epochs,\n        shuffle=shuffle,\n        seed=seed,\n        capacity=capacity,\n        shared_name=shared_name,\n        name=name,\n        summary_name=\"fraction_of_%d_full\" % capacity,\n        cancel_op=cancel_op)\n\n\n@tf_export(v1=[\"train.range_input_producer\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If \"\n    \"`shuffle=False`, omit the `.shuffle(...)`.\")\ndef range_input_producer(limit, num_epochs=None, shuffle=True, seed=None,\n                         capacity=32, shared_name=None, name=None):\n  \"\"\"Produces the integers from 0 to limit-1 in a queue.\n\n  Note: if `num_epochs` is not `None`, this function creates local counter\n  `epochs`. Use `local_variables_initializer()` to initialize local variables.\n\n  Args:\n    limit: An int32 scalar tensor.\n    num_epochs: An integer (optional). If specified, `range_input_producer`\n      produces each integer `num_epochs` times before generating an\n      OutOfRange error. If not specified, `range_input_producer` can cycle\n      through the integers an unlimited number of times.\n    shuffle: Boolean. If true, the integers are randomly shuffled within each\n      epoch.\n    seed: An integer (optional). Seed used if shuffle == True.\n    capacity: An integer. Sets the queue capacity.\n    shared_name: (optional). If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: A name for the operations (optional).\n\n  Returns:\n    A Queue with the output integers.  A `QueueRunner` for the Queue\n    is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  with ops.name_scope(name, \"input_producer\", [limit]) as name:\n    range_tensor = math_ops.range(limit)\n    return input_producer(\n        range_tensor, [], num_epochs, shuffle, seed, capacity,\n        shared_name, \"fraction_of_%d_full\" % capacity, name)\n\n\n@tf_export(v1=[\"train.slice_input_producer\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle\"\n    \"(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If \"\n    \"`shuffle=False`, omit the `.shuffle(...)`.\")\ndef slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None,\n                         capacity=32, shared_name=None, name=None):\n  \"\"\"Produces a slice of each `Tensor` in `tensor_list`.\n\n  Implemented using a Queue -- a `QueueRunner` for the Queue\n  is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n\n  Args:\n    tensor_list: A list of `Tensor` objects. Every `Tensor` in\n      `tensor_list` must have the same size in the first dimension.\n    num_epochs: An integer (optional). If specified, `slice_input_producer`\n      produces each slice `num_epochs` times before generating\n      an `OutOfRange` error. If not specified, `slice_input_producer` can cycle\n      through the slices an unlimited number of times.\n    shuffle: Boolean. If true, the integers are randomly shuffled within each\n      epoch.\n    seed: An integer (optional). Seed used if shuffle == True.\n    capacity: An integer. Sets the queue capacity.\n    shared_name: (optional). If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: A name for the operations (optional).\n\n  Returns:\n    A list of tensors, one for each element of `tensor_list`.  If the tensor\n    in `tensor_list` has shape `[N, a, b, .., z]`, then the corresponding output\n    tensor will have shape `[a, b, ..., z]`.\n\n  Raises:\n    ValueError: if `slice_input_producer` produces nothing from `tensor_list`.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  with ops.name_scope(name, \"input_producer\", tensor_list):\n    tensor_list = ops.convert_n_to_tensor_or_indexed_slices(tensor_list)\n    if not tensor_list:\n      raise ValueError(\n          \"Expected at least one tensor in slice_input_producer().\")\n    range_size = array_ops.shape(tensor_list[0])[0]\n    # TODO(josh11b): Add an assertion that the first dimension of\n    # everything in TensorList matches. Maybe just check the inferred shapes?\n    queue = range_input_producer(range_size, num_epochs=num_epochs,\n                                 shuffle=shuffle, seed=seed, capacity=capacity,\n                                 shared_name=shared_name)\n    index = queue.dequeue()\n    output = [array_ops.gather(t, index) for t in tensor_list]\n    return output\n\n\n# Helpers for the batching functions ------------------------------------------\n\n\ndef _flatten(tensor_list_list):\n  return [tensor for tensor_list in tensor_list_list for tensor in tensor_list]\n\n\nclass _SparseMetaData(object):\n  \"\"\"Store information about the Tensor: Is it sparse?, map_op, and rank.\"\"\"\n\n  def __init__(self, sparse, map_op, rank):\n    \"\"\"Create the metadata.\n\n    Args:\n      sparse: Python boolean.\n      map_op: The `Operation` that created the `SparseTensorsMap` in question.\n        This Op contains information about the underlying Map object and the\n        dtype of the original data.\n      rank: The statically known rank of the `SparseTensor`.\n    \"\"\"\n    self._sparse = sparse\n    self._map_op = map_op\n    self._rank = tensor_shape.as_dimension(rank)\n\n  def __eq__(self, other):\n    if self.sparse != other.sparse:\n      return False\n    if not self.sparse:\n      return True\n    # If map_ops are not the same, the data source is not the same.\n    if (self.map_op is not None) != (other.map_op is not None):\n      return False\n    if self.map_op != other.map_op:\n      return False\n    if not self.rank.is_compatible_with(other.rank):\n      return False\n    return True\n\n  def __ne__(self, other):\n    return not self.__eq__(other)\n\n  def __str__(self):\n    return \"[SparseMetaData(%s, %s, %s)]\" % (self.sparse, self.map_op.name,\n                                             self.rank)\n\n  def merge_with(self, other):\n    if self != other:\n      raise ValueError(\"SparseMetaData objects are incompatible: %s vs. %s\"\n                       % (self, other))\n    if self.sparse:\n      self.rank.merge_with(other.rank)\n    return self\n\n  @property\n  def map_op(self):\n    return self._map_op\n\n  @property\n  def sparse(self):\n    return self._sparse\n\n  @property\n  def rank(self):\n    return self._rank\n\n\ndef _as_tensor_list(tensors):\n  if isinstance(tensors, dict):\n    return [tensors[k] for k in sorted(tensors, key=str)]\n  else:\n    return tensors\n\n\ndef _as_tensor_list_list(tensors_list):\n  if not tensors_list:\n    raise ValueError(\"Expected at least one set of tensors\")\n  if isinstance(tensors_list[0], dict):\n    expected_keys = set(tensors_list[0].keys())\n    for tensors in tensors_list[1:]:\n      if set(tensors.keys()) != expected_keys:\n        raise ValueError(\"All dictionaries in tensors_list must have \"\n                         \"the same keys\")\n    return [_as_tensor_list(tensors) for tensors in tensors_list]\n  else:\n    return tensors_list\n\n\ndef _as_original_type(original_tensors, tensor_list):\n  if isinstance(original_tensors, dict):\n    if len(original_tensors) == 1:\n      # tensor_list is bogusly returned as a single tensor if only one tensor\n      # was enqueued.  Make it a list again.  See b/28117485.\n      tensor_list = [tensor_list]\n    return {k: tensor_list[i]\n            for i, k in enumerate(sorted(original_tensors, key=str))}\n  else:\n    return tensor_list\n\n\ndef _store_sparse_tensors(tensor_list, enqueue_many, keep_input,\n                          shared_map_ops=None):\n  \"\"\"Store SparseTensors for feeding into batch, etc.\n\n  If `shared_map_ops` is provided, the underlying `SparseTensorsMap` objects\n  are reused (shared).  This argument is useful for, e.g., `batch_join`\n  where multiple enqueue operations write to the same Queue component,\n  and another (dequeue) thread reads from that same location and must then\n  restore the associated `SparseTensor` objects.  In this case, the sparse\n  restore must have a single `SparseTensorMap` from which to read out the\n  handles; so a single `SparseTensorMap` must be shared for storing\n  across the multiple enqueue operations.  This sharing is performed by\n  calling `_store_sparse_tensors` the first time with `shared_map_ops=None`,\n  and then in subsequent times with this value set to the list of `Operation`\n  objects created in the first call.\n\n  Args:\n    tensor_list: List of `Tensor` and `SparseTensor` objects.\n    enqueue_many: Python `Boolean`.\n    keep_input: Must be a scalar bool Tensor (not a Python bool). If False,\n      don't store.\n    shared_map_ops: (optional) List of `Operation` objects from a previous\n      call to `_store_sparse_tensors`.  If not `None`, the op types should be\n      one of `AddSparseToTensorsMap` or `AddManySparseToTensorsMap` in the\n      locations corresponding to `SparseTensors` in `tensor_list`.\n\n  Returns:\n    A tuple `(stored_list, sparse_info_list)` where `stored_list` is a list\n    of `Tensor` objects (same length as `tensor_list`) and `sparse_info_list`\n    is a list of the same length of `_SparseMetaData` objects.\n  \"\"\"\n  maybe_shared_map_ops = shared_map_ops or [None] * len(tensor_list)\n\n  def _sparse_meta_data(t, storing_op, map_op):\n    if not isinstance(t, sparse_tensor.SparseTensor):\n      return _SparseMetaData(False, None, None)\n    rank = t.dense_shape.shape.with_rank(1).dims[0]\n    if enqueue_many:\n      rank -= 1\n    # If a shared map_op was provided, use that. Otherwise use the name of\n    # the operation used to store the SparseTensor.\n    return _SparseMetaData(\n        sparse=True, map_op=map_op or storing_op, rank=rank)\n\n  def _maybe_store(t, shared_map_op):\n    \"\"\"Store Sparse tensor, if necessary.\"\"\"\n    if not isinstance(t, sparse_tensor.SparseTensor):\n      return t\n    map_op_name = shared_map_op.name if shared_map_op else None\n    def _maybe_store_sparse(t, map_op_name, keep_input):\n      \"\"\"Conditionally store a single sparse Tensor.\"\"\"\n      return utils.smart_cond(\n          keep_input,\n          lambda: _store_sparse(t, shared_name=map_op_name),\n          lambda: constant_op.constant(-1, dtypes.int64))\n    def _maybe_store_many_sparse(t, map_op_name, keep_input):\n      \"\"\"Conditionally store multiple sparse Tensors.\"\"\"\n      out_tensor = utils.smart_cond(\n          keep_input,\n          lambda: _store_many_sparse(t, shared_name=map_op_name),\n          lambda: -1 * array_ops.ones(array_ops.shape(t)[0:1], dtypes.int64))\n      out_tensor.set_shape([None])  # necessary when t.ndims is unknown\n      return out_tensor\n    def _sparse_values_to_keep(t, keep_input):\n      \"\"\"Convert a per-row `keep_input` vector to a per-value one.\"\"\"\n      # Get the rows of every value in the sparse Tensor.\n      row_values = t.indices[:, 0]\n      # The value should be kept iff the row should be kept.\n      return array_ops.gather(keep_input, row_values)\n    if keep_input.shape.ndims == 1:\n      t = sparse_ops.sparse_retain(t, _sparse_values_to_keep(t, keep_input))\n      store_f = lambda t, name, _: _store_many_sparse(t, shared_name=name)\n    elif enqueue_many:\n      store_f = _maybe_store_many_sparse\n    else:\n      store_f = _maybe_store_sparse\n    return store_f(t, map_op_name, keep_input)\n\n  stored_list = [\n      _maybe_store(t, shared_map_op) for t, shared_map_op\n      in zip(tensor_list, maybe_shared_map_ops)]\n  # Since the output of `_store{_many}_sparse is wrapped in a tf.cond `Merge`,\n  # we can't just get the Op of the resulting tensor.\n  def _sparse_op(stored):\n    for input_tensor in stored.op.inputs:\n      if input_tensor.op.type in (\"AddSparseToTensorsMap\",\n                                  \"AddManySparseToTensorsMap\"):\n        return input_tensor.op\n    # If there was no sparse input, then the original stored Tensor wasn't\n    # sparse and we can just return the original Tensor's Op.\n    return stored.op\n  sparse_info_list = [\n      _sparse_meta_data(t, _sparse_op(stored), shared_map_op)\n      for t, stored, shared_map_op\n      in zip(tensor_list, stored_list, maybe_shared_map_ops)]\n  # Expand dims of stored tensors by 1 for proper enqueue shape\n  stored_list = [\n      array_ops.expand_dims(s, [-1]) if s_info.sparse else s\n      for s, s_info in zip(stored_list, sparse_info_list)]\n  return stored_list, sparse_info_list\n\n\ndef _store_sparse_tensors_join(tensor_list_list, enqueue_many, keep_input):\n  \"\"\"Store SparseTensors for feeding into batch_join, etc.\"\"\"\n  (s0, sparse_info_list) = _store_sparse_tensors(\n      tensor_list_list[0], enqueue_many, keep_input)\n  stored_list_list = [s0]\n  for tensor_list in tensor_list_list[1:]:\n    s, sparse_info_candidate = _store_sparse_tensors(\n        tensor_list, enqueue_many, keep_input,\n        [st.map_op for st in sparse_info_list])\n    if sparse_info_list != sparse_info_candidate:\n      raise ValueError(\"Inconsistent SparseTensors list: %s vs. %s\"\n                       % (tensor_list_list[0], tensor_list))\n    sparse_info_list = [\n        info.merge_with(candidate)\n        for (info, candidate) in zip(sparse_info_list, sparse_info_candidate)]\n    stored_list_list.append(s)\n\n  return (stored_list_list, sparse_info_list)\n\n\ndef _restore_sparse_tensors(stored_list, sparse_info_list):\n  \"\"\"Restore SparseTensors after dequeue in batch, batch_join, etc.\"\"\"\n  received_sequence = isinstance(stored_list, collections_abc.Sequence)\n  if not received_sequence:\n    stored_list = (stored_list,)\n  tensors = [\n      _restore_sparse(sparse_map_op=info.map_op,\n                      sparse_handles=array_ops.squeeze(s, [1]),\n                      rank=tensor_shape.dimension_value(info.rank + 1))\n      if info.sparse else s\n      for (s, info) in zip(stored_list, sparse_info_list)]\n  has_st = any(isinstance(x, sparse_tensor.SparseTensor) for x in tensors)\n  if has_st:\n    t_values = [\n        x.values if isinstance(x, sparse_tensor.SparseTensor)\n        else x\n        for x in tensors]\n    with_deps = lambda x: control_flow_ops.with_dependencies(t_values, x)\n    ensure_restore_tensors = [\n        sparse_tensor.SparseTensor(indices=with_deps(x.indices),\n                                   values=with_deps(x.values),\n                                   dense_shape=with_deps(x.dense_shape))\n        if isinstance(x, sparse_tensor.SparseTensor)\n        else with_deps(x)\n        for x in tensors]\n  else:\n    ensure_restore_tensors = tensors\n  return ensure_restore_tensors if received_sequence else tensors[0]\n\n\ndef _validate(tensor_list):\n  tensor_list = ops.convert_n_to_tensor_or_indexed_slices(tensor_list)\n  if not tensor_list:\n    raise ValueError(\"Expected at least one tensor in batch().\")\n  return tensor_list\n\n\ndef _validate_join(tensor_list_list):\n  tensor_list_list = [ops.convert_n_to_tensor_or_indexed_slices(tl)\n                      for tl in tensor_list_list]\n  if not tensor_list_list:\n    raise ValueError(\"Expected at least one input in batch_join().\")\n  return tensor_list_list\n\n\ndef _validate_keep_input(keep_input, enqueue_many):\n  \"\"\"Validate `keep_input` argument to conditional batching functions.\"\"\"\n  keep_input = ops.convert_to_tensor(keep_input)\n  if keep_input.shape.ndims is None:\n    raise ValueError(\n        \"`keep_input` dimensions must be known at graph construction.\")\n  if not enqueue_many and keep_input.shape.ndims == 1:\n    raise ValueError(\n        \"`keep_input` cannot be a vector when `enqueue_many=False`.\")\n  if keep_input.shape.ndims > 1:\n    raise ValueError(\"`keep_input` must be 0 or 1 dimensions.\")\n  return keep_input\n\n\ndef _dtypes(tensor_list_list):\n  all_types = [[t.dtype for t in tl] for tl in tensor_list_list]\n  types = all_types[0]\n  for other_types in all_types[1:]:\n    if other_types != types:\n      raise TypeError(\"Expected types to be consistent: %s vs. %s.\" %\n                      (\", \".join(x.name for x in types),\n                       \", \".join(x.name for x in other_types)))\n  return types\n\n\ndef _merge_shapes(shape_list, enqueue_many):\n  shape_list = [tensor_shape.as_shape(s) for s in shape_list]\n  if enqueue_many:\n    # We want the shapes without the leading batch dimension.\n    shape_list = [s.with_rank_at_least(1)[1:] for s in shape_list]\n  merged_shape = shape_list[0]\n  for s in shape_list[1:]:\n    merged_shape.merge_with(s)\n  return merged_shape.as_list()\n\n\ndef _shapes(tensor_list_list, shapes, enqueue_many):\n  \"\"\"Calculate and merge the shapes of incoming tensors.\n\n  Args:\n    tensor_list_list: List of tensor lists.\n    shapes: List of shape tuples corresponding to tensors within the lists.\n    enqueue_many: Boolean describing whether shapes will be enqueued as\n      batches or individual entries.\n\n  Returns:\n    A list of shapes aggregating shape inference info from `tensor_list_list`,\n    or returning `shapes` if it is not `None`.\n\n  Raises:\n    ValueError: If any of the inferred shapes in `tensor_list_list` lack a\n      well defined rank.\n  \"\"\"\n  if shapes is None:\n    len0 = len(tensor_list_list[0])\n\n    for tl in tensor_list_list:\n      for i in range(len0):\n        if tl[i].shape.ndims is None:\n          raise ValueError(\"Cannot infer Tensor's rank: %s\" % tl[i])\n\n    shapes = [\n        _merge_shapes([tl[i].shape.as_list()\n                       for tl in tensor_list_list], enqueue_many)\n        for i in range(len0)\n    ]\n  return shapes\n\n\ndef _select_which_to_enqueue(tensor_list, keep_input):\n  \"\"\"Select which examples to enqueue based on vector `keep_input`.\"\"\"\n  select_i = math_ops.cast(keep_input, dtypes.int32)\n  tensor_list = [\n      data_flow_ops.dynamic_partition(x, select_i, num_partitions=2)[1]\n      for x in tensor_list]\n  return tensor_list\n\n\ndef _enqueue_join(queue, tensor_list_list, enqueue_many, keep_input):\n  \"\"\"Enqueue `tensor_list_list` in `queue`.\"\"\"\n  if enqueue_many:\n    enqueue_fn = queue.enqueue_many\n  else:\n    enqueue_fn = queue.enqueue\n  if keep_input.shape.ndims == 1:\n    enqueue_ops = [enqueue_fn(_select_which_to_enqueue(x, keep_input))\n                   for x in tensor_list_list]\n  else:\n    enqueue_ops = [utils.smart_cond(\n        keep_input,\n        lambda: enqueue_fn(tl),  # pylint:disable=cell-var-from-loop\n        control_flow_ops.no_op) for tl in tensor_list_list]\n  queue_runner.add_queue_runner(queue_runner.QueueRunner(queue, enqueue_ops))\n\n\ndef _enqueue(queue, tensor_list, threads, enqueue_many, keep_input):\n  \"\"\"Enqueue `tensor_list` in `queue`.\"\"\"\n  if enqueue_many:\n    enqueue_fn = queue.enqueue_many\n  else:\n    enqueue_fn = queue.enqueue\n  if keep_input.shape.ndims == 1:\n    enqueue_ops = [\n        enqueue_fn(_select_which_to_enqueue(tensor_list, keep_input))] * threads\n  else:\n    enqueue_ops = [utils.smart_cond(\n        keep_input,\n        lambda: enqueue_fn(tensor_list),\n        control_flow_ops.no_op)] * threads\n  queue_runner.add_queue_runner(queue_runner.QueueRunner(queue, enqueue_ops))\n\n\ndef _which_queue(dynamic_pad):\n  return (data_flow_ops.PaddingFIFOQueue if dynamic_pad\n          else data_flow_ops.FIFOQueue)\n\n\ndef _batch(tensors, batch_size, keep_input, num_threads=1, capacity=32,\n           enqueue_many=False, shapes=None, dynamic_pad=False,\n           allow_smaller_final_batch=False, shared_name=None,\n           name=None):\n  \"\"\"Helper function for `batch` and `maybe_batch`.\"\"\"\n  if context.executing_eagerly():\n    raise ValueError(\n        \"Input pipelines based on Queues are not supported when eager execution\"\n        \" is enabled. Please use tf.data to ingest data into your model\"\n        \" instead.\")\n  tensor_list = _as_tensor_list(tensors)\n  with ops.name_scope(name, \"batch\", list(tensor_list) + [keep_input]) as name:\n    tensor_list = _validate(tensor_list)\n    keep_input = _validate_keep_input(keep_input, enqueue_many)\n    (tensor_list, sparse_info) = _store_sparse_tensors(\n        tensor_list, enqueue_many, keep_input)\n    types = _dtypes([tensor_list])\n    shapes = _shapes([tensor_list], shapes, enqueue_many)\n    # TODO(josh11b,mrry): Switch to BatchQueue once it is written.\n    queue = _which_queue(dynamic_pad)(\n        capacity=capacity, dtypes=types, shapes=shapes, shared_name=shared_name)\n    _enqueue(queue, tensor_list, num_threads, enqueue_many, keep_input)\n    summary.scalar(\n        \"fraction_of_%d_full\" % capacity,\n        math_ops.cast(queue.size(), dtypes.float32) * (1. / capacity))\n\n    if allow_smaller_final_batch:\n      dequeued = queue.dequeue_up_to(batch_size, name=name)\n    else:\n      dequeued = queue.dequeue_many(batch_size, name=name)\n    dequeued = _restore_sparse_tensors(dequeued, sparse_info)\n    return _as_original_type(tensors, dequeued)\n\n\n# TODO(josh11b): Add a thread_multiplier or num_threads (that has to be\n# a multiple of len(tensor_list_list)?) parameter, to address the use\n# case where you want more parallelism than you can support different\n# readers (either because you don't have that many files or can't\n# read that many files in parallel due to the number of seeks required).\n# Once this is done, batch() can be written as a call to batch_join().\ndef _batch_join(tensors_list, batch_size, keep_input, capacity=32,\n                enqueue_many=False, shapes=None, dynamic_pad=False,\n                allow_smaller_final_batch=False, shared_name=None, name=None):\n  \"\"\"Helper function for `batch_join` and `maybe_batch_join`.\"\"\"\n  if context.executing_eagerly():\n    raise ValueError(\n        \"Input pipelines based on Queues are not supported when eager execution\"\n        \" is enabled. Please use tf.data to ingest data into your model\"\n        \" instead.\")\n  tensor_list_list = _as_tensor_list_list(tensors_list)\n  with ops.name_scope(name, \"batch_join\",\n                      _flatten(tensor_list_list) + [keep_input]) as name:\n    tensor_list_list = _validate_join(tensor_list_list)\n    keep_input = _validate_keep_input(keep_input, enqueue_many)\n    tensor_list_list, sparse_info = _store_sparse_tensors_join(\n        tensor_list_list, enqueue_many, keep_input)\n    types = _dtypes(tensor_list_list)\n    shapes = _shapes(tensor_list_list, shapes, enqueue_many)\n    # TODO(josh11b,mrry): Switch to BatchQueue once it is written.\n    queue = _which_queue(dynamic_pad)(\n        capacity=capacity, dtypes=types, shapes=shapes, shared_name=shared_name)\n    _enqueue_join(queue, tensor_list_list, enqueue_many, keep_input)\n    summary.scalar(\n        \"fraction_of_%d_full\" % capacity,\n        math_ops.cast(queue.size(), dtypes.float32) * (1. / capacity))\n\n    if allow_smaller_final_batch:\n      dequeued = queue.dequeue_up_to(batch_size, name=name)\n    else:\n      dequeued = queue.dequeue_many(batch_size, name=name)\n    dequeued = _restore_sparse_tensors(dequeued, sparse_info)\n    # tensors_list was validated to not be empty.\n    return _as_original_type(tensors_list[0], dequeued)\n\n\ndef _shuffle_batch(tensors, batch_size, capacity, min_after_dequeue,\n                   keep_input, num_threads=1, seed=None, enqueue_many=False,\n                   shapes=None, allow_smaller_final_batch=False,\n                   shared_name=None, name=None):\n  \"\"\"Helper function for `shuffle_batch` and `maybe_shuffle_batch`.\"\"\"\n  if context.executing_eagerly():\n    raise ValueError(\n        \"Input pipelines based on Queues are not supported when eager execution\"\n        \" is enabled. Please use tf.data to ingest data into your model\"\n        \" instead.\")\n  tensor_list = _as_tensor_list(tensors)\n  with ops.name_scope(name, \"shuffle_batch\",\n                      list(tensor_list) + [keep_input]) as name:\n    if capacity <= min_after_dequeue:\n      raise ValueError(\"capacity %d must be bigger than min_after_dequeue %d.\"\n                       % (capacity, min_after_dequeue))\n    tensor_list = _validate(tensor_list)\n    keep_input = _validate_keep_input(keep_input, enqueue_many)\n    tensor_list, sparse_info = _store_sparse_tensors(\n        tensor_list, enqueue_many, keep_input)\n    types = _dtypes([tensor_list])\n    shapes = _shapes([tensor_list], shapes, enqueue_many)\n    queue = data_flow_ops.RandomShuffleQueue(\n        capacity=capacity, min_after_dequeue=min_after_dequeue, seed=seed,\n        dtypes=types, shapes=shapes, shared_name=shared_name)\n    _enqueue(queue, tensor_list, num_threads, enqueue_many, keep_input)\n    full = (math_ops.cast(\n        math_ops.maximum(0, queue.size() - min_after_dequeue), dtypes.float32) *\n            (1. / (capacity - min_after_dequeue)))\n    # Note that name contains a '/' at the end so we intentionally do not place\n    # a '/' after %s below.\n    summary_name = (\n        \"fraction_over_%d_of_%d_full\" %\n        (min_after_dequeue, capacity - min_after_dequeue))\n    summary.scalar(summary_name, full)\n\n    if allow_smaller_final_batch:\n      dequeued = queue.dequeue_up_to(batch_size, name=name)\n    else:\n      dequeued = queue.dequeue_many(batch_size, name=name)\n    dequeued = _restore_sparse_tensors(dequeued, sparse_info)\n    return _as_original_type(tensors, dequeued)\n\n\ndef _shuffle_batch_join(tensors_list, batch_size, capacity,\n                        min_after_dequeue, keep_input, seed=None,\n                        enqueue_many=False, shapes=None,\n                        allow_smaller_final_batch=False, shared_name=None,\n                        name=None):\n  \"\"\"Helper function for `shuffle_batch_join` and `maybe_shuffle_batch_join`.\"\"\"\n  if context.executing_eagerly():\n    raise ValueError(\n        \"Input pipelines based on Queues are not supported when eager execution\"\n        \" is enabled. Please use tf.data to ingest data into your model\"\n        \" instead.\")\n  tensor_list_list = _as_tensor_list_list(tensors_list)\n  with ops.name_scope(name, \"shuffle_batch_join\",\n                      _flatten(tensor_list_list) + [keep_input]) as name:\n    tensor_list_list = _validate_join(tensor_list_list)\n    keep_input = _validate_keep_input(keep_input, enqueue_many)\n    tensor_list_list, sparse_info = _store_sparse_tensors_join(\n        tensor_list_list, enqueue_many, keep_input)\n    types = _dtypes(tensor_list_list)\n    shapes = _shapes(tensor_list_list, shapes, enqueue_many)\n    queue = data_flow_ops.RandomShuffleQueue(\n        capacity=capacity, min_after_dequeue=min_after_dequeue, seed=seed,\n        dtypes=types, shapes=shapes, shared_name=shared_name)\n    _enqueue_join(queue, tensor_list_list, enqueue_many, keep_input)\n    full = (math_ops.cast(\n        math_ops.maximum(0, queue.size() - min_after_dequeue), dtypes.float32) *\n            (1. / (capacity - min_after_dequeue)))\n    # Note that name contains a '/' at the end so we intentionally do not place\n    # a '/' after %s below.\n    summary_name = (\n        \"fraction_over_%d_of_%d_full\" %\n        (min_after_dequeue, capacity - min_after_dequeue))\n    summary.scalar(summary_name, full)\n\n    if allow_smaller_final_batch:\n      dequeued = queue.dequeue_up_to(batch_size, name=name)\n    else:\n      dequeued = queue.dequeue_many(batch_size, name=name)\n    dequeued = _restore_sparse_tensors(dequeued, sparse_info)\n    # tensors_list was validated to not be empty.\n    return _as_original_type(tensors_list[0], dequeued)\n\n# Batching functions ----------------------------------------------------------\n\n\n@tf_export(v1=[\"train.batch\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if \"\n    \"`dynamic_pad=True`).\")\ndef batch(tensors, batch_size, num_threads=1, capacity=32,\n          enqueue_many=False, shapes=None, dynamic_pad=False,\n          allow_smaller_final_batch=False, shared_name=None, name=None):\n  \"\"\"Creates batches of tensors in `tensors`.\n\n  The argument `tensors` can be a list or a dictionary of tensors.\n  The value returned by the function will be of the same type\n  as `tensors`.\n\n  This function is implemented using a queue. A `QueueRunner` for the\n  queue is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n\n  If `enqueue_many` is `False`, `tensors` is assumed to represent a single\n  example.  An input tensor with shape `[x, y, z]` will be output as a tensor\n  with shape `[batch_size, x, y, z]`.\n\n  If `enqueue_many` is `True`, `tensors` is assumed to represent a batch of\n  examples, where the first dimension is indexed by example, and all members of\n  `tensors` should have the same size in the first dimension.  If an input\n  tensor has shape `[*, x, y, z]`, the output will have shape `[batch_size, x,\n  y, z]`.  The `capacity` argument controls the how long the prefetching is\n  allowed to grow the queues.\n\n  The returned operation is a dequeue operation and will throw\n  `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n  operation is feeding another input queue, its queue runner will catch\n  this exception, however, if this operation is used in your main thread\n  you are responsible for catching this yourself.\n\n  *N.B.:* If `dynamic_pad` is `False`, you must ensure that either\n  (i) the `shapes` argument is passed, or (ii) all of the tensors in\n  `tensors` must have fully-defined shapes. `ValueError` will be\n  raised if neither of these conditions holds.\n\n  If `dynamic_pad` is `True`, it is sufficient that the *rank* of the\n  tensors is known, but individual dimensions may have shape `None`.\n  In this case, for each enqueue the dimensions with value `None`\n  may have a variable length; upon dequeue, the output tensors will be padded\n  on the right to the maximum shape of the tensors in the current minibatch.\n  For numbers, this padding takes value 0.  For strings, this padding is\n  the empty string.  See `PaddingFIFOQueue` for more info.\n\n  If `allow_smaller_final_batch` is `True`, a smaller batch value than\n  `batch_size` is returned when the queue is closed and there are not enough\n  elements to fill the batch, otherwise the pending elements are discarded.\n  In addition, all output tensors' static shapes, as accessed via the\n  `shape` property will have a first `Dimension` value of `None`, and\n  operations that depend on fixed batch_size would fail.\n\n  Args:\n    tensors: The list or dictionary of tensors to enqueue.\n    batch_size: The new batch size pulled from the queue.\n    num_threads: The number of threads enqueuing `tensors`.  The batching will\n      be nondeterministic if `num_threads > 1`.\n    capacity: An integer. The maximum number of elements in the queue.\n    enqueue_many: Whether each tensor in `tensors` is a single example.\n    shapes: (Optional) The shapes for each example.  Defaults to the\n      inferred shapes for `tensors`.\n    dynamic_pad: Boolean.  Allow variable dimensions in input shapes.\n      The given dimensions are padded upon dequeue so that tensors within a\n      batch have the same shapes.\n    allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n      batch to be smaller if there are insufficient items left in the queue.\n    shared_name: (Optional). If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: (Optional) A name for the operations.\n\n  Returns:\n    A list or dictionary of tensors with the same types as `tensors` (except if\n    the input is a list of one element, then it returns a tensor, not a list).\n\n  Raises:\n    ValueError: If the `shapes` are not specified, and cannot be\n      inferred from the elements of `tensors`.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  return _batch(\n      tensors,\n      batch_size,\n      keep_input=True,\n      num_threads=num_threads,\n      capacity=capacity,\n      enqueue_many=enqueue_many,\n      shapes=shapes,\n      dynamic_pad=dynamic_pad,\n      allow_smaller_final_batch=allow_smaller_final_batch,\n      shared_name=shared_name,\n      name=name)\n\n\n@tf_export(v1=[\"train.maybe_batch\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)`\"\n    \" if `dynamic_pad=True`).\")\ndef maybe_batch(tensors, keep_input, batch_size, num_threads=1, capacity=32,\n                enqueue_many=False, shapes=None, dynamic_pad=False,\n                allow_smaller_final_batch=False, shared_name=None, name=None):\n  \"\"\"Conditionally creates batches of tensors based on `keep_input`.\n\n  See docstring in `batch` for more details.\n\n  Args:\n    tensors: The list or dictionary of tensors to enqueue.\n    keep_input: A `bool` Tensor.  This tensor controls whether the input is\n      added to the queue or not.  If it is a scalar and evaluates `True`, then\n      `tensors` are all added to the queue. If it is a vector and `enqueue_many`\n      is `True`, then each example is added to the queue only if the\n      corresponding value in `keep_input` is `True`. This tensor essentially\n      acts as a filtering mechanism.\n    batch_size: The new batch size pulled from the queue.\n    num_threads: The number of threads enqueuing `tensors`.  The batching will\n      be nondeterministic if `num_threads > 1`.\n    capacity: An integer. The maximum number of elements in the queue.\n    enqueue_many: Whether each tensor in `tensors` is a single example.\n    shapes: (Optional) The shapes for each example.  Defaults to the\n      inferred shapes for `tensors`.\n    dynamic_pad: Boolean.  Allow variable dimensions in input shapes.\n      The given dimensions are padded upon dequeue so that tensors within a\n      batch have the same shapes.\n    allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n      batch to be smaller if there are insufficient items left in the queue.\n    shared_name: (Optional). If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: (Optional) A name for the operations.\n\n  Returns:\n    A list or dictionary of tensors with the same types as `tensors`.\n\n  Raises:\n    ValueError: If the `shapes` are not specified, and cannot be\n      inferred from the elements of `tensors`.\n  \"\"\"\n  return _batch(\n      tensors,\n      batch_size,\n      keep_input,\n      num_threads=num_threads,\n      capacity=capacity,\n      enqueue_many=enqueue_many,\n      shapes=shapes,\n      dynamic_pad=dynamic_pad,\n      allow_smaller_final_batch=allow_smaller_final_batch,\n      shared_name=shared_name,\n      name=name)\n\n\n@tf_export(v1=[\"train.batch_join\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.interleave(...).batch(batch_size)` (or \"\n    \"`padded_batch(...)` if `dynamic_pad=True`).\")\ndef batch_join(tensors_list, batch_size, capacity=32, enqueue_many=False,\n               shapes=None, dynamic_pad=False, allow_smaller_final_batch=False,\n               shared_name=None, name=None):\n  \"\"\"Runs a list of tensors to fill a queue to create batches of examples.\n\n  The `tensors_list` argument is a list of tuples of tensors, or a list of\n  dictionaries of tensors.  Each element in the list is treated similarly\n  to the `tensors` argument of `tf.compat.v1.train.batch()`.\n\n  WARNING: This function is nondeterministic, since it starts a separate thread\n  for each tensor.\n\n  Enqueues a different list of tensors in different threads.\n  Implemented using a queue -- a `QueueRunner` for the queue\n  is added to the current `Graph`'s `QUEUE_RUNNER` collection.\n\n  `len(tensors_list)` threads will be started,\n  with thread `i` enqueuing the tensors from\n  `tensors_list[i]`. `tensors_list[i1][j]` must match\n  `tensors_list[i2][j]` in type and shape, except in the first\n  dimension if `enqueue_many` is true.\n\n  If `enqueue_many` is `False`, each `tensors_list[i]` is assumed\n  to represent a single example. An input tensor `x` will be output as a\n  tensor with shape `[batch_size] + x.shape`.\n\n  If `enqueue_many` is `True`, `tensors_list[i]` is assumed to\n  represent a batch of examples, where the first dimension is indexed\n  by example, and all members of `tensors_list[i]` should have the\n  same size in the first dimension.  The slices of any input tensor\n  `x` are treated as examples, and the output tensors will have shape\n  `[batch_size] + x.shape[1:]`.\n\n  The `capacity` argument controls the how long the prefetching is allowed to\n  grow the queues.\n\n  The returned operation is a dequeue operation and will throw\n  `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n  operation is feeding another input queue, its queue runner will catch\n  this exception, however, if this operation is used in your main thread\n  you are responsible for catching this yourself.\n\n  *N.B.:* If `dynamic_pad` is `False`, you must ensure that either\n  (i) the `shapes` argument is passed, or (ii) all of the tensors in\n  `tensors_list` must have fully-defined shapes. `ValueError` will be\n  raised if neither of these conditions holds.\n\n  If `dynamic_pad` is `True`, it is sufficient that the *rank* of the\n  tensors is known, but individual dimensions may have value `None`.\n  In this case, for each enqueue the dimensions with value `None`\n  may have a variable length; upon dequeue, the output tensors will be padded\n  on the right to the maximum shape of the tensors in the current minibatch.\n  For numbers, this padding takes value 0.  For strings, this padding is\n  the empty string.  See `PaddingFIFOQueue` for more info.\n\n  If `allow_smaller_final_batch` is `True`, a smaller batch value than\n  `batch_size` is returned when the queue is closed and there are not enough\n  elements to fill the batch, otherwise the pending elements are discarded.\n  In addition, all output tensors' static shapes, as accessed via the\n  `shape` property will have a first `Dimension` value of `None`, and\n  operations that depend on fixed batch_size would fail.\n\n  Args:\n    tensors_list: A list of tuples or dictionaries of tensors to enqueue.\n    batch_size: An integer. The new batch size pulled from the queue.\n    capacity: An integer. The maximum number of elements in the queue.\n    enqueue_many: Whether each tensor in `tensor_list_list` is a single\n      example.\n    shapes: (Optional) The shapes for each example.  Defaults to the\n      inferred shapes for `tensor_list_list[i]`.\n    dynamic_pad: Boolean.  Allow variable dimensions in input shapes.\n      The given dimensions are padded upon dequeue so that tensors within a\n      batch have the same shapes.\n    allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n      batch to be smaller if there are insufficient items left in the queue.\n    shared_name: (Optional) If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: (Optional) A name for the operations.\n\n  Returns:\n    A list or dictionary of tensors with the same number and types as\n    `tensors_list[i]`.\n\n  Raises:\n    ValueError: If the `shapes` are not specified, and cannot be\n      inferred from the elements of `tensor_list_list`.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  return _batch_join(\n      tensors_list,\n      batch_size,\n      keep_input=True,\n      capacity=capacity,\n      enqueue_many=enqueue_many,\n      shapes=shapes,\n      dynamic_pad=dynamic_pad,\n      allow_smaller_final_batch=allow_smaller_final_batch,\n      shared_name=shared_name,\n      name=name)\n\n\n@tf_export(v1=[\"train.maybe_batch_join\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.interleave(...).filter(...).batch(batch_size)` (or \"\n    \"`padded_batch(...)` if `dynamic_pad=True`).\")\ndef maybe_batch_join(tensors_list, keep_input, batch_size, capacity=32,\n                     enqueue_many=False, shapes=None, dynamic_pad=False,\n                     allow_smaller_final_batch=False, shared_name=None,\n                     name=None):\n  \"\"\"Runs a list of tensors to conditionally fill a queue to create batches.\n\n  See docstring in `batch_join` for more details.\n\n  Args:\n    tensors_list: A list of tuples or dictionaries of tensors to enqueue.\n    keep_input: A `bool` Tensor.  This tensor controls whether the input is\n      added to the queue or not.  If it is a scalar and evaluates `True`, then\n      `tensors` are all added to the queue. If it is a vector and `enqueue_many`\n      is `True`, then each example is added to the queue only if the\n      corresponding value in `keep_input` is `True`. This tensor essentially\n      acts as a filtering mechanism.\n    batch_size: An integer. The new batch size pulled from the queue.\n    capacity: An integer. The maximum number of elements in the queue.\n    enqueue_many: Whether each tensor in `tensor_list_list` is a single\n      example.\n    shapes: (Optional) The shapes for each example.  Defaults to the\n      inferred shapes for `tensor_list_list[i]`.\n    dynamic_pad: Boolean.  Allow variable dimensions in input shapes.\n      The given dimensions are padded upon dequeue so that tensors within a\n      batch have the same shapes.\n    allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n      batch to be smaller if there are insufficient items left in the queue.\n    shared_name: (Optional) If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: (Optional) A name for the operations.\n\n  Returns:\n    A list or dictionary of tensors with the same number and types as\n    `tensors_list[i]`.\n\n  Raises:\n    ValueError: If the `shapes` are not specified, and cannot be\n      inferred from the elements of `tensor_list_list`.\n  \"\"\"\n  return _batch_join(\n      tensors_list,\n      batch_size,\n      keep_input,\n      capacity=capacity,\n      enqueue_many=enqueue_many,\n      shapes=shapes,\n      dynamic_pad=dynamic_pad,\n      allow_smaller_final_batch=allow_smaller_final_batch,\n      shared_name=shared_name,\n      name=name)\n\n\n@tf_export(v1=[\"train.shuffle_batch\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\")\ndef shuffle_batch(tensors, batch_size, capacity, min_after_dequeue,\n                  num_threads=1, seed=None, enqueue_many=False, shapes=None,\n                  allow_smaller_final_batch=False, shared_name=None, name=None):\n  \"\"\"Creates batches by randomly shuffling tensors.\n\n  This function adds the following to the current `Graph`:\n\n  * A shuffling queue into which tensors from `tensors` are enqueued.\n  * A `dequeue_many` operation to create batches from the queue.\n  * A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors\n    from `tensors`.\n\n  If `enqueue_many` is `False`, `tensors` is assumed to represent a\n  single example.  An input tensor with shape `[x, y, z]` will be output\n  as a tensor with shape `[batch_size, x, y, z]`.\n\n  If `enqueue_many` is `True`, `tensors` is assumed to represent a\n  batch of examples, where the first dimension is indexed by example,\n  and all members of `tensors` should have the same size in the\n  first dimension.  If an input tensor has shape `[*, x, y, z]`, the\n  output will have shape `[batch_size, x, y, z]`.\n\n  The `capacity` argument controls the how long the prefetching is allowed to\n  grow the queues.\n\n  The returned operation is a dequeue operation and will throw\n  `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n  operation is feeding another input queue, its queue runner will catch\n  this exception, however, if this operation is used in your main thread\n  you are responsible for catching this yourself.\n\n  For example:\n\n  ```python\n  # Creates batches of 32 images and 32 labels.\n  image_batch, label_batch = tf.compat.v1.train.shuffle_batch(\n        [single_image, single_label],\n        batch_size=32,\n        num_threads=4,\n        capacity=50000,\n        min_after_dequeue=10000)\n  ```\n\n  *N.B.:* You must ensure that either (i) the `shapes` argument is\n  passed, or (ii) all of the tensors in `tensors` must have\n  fully-defined shapes. `ValueError` will be raised if neither of\n  these conditions holds.\n\n  If `allow_smaller_final_batch` is `True`, a smaller batch value than\n  `batch_size` is returned when the queue is closed and there are not enough\n  elements to fill the batch, otherwise the pending elements are discarded.\n  In addition, all output tensors' static shapes, as accessed via the\n  `shape` property will have a first `Dimension` value of `None`, and\n  operations that depend on fixed batch_size would fail.\n\n  Args:\n    tensors: The list or dictionary of tensors to enqueue.\n    batch_size: The new batch size pulled from the queue.\n    capacity: An integer. The maximum number of elements in the queue.\n    min_after_dequeue: Minimum number elements in the queue after a\n      dequeue, used to ensure a level of mixing of elements.\n    num_threads: The number of threads enqueuing `tensor_list`.\n    seed: Seed for the random shuffling within the queue.\n    enqueue_many: Whether each tensor in `tensor_list` is a single example.\n    shapes: (Optional) The shapes for each example.  Defaults to the\n      inferred shapes for `tensor_list`.\n    allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n      batch to be smaller if there are insufficient items left in the queue.\n    shared_name: (Optional) If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: (Optional) A name for the operations.\n\n  Returns:\n    A list or dictionary of tensors with the types as `tensors`.\n\n  Raises:\n    ValueError: If the `shapes` are not specified, and cannot be\n      inferred from the elements of `tensors`.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  return _shuffle_batch(\n      tensors,\n      batch_size,\n      capacity,\n      min_after_dequeue,\n      keep_input=True,\n      num_threads=num_threads,\n      seed=seed,\n      enqueue_many=enqueue_many,\n      shapes=shapes,\n      allow_smaller_final_batch=allow_smaller_final_batch,\n      shared_name=shared_name,\n      name=name)\n\n\n@tf_export(v1=[\"train.maybe_shuffle_batch\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`\"\n    \".\")\ndef maybe_shuffle_batch(tensors, batch_size, capacity, min_after_dequeue,\n                        keep_input, num_threads=1, seed=None,\n                        enqueue_many=False, shapes=None,\n                        allow_smaller_final_batch=False, shared_name=None,\n                        name=None):\n  \"\"\"Creates batches by randomly shuffling conditionally-enqueued tensors.\n\n  See docstring in `shuffle_batch` for more details.\n\n  Args:\n    tensors: The list or dictionary of tensors to enqueue.\n    batch_size: The new batch size pulled from the queue.\n    capacity: An integer. The maximum number of elements in the queue.\n    min_after_dequeue: Minimum number elements in the queue after a\n      dequeue, used to ensure a level of mixing of elements.\n    keep_input: A `bool` Tensor.  This tensor controls whether the input is\n      added to the queue or not.  If it is a scalar and evaluates `True`, then\n      `tensors` are all added to the queue. If it is a vector and `enqueue_many`\n      is `True`, then each example is added to the queue only if the\n      corresponding value in `keep_input` is `True`. This tensor essentially\n      acts as a filtering mechanism.\n    num_threads: The number of threads enqueuing `tensor_list`.\n    seed: Seed for the random shuffling within the queue.\n    enqueue_many: Whether each tensor in `tensor_list` is a single example.\n    shapes: (Optional) The shapes for each example.  Defaults to the\n      inferred shapes for `tensor_list`.\n    allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n      batch to be smaller if there are insufficient items left in the queue.\n    shared_name: (Optional) If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: (Optional) A name for the operations.\n\n  Returns:\n    A list or dictionary of tensors with the types as `tensors`.\n\n  Raises:\n    ValueError: If the `shapes` are not specified, and cannot be\n      inferred from the elements of `tensors`.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  return _shuffle_batch(\n      tensors,\n      batch_size,\n      capacity,\n      min_after_dequeue,\n      keep_input,\n      num_threads=num_threads,\n      seed=seed,\n      enqueue_many=enqueue_many,\n      shapes=shapes,\n      allow_smaller_final_batch=allow_smaller_final_batch,\n      shared_name=shared_name,\n      name=name)\n\n\n@tf_export(v1=[\"train.shuffle_batch_join\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch\"\n    \"(batch_size)`.\")\ndef shuffle_batch_join(tensors_list, batch_size, capacity,\n                       min_after_dequeue, seed=None, enqueue_many=False,\n                       shapes=None, allow_smaller_final_batch=False,\n                       shared_name=None, name=None):\n  \"\"\"Create batches by randomly shuffling tensors.\n\n  The `tensors_list` argument is a list of tuples of tensors, or a list of\n  dictionaries of tensors.  Each element in the list is treated similarly\n  to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`.\n\n  This version enqueues a different list of tensors in different threads.\n  It adds the following to the current `Graph`:\n\n  * A shuffling queue into which tensors from `tensors_list` are enqueued.\n  * A `dequeue_many` operation to create batches from the queue.\n  * A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors\n    from `tensors_list`.\n\n  `len(tensors_list)` threads will be started, with thread `i` enqueuing\n  the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match\n  `tensors_list[i2][j]` in type and shape, except in the first dimension if\n  `enqueue_many` is true.\n\n  If `enqueue_many` is `False`, each `tensors_list[i]` is assumed\n  to represent a single example.  An input tensor with shape `[x, y, z]`\n  will be output as a tensor with shape `[batch_size, x, y, z]`.\n\n  If `enqueue_many` is `True`, `tensors_list[i]` is assumed to\n  represent a batch of examples, where the first dimension is indexed\n  by example, and all members of `tensors_list[i]` should have the\n  same size in the first dimension.  If an input tensor has shape `[*, x,\n  y, z]`, the output will have shape `[batch_size, x, y, z]`.\n\n  The `capacity` argument controls the how long the prefetching is allowed to\n  grow the queues.\n\n  The returned operation is a dequeue operation and will throw\n  `tf.errors.OutOfRangeError` if the input queue is exhausted. If this\n  operation is feeding another input queue, its queue runner will catch\n  this exception, however, if this operation is used in your main thread\n  you are responsible for catching this yourself.\n\n  If `allow_smaller_final_batch` is `True`, a smaller batch value than\n  `batch_size` is returned when the queue is closed and there are not enough\n  elements to fill the batch, otherwise the pending elements are discarded.\n  In addition, all output tensors' static shapes, as accessed via the\n  `shape` property will have a first `Dimension` value of `None`, and\n  operations that depend on fixed batch_size would fail.\n\n  Args:\n    tensors_list: A list of tuples or dictionaries of tensors to enqueue.\n    batch_size: An integer. The new batch size pulled from the queue.\n    capacity: An integer. The maximum number of elements in the queue.\n    min_after_dequeue: Minimum number elements in the queue after a\n      dequeue, used to ensure a level of mixing of elements.\n    seed: Seed for the random shuffling within the queue.\n    enqueue_many: Whether each tensor in `tensor_list_list` is a single\n      example.\n    shapes: (Optional) The shapes for each example.  Defaults to the\n      inferred shapes for `tensors_list[i]`.\n    allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n      batch to be smaller if there are insufficient items left in the queue.\n    shared_name: (optional). If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: (Optional) A name for the operations.\n\n  Returns:\n    A list or dictionary of tensors with the same number and types as\n    `tensors_list[i]`.\n\n  Raises:\n    ValueError: If the `shapes` are not specified, and cannot be\n      inferred from the elements of `tensors_list`.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  return _shuffle_batch_join(\n      tensors_list,\n      batch_size,\n      capacity,\n      min_after_dequeue,\n      keep_input=True,\n      seed=seed,\n      enqueue_many=enqueue_many,\n      shapes=shapes,\n      allow_smaller_final_batch=allow_smaller_final_batch,\n      shared_name=shared_name,\n      name=name)\n\n\n@tf_export(v1=[\"train.maybe_shuffle_batch_join\"])\n@deprecation.deprecated(\n    None, \"Queue-based input pipelines have been replaced by `tf.data`. Use \"\n    \"`tf.data.Dataset.interleave(...).filter(...).shuffle(min_after_dequeue)\"\n    \".batch(batch_size)`.\")\ndef maybe_shuffle_batch_join(tensors_list, batch_size, capacity,\n                             min_after_dequeue, keep_input, seed=None,\n                             enqueue_many=False, shapes=None,\n                             allow_smaller_final_batch=False, shared_name=None,\n                             name=None):\n  \"\"\"Create batches by randomly shuffling conditionally-enqueued tensors.\n\n  See docstring in `shuffle_batch_join` for more details.\n\n  Args:\n    tensors_list: A list of tuples or dictionaries of tensors to enqueue.\n    batch_size: An integer. The new batch size pulled from the queue.\n    capacity: An integer. The maximum number of elements in the queue.\n    min_after_dequeue: Minimum number elements in the queue after a\n      dequeue, used to ensure a level of mixing of elements.\n    keep_input: A `bool` Tensor.  This tensor controls whether the input is\n      added to the queue or not.  If it is a scalar and evaluates `True`, then\n      `tensors` are all added to the queue. If it is a vector and `enqueue_many`\n      is `True`, then each example is added to the queue only if the\n      corresponding value in `keep_input` is `True`. This tensor essentially\n      acts as a filtering mechanism.\n    seed: Seed for the random shuffling within the queue.\n    enqueue_many: Whether each tensor in `tensor_list_list` is a single\n      example.\n    shapes: (Optional) The shapes for each example.  Defaults to the\n      inferred shapes for `tensors_list[i]`.\n    allow_smaller_final_batch: (Optional) Boolean. If `True`, allow the final\n      batch to be smaller if there are insufficient items left in the queue.\n    shared_name: (optional). If set, this queue will be shared under the given\n      name across multiple sessions.\n    name: (Optional) A name for the operations.\n\n  Returns:\n    A list or dictionary of tensors with the same number and types as\n    `tensors_list[i]`.\n\n  Raises:\n    ValueError: If the `shapes` are not specified, and cannot be\n      inferred from the elements of `tensors_list`.\n\n  @compatibility(eager)\n  Input pipelines based on Queues are not supported when eager execution is\n  enabled. Please use the `tf.data` API to ingest data under eager execution.\n  @end_compatibility\n  \"\"\"\n  return _shuffle_batch_join(\n      tensors_list,\n      batch_size,\n      capacity,\n      min_after_dequeue,\n      keep_input,\n      seed=seed,\n      enqueue_many=enqueue_many,\n      shapes=shapes,\n      allow_smaller_final_batch=allow_smaller_final_batch,\n      shared_name=shared_name,\n      name=name)\n", "framework": "tensorflow"}
{"repo_name": "aselle/tensorflow", "file_path": "tensorflow/python/training/sync_replicas_optimizer.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Synchronize replicas for training.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import queue_runner\nfrom tensorflow.python.training import session_manager\nfrom tensorflow.python.training import session_run_hook\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n# Please note that the gradients from replicas are averaged instead of summed\n# (as in the old sync_replicas_optimizer) so you need to increase the learning\n# rate according to the number of replicas. This change is introduced to be\n# consistent with how gradients are aggregated (averaged) within a batch in a\n# replica.\n@tf_export(\"train.SyncReplicasOptimizer\")\nclass SyncReplicasOptimizer(optimizer.Optimizer):\n  \"\"\"Class to synchronize, aggregate gradients and pass them to the optimizer.\n\n  In a typical asynchronous training environment, it's common to have some\n  stale gradients. For example, with a N-replica asynchronous training,\n  gradients will be applied to the variables N times independently. Depending\n  on each replica's training speed, some gradients might be calculated from\n  copies of the variable from several steps back (N-1 steps on average). This\n  optimizer avoids stale gradients by collecting gradients from all replicas,\n  averaging them, then applying them to the variables in one shot, after\n  which replicas can fetch the new variables and continue.\n\n  The following accumulators/queue are created:\n  <empty line>\n  * N `gradient accumulators`, one per variable to train. Gradients are pushed\n    to them and the chief worker will wait until enough gradients are collected\n    and then average them before applying to variables. The accumulator will\n    drop all stale gradients (more details in the accumulator op).\n  * 1 `token` queue where the optimizer pushes the new global_step value after\n    all variables are updated.\n\n  The following local variable is created:\n  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n    each accumulator to check for staleness of the gradients.\n\n  The optimizer adds nodes to the graph to collect gradients and pause the\n  trainers until variables are updated.\n  For the Parameter Server job:\n  <empty line>\n  1. An accumulator is created for each variable, and each replica pushes the\n     gradients into the accumulators instead of directly applying them to the\n     variables.\n  2. Each accumulator averages once enough gradients (replicas_to_aggregate)\n     have been accumulated.\n  3. Apply the averaged gradients to the variables.\n  4. Only after all variables have been updated, increment the global step.\n  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n     each worker replica. The workers can now fetch the global step, use it to\n     update its local_step variable and start the next batch.\n\n  For the replicas:\n  <empty line>\n  1. Start a step: fetch variables and compute gradients.\n  2. Once the gradients have been computed, push them into gradient\n     accumulators. Each accumulator will check the staleness and drop the stale.\n  3. After pushing all the gradients, dequeue an updated value of global_step\n     from the token queue and record that step to its local_step variable. Note\n     that this is effectively a barrier.\n  4. Start the next batch.\n\n  ### Usage\n\n  ```python\n  # Create any optimizer to update the variables, say a simple SGD:\n  opt = GradientDescentOptimizer(learning_rate=0.1)\n\n  # Wrap the optimizer with sync_replicas_optimizer with 50 replicas: at each\n  # step the optimizer collects 50 gradients before applying to variables.\n  # Note that if you want to have 2 backup replicas, you can change\n  # total_num_replicas=52 and make sure this number matches how many physical\n  # replicas you started in your job.\n  opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=50,\n                                 total_num_replicas=50)\n\n  # Some models have startup_delays to help stabilize the model but when using\n  # sync_replicas training, set it to 0.\n\n  # Now you can call `minimize()` or `compute_gradients()` and\n  # `apply_gradients()` normally\n  training_op = opt.minimize(total_loss, global_step=self.global_step)\n\n\n  # You can create the hook which handles initialization and queues.\n  sync_replicas_hook = opt.make_session_run_hook(is_chief)\n  ```\n\n  In the training program, every worker will run the train_op as if not\n  synchronized.\n\n  ```python\n  with training.MonitoredTrainingSession(\n      master=workers[worker_id].target, is_chief=is_chief,\n      hooks=[sync_replicas_hook]) as mon_sess:\n    while not mon_sess.should_stop():\n      mon_sess.run(training_op)\n  ```\n\n  To use SyncReplicasOptimizer with an `Estimator`, you need to send\n  sync_replicas_hook while calling the fit.\n  ```python\n  my_estimator = DNNClassifier(..., optimizer=opt)\n  my_estimator.fit(..., hooks=[sync_replicas_hook])\n  ```\n  \"\"\"\n\n  def __init__(self,\n               opt,\n               replicas_to_aggregate,\n               total_num_replicas=None,\n               variable_averages=None,\n               variables_to_average=None,\n               use_locking=False,\n               name=\"sync_replicas\"):\n    \"\"\"Construct a sync_replicas optimizer.\n\n    Args:\n      opt: The actual optimizer that will be used to compute and apply the\n        gradients. Must be one of the Optimizer classes.\n      replicas_to_aggregate: number of replicas to aggregate for each variable\n        update.\n      total_num_replicas: Total number of tasks/workers/replicas, could be\n        different from replicas_to_aggregate.\n        If total_num_replicas > replicas_to_aggregate: it is backup_replicas +\n        replicas_to_aggregate.\n        If total_num_replicas < replicas_to_aggregate: Replicas compute\n        multiple batches per update to variables.\n      variable_averages: Optional `ExponentialMovingAverage` object, used to\n        maintain moving averages for the variables passed in\n        `variables_to_average`.\n      variables_to_average: a list of variables that need to be averaged. Only\n        needed if variable_averages is passed in.\n      use_locking: If True use locks for update operation.\n      name: string. Optional name of the returned operation.\n    \"\"\"\n    if total_num_replicas is None:\n      total_num_replicas = replicas_to_aggregate\n\n    super(SyncReplicasOptimizer, self).__init__(use_locking, name)\n    logging.info(\n        \"SyncReplicasV2: replicas_to_aggregate=%s; total_num_replicas=%s\",\n        replicas_to_aggregate, total_num_replicas)\n    self._opt = opt\n    self._replicas_to_aggregate = replicas_to_aggregate\n    self._gradients_applied = False\n    self._variable_averages = variable_averages\n    self._variables_to_average = variables_to_average\n    self._total_num_replicas = total_num_replicas\n    self._tokens_per_step = max(total_num_replicas, replicas_to_aggregate)\n    self._global_step = None\n    self._sync_token_queue = None\n\n    # The synchronization op will be executed in a queue runner which should\n    # only be executed by one of the replicas (usually the chief).\n    self._chief_queue_runner = None\n\n    # Remember which accumulator is on which device to set the initial step in\n    # the accumulator to be global step. This list contains list of the\n    # following format: (accumulator, device).\n    self._accumulator_list = []\n\n  def compute_gradients(self, *args, **kwargs):\n    \"\"\"Compute gradients of \"loss\" for the variables in \"var_list\".\n\n    This simply wraps the compute_gradients() from the real optimizer. The\n    gradients will be aggregated in the apply_gradients() so that user can\n    modify the gradients like clipping with per replica global norm if needed.\n    The global norm with aggregated gradients can be bad as one replica's huge\n    gradients can hurt the gradients from other replicas.\n\n    Args:\n      *args: Arguments for compute_gradients().\n      **kwargs: Keyword arguments for compute_gradients().\n\n    Returns:\n      A list of (gradient, variable) pairs.\n    \"\"\"\n    return self._opt.compute_gradients(*args, **kwargs)\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    \"\"\"Apply gradients to variables.\n\n    This contains most of the synchronization implementation and also wraps the\n    apply_gradients() from the real optimizer.\n\n    Args:\n      grads_and_vars: List of (gradient, variable) pairs as returned by\n        compute_gradients().\n      global_step: Optional Variable to increment by one after the\n        variables have been updated.\n      name: Optional name for the returned operation.  Default to the\n        name passed to the Optimizer constructor.\n\n    Returns:\n      train_op: The op to dequeue a token so the replicas can exit this batch\n      and start the next one. This is executed by each replica.\n\n    Raises:\n      ValueError: If the grads_and_vars is empty.\n      ValueError: If global step is not provided, the staleness cannot be\n        checked.\n    \"\"\"\n    if not grads_and_vars:\n      raise ValueError(\"Must supply at least one variable\")\n\n    if global_step is None:\n      raise ValueError(\"Global step is required to check staleness\")\n\n    self._global_step = global_step\n    train_ops = []\n    aggregated_grad = []\n    var_list = []\n\n    # local_anchor op will be placed on this worker task by default.\n    local_anchor = control_flow_ops.no_op()\n    # Colocating local_step variable prevents it being placed on the PS.\n    with ops.colocate_with(local_anchor):\n      self._local_step = variable_scope.variable(\n          initial_value=0,\n          trainable=False,\n          collections=[ops.GraphKeys.LOCAL_VARIABLES],\n          dtype=global_step.dtype.base_dtype,\n          name=\"sync_rep_local_step\")\n\n    self.local_step_init_op = state_ops.assign(self._local_step, global_step)\n    chief_init_ops = [self.local_step_init_op]\n    self.ready_for_local_init_op = variables.report_uninitialized_variables(\n        variables.global_variables())\n\n    with ops.name_scope(None, self._name):\n      for grad, var in grads_and_vars:\n        var_list.append(var)\n        with ops.device(var.device):\n          # Dense gradients.\n          if grad is None:\n            aggregated_grad.append(None)  # pass-through.\n            continue\n          elif isinstance(grad, ops.Tensor):\n            grad_accum = data_flow_ops.ConditionalAccumulator(\n                grad.dtype,\n                shape=var.get_shape(),\n                shared_name=var.name + \"/grad_accum\")\n            train_ops.append(grad_accum.apply_grad(\n                grad, local_step=self._local_step))\n            aggregated_grad.append(grad_accum.take_grad(\n                self._replicas_to_aggregate))\n          else:\n            if not isinstance(grad, ops.IndexedSlices):\n              raise ValueError(\"Unknown grad type!\")\n            grad_accum = data_flow_ops.SparseConditionalAccumulator(\n                grad.dtype, shape=(), shared_name=var.name + \"/grad_accum\")\n            train_ops.append(grad_accum.apply_indexed_slices_grad(\n                grad, local_step=self._local_step))\n            aggregated_grad.append(grad_accum.take_indexed_slices_grad(\n                self._replicas_to_aggregate))\n\n          self._accumulator_list.append((grad_accum, var.device))\n\n      aggregated_grads_and_vars = zip(aggregated_grad, var_list)\n\n      # sync_op will be assigned to the same device as the global step.\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        update_op = self._opt.apply_gradients(aggregated_grads_and_vars,\n                                              global_step)\n\n      # Create token queue.\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        sync_token_queue = (\n            data_flow_ops.FIFOQueue(-1,\n                                    global_step.dtype.base_dtype,\n                                    shapes=(),\n                                    name=\"sync_token_q\",\n                                    shared_name=\"sync_token_q\"))\n        self._sync_token_queue = sync_token_queue\n\n        # dummy_queue is passed to the queue runner. Don't use the real queues\n        # because the queue runner doesn't automatically reopen it once it\n        # closed queues in PS devices.\n        dummy_queue = (\n            data_flow_ops.FIFOQueue(1,\n                                    types_pb2.DT_INT32,\n                                    shapes=(),\n                                    name=\"dummy_queue\",\n                                    shared_name=\"dummy_queue\"))\n\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        # Replicas have to wait until they can get a token from the token queue.\n        with ops.control_dependencies(train_ops):\n          token = sync_token_queue.dequeue()\n        train_op = state_ops.assign(self._local_step, token)\n\n        with ops.control_dependencies([update_op]):\n          # Sync_op needs to insert tokens to the token queue at the end of the\n          # step so the replicas can fetch them to start the next step.\n          tokens = array_ops.fill([self._tokens_per_step], global_step)\n          sync_op = sync_token_queue.enqueue_many((tokens,))\n\n        if self._variable_averages is not None:\n          with ops.control_dependencies([sync_op]), ops.name_scope(\"\"):\n            sync_op = self._variable_averages.apply(\n                self._variables_to_average)\n\n        self._chief_queue_runner = queue_runner.QueueRunner(dummy_queue,\n                                                            [sync_op])\n      for accum, dev in self._accumulator_list:\n        with ops.device(dev):\n          chief_init_ops.append(\n              accum.set_global_step(\n                  global_step, name=\"SetGlobalStep\"))\n      self.chief_init_op = control_flow_ops.group(*(chief_init_ops))\n      self._gradients_applied = True\n      return train_op\n\n  def get_chief_queue_runner(self):\n    \"\"\"Returns the QueueRunner for the chief to execute.\n\n    This includes the operations to synchronize replicas: aggregate gradients,\n    apply to variables, increment global step, insert tokens to token queue.\n\n    Note that this can only be called after calling apply_gradients() which\n    actually generates this queuerunner.\n\n    Returns:\n      A `QueueRunner` for chief to execute.\n\n    Raises:\n      ValueError: If this is called before apply_gradients().\n    \"\"\"\n    if self._gradients_applied is False:\n      raise ValueError(\"Should be called after apply_gradients().\")\n\n    return self._chief_queue_runner\n\n  def get_slot(self, *args, **kwargs):\n    \"\"\"Return a slot named \"name\" created for \"var\" by the Optimizer.\n\n    This simply wraps the get_slot() from the actual optimizer.\n\n    Args:\n      *args: Arguments for get_slot().\n      **kwargs: Keyword arguments for get_slot().\n\n    Returns:\n      The `Variable` for the slot if it was created, `None` otherwise.\n    \"\"\"\n    return self._opt.get_slot(*args, **kwargs)\n\n  def variables(self):\n    \"\"\"Fetches a list of optimizer variables in the default graph.\n\n    This wraps `variables()` from the actual optimizer. It does not include\n    the `SyncReplicasOptimizer`'s local step.\n\n    Returns:\n      A list of variables.\n    \"\"\"\n    return self._opt.variables()\n\n  def get_slot_names(self, *args, **kwargs):\n    \"\"\"Return a list of the names of slots created by the `Optimizer`.\n\n    This simply wraps the get_slot_names() from the actual optimizer.\n\n    Args:\n      *args: Arguments for get_slot().\n      **kwargs: Keyword arguments for get_slot().\n\n    Returns:\n      A list of strings.\n    \"\"\"\n    return self._opt.get_slot_names(*args, **kwargs)\n\n  def get_init_tokens_op(self, num_tokens=-1):\n    \"\"\"Returns the op to fill the sync_token_queue with the tokens.\n\n    This is supposed to be executed in the beginning of the chief/sync thread\n    so that even if the total_num_replicas is less than replicas_to_aggregate,\n    the model can still proceed as the replicas can compute multiple steps per\n    variable update. Make sure:\n    `num_tokens >= replicas_to_aggregate - total_num_replicas`.\n\n    Args:\n      num_tokens: Number of tokens to add to the queue.\n\n    Returns:\n      An op for the chief/sync replica to fill the token queue.\n\n    Raises:\n      ValueError: If this is called before apply_gradients().\n      ValueError: If num_tokens are smaller than replicas_to_aggregate -\n        total_num_replicas.\n    \"\"\"\n    if self._gradients_applied is False:\n      raise ValueError(\n          \"get_init_tokens_op() should be called after apply_gradients().\")\n\n    tokens_needed = self._replicas_to_aggregate - self._total_num_replicas\n    if num_tokens == -1:\n      num_tokens = self._replicas_to_aggregate\n    elif num_tokens < tokens_needed:\n      raise ValueError(\n          \"Too few tokens to finish the first step: %d (given) vs %d (needed)\" %\n          (num_tokens, tokens_needed))\n\n    if num_tokens > 0:\n      with ops.device(self._global_step.device), ops.name_scope(\"\"):\n        tokens = array_ops.fill([num_tokens], self._global_step)\n        init_tokens = self._sync_token_queue.enqueue_many((tokens,))\n    else:\n      init_tokens = control_flow_ops.no_op(name=\"no_init_tokens\")\n\n    return init_tokens\n\n  def make_session_run_hook(self, is_chief, num_tokens=-1):\n    \"\"\"Creates a hook to handle SyncReplicasHook ops such as initialization.\"\"\"\n    return _SyncReplicasOptimizerHook(self, is_chief, num_tokens)\n\n\nclass _SyncReplicasOptimizerHook(session_run_hook.SessionRunHook):\n  \"\"\"A SessionRunHook handles ops related to SyncReplicasOptimizer.\"\"\"\n\n  def __init__(self, sync_optimizer, is_chief, num_tokens):\n    \"\"\"Creates hook to handle SyncReplicasOptimizer initialization ops.\n\n    Args:\n      sync_optimizer: `SyncReplicasOptimizer` which this hook will initialize.\n      is_chief: `Bool`, whether is this a chief replica or not.\n      num_tokens: Number of tokens to add to the queue.\n    \"\"\"\n    self._sync_optimizer = sync_optimizer\n    self._is_chief = is_chief\n    self._num_tokens = num_tokens\n\n  def begin(self):\n    if self._sync_optimizer._gradients_applied is False:  # pylint: disable=protected-access\n      raise ValueError(\n          \"SyncReplicasOptimizer.apply_gradient should be called before using \"\n          \"the hook.\")\n    if self._is_chief:\n      self._local_init_op = self._sync_optimizer.chief_init_op\n      self._ready_for_local_init_op = (\n          self._sync_optimizer.ready_for_local_init_op)\n      self._q_runner = self._sync_optimizer.get_chief_queue_runner()\n      self._init_tokens_op = self._sync_optimizer.get_init_tokens_op(\n          self._num_tokens)\n    else:\n      self._local_init_op = self._sync_optimizer.local_step_init_op\n      self._ready_for_local_init_op = (\n          self._sync_optimizer.ready_for_local_init_op)\n      self._q_runner = None\n      self._init_tokens_op = None\n\n  def after_create_session(self, session, coord):\n    \"\"\"Runs SyncReplicasOptimizer initialization ops.\"\"\"\n    local_init_success, msg = session_manager._ready(  # pylint: disable=protected-access\n        self._ready_for_local_init_op, session,\n        \"Model is not ready for SyncReplicasOptimizer local init.\")\n    if not local_init_success:\n      raise RuntimeError(\n          \"Init operations did not make model ready for SyncReplicasOptimizer \"\n          \"local_init. Init op: %s, error: %s\" %\n          (self._local_init_op.name, msg))\n    session.run(self._local_init_op)\n    if self._init_tokens_op is not None:\n      session.run(self._init_tokens_op)\n    if self._q_runner is not None:\n      self._q_runner.create_threads(\n          session, coord=coord, daemon=True, start=True)\n", "framework": "tensorflow"}
{"repo_name": "allenlavoie/tensorflow", "file_path": "tensorflow/python/training/sync_replicas_optimizer.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Synchronize replicas for training.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import queue_runner\nfrom tensorflow.python.training import session_manager\nfrom tensorflow.python.training import session_run_hook\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n# Please note that the gradients from replicas are averaged instead of summed\n# (as in the old sync_replicas_optimizer) so you need to increase the learning\n# rate according to the number of replicas. This change is introduced to be\n# consistent with how gradients are aggregated (averaged) within a batch in a\n# replica.\n@tf_export(\"train.SyncReplicasOptimizer\")\nclass SyncReplicasOptimizer(optimizer.Optimizer):\n  \"\"\"Class to synchronize, aggregate gradients and pass them to the optimizer.\n\n  In a typical asynchronous training environment, it's common to have some\n  stale gradients. For example, with a N-replica asynchronous training,\n  gradients will be applied to the variables N times independently. Depending\n  on each replica's training speed, some gradients might be calculated from\n  copies of the variable from several steps back (N-1 steps on average). This\n  optimizer avoids stale gradients by collecting gradients from all replicas,\n  averaging them, then applying them to the variables in one shot, after\n  which replicas can fetch the new variables and continue.\n\n  The following accumulators/queue are created:\n  <empty line>\n  * N `gradient accumulators`, one per variable to train. Gradients are pushed\n    to them and the chief worker will wait until enough gradients are collected\n    and then average them before applying to variables. The accumulator will\n    drop all stale gradients (more details in the accumulator op).\n  * 1 `token` queue where the optimizer pushes the new global_step value after\n    all variables are updated.\n\n  The following local variable is created:\n  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n    each accumulator to check for staleness of the gradients.\n\n  The optimizer adds nodes to the graph to collect gradients and pause the\n  trainers until variables are updated.\n  For the Parameter Server job:\n  <empty line>\n  1. An accumulator is created for each variable, and each replica pushes the\n     gradients into the accumulators instead of directly applying them to the\n     variables.\n  2. Each accumulator averages once enough gradients (replicas_to_aggregate)\n     have been accumulated.\n  3. Apply the averaged gradients to the variables.\n  4. Only after all variables have been updated, increment the global step.\n  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n     each worker replica. The workers can now fetch the global step, use it to\n     update its local_step variable and start the next batch.\n\n  For the replicas:\n  <empty line>\n  1. Start a step: fetch variables and compute gradients.\n  2. Once the gradients have been computed, push them into gradient\n     accumulators. Each accumulator will check the staleness and drop the stale.\n  3. After pushing all the gradients, dequeue an updated value of global_step\n     from the token queue and record that step to its local_step variable. Note\n     that this is effectively a barrier.\n  4. Start the next batch.\n\n  ### Usage\n\n  ```python\n  # Create any optimizer to update the variables, say a simple SGD:\n  opt = GradientDescentOptimizer(learning_rate=0.1)\n\n  # Wrap the optimizer with sync_replicas_optimizer with 50 replicas: at each\n  # step the optimizer collects 50 gradients before applying to variables.\n  # Note that if you want to have 2 backup replicas, you can change\n  # total_num_replicas=52 and make sure this number matches how many physical\n  # replicas you started in your job.\n  opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=50,\n                                 total_num_replicas=50)\n\n  # Some models have startup_delays to help stabilize the model but when using\n  # sync_replicas training, set it to 0.\n\n  # Now you can call `minimize()` or `compute_gradients()` and\n  # `apply_gradients()` normally\n  training_op = opt.minimize(total_loss, global_step=self.global_step)\n\n\n  # You can create the hook which handles initialization and queues.\n  sync_replicas_hook = opt.make_session_run_hook(is_chief)\n  ```\n\n  In the training program, every worker will run the train_op as if not\n  synchronized.\n\n  ```python\n  with training.MonitoredTrainingSession(\n      master=workers[worker_id].target, is_chief=is_chief,\n      hooks=[sync_replicas_hook]) as mon_sess:\n    while not mon_sess.should_stop():\n      mon_sess.run(training_op)\n  ```\n\n  To use SyncReplicasOptimizer with an `Estimator`, you need to send\n  sync_replicas_hook while calling the fit.\n  ```python\n  my_estimator = DNNClassifier(..., optimizer=opt)\n  my_estimator.fit(..., hooks=[sync_replicas_hook])\n  ```\n  \"\"\"\n\n  def __init__(self,\n               opt,\n               replicas_to_aggregate,\n               total_num_replicas=None,\n               variable_averages=None,\n               variables_to_average=None,\n               use_locking=False,\n               name=\"sync_replicas\"):\n    \"\"\"Construct a sync_replicas optimizer.\n\n    Args:\n      opt: The actual optimizer that will be used to compute and apply the\n        gradients. Must be one of the Optimizer classes.\n      replicas_to_aggregate: number of replicas to aggregate for each variable\n        update.\n      total_num_replicas: Total number of tasks/workers/replicas, could be\n        different from replicas_to_aggregate.\n        If total_num_replicas > replicas_to_aggregate: it is backup_replicas +\n        replicas_to_aggregate.\n        If total_num_replicas < replicas_to_aggregate: Replicas compute\n        multiple batches per update to variables.\n      variable_averages: Optional `ExponentialMovingAverage` object, used to\n        maintain moving averages for the variables passed in\n        `variables_to_average`.\n      variables_to_average: a list of variables that need to be averaged. Only\n        needed if variable_averages is passed in.\n      use_locking: If True use locks for update operation.\n      name: string. Optional name of the returned operation.\n    \"\"\"\n    if total_num_replicas is None:\n      total_num_replicas = replicas_to_aggregate\n\n    super(SyncReplicasOptimizer, self).__init__(use_locking, name)\n    logging.info(\n        \"SyncReplicasV2: replicas_to_aggregate=%s; total_num_replicas=%s\",\n        replicas_to_aggregate, total_num_replicas)\n    self._opt = opt\n    self._replicas_to_aggregate = replicas_to_aggregate\n    self._gradients_applied = False\n    self._variable_averages = variable_averages\n    self._variables_to_average = variables_to_average\n    self._total_num_replicas = total_num_replicas\n    self._tokens_per_step = max(total_num_replicas, replicas_to_aggregate)\n    self._global_step = None\n    self._sync_token_queue = None\n\n    # The synchronization op will be executed in a queue runner which should\n    # only be executed by one of the replicas (usually the chief).\n    self._chief_queue_runner = None\n\n    # Remember which accumulator is on which device to set the initial step in\n    # the accumulator to be global step. This list contains list of the\n    # following format: (accumulator, device).\n    self._accumulator_list = []\n\n  def compute_gradients(self, *args, **kwargs):\n    \"\"\"Compute gradients of \"loss\" for the variables in \"var_list\".\n\n    This simply wraps the compute_gradients() from the real optimizer. The\n    gradients will be aggregated in the apply_gradients() so that user can\n    modify the gradients like clipping with per replica global norm if needed.\n    The global norm with aggregated gradients can be bad as one replica's huge\n    gradients can hurt the gradients from other replicas.\n\n    Args:\n      *args: Arguments for compute_gradients().\n      **kwargs: Keyword arguments for compute_gradients().\n\n    Returns:\n      A list of (gradient, variable) pairs.\n    \"\"\"\n    return self._opt.compute_gradients(*args, **kwargs)\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    \"\"\"Apply gradients to variables.\n\n    This contains most of the synchronization implementation and also wraps the\n    apply_gradients() from the real optimizer.\n\n    Args:\n      grads_and_vars: List of (gradient, variable) pairs as returned by\n        compute_gradients().\n      global_step: Optional Variable to increment by one after the\n        variables have been updated.\n      name: Optional name for the returned operation.  Default to the\n        name passed to the Optimizer constructor.\n\n    Returns:\n      train_op: The op to dequeue a token so the replicas can exit this batch\n      and start the next one. This is executed by each replica.\n\n    Raises:\n      ValueError: If the grads_and_vars is empty.\n      ValueError: If global step is not provided, the staleness cannot be\n        checked.\n    \"\"\"\n    if not grads_and_vars:\n      raise ValueError(\"Must supply at least one variable\")\n\n    if global_step is None:\n      raise ValueError(\"Global step is required to check staleness\")\n\n    self._global_step = global_step\n    train_ops = []\n    aggregated_grad = []\n    var_list = []\n\n    # local_anchor op will be placed on this worker task by default.\n    local_anchor = control_flow_ops.no_op()\n    # Colocating local_step variable prevents it being placed on the PS.\n    with ops.colocate_with(local_anchor):\n      self._local_step = variable_scope.variable(\n          initial_value=0,\n          trainable=False,\n          collections=[ops.GraphKeys.LOCAL_VARIABLES],\n          dtype=global_step.dtype.base_dtype,\n          name=\"sync_rep_local_step\")\n\n    self.local_step_init_op = state_ops.assign(self._local_step, global_step)\n    chief_init_ops = [self.local_step_init_op]\n    self.ready_for_local_init_op = variables.report_uninitialized_variables(\n        variables.global_variables())\n\n    with ops.name_scope(None, self._name):\n      for grad, var in grads_and_vars:\n        var_list.append(var)\n        with ops.device(var.device):\n          # Dense gradients.\n          if grad is None:\n            aggregated_grad.append(None)  # pass-through.\n            continue\n          elif isinstance(grad, ops.Tensor):\n            grad_accum = data_flow_ops.ConditionalAccumulator(\n                grad.dtype,\n                shape=var.get_shape(),\n                shared_name=var.name + \"/grad_accum\")\n            train_ops.append(grad_accum.apply_grad(\n                grad, local_step=self._local_step))\n            aggregated_grad.append(grad_accum.take_grad(\n                self._replicas_to_aggregate))\n          else:\n            if not isinstance(grad, ops.IndexedSlices):\n              raise ValueError(\"Unknown grad type!\")\n            grad_accum = data_flow_ops.SparseConditionalAccumulator(\n                grad.dtype, shape=(), shared_name=var.name + \"/grad_accum\")\n            train_ops.append(grad_accum.apply_indexed_slices_grad(\n                grad, local_step=self._local_step))\n            aggregated_grad.append(grad_accum.take_indexed_slices_grad(\n                self._replicas_to_aggregate))\n\n          self._accumulator_list.append((grad_accum, var.device))\n\n      aggregated_grads_and_vars = zip(aggregated_grad, var_list)\n\n      # sync_op will be assigned to the same device as the global step.\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        update_op = self._opt.apply_gradients(aggregated_grads_and_vars,\n                                              global_step)\n\n      # Create token queue.\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        sync_token_queue = (\n            data_flow_ops.FIFOQueue(-1,\n                                    global_step.dtype.base_dtype,\n                                    shapes=(),\n                                    name=\"sync_token_q\",\n                                    shared_name=\"sync_token_q\"))\n        self._sync_token_queue = sync_token_queue\n\n        # dummy_queue is passed to the queue runner. Don't use the real queues\n        # because the queue runner doesn't automatically reopen it once it\n        # closed queues in PS devices.\n        dummy_queue = (\n            data_flow_ops.FIFOQueue(1,\n                                    types_pb2.DT_INT32,\n                                    shapes=(),\n                                    name=\"dummy_queue\",\n                                    shared_name=\"dummy_queue\"))\n\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        # Replicas have to wait until they can get a token from the token queue.\n        with ops.control_dependencies(train_ops):\n          token = sync_token_queue.dequeue()\n        train_op = state_ops.assign(self._local_step, token)\n\n        with ops.control_dependencies([update_op]):\n          # Sync_op needs to insert tokens to the token queue at the end of the\n          # step so the replicas can fetch them to start the next step.\n          tokens = array_ops.fill([self._tokens_per_step], global_step)\n          sync_op = sync_token_queue.enqueue_many((tokens,))\n\n        if self._variable_averages is not None:\n          with ops.control_dependencies([sync_op]), ops.name_scope(\"\"):\n            sync_op = self._variable_averages.apply(\n                self._variables_to_average)\n\n        self._chief_queue_runner = queue_runner.QueueRunner(dummy_queue,\n                                                            [sync_op])\n      for accum, dev in self._accumulator_list:\n        with ops.device(dev):\n          chief_init_ops.append(\n              accum.set_global_step(\n                  global_step, name=\"SetGlobalStep\"))\n      self.chief_init_op = control_flow_ops.group(*(chief_init_ops))\n      self._gradients_applied = True\n      return train_op\n\n  def get_chief_queue_runner(self):\n    \"\"\"Returns the QueueRunner for the chief to execute.\n\n    This includes the operations to synchronize replicas: aggregate gradients,\n    apply to variables, increment global step, insert tokens to token queue.\n\n    Note that this can only be called after calling apply_gradients() which\n    actually generates this queuerunner.\n\n    Returns:\n      A `QueueRunner` for chief to execute.\n\n    Raises:\n      ValueError: If this is called before apply_gradients().\n    \"\"\"\n    if self._gradients_applied is False:\n      raise ValueError(\"Should be called after apply_gradients().\")\n\n    return self._chief_queue_runner\n\n  def get_slot(self, *args, **kwargs):\n    \"\"\"Return a slot named \"name\" created for \"var\" by the Optimizer.\n\n    This simply wraps the get_slot() from the actual optimizer.\n\n    Args:\n      *args: Arguments for get_slot().\n      **kwargs: Keyword arguments for get_slot().\n\n    Returns:\n      The `Variable` for the slot if it was created, `None` otherwise.\n    \"\"\"\n    return self._opt.get_slot(*args, **kwargs)\n\n  def variables(self):\n    \"\"\"Fetches a list of optimizer variables in the default graph.\n\n    This wraps `variables()` from the actual optimizer. It does not include\n    the `SyncReplicasOptimizer`'s local step.\n\n    Returns:\n      A list of variables.\n    \"\"\"\n    return self._opt.variables()\n\n  def get_slot_names(self, *args, **kwargs):\n    \"\"\"Return a list of the names of slots created by the `Optimizer`.\n\n    This simply wraps the get_slot_names() from the actual optimizer.\n\n    Args:\n      *args: Arguments for get_slot().\n      **kwargs: Keyword arguments for get_slot().\n\n    Returns:\n      A list of strings.\n    \"\"\"\n    return self._opt.get_slot_names(*args, **kwargs)\n\n  def get_init_tokens_op(self, num_tokens=-1):\n    \"\"\"Returns the op to fill the sync_token_queue with the tokens.\n\n    This is supposed to be executed in the beginning of the chief/sync thread\n    so that even if the total_num_replicas is less than replicas_to_aggregate,\n    the model can still proceed as the replicas can compute multiple steps per\n    variable update. Make sure:\n    `num_tokens >= replicas_to_aggregate - total_num_replicas`.\n\n    Args:\n      num_tokens: Number of tokens to add to the queue.\n\n    Returns:\n      An op for the chief/sync replica to fill the token queue.\n\n    Raises:\n      ValueError: If this is called before apply_gradients().\n      ValueError: If num_tokens are smaller than replicas_to_aggregate -\n        total_num_replicas.\n    \"\"\"\n    if self._gradients_applied is False:\n      raise ValueError(\n          \"get_init_tokens_op() should be called after apply_gradients().\")\n\n    tokens_needed = self._replicas_to_aggregate - self._total_num_replicas\n    if num_tokens == -1:\n      num_tokens = self._replicas_to_aggregate\n    elif num_tokens < tokens_needed:\n      raise ValueError(\n          \"Too few tokens to finish the first step: %d (given) vs %d (needed)\" %\n          (num_tokens, tokens_needed))\n\n    if num_tokens > 0:\n      with ops.device(self._global_step.device), ops.name_scope(\"\"):\n        tokens = array_ops.fill([num_tokens], self._global_step)\n        init_tokens = self._sync_token_queue.enqueue_many((tokens,))\n    else:\n      init_tokens = control_flow_ops.no_op(name=\"no_init_tokens\")\n\n    return init_tokens\n\n  def make_session_run_hook(self, is_chief, num_tokens=-1):\n    \"\"\"Creates a hook to handle SyncReplicasHook ops such as initialization.\"\"\"\n    return _SyncReplicasOptimizerHook(self, is_chief, num_tokens)\n\n\nclass _SyncReplicasOptimizerHook(session_run_hook.SessionRunHook):\n  \"\"\"A SessionRunHook handles ops related to SyncReplicasOptimizer.\"\"\"\n\n  def __init__(self, sync_optimizer, is_chief, num_tokens):\n    \"\"\"Creates hook to handle SyncReplicasOptimizer initialization ops.\n\n    Args:\n      sync_optimizer: `SyncReplicasOptimizer` which this hook will initialize.\n      is_chief: `Bool`, whether is this a chief replica or not.\n      num_tokens: Number of tokens to add to the queue.\n    \"\"\"\n    self._sync_optimizer = sync_optimizer\n    self._is_chief = is_chief\n    self._num_tokens = num_tokens\n\n  def begin(self):\n    if self._sync_optimizer._gradients_applied is False:  # pylint: disable=protected-access\n      raise ValueError(\n          \"SyncReplicasOptimizer.apply_gradient should be called before using \"\n          \"the hook.\")\n    if self._is_chief:\n      self._local_init_op = self._sync_optimizer.chief_init_op\n      self._ready_for_local_init_op = (\n          self._sync_optimizer.ready_for_local_init_op)\n      self._q_runner = self._sync_optimizer.get_chief_queue_runner()\n      self._init_tokens_op = self._sync_optimizer.get_init_tokens_op(\n          self._num_tokens)\n    else:\n      self._local_init_op = self._sync_optimizer.local_step_init_op\n      self._ready_for_local_init_op = (\n          self._sync_optimizer.ready_for_local_init_op)\n      self._q_runner = None\n      self._init_tokens_op = None\n\n  def after_create_session(self, session, coord):\n    \"\"\"Runs SyncReplicasOptimizer initialization ops.\"\"\"\n    local_init_success, msg = session_manager._ready(  # pylint: disable=protected-access\n        self._ready_for_local_init_op, session,\n        \"Model is not ready for SyncReplicasOptimizer local init.\")\n    if not local_init_success:\n      raise RuntimeError(\n          \"Init operations did not make model ready for SyncReplicasOptimizer \"\n          \"local_init. Init op: %s, error: %s\" %\n          (self._local_init_op.name, msg))\n    session.run(self._local_init_op)\n    if self._init_tokens_op is not None:\n      session.run(self._init_tokens_op)\n    if self._q_runner is not None:\n      self._q_runner.create_threads(\n          session, coord=coord, daemon=True, start=True)\n", "framework": "tensorflow"}
{"repo_name": "caisq/tensorflow", "file_path": "tensorflow/python/training/sync_replicas_optimizer.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Synchronize replicas for training.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import queue_runner\nfrom tensorflow.python.training import session_manager\nfrom tensorflow.python.training import session_run_hook\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n# Please note that the gradients from replicas are averaged instead of summed\n# (as in the old sync_replicas_optimizer) so you need to increase the learning\n# rate according to the number of replicas. This change is introduced to be\n# consistent with how gradients are aggregated (averaged) within a batch in a\n# replica.\n@tf_export(\"train.SyncReplicasOptimizer\")\nclass SyncReplicasOptimizer(optimizer.Optimizer):\n  \"\"\"Class to synchronize, aggregate gradients and pass them to the optimizer.\n\n  In a typical asynchronous training environment, it's common to have some\n  stale gradients. For example, with a N-replica asynchronous training,\n  gradients will be applied to the variables N times independently. Depending\n  on each replica's training speed, some gradients might be calculated from\n  copies of the variable from several steps back (N-1 steps on average). This\n  optimizer avoids stale gradients by collecting gradients from all replicas,\n  averaging them, then applying them to the variables in one shot, after\n  which replicas can fetch the new variables and continue.\n\n  The following accumulators/queue are created:\n  <empty line>\n  * N `gradient accumulators`, one per variable to train. Gradients are pushed\n    to them and the chief worker will wait until enough gradients are collected\n    and then average them before applying to variables. The accumulator will\n    drop all stale gradients (more details in the accumulator op).\n  * 1 `token` queue where the optimizer pushes the new global_step value after\n    all variables are updated.\n\n  The following local variable is created:\n  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n    each accumulator to check for staleness of the gradients.\n\n  The optimizer adds nodes to the graph to collect gradients and pause the\n  trainers until variables are updated.\n  For the Parameter Server job:\n  <empty line>\n  1. An accumulator is created for each variable, and each replica pushes the\n     gradients into the accumulators instead of directly applying them to the\n     variables.\n  2. Each accumulator averages once enough gradients (replicas_to_aggregate)\n     have been accumulated.\n  3. Apply the averaged gradients to the variables.\n  4. Only after all variables have been updated, increment the global step.\n  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n     each worker replica. The workers can now fetch the global step, use it to\n     update its local_step variable and start the next batch.\n\n  For the replicas:\n  <empty line>\n  1. Start a step: fetch variables and compute gradients.\n  2. Once the gradients have been computed, push them into gradient\n     accumulators. Each accumulator will check the staleness and drop the stale.\n  3. After pushing all the gradients, dequeue an updated value of global_step\n     from the token queue and record that step to its local_step variable. Note\n     that this is effectively a barrier.\n  4. Start the next batch.\n\n  ### Usage\n\n  ```python\n  # Create any optimizer to update the variables, say a simple SGD:\n  opt = GradientDescentOptimizer(learning_rate=0.1)\n\n  # Wrap the optimizer with sync_replicas_optimizer with 50 replicas: at each\n  # step the optimizer collects 50 gradients before applying to variables.\n  # Note that if you want to have 2 backup replicas, you can change\n  # total_num_replicas=52 and make sure this number matches how many physical\n  # replicas you started in your job.\n  opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=50,\n                                 total_num_replicas=50)\n\n  # Some models have startup_delays to help stabilize the model but when using\n  # sync_replicas training, set it to 0.\n\n  # Now you can call `minimize()` or `compute_gradients()` and\n  # `apply_gradients()` normally\n  training_op = opt.minimize(total_loss, global_step=self.global_step)\n\n\n  # You can create the hook which handles initialization and queues.\n  sync_replicas_hook = opt.make_session_run_hook(is_chief)\n  ```\n\n  In the training program, every worker will run the train_op as if not\n  synchronized.\n\n  ```python\n  with training.MonitoredTrainingSession(\n      master=workers[worker_id].target, is_chief=is_chief,\n      hooks=[sync_replicas_hook]) as mon_sess:\n    while not mon_sess.should_stop():\n      mon_sess.run(training_op)\n  ```\n\n  To use SyncReplicasOptimizer with an `Estimator`, you need to send\n  sync_replicas_hook while calling the fit.\n  ```python\n  my_estimator = DNNClassifier(..., optimizer=opt)\n  my_estimator.fit(..., hooks=[sync_replicas_hook])\n  ```\n  \"\"\"\n\n  def __init__(self,\n               opt,\n               replicas_to_aggregate,\n               total_num_replicas=None,\n               variable_averages=None,\n               variables_to_average=None,\n               use_locking=False,\n               name=\"sync_replicas\"):\n    \"\"\"Construct a sync_replicas optimizer.\n\n    Args:\n      opt: The actual optimizer that will be used to compute and apply the\n        gradients. Must be one of the Optimizer classes.\n      replicas_to_aggregate: number of replicas to aggregate for each variable\n        update.\n      total_num_replicas: Total number of tasks/workers/replicas, could be\n        different from replicas_to_aggregate.\n        If total_num_replicas > replicas_to_aggregate: it is backup_replicas +\n        replicas_to_aggregate.\n        If total_num_replicas < replicas_to_aggregate: Replicas compute\n        multiple batches per update to variables.\n      variable_averages: Optional `ExponentialMovingAverage` object, used to\n        maintain moving averages for the variables passed in\n        `variables_to_average`.\n      variables_to_average: a list of variables that need to be averaged. Only\n        needed if variable_averages is passed in.\n      use_locking: If True use locks for update operation.\n      name: string. Optional name of the returned operation.\n    \"\"\"\n    if total_num_replicas is None:\n      total_num_replicas = replicas_to_aggregate\n\n    super(SyncReplicasOptimizer, self).__init__(use_locking, name)\n    logging.info(\n        \"SyncReplicasV2: replicas_to_aggregate=%s; total_num_replicas=%s\",\n        replicas_to_aggregate, total_num_replicas)\n    self._opt = opt\n    self._replicas_to_aggregate = replicas_to_aggregate\n    self._gradients_applied = False\n    self._variable_averages = variable_averages\n    self._variables_to_average = variables_to_average\n    self._total_num_replicas = total_num_replicas\n    self._tokens_per_step = max(total_num_replicas, replicas_to_aggregate)\n    self._global_step = None\n    self._sync_token_queue = None\n\n    # The synchronization op will be executed in a queue runner which should\n    # only be executed by one of the replicas (usually the chief).\n    self._chief_queue_runner = None\n\n    # Remember which accumulator is on which device to set the initial step in\n    # the accumulator to be global step. This list contains list of the\n    # following format: (accumulator, device).\n    self._accumulator_list = []\n\n  def compute_gradients(self, *args, **kwargs):\n    \"\"\"Compute gradients of \"loss\" for the variables in \"var_list\".\n\n    This simply wraps the compute_gradients() from the real optimizer. The\n    gradients will be aggregated in the apply_gradients() so that user can\n    modify the gradients like clipping with per replica global norm if needed.\n    The global norm with aggregated gradients can be bad as one replica's huge\n    gradients can hurt the gradients from other replicas.\n\n    Args:\n      *args: Arguments for compute_gradients().\n      **kwargs: Keyword arguments for compute_gradients().\n\n    Returns:\n      A list of (gradient, variable) pairs.\n    \"\"\"\n    return self._opt.compute_gradients(*args, **kwargs)\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    \"\"\"Apply gradients to variables.\n\n    This contains most of the synchronization implementation and also wraps the\n    apply_gradients() from the real optimizer.\n\n    Args:\n      grads_and_vars: List of (gradient, variable) pairs as returned by\n        compute_gradients().\n      global_step: Optional Variable to increment by one after the\n        variables have been updated.\n      name: Optional name for the returned operation.  Default to the\n        name passed to the Optimizer constructor.\n\n    Returns:\n      train_op: The op to dequeue a token so the replicas can exit this batch\n      and start the next one. This is executed by each replica.\n\n    Raises:\n      ValueError: If the grads_and_vars is empty.\n      ValueError: If global step is not provided, the staleness cannot be\n        checked.\n    \"\"\"\n    if not grads_and_vars:\n      raise ValueError(\"Must supply at least one variable\")\n\n    if global_step is None:\n      raise ValueError(\"Global step is required to check staleness\")\n\n    self._global_step = global_step\n    train_ops = []\n    aggregated_grad = []\n    var_list = []\n\n    # local_anchor op will be placed on this worker task by default.\n    local_anchor = control_flow_ops.no_op()\n    # Colocating local_step variable prevents it being placed on the PS.\n    with ops.colocate_with(local_anchor):\n      self._local_step = variable_scope.variable(\n          initial_value=0,\n          trainable=False,\n          collections=[ops.GraphKeys.LOCAL_VARIABLES],\n          dtype=global_step.dtype.base_dtype,\n          name=\"sync_rep_local_step\")\n\n    self.local_step_init_op = state_ops.assign(self._local_step, global_step)\n    chief_init_ops = [self.local_step_init_op]\n    self.ready_for_local_init_op = variables.report_uninitialized_variables(\n        variables.global_variables())\n\n    with ops.name_scope(None, self._name):\n      for grad, var in grads_and_vars:\n        var_list.append(var)\n        with ops.device(var.device):\n          # Dense gradients.\n          if grad is None:\n            aggregated_grad.append(None)  # pass-through.\n            continue\n          elif isinstance(grad, ops.Tensor):\n            grad_accum = data_flow_ops.ConditionalAccumulator(\n                grad.dtype,\n                shape=var.get_shape(),\n                shared_name=var.name + \"/grad_accum\")\n            train_ops.append(grad_accum.apply_grad(\n                grad, local_step=self._local_step))\n            aggregated_grad.append(grad_accum.take_grad(\n                self._replicas_to_aggregate))\n          else:\n            if not isinstance(grad, ops.IndexedSlices):\n              raise ValueError(\"Unknown grad type!\")\n            grad_accum = data_flow_ops.SparseConditionalAccumulator(\n                grad.dtype, shape=(), shared_name=var.name + \"/grad_accum\")\n            train_ops.append(grad_accum.apply_indexed_slices_grad(\n                grad, local_step=self._local_step))\n            aggregated_grad.append(grad_accum.take_indexed_slices_grad(\n                self._replicas_to_aggregate))\n\n          self._accumulator_list.append((grad_accum, var.device))\n\n      aggregated_grads_and_vars = zip(aggregated_grad, var_list)\n\n      # sync_op will be assigned to the same device as the global step.\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        update_op = self._opt.apply_gradients(aggregated_grads_and_vars,\n                                              global_step)\n\n      # Create token queue.\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        sync_token_queue = (\n            data_flow_ops.FIFOQueue(-1,\n                                    global_step.dtype.base_dtype,\n                                    shapes=(),\n                                    name=\"sync_token_q\",\n                                    shared_name=\"sync_token_q\"))\n        self._sync_token_queue = sync_token_queue\n\n        # dummy_queue is passed to the queue runner. Don't use the real queues\n        # because the queue runner doesn't automatically reopen it once it\n        # closed queues in PS devices.\n        dummy_queue = (\n            data_flow_ops.FIFOQueue(1,\n                                    types_pb2.DT_INT32,\n                                    shapes=(),\n                                    name=\"dummy_queue\",\n                                    shared_name=\"dummy_queue\"))\n\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        # Replicas have to wait until they can get a token from the token queue.\n        with ops.control_dependencies(train_ops):\n          token = sync_token_queue.dequeue()\n        train_op = state_ops.assign(self._local_step, token)\n\n        with ops.control_dependencies([update_op]):\n          # Sync_op needs to insert tokens to the token queue at the end of the\n          # step so the replicas can fetch them to start the next step.\n          tokens = array_ops.fill([self._tokens_per_step], global_step)\n          sync_op = sync_token_queue.enqueue_many((tokens,))\n\n        if self._variable_averages is not None:\n          with ops.control_dependencies([sync_op]), ops.name_scope(\"\"):\n            sync_op = self._variable_averages.apply(\n                self._variables_to_average)\n\n        self._chief_queue_runner = queue_runner.QueueRunner(dummy_queue,\n                                                            [sync_op])\n      for accum, dev in self._accumulator_list:\n        with ops.device(dev):\n          chief_init_ops.append(\n              accum.set_global_step(\n                  global_step, name=\"SetGlobalStep\"))\n      self.chief_init_op = control_flow_ops.group(*(chief_init_ops))\n      self._gradients_applied = True\n      return train_op\n\n  def get_chief_queue_runner(self):\n    \"\"\"Returns the QueueRunner for the chief to execute.\n\n    This includes the operations to synchronize replicas: aggregate gradients,\n    apply to variables, increment global step, insert tokens to token queue.\n\n    Note that this can only be called after calling apply_gradients() which\n    actually generates this queuerunner.\n\n    Returns:\n      A `QueueRunner` for chief to execute.\n\n    Raises:\n      ValueError: If this is called before apply_gradients().\n    \"\"\"\n    if self._gradients_applied is False:\n      raise ValueError(\"Should be called after apply_gradients().\")\n\n    return self._chief_queue_runner\n\n  def get_slot(self, *args, **kwargs):\n    \"\"\"Return a slot named \"name\" created for \"var\" by the Optimizer.\n\n    This simply wraps the get_slot() from the actual optimizer.\n\n    Args:\n      *args: Arguments for get_slot().\n      **kwargs: Keyword arguments for get_slot().\n\n    Returns:\n      The `Variable` for the slot if it was created, `None` otherwise.\n    \"\"\"\n    return self._opt.get_slot(*args, **kwargs)\n\n  def variables(self):\n    \"\"\"Fetches a list of optimizer variables in the default graph.\n\n    This wraps `variables()` from the actual optimizer. It does not include\n    the `SyncReplicasOptimizer`'s local step.\n\n    Returns:\n      A list of variables.\n    \"\"\"\n    return self._opt.variables()\n\n  def get_slot_names(self, *args, **kwargs):\n    \"\"\"Return a list of the names of slots created by the `Optimizer`.\n\n    This simply wraps the get_slot_names() from the actual optimizer.\n\n    Args:\n      *args: Arguments for get_slot().\n      **kwargs: Keyword arguments for get_slot().\n\n    Returns:\n      A list of strings.\n    \"\"\"\n    return self._opt.get_slot_names(*args, **kwargs)\n\n  def get_init_tokens_op(self, num_tokens=-1):\n    \"\"\"Returns the op to fill the sync_token_queue with the tokens.\n\n    This is supposed to be executed in the beginning of the chief/sync thread\n    so that even if the total_num_replicas is less than replicas_to_aggregate,\n    the model can still proceed as the replicas can compute multiple steps per\n    variable update. Make sure:\n    `num_tokens >= replicas_to_aggregate - total_num_replicas`.\n\n    Args:\n      num_tokens: Number of tokens to add to the queue.\n\n    Returns:\n      An op for the chief/sync replica to fill the token queue.\n\n    Raises:\n      ValueError: If this is called before apply_gradients().\n      ValueError: If num_tokens are smaller than replicas_to_aggregate -\n        total_num_replicas.\n    \"\"\"\n    if self._gradients_applied is False:\n      raise ValueError(\n          \"get_init_tokens_op() should be called after apply_gradients().\")\n\n    tokens_needed = self._replicas_to_aggregate - self._total_num_replicas\n    if num_tokens == -1:\n      num_tokens = self._replicas_to_aggregate\n    elif num_tokens < tokens_needed:\n      raise ValueError(\n          \"Too few tokens to finish the first step: %d (given) vs %d (needed)\" %\n          (num_tokens, tokens_needed))\n\n    if num_tokens > 0:\n      with ops.device(self._global_step.device), ops.name_scope(\"\"):\n        tokens = array_ops.fill([num_tokens], self._global_step)\n        init_tokens = self._sync_token_queue.enqueue_many((tokens,))\n    else:\n      init_tokens = control_flow_ops.no_op(name=\"no_init_tokens\")\n\n    return init_tokens\n\n  def make_session_run_hook(self, is_chief, num_tokens=-1):\n    \"\"\"Creates a hook to handle SyncReplicasHook ops such as initialization.\"\"\"\n    return _SyncReplicasOptimizerHook(self, is_chief, num_tokens)\n\n\nclass _SyncReplicasOptimizerHook(session_run_hook.SessionRunHook):\n  \"\"\"A SessionRunHook handles ops related to SyncReplicasOptimizer.\"\"\"\n\n  def __init__(self, sync_optimizer, is_chief, num_tokens):\n    \"\"\"Creates hook to handle SyncReplicasOptimizer initialization ops.\n\n    Args:\n      sync_optimizer: `SyncReplicasOptimizer` which this hook will initialize.\n      is_chief: `Bool`, whether is this a chief replica or not.\n      num_tokens: Number of tokens to add to the queue.\n    \"\"\"\n    self._sync_optimizer = sync_optimizer\n    self._is_chief = is_chief\n    self._num_tokens = num_tokens\n\n  def begin(self):\n    if self._sync_optimizer._gradients_applied is False:  # pylint: disable=protected-access\n      raise ValueError(\n          \"SyncReplicasOptimizer.apply_gradient should be called before using \"\n          \"the hook.\")\n    if self._is_chief:\n      self._local_init_op = self._sync_optimizer.chief_init_op\n      self._ready_for_local_init_op = (\n          self._sync_optimizer.ready_for_local_init_op)\n      self._q_runner = self._sync_optimizer.get_chief_queue_runner()\n      self._init_tokens_op = self._sync_optimizer.get_init_tokens_op(\n          self._num_tokens)\n    else:\n      self._local_init_op = self._sync_optimizer.local_step_init_op\n      self._ready_for_local_init_op = (\n          self._sync_optimizer.ready_for_local_init_op)\n      self._q_runner = None\n      self._init_tokens_op = None\n\n  def after_create_session(self, session, coord):\n    \"\"\"Runs SyncReplicasOptimizer initialization ops.\"\"\"\n    local_init_success, msg = session_manager._ready(  # pylint: disable=protected-access\n        self._ready_for_local_init_op, session,\n        \"Model is not ready for SyncReplicasOptimizer local init.\")\n    if not local_init_success:\n      raise RuntimeError(\n          \"Init operations did not make model ready for SyncReplicasOptimizer \"\n          \"local_init. Init op: %s, error: %s\" %\n          (self._local_init_op.name, msg))\n    session.run(self._local_init_op)\n    if self._init_tokens_op is not None:\n      session.run(self._init_tokens_op)\n    if self._q_runner is not None:\n      self._q_runner.create_threads(\n          session, coord=coord, daemon=True, start=True)\n", "framework": "tensorflow"}
{"repo_name": "av8ramit/tensorflow", "file_path": "tensorflow/python/training/sync_replicas_optimizer.py", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Synchronize replicas for training.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.core.framework import types_pb2\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.training import optimizer\nfrom tensorflow.python.training import queue_runner\nfrom tensorflow.python.training import session_manager\nfrom tensorflow.python.training import session_run_hook\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n# Please note that the gradients from replicas are averaged instead of summed\n# (as in the old sync_replicas_optimizer) so you need to increase the learning\n# rate according to the number of replicas. This change is introduced to be\n# consistent with how gradients are aggregated (averaged) within a batch in a\n# replica.\n@tf_export(\"train.SyncReplicasOptimizer\")\nclass SyncReplicasOptimizer(optimizer.Optimizer):\n  \"\"\"Class to synchronize, aggregate gradients and pass them to the optimizer.\n\n  In a typical asynchronous training environment, it's common to have some\n  stale gradients. For example, with a N-replica asynchronous training,\n  gradients will be applied to the variables N times independently. Depending\n  on each replica's training speed, some gradients might be calculated from\n  copies of the variable from several steps back (N-1 steps on average). This\n  optimizer avoids stale gradients by collecting gradients from all replicas,\n  averaging them, then applying them to the variables in one shot, after\n  which replicas can fetch the new variables and continue.\n\n  The following accumulators/queue are created:\n  <empty line>\n  * N `gradient accumulators`, one per variable to train. Gradients are pushed\n    to them and the chief worker will wait until enough gradients are collected\n    and then average them before applying to variables. The accumulator will\n    drop all stale gradients (more details in the accumulator op).\n  * 1 `token` queue where the optimizer pushes the new global_step value after\n    all variables are updated.\n\n  The following local variable is created:\n  * `sync_rep_local_step`, one per replica. Compared against the global_step in\n    each accumulator to check for staleness of the gradients.\n\n  The optimizer adds nodes to the graph to collect gradients and pause the\n  trainers until variables are updated.\n  For the Parameter Server job:\n  <empty line>\n  1. An accumulator is created for each variable, and each replica pushes the\n     gradients into the accumulators instead of directly applying them to the\n     variables.\n  2. Each accumulator averages once enough gradients (replicas_to_aggregate)\n     have been accumulated.\n  3. Apply the averaged gradients to the variables.\n  4. Only after all variables have been updated, increment the global step.\n  5. Only after step 4, pushes `global_step` in the `token_queue`, once for\n     each worker replica. The workers can now fetch the global step, use it to\n     update its local_step variable and start the next batch.\n\n  For the replicas:\n  <empty line>\n  1. Start a step: fetch variables and compute gradients.\n  2. Once the gradients have been computed, push them into gradient\n     accumulators. Each accumulator will check the staleness and drop the stale.\n  3. After pushing all the gradients, dequeue an updated value of global_step\n     from the token queue and record that step to its local_step variable. Note\n     that this is effectively a barrier.\n  4. Start the next batch.\n\n  ### Usage\n\n  ```python\n  # Create any optimizer to update the variables, say a simple SGD:\n  opt = GradientDescentOptimizer(learning_rate=0.1)\n\n  # Wrap the optimizer with sync_replicas_optimizer with 50 replicas: at each\n  # step the optimizer collects 50 gradients before applying to variables.\n  # Note that if you want to have 2 backup replicas, you can change\n  # total_num_replicas=52 and make sure this number matches how many physical\n  # replicas you started in your job.\n  opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=50,\n                                 total_num_replicas=50)\n\n  # Some models have startup_delays to help stabilize the model but when using\n  # sync_replicas training, set it to 0.\n\n  # Now you can call `minimize()` or `compute_gradients()` and\n  # `apply_gradients()` normally\n  training_op = opt.minimize(total_loss, global_step=self.global_step)\n\n\n  # You can create the hook which handles initialization and queues.\n  sync_replicas_hook = opt.make_session_run_hook(is_chief)\n  ```\n\n  In the training program, every worker will run the train_op as if not\n  synchronized.\n\n  ```python\n  with training.MonitoredTrainingSession(\n      master=workers[worker_id].target, is_chief=is_chief,\n      hooks=[sync_replicas_hook]) as mon_sess:\n    while not mon_sess.should_stop():\n      mon_sess.run(training_op)\n  ```\n\n  To use SyncReplicasOptimizer with an `Estimator`, you need to send\n  sync_replicas_hook while calling the fit.\n  ```python\n  my_estimator = DNNClassifier(..., optimizer=opt)\n  my_estimator.fit(..., hooks=[sync_replicas_hook])\n  ```\n  \"\"\"\n\n  def __init__(self,\n               opt,\n               replicas_to_aggregate,\n               total_num_replicas=None,\n               variable_averages=None,\n               variables_to_average=None,\n               use_locking=False,\n               name=\"sync_replicas\"):\n    \"\"\"Construct a sync_replicas optimizer.\n\n    Args:\n      opt: The actual optimizer that will be used to compute and apply the\n        gradients. Must be one of the Optimizer classes.\n      replicas_to_aggregate: number of replicas to aggregate for each variable\n        update.\n      total_num_replicas: Total number of tasks/workers/replicas, could be\n        different from replicas_to_aggregate.\n        If total_num_replicas > replicas_to_aggregate: it is backup_replicas +\n        replicas_to_aggregate.\n        If total_num_replicas < replicas_to_aggregate: Replicas compute\n        multiple batches per update to variables.\n      variable_averages: Optional `ExponentialMovingAverage` object, used to\n        maintain moving averages for the variables passed in\n        `variables_to_average`.\n      variables_to_average: a list of variables that need to be averaged. Only\n        needed if variable_averages is passed in.\n      use_locking: If True use locks for update operation.\n      name: string. Optional name of the returned operation.\n    \"\"\"\n    if total_num_replicas is None:\n      total_num_replicas = replicas_to_aggregate\n\n    super(SyncReplicasOptimizer, self).__init__(use_locking, name)\n    logging.info(\n        \"SyncReplicasV2: replicas_to_aggregate=%s; total_num_replicas=%s\",\n        replicas_to_aggregate, total_num_replicas)\n    self._opt = opt\n    self._replicas_to_aggregate = replicas_to_aggregate\n    self._gradients_applied = False\n    self._variable_averages = variable_averages\n    self._variables_to_average = variables_to_average\n    self._total_num_replicas = total_num_replicas\n    self._tokens_per_step = max(total_num_replicas, replicas_to_aggregate)\n    self._global_step = None\n    self._sync_token_queue = None\n\n    # The synchronization op will be executed in a queue runner which should\n    # only be executed by one of the replicas (usually the chief).\n    self._chief_queue_runner = None\n\n    # Remember which accumulator is on which device to set the initial step in\n    # the accumulator to be global step. This list contains list of the\n    # following format: (accumulator, device).\n    self._accumulator_list = []\n\n  def compute_gradients(self, *args, **kwargs):\n    \"\"\"Compute gradients of \"loss\" for the variables in \"var_list\".\n\n    This simply wraps the compute_gradients() from the real optimizer. The\n    gradients will be aggregated in the apply_gradients() so that user can\n    modify the gradients like clipping with per replica global norm if needed.\n    The global norm with aggregated gradients can be bad as one replica's huge\n    gradients can hurt the gradients from other replicas.\n\n    Args:\n      *args: Arguments for compute_gradients().\n      **kwargs: Keyword arguments for compute_gradients().\n\n    Returns:\n      A list of (gradient, variable) pairs.\n    \"\"\"\n    return self._opt.compute_gradients(*args, **kwargs)\n\n  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n    \"\"\"Apply gradients to variables.\n\n    This contains most of the synchronization implementation and also wraps the\n    apply_gradients() from the real optimizer.\n\n    Args:\n      grads_and_vars: List of (gradient, variable) pairs as returned by\n        compute_gradients().\n      global_step: Optional Variable to increment by one after the\n        variables have been updated.\n      name: Optional name for the returned operation.  Default to the\n        name passed to the Optimizer constructor.\n\n    Returns:\n      train_op: The op to dequeue a token so the replicas can exit this batch\n      and start the next one. This is executed by each replica.\n\n    Raises:\n      ValueError: If the grads_and_vars is empty.\n      ValueError: If global step is not provided, the staleness cannot be\n        checked.\n    \"\"\"\n    if not grads_and_vars:\n      raise ValueError(\"Must supply at least one variable\")\n\n    if global_step is None:\n      raise ValueError(\"Global step is required to check staleness\")\n\n    self._global_step = global_step\n    train_ops = []\n    aggregated_grad = []\n    var_list = []\n\n    # local_anchor op will be placed on this worker task by default.\n    local_anchor = control_flow_ops.no_op()\n    # Colocating local_step variable prevents it being placed on the PS.\n    with ops.colocate_with(local_anchor):\n      self._local_step = variable_scope.variable(\n          initial_value=0,\n          trainable=False,\n          collections=[ops.GraphKeys.LOCAL_VARIABLES],\n          dtype=global_step.dtype.base_dtype,\n          name=\"sync_rep_local_step\")\n\n    self.local_step_init_op = state_ops.assign(self._local_step, global_step)\n    chief_init_ops = [self.local_step_init_op]\n    self.ready_for_local_init_op = variables.report_uninitialized_variables(\n        variables.global_variables())\n\n    with ops.name_scope(None, self._name):\n      for grad, var in grads_and_vars:\n        var_list.append(var)\n        with ops.device(var.device):\n          # Dense gradients.\n          if grad is None:\n            aggregated_grad.append(None)  # pass-through.\n            continue\n          elif isinstance(grad, ops.Tensor):\n            grad_accum = data_flow_ops.ConditionalAccumulator(\n                grad.dtype,\n                shape=var.get_shape(),\n                shared_name=var.name + \"/grad_accum\")\n            train_ops.append(grad_accum.apply_grad(\n                grad, local_step=self._local_step))\n            aggregated_grad.append(grad_accum.take_grad(\n                self._replicas_to_aggregate))\n          else:\n            if not isinstance(grad, ops.IndexedSlices):\n              raise ValueError(\"Unknown grad type!\")\n            grad_accum = data_flow_ops.SparseConditionalAccumulator(\n                grad.dtype, shape=(), shared_name=var.name + \"/grad_accum\")\n            train_ops.append(grad_accum.apply_indexed_slices_grad(\n                grad, local_step=self._local_step))\n            aggregated_grad.append(grad_accum.take_indexed_slices_grad(\n                self._replicas_to_aggregate))\n\n          self._accumulator_list.append((grad_accum, var.device))\n\n      aggregated_grads_and_vars = zip(aggregated_grad, var_list)\n\n      # sync_op will be assigned to the same device as the global step.\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        update_op = self._opt.apply_gradients(aggregated_grads_and_vars,\n                                              global_step)\n\n      # Create token queue.\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        sync_token_queue = (\n            data_flow_ops.FIFOQueue(-1,\n                                    global_step.dtype.base_dtype,\n                                    shapes=(),\n                                    name=\"sync_token_q\",\n                                    shared_name=\"sync_token_q\"))\n        self._sync_token_queue = sync_token_queue\n\n        # dummy_queue is passed to the queue runner. Don't use the real queues\n        # because the queue runner doesn't automatically reopen it once it\n        # closed queues in PS devices.\n        dummy_queue = (\n            data_flow_ops.FIFOQueue(1,\n                                    types_pb2.DT_INT32,\n                                    shapes=(),\n                                    name=\"dummy_queue\",\n                                    shared_name=\"dummy_queue\"))\n\n      with ops.device(global_step.device), ops.name_scope(\"\"):\n        # Replicas have to wait until they can get a token from the token queue.\n        with ops.control_dependencies(train_ops):\n          token = sync_token_queue.dequeue()\n        train_op = state_ops.assign(self._local_step, token)\n\n        with ops.control_dependencies([update_op]):\n          # Sync_op needs to insert tokens to the token queue at the end of the\n          # step so the replicas can fetch them to start the next step.\n          tokens = array_ops.fill([self._tokens_per_step], global_step)\n          sync_op = sync_token_queue.enqueue_many((tokens,))\n\n        if self._variable_averages is not None:\n          with ops.control_dependencies([sync_op]), ops.name_scope(\"\"):\n            sync_op = self._variable_averages.apply(\n                self._variables_to_average)\n\n        self._chief_queue_runner = queue_runner.QueueRunner(dummy_queue,\n                                                            [sync_op])\n      for accum, dev in self._accumulator_list:\n        with ops.device(dev):\n          chief_init_ops.append(\n              accum.set_global_step(\n                  global_step, name=\"SetGlobalStep\"))\n      self.chief_init_op = control_flow_ops.group(*(chief_init_ops))\n      self._gradients_applied = True\n      return train_op\n\n  def get_chief_queue_runner(self):\n    \"\"\"Returns the QueueRunner for the chief to execute.\n\n    This includes the operations to synchronize replicas: aggregate gradients,\n    apply to variables, increment global step, insert tokens to token queue.\n\n    Note that this can only be called after calling apply_gradients() which\n    actually generates this queuerunner.\n\n    Returns:\n      A `QueueRunner` for chief to execute.\n\n    Raises:\n      ValueError: If this is called before apply_gradients().\n    \"\"\"\n    if self._gradients_applied is False:\n      raise ValueError(\"Should be called after apply_gradients().\")\n\n    return self._chief_queue_runner\n\n  def get_slot(self, *args, **kwargs):\n    \"\"\"Return a slot named \"name\" created for \"var\" by the Optimizer.\n\n    This simply wraps the get_slot() from the actual optimizer.\n\n    Args:\n      *args: Arguments for get_slot().\n      **kwargs: Keyword arguments for get_slot().\n\n    Returns:\n      The `Variable` for the slot if it was created, `None` otherwise.\n    \"\"\"\n    return self._opt.get_slot(*args, **kwargs)\n\n  def variables(self):\n    \"\"\"Fetches a list of optimizer variables in the default graph.\n\n    This wraps `variables()` from the actual optimizer. It does not include\n    the `SyncReplicasOptimizer`'s local step.\n\n    Returns:\n      A list of variables.\n    \"\"\"\n    return self._opt.variables()\n\n  def get_slot_names(self, *args, **kwargs):\n    \"\"\"Return a list of the names of slots created by the `Optimizer`.\n\n    This simply wraps the get_slot_names() from the actual optimizer.\n\n    Args:\n      *args: Arguments for get_slot().\n      **kwargs: Keyword arguments for get_slot().\n\n    Returns:\n      A list of strings.\n    \"\"\"\n    return self._opt.get_slot_names(*args, **kwargs)\n\n  def get_init_tokens_op(self, num_tokens=-1):\n    \"\"\"Returns the op to fill the sync_token_queue with the tokens.\n\n    This is supposed to be executed in the beginning of the chief/sync thread\n    so that even if the total_num_replicas is less than replicas_to_aggregate,\n    the model can still proceed as the replicas can compute multiple steps per\n    variable update. Make sure:\n    `num_tokens >= replicas_to_aggregate - total_num_replicas`.\n\n    Args:\n      num_tokens: Number of tokens to add to the queue.\n\n    Returns:\n      An op for the chief/sync replica to fill the token queue.\n\n    Raises:\n      ValueError: If this is called before apply_gradients().\n      ValueError: If num_tokens are smaller than replicas_to_aggregate -\n        total_num_replicas.\n    \"\"\"\n    if self._gradients_applied is False:\n      raise ValueError(\n          \"get_init_tokens_op() should be called after apply_gradients().\")\n\n    tokens_needed = self._replicas_to_aggregate - self._total_num_replicas\n    if num_tokens == -1:\n      num_tokens = self._replicas_to_aggregate\n    elif num_tokens < tokens_needed:\n      raise ValueError(\n          \"Too few tokens to finish the first step: %d (given) vs %d (needed)\" %\n          (num_tokens, tokens_needed))\n\n    if num_tokens > 0:\n      with ops.device(self._global_step.device), ops.name_scope(\"\"):\n        tokens = array_ops.fill([num_tokens], self._global_step)\n        init_tokens = self._sync_token_queue.enqueue_many((tokens,))\n    else:\n      init_tokens = control_flow_ops.no_op(name=\"no_init_tokens\")\n\n    return init_tokens\n\n  def make_session_run_hook(self, is_chief, num_tokens=-1):\n    \"\"\"Creates a hook to handle SyncReplicasHook ops such as initialization.\"\"\"\n    return _SyncReplicasOptimizerHook(self, is_chief, num_tokens)\n\n\nclass _SyncReplicasOptimizerHook(session_run_hook.SessionRunHook):\n  \"\"\"A SessionRunHook handles ops related to SyncReplicasOptimizer.\"\"\"\n\n  def __init__(self, sync_optimizer, is_chief, num_tokens):\n    \"\"\"Creates hook to handle SyncReplicasOptimizer initialization ops.\n\n    Args:\n      sync_optimizer: `SyncReplicasOptimizer` which this hook will initialize.\n      is_chief: `Bool`, whether is this a chief replica or not.\n      num_tokens: Number of tokens to add to the queue.\n    \"\"\"\n    self._sync_optimizer = sync_optimizer\n    self._is_chief = is_chief\n    self._num_tokens = num_tokens\n\n  def begin(self):\n    if self._sync_optimizer._gradients_applied is False:  # pylint: disable=protected-access\n      raise ValueError(\n          \"SyncReplicasOptimizer.apply_gradient should be called before using \"\n          \"the hook.\")\n    if self._is_chief:\n      self._local_init_op = self._sync_optimizer.chief_init_op\n      self._ready_for_local_init_op = (\n          self._sync_optimizer.ready_for_local_init_op)\n      self._q_runner = self._sync_optimizer.get_chief_queue_runner()\n      self._init_tokens_op = self._sync_optimizer.get_init_tokens_op(\n          self._num_tokens)\n    else:\n      self._local_init_op = self._sync_optimizer.local_step_init_op\n      self._ready_for_local_init_op = (\n          self._sync_optimizer.ready_for_local_init_op)\n      self._q_runner = None\n      self._init_tokens_op = None\n\n  def after_create_session(self, session, coord):\n    \"\"\"Runs SyncReplicasOptimizer initialization ops.\"\"\"\n    local_init_success, msg = session_manager._ready(  # pylint: disable=protected-access\n        self._ready_for_local_init_op, session,\n        \"Model is not ready for SyncReplicasOptimizer local init.\")\n    if not local_init_success:\n      raise RuntimeError(\n          \"Init operations did not make model ready for SyncReplicasOptimizer \"\n          \"local_init. Init op: %s, error: %s\" %\n          (self._local_init_op.name, msg))\n    session.run(self._local_init_op)\n    if self._init_tokens_op is not None:\n      session.run(self._init_tokens_op)\n    if self._q_runner is not None:\n      self._q_runner.create_threads(\n          session, coord=coord, daemon=True, start=True)\n", "framework": "tensorflow"}
{"repo_name": "hiraditya/fool", "file_path": "tensorflow/taxifare-trainer-model.py", "content": "#!/usr/bin/env python\n\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport shutil\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# List the CSV columns\nCSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\n\n#Choose which column is your label\nLABEL_COLUMN = 'fare_amount'\n\n# Set the default values for each CSV column in case there is a missing value\nDEFAULTS = [[0.0], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]\n\n# Create an input function that stores your data into a dataset\ndef read_dataset(filename, mode, batch_size = 512):\n    def _input_fn():\n        def decode_csv(value_column):\n            columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n            features = dict(zip(CSV_COLUMNS, columns))\n            label = features.pop(LABEL_COLUMN)\n            return features, label\n    \n        # Create list of files that match pattern\n        file_list = tf.gfile.Glob(filename)\n\n        # Create dataset from file list\n        dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n        \n        if mode == tf.estimator.ModeKeys.TRAIN:\n            num_epochs = None # indefinitely\n            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n        else:\n            num_epochs = 1 # end-of-input after this\n\n        dataset = dataset.repeat(num_epochs).batch(batch_size)\n        return dataset.make_one_shot_iterator().get_next()\n    return _input_fn\n\n# Define your feature columns\nINPUT_COLUMNS = [\n    tf.feature_column.numeric_column('pickuplon'),\n    tf.feature_column.numeric_column('pickuplat'),\n    tf.feature_column.numeric_column('dropofflat'),\n    tf.feature_column.numeric_column('dropofflon'),\n    tf.feature_column.numeric_column('passengers'),\n]\n\n# Create a function that will augment your feature set\ndef add_more_features(feats):\n    # Nothing to add (yet!)\n    return feats\n\nfeature_cols = add_more_features(INPUT_COLUMNS)\n\n# Create your serving input function so that your trained model will be able to serve predictions\ndef serving_input_fn():\n    feature_placeholders = {\n        column.name: tf.placeholder(tf.float32, [None]) for column in INPUT_COLUMNS\n    }\n    features = {\n        key: tf.expand_dims(tensor, -1)\n        for key, tensor in feature_placeholders.items()\n    }\n    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n\n# Create an estimator that we are going to train and evaluate\ndef train_and_evaluate(args):\n    estimator = tf.estimator.DNNRegressor(\n        model_dir = args['output_dir'],\n        feature_columns = feature_cols,\n        hidden_units = args['hidden_units'])\n    train_spec = tf.estimator.TrainSpec(\n        input_fn = read_dataset(args['train_data_paths'],\n                                batch_size = args['train_batch_size'],\n                                mode = tf.estimator.ModeKeys.TRAIN),\n        max_steps = args['train_steps'])\n    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn = read_dataset(args['eval_data_paths'],\n                                batch_size = 10000,\n                                mode = tf.estimator.ModeKeys.EVAL),\n        steps = None,\n        start_delay_secs = args['eval_delay_secs'],\n        throttle_secs = args['min_eval_frequency'],\n        exporters = exporter)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/estimator", "file_path": "tensorflow_estimator/python/estimator/canned/dnn_linear_combined.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"TensorFlow estimators for Linear and DNN joined training models.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport six\nimport tensorflow as tf\nfrom tensorflow.python.util.tf_export import estimator_export\nfrom tensorflow_estimator.python.estimator import estimator\nfrom tensorflow_estimator.python.estimator.canned import dnn\nfrom tensorflow_estimator.python.estimator.canned import head as head_lib\nfrom tensorflow_estimator.python.estimator.canned import linear\nfrom tensorflow_estimator.python.estimator.canned import optimizers\nfrom tensorflow_estimator.python.estimator.head import head_utils\nfrom tensorflow_estimator.python.estimator.head import regression_head\nfrom tensorflow_estimator.python.estimator.mode_keys import ModeKeys\n\n# The default learning rates are a historical artifact of the initial\n# implementation.\n_DNN_LEARNING_RATE = 0.001\n_LINEAR_LEARNING_RATE = 0.005\n\n\ndef _check_no_sync_replicas_optimizer(optimizer):\n  if isinstance(optimizer, tf.compat.v1.train.SyncReplicasOptimizer):\n    raise ValueError(\n        'SyncReplicasOptimizer does not support multi optimizers case. '\n        'Therefore, it is not supported in DNNLinearCombined model. '\n        'If you want to use this optimizer, please use either DNN or Linear '\n        'model.')\n\n\ndef _linear_learning_rate(num_linear_feature_columns):\n  \"\"\"Returns the default learning rate of the linear model.\n\n  The calculation is a historical artifact of this initial implementation, but\n  has proven a reasonable choice.\n\n  Args:\n    num_linear_feature_columns: The number of feature columns of the linear\n      model.\n\n  Returns:\n    A float.\n  \"\"\"\n  default_learning_rate = 1. / math.sqrt(num_linear_feature_columns)\n  return min(_LINEAR_LEARNING_RATE, default_learning_rate)\n\n\ndef _add_layer_summary(value, tag):\n  tf.compat.v1.summary.scalar('%s/fraction_of_zero_values' % tag,\n                              tf.math.zero_fraction(value))\n  tf.compat.v1.summary.histogram('%s/activation' % tag, value)\n\n\ndef _validate_feature_columns(linear_feature_columns, dnn_feature_columns):\n  \"\"\"Validates feature columns DNNLinearCombinedRegressor.\"\"\"\n  linear_feature_columns = linear_feature_columns or []\n  dnn_feature_columns = dnn_feature_columns or []\n  feature_columns = (list(linear_feature_columns) + list(dnn_feature_columns))\n  if not feature_columns:\n    raise ValueError('Either linear_feature_columns or dnn_feature_columns '\n                     'must be defined.')\n  return feature_columns\n\n\ndef _dnn_linear_combined_model_fn_v2(\n    features,\n    labels,\n    mode,\n    head,\n    linear_feature_columns=None,\n    linear_optimizer='Ftrl',\n    dnn_feature_columns=None,\n    dnn_optimizer='Adagrad',\n    dnn_hidden_units=None,\n    dnn_activation_fn=tf.nn.relu,\n    dnn_dropout=None,\n    config=None,\n    batch_norm=False,\n    linear_sparse_combiner='sum',\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE):\n  \"\"\"Deep Neural Net and Linear combined model_fn.\n\n  Args:\n    features: dict of `Tensor`.\n    labels: `Tensor` of shape [batch_size, 1] or [batch_size] labels of dtype\n      `int32` or `int64` in the range `[0, n_classes)`.\n    mode: Defines whether this is training, evaluation or prediction. See\n      `ModeKeys`.\n    head: A `Head` instance.\n    linear_feature_columns: An iterable containing all the feature columns used\n      by the Linear model.\n    linear_optimizer: string, `Optimizer` object, or callable that defines the\n      optimizer to use for training the Linear model. Defaults to the Ftrl\n      optimizer.\n    dnn_feature_columns: An iterable containing all the feature columns used by\n      the DNN model.\n    dnn_optimizer: string, `Optimizer` object, or callable that defines the\n      optimizer to use for training the DNN model. Defaults to the Adagrad\n      optimizer.\n    dnn_hidden_units: List of hidden units per DNN layer.\n    dnn_activation_fn: Activation function applied to each DNN layer. If `None`,\n      will use `tf.nn.relu`.\n    dnn_dropout: When not `None`, the probability we will drop out a given DNN\n      coordinate.\n    config: `RunConfig` object to configure the runtime settings.\n    batch_norm: Whether to use batch normalization after each hidden layer.\n    linear_sparse_combiner: A string specifying how to reduce the linear model\n      if a categorical column is multivalent.  One of \"mean\", \"sqrtn\", and\n      \"sum\".\n    loss_reduction: One of `tf.keras.losses.Reduction` except `NONE`. Describes\n      how to reduce training loss over batch. Defaults to `SUM_OVER_BATCH_SIZE`.\n\n  Returns:\n    An `EstimatorSpec` instance.\n\n  Raises:\n    ValueError: If both `linear_feature_columns` and `dnn_features_columns`\n      are empty at the same time, or `input_layer_partitioner` is missing,\n      or features has the wrong type.\n  \"\"\"\n  if not isinstance(features, dict):\n    raise ValueError('features should be a dictionary of `Tensor`s. '\n                     'Given type: {}'.format(type(features)))\n  if not linear_feature_columns and not dnn_feature_columns:\n    raise ValueError(\n        'Either linear_feature_columns or dnn_feature_columns must be defined.')\n\n  del config\n\n  # Build DNN Logits.\n  if not dnn_feature_columns:\n    dnn_logits = None\n  else:\n    if mode == ModeKeys.TRAIN:\n      dnn_optimizer = optimizers.get_optimizer_instance_v2(\n          dnn_optimizer, learning_rate=_DNN_LEARNING_RATE)\n      _check_no_sync_replicas_optimizer(dnn_optimizer)\n\n    if not dnn_hidden_units:\n      raise ValueError(\n          'dnn_hidden_units must be defined when dnn_feature_columns is '\n          'specified.')\n    dnn_logits, dnn_trainable_variables, dnn_update_ops = (\n        dnn._dnn_model_fn_builder_v2(  # pylint: disable=protected-access\n            units=head.logits_dimension,\n            hidden_units=dnn_hidden_units,\n            feature_columns=dnn_feature_columns,\n            activation_fn=dnn_activation_fn,\n            dropout=dnn_dropout,\n            batch_norm=batch_norm,\n            features=features,\n            mode=mode))\n\n  if not linear_feature_columns:\n    linear_logits = None\n  else:\n    if mode == ModeKeys.TRAIN:\n      linear_optimizer = optimizers.get_optimizer_instance_v2(\n          linear_optimizer,\n          learning_rate=_linear_learning_rate(len(linear_feature_columns)))\n      _check_no_sync_replicas_optimizer(linear_optimizer)\n\n    linear_logits, linear_trainable_variables = (\n        linear._linear_model_fn_builder_v2(  # pylint: disable=protected-access\n            units=head.logits_dimension,\n            feature_columns=linear_feature_columns,\n            sparse_combiner=linear_sparse_combiner,\n            features=features))\n    _add_layer_summary(linear_logits, 'linear')\n\n  # Combine logits and build full model.\n  if dnn_logits is not None and linear_logits is not None:\n    logits = dnn_logits + linear_logits\n  elif dnn_logits is not None:\n    logits = dnn_logits\n  else:\n    logits = linear_logits\n\n  def _train_op_fn(loss):\n    \"\"\"Returns the op to optimize the loss.\"\"\"\n    train_ops = []\n    # Scale loss by number of replicas.\n    if loss_reduction == tf.losses.Reduction.SUM_OVER_BATCH_SIZE:\n      num_replicas = tf.distribute.get_strategy().num_replicas_in_sync\n      if num_replicas > 1:\n        loss *= (1. / num_replicas)\n\n    if dnn_logits is not None:\n      train_ops.extend(dnn_optimizer.get_updates(loss, dnn_trainable_variables))\n      if dnn_update_ops is not None:\n        train_ops.extend(dnn_update_ops)\n    if linear_logits is not None:\n      train_ops.extend(\n          linear_optimizer.get_updates(loss, linear_trainable_variables))\n    train_op = tf.group(*train_ops)\n    return train_op\n\n  # In TRAIN mode, asssign global_step variable to optimizer.iterations to\n  # make global_step increased correctly, as Hooks relies on global step as\n  # step counter. Note that, Only one model's optimizer needs this assignment.\n  if mode == ModeKeys.TRAIN:\n    if dnn_logits is not None:\n      dnn_optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()\n    else:\n      linear_optimizer.iterations = \\\n        tf.compat.v1.train.get_or_create_global_step()\n\n  return head.create_estimator_spec(\n      features=features,\n      mode=mode,\n      labels=labels,\n      train_op_fn=_train_op_fn,\n      logits=logits)\n\n\ndef _dnn_linear_combined_model_fn(features,\n                                  labels,\n                                  mode,\n                                  head,\n                                  linear_feature_columns=None,\n                                  linear_optimizer='Ftrl',\n                                  dnn_feature_columns=None,\n                                  dnn_optimizer='Adagrad',\n                                  dnn_hidden_units=None,\n                                  dnn_activation_fn=tf.nn.relu,\n                                  dnn_dropout=None,\n                                  input_layer_partitioner=None,\n                                  config=None,\n                                  batch_norm=False,\n                                  linear_sparse_combiner='sum'):\n  \"\"\"Deep Neural Net and Linear combined model_fn.\n\n  Args:\n    features: dict of `Tensor`.\n    labels: `Tensor` of shape [batch_size, 1] or [batch_size] labels of dtype\n      `int32` or `int64` in the range `[0, n_classes)`.\n    mode: Defines whether this is training, evaluation or prediction. See\n      `ModeKeys`.\n    head: A `Head` instance.\n    linear_feature_columns: An iterable containing all the feature columns used\n      by the Linear model.\n    linear_optimizer: string, `Optimizer` object, or callable that defines the\n      optimizer to use for training the Linear model. Defaults to the Ftrl\n      optimizer.\n    dnn_feature_columns: An iterable containing all the feature columns used by\n      the DNN model.\n    dnn_optimizer: string, `Optimizer` object, or callable that defines the\n      optimizer to use for training the DNN model. Defaults to the Adagrad\n      optimizer.\n    dnn_hidden_units: List of hidden units per DNN layer.\n    dnn_activation_fn: Activation function applied to each DNN layer. If `None`,\n      will use `tf.nn.relu`.\n    dnn_dropout: When not `None`, the probability we will drop out a given DNN\n      coordinate.\n    input_layer_partitioner: Partitioner for input layer.\n    config: `RunConfig` object to configure the runtime settings.\n    batch_norm: Whether to use batch normalization after each hidden layer.\n    linear_sparse_combiner: A string specifying how to reduce the linear model\n      if a categorical column is multivalent.  One of \"mean\", \"sqrtn\", and\n      \"sum\".\n\n  Returns:\n    An `EstimatorSpec` instance.\n\n  Raises:\n    ValueError: If both `linear_feature_columns` and `dnn_features_columns`\n      are empty at the same time, or `input_layer_partitioner` is missing,\n      or features has the wrong type.\n  \"\"\"\n  if not isinstance(features, dict):\n    raise ValueError('features should be a dictionary of `Tensor`s. '\n                     'Given type: {}'.format(type(features)))\n  if not linear_feature_columns and not dnn_feature_columns:\n    raise ValueError(\n        'Either linear_feature_columns or dnn_feature_columns must be defined.')\n\n  num_ps_replicas = config.num_ps_replicas if config else 0\n  input_layer_partitioner = input_layer_partitioner or (\n      tf.compat.v1.min_max_variable_partitioner(\n          max_partitions=num_ps_replicas, min_slice_size=64 << 20))\n\n  # Build DNN Logits.\n  dnn_parent_scope = 'dnn'\n\n  if not dnn_feature_columns:\n    dnn_logits = None\n  else:\n    dnn_optimizer = optimizers.get_optimizer_instance(\n        dnn_optimizer, learning_rate=_DNN_LEARNING_RATE)\n    _check_no_sync_replicas_optimizer(dnn_optimizer)\n    if not dnn_hidden_units:\n      raise ValueError(\n          'dnn_hidden_units must be defined when dnn_feature_columns is '\n          'specified.')\n    dnn_partitioner = (\n        tf.compat.v1.min_max_variable_partitioner(\n            max_partitions=num_ps_replicas))\n    with tf.compat.v1.variable_scope(\n        dnn_parent_scope,\n        values=tuple(six.itervalues(features)),\n        partitioner=dnn_partitioner) as scope:\n      dnn_absolute_scope = scope.name\n      dnn_logit_fn = dnn.dnn_logit_fn_builder(\n          units=head.logits_dimension,\n          hidden_units=dnn_hidden_units,\n          feature_columns=dnn_feature_columns,\n          activation_fn=dnn_activation_fn,\n          dropout=dnn_dropout,\n          batch_norm=batch_norm,\n          input_layer_partitioner=input_layer_partitioner)\n      dnn_logits = dnn_logit_fn(features=features, mode=mode)\n\n  linear_parent_scope = 'linear'\n\n  if not linear_feature_columns:\n    linear_logits = None\n  else:\n    linear_optimizer = optimizers.get_optimizer_instance(\n        linear_optimizer,\n        learning_rate=_linear_learning_rate(len(linear_feature_columns)))\n    _check_no_sync_replicas_optimizer(linear_optimizer)\n    with tf.compat.v1.variable_scope(\n        linear_parent_scope,\n        values=tuple(six.itervalues(features)),\n        partitioner=input_layer_partitioner) as scope:\n      linear_absolute_scope = scope.name\n      logit_fn = linear.linear_logit_fn_builder(\n          units=head.logits_dimension,\n          feature_columns=linear_feature_columns,\n          sparse_combiner=linear_sparse_combiner)\n      linear_logits = logit_fn(features=features)\n      _add_layer_summary(linear_logits, scope.name)\n\n  # Combine logits and build full model.\n  if dnn_logits is not None and linear_logits is not None:\n    logits = dnn_logits + linear_logits\n  elif dnn_logits is not None:\n    logits = dnn_logits\n  else:\n    logits = linear_logits\n\n  def _train_op_fn(loss):\n    \"\"\"Returns the op to optimize the loss.\"\"\"\n    train_ops = []\n    global_step = tf.compat.v1.train.get_global_step()\n    if dnn_logits is not None:\n      train_ops.append(\n          dnn_optimizer.minimize(\n              loss,\n              var_list=tf.compat.v1.get_collection(\n                  tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES,\n                  scope=dnn_absolute_scope)))\n    if linear_logits is not None:\n      train_ops.append(\n          linear_optimizer.minimize(\n              loss,\n              var_list=tf.compat.v1.get_collection(\n                  tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES,\n                  scope=linear_absolute_scope)))\n\n    train_op = tf.group(*train_ops)\n    with tf.control_dependencies([train_op]):\n      return tf.compat.v1.assign_add(global_step, 1).op\n\n  return head.create_estimator_spec(\n      features=features,\n      mode=mode,\n      labels=labels,\n      train_op_fn=_train_op_fn,\n      logits=logits)\n\n\n@estimator_export('estimator.DNNLinearCombinedClassifier', v1=[])\nclass DNNLinearCombinedClassifierV2(estimator.EstimatorV2):\n  \"\"\"An estimator for TensorFlow Linear and DNN joined classification models.\n\n  Note: This estimator is also known as wide-n-deep.\n\n  Example:\n\n  ```python\n  numeric_feature = numeric_column(...)\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n  categorical_feature_a_emb = embedding_column(\n      categorical_column=categorical_feature_a, ...)\n  categorical_feature_b_emb = embedding_column(\n      categorical_id_column=categorical_feature_b, ...)\n\n  estimator = tf.estimator.DNNLinearCombinedClassifier(\n      # wide settings\n      linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],\n      linear_optimizer=tf.keras.optimizers.Ftrl(...),\n      # deep settings\n      dnn_feature_columns=[\n          categorical_feature_a_emb, categorical_feature_b_emb,\n          numeric_feature],\n      dnn_hidden_units=[1000, 500, 100],\n      dnn_optimizer=tf.keras.optimizers.Adagrad(...),\n      # warm-start settings\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n  # To apply L1 and L2 regularization, you can set dnn_optimizer to:\n  tf.compat.v1.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001,\n      l2_regularization_strength=0.001)\n  # To apply learning rate decay, you can set dnn_optimizer to a callable:\n  lambda: tf.keras.optimizers.Adam(\n      learning_rate=tf.compat.v1.train.exponential_decay(\n          learning_rate=0.1,\n          global_step=tf.compat.v1.train.get_global_step(),\n          decay_steps=10000,\n          decay_rate=0.96)\n  # It is the same for linear_optimizer.\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train, steps=100)\n  metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n  otherwise there will be a `KeyError`:\n\n  * for each `column` in `dnn_feature_columns` + `linear_feature_columns`:\n    - if `column` is a `CategoricalColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedCategoricalColumn`, two features: the first\n      with `key` the id column name, the second with `key` the weight column\n      name. Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `DenseColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using softmax cross entropy.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  \"\"\"\n\n  def __init__(self,\n               model_dir=None,\n               linear_feature_columns=None,\n               linear_optimizer='Ftrl',\n               dnn_feature_columns=None,\n               dnn_optimizer='Adagrad',\n               dnn_hidden_units=None,\n               dnn_activation_fn=tf.nn.relu,\n               dnn_dropout=None,\n               n_classes=2,\n               weight_column=None,\n               label_vocabulary=None,\n               config=None,\n               warm_start_from=None,\n               loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n               batch_norm=False,\n               linear_sparse_combiner='sum'):\n    \"\"\"Initializes a DNNLinearCombinedClassifier instance.\n\n    Args:\n      model_dir: Directory to save model parameters, graph and etc. This can\n        also be used to load checkpoints from the directory into a estimator to\n        continue training a previously saved model.\n      linear_feature_columns: An iterable containing all the feature columns\n        used by linear part of the model. All items in the set must be instances\n        of classes derived from `FeatureColumn`.\n      linear_optimizer: An instance of `tf.keras.optimizers.*` used to apply\n        gradients to the linear part of the model. Can also be a string (one of\n        'Adagrad', 'Adam', 'Ftrl', 'RMSProp', 'SGD'), or callable. Defaults to\n        FTRL optimizer.\n      dnn_feature_columns: An iterable containing all the feature columns used\n        by deep part of the model. All items in the set must be instances of\n        classes derived from `FeatureColumn`.\n      dnn_optimizer: An instance of `tf.keras.optimizers.*` used to apply\n        gradients to the deep part of the model. Can also be a string (one of\n        'Adagrad', 'Adam', 'Ftrl', 'RMSProp', 'SGD'), or callable. Defaults to\n        Adagrad optimizer.\n      dnn_hidden_units: List of hidden units per layer. All layers are fully\n        connected.\n      dnn_activation_fn: Activation function applied to each layer. If None,\n        will use `tf.nn.relu`.\n      dnn_dropout: When not None, the probability we will drop out a given\n        coordinate.\n      n_classes: Number of label classes. Defaults to 2, namely binary\n        classification. Must be > 1.\n      weight_column: A string or a `_NumericColumn` created by\n        `tf.feature_column.numeric_column` defining feature column representing\n        weights. It is used to down weight or boost examples during training. It\n        will be multiplied by the loss of the example. If it is a string, it is\n        used as a key to fetch weight tensor from the `features`. If it is a\n        `_NumericColumn`, raw tensor is fetched by key `weight_column.key`, then\n        weight_column.normalizer_fn is applied on it to get weight tensor.\n      label_vocabulary: A list of strings represents possible label values. If\n        given, labels must be string type and have any value in\n        `label_vocabulary`. If it is not given, that means labels are already\n        encoded as integer or float within [0, 1] for `n_classes=2` and encoded\n        as integer values in {0, 1,..., n_classes-1} for `n_classes`>2 . Also\n        there will be errors if vocabulary is not provided and labels are\n        string.\n      config: RunConfig object to configure the runtime settings.\n      warm_start_from: A string filepath to a checkpoint to warm-start from, or\n        a `WarmStartSettings` object to fully configure warm-starting.  If the\n        string filepath is provided instead of a `WarmStartSettings`, then all\n        weights are warm-started, and it is assumed that vocabularies and Tensor\n        names are unchanged.\n      loss_reduction: One of `tf.losses.Reduction` except `NONE`. Describes how\n        to reduce training loss over batch. Defaults to `SUM_OVER_BATCH_SIZE`.\n      batch_norm: Whether to use batch normalization after each hidden layer.\n      linear_sparse_combiner: A string specifying how to reduce the linear model\n        if a categorical column is multivalent.  One of \"mean\", \"sqrtn\", and\n        \"sum\" -- these are effectively different ways to do example-level\n        normalization, which can be useful for bag-of-words features.  For more\n        details, see `tf.feature_column.linear_model`.\n\n    Raises:\n      ValueError: If both linear_feature_columns and dnn_features_columns are\n        empty at the same time.\n    \"\"\"\n    self._feature_columns = _validate_feature_columns(\n        linear_feature_columns=linear_feature_columns,\n        dnn_feature_columns=dnn_feature_columns)\n\n    head = head_utils.binary_or_multi_class_head(\n        n_classes,\n        weight_column=weight_column,\n        label_vocabulary=label_vocabulary,\n        loss_reduction=loss_reduction)\n    estimator._canned_estimator_api_gauge.get_cell('Classifier').set(  # pylint: disable=protected-access\n        'DNNLinearCombined')\n\n    def _model_fn(features, labels, mode, config):\n      \"\"\"Call the _dnn_linear_combined_model_fn.\"\"\"\n      return _dnn_linear_combined_model_fn_v2(\n          features=features,\n          labels=labels,\n          mode=mode,\n          head=head,\n          linear_feature_columns=linear_feature_columns,\n          linear_optimizer=linear_optimizer,\n          dnn_feature_columns=dnn_feature_columns,\n          dnn_optimizer=dnn_optimizer,\n          dnn_hidden_units=dnn_hidden_units,\n          dnn_activation_fn=dnn_activation_fn,\n          dnn_dropout=dnn_dropout,\n          config=config,\n          batch_norm=batch_norm,\n          linear_sparse_combiner=linear_sparse_combiner,\n          loss_reduction=loss_reduction)\n\n    super(DNNLinearCombinedClassifierV2, self).__init__(\n        model_fn=_model_fn,\n        model_dir=model_dir,\n        config=config,\n        warm_start_from=warm_start_from)\n\n\n@estimator_export(v1=['estimator.DNNLinearCombinedClassifier'])  # pylint: disable=missing-docstring\nclass DNNLinearCombinedClassifier(estimator.Estimator):\n  __doc__ = DNNLinearCombinedClassifierV2.__doc__.replace(\n      'SUM_OVER_BATCH_SIZE', 'SUM')\n\n  def __init__(self,\n               model_dir=None,\n               linear_feature_columns=None,\n               linear_optimizer='Ftrl',\n               dnn_feature_columns=None,\n               dnn_optimizer='Adagrad',\n               dnn_hidden_units=None,\n               dnn_activation_fn=tf.nn.relu,\n               dnn_dropout=None,\n               n_classes=2,\n               weight_column=None,\n               label_vocabulary=None,\n               input_layer_partitioner=None,\n               config=None,\n               warm_start_from=None,\n               loss_reduction=tf.compat.v1.losses.Reduction.SUM,\n               batch_norm=False,\n               linear_sparse_combiner='sum'):\n    self._feature_columns = _validate_feature_columns(\n        linear_feature_columns=linear_feature_columns,\n        dnn_feature_columns=dnn_feature_columns)\n\n    head = head_lib._binary_logistic_or_multi_class_head(  # pylint: disable=protected-access\n        n_classes, weight_column, label_vocabulary, loss_reduction)\n    estimator._canned_estimator_api_gauge.get_cell('Classifier').set(\n        'DNNLinearCombined')  # pylint: disable=protected-access\n\n    def _model_fn(features, labels, mode, config):\n      \"\"\"Call the _dnn_linear_combined_model_fn.\"\"\"\n      return _dnn_linear_combined_model_fn(\n          features=features,\n          labels=labels,\n          mode=mode,\n          head=head,\n          linear_feature_columns=linear_feature_columns,\n          linear_optimizer=linear_optimizer,\n          dnn_feature_columns=dnn_feature_columns,\n          dnn_optimizer=dnn_optimizer,\n          dnn_hidden_units=dnn_hidden_units,\n          dnn_activation_fn=dnn_activation_fn,\n          dnn_dropout=dnn_dropout,\n          input_layer_partitioner=input_layer_partitioner,\n          config=config,\n          batch_norm=batch_norm,\n          linear_sparse_combiner=linear_sparse_combiner)\n\n    super(DNNLinearCombinedClassifier, self).__init__(\n        model_fn=_model_fn,\n        model_dir=model_dir,\n        config=config,\n        warm_start_from=warm_start_from)\n\n\ndef _init_dnn_linear_combined_estimator(head, linear_feature_columns,\n                                        linear_optimizer, dnn_feature_columns,\n                                        dnn_optimizer, dnn_hidden_units,\n                                        dnn_activation_fn, dnn_dropout,\n                                        input_layer_partitioner,\n                                        linear_sparse_combiner):\n  \"\"\"Helper function for the initialization of DNNLinearCombinedEstimator.\"\"\"\n  linear_feature_columns = linear_feature_columns or []\n  dnn_feature_columns = dnn_feature_columns or []\n  feature_columns = (list(linear_feature_columns) + list(dnn_feature_columns))\n  if not feature_columns:\n    raise ValueError('Either linear_feature_columns or dnn_feature_columns '\n                     'must be defined.')\n\n  def _model_fn(features, labels, mode, config):\n    \"\"\"Call the _dnn_linear_combined_model_fn.\"\"\"\n    return _dnn_linear_combined_model_fn(\n        features=features,\n        labels=labels,\n        mode=mode,\n        head=head,\n        linear_feature_columns=linear_feature_columns,\n        linear_optimizer=linear_optimizer,\n        dnn_feature_columns=dnn_feature_columns,\n        dnn_optimizer=dnn_optimizer,\n        dnn_hidden_units=dnn_hidden_units,\n        dnn_activation_fn=dnn_activation_fn,\n        dnn_dropout=dnn_dropout,\n        input_layer_partitioner=input_layer_partitioner,\n        config=config,\n        linear_sparse_combiner=linear_sparse_combiner)\n\n  return feature_columns, _model_fn\n\n\n@estimator_export('estimator.DNNLinearCombinedEstimator', v1=[])\nclass DNNLinearCombinedEstimatorV2(estimator.EstimatorV2):\n  \"\"\"An estimator for TensorFlow Linear and DNN joined models with custom head.\n\n  Note: This estimator is also known as wide-n-deep.\n\n  Example:\n\n  ```python\n  numeric_feature = numeric_column(...)\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n  categorical_feature_a_emb = embedding_column(\n      categorical_column=categorical_feature_a, ...)\n  categorical_feature_b_emb = embedding_column(\n      categorical_column=categorical_feature_b, ...)\n\n  estimator = tf.estimator.DNNLinearCombinedEstimator(\n      head=tf.estimator.MultiLabelHead(n_classes=3),\n      # wide settings\n      linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],\n      linear_optimizer=tf.keras.optimizers.Ftrl(...),\n      # deep settings\n      dnn_feature_columns=[\n          categorical_feature_a_emb, categorical_feature_b_emb,\n          numeric_feature],\n      dnn_hidden_units=[1000, 500, 100],\n      dnn_optimizer=tf.keras.optimizers.Adagrad(...))\n\n  # To apply L1 and L2 regularization, you can set dnn_optimizer to:\n  tf.compat.v1.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001,\n      l2_regularization_strength=0.001)\n  # To apply learning rate decay, you can set dnn_optimizer to a callable:\n  lambda: tf.keras.optimizers.Adam(\n      learning_rate=tf.compat.v1.train.exponential_decay(\n          learning_rate=0.1,\n          global_step=tf.compat.v1.train.get_global_step(),\n          decay_steps=10000,\n          decay_rate=0.96)\n  # It is the same for linear_optimizer.\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train, steps=100)\n  metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n  otherwise there will be a `KeyError`:\n\n  * for each `column` in `dnn_feature_columns` + `linear_feature_columns`:\n    - if `column` is a `CategoricalColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedCategoricalColumn`, two features: the first\n      with `key` the id column name, the second with `key` the weight column\n      name. Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `DenseColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using mean squared error.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  \"\"\"\n\n  def __init__(self,\n               head,\n               model_dir=None,\n               linear_feature_columns=None,\n               linear_optimizer='Ftrl',\n               dnn_feature_columns=None,\n               dnn_optimizer='Adagrad',\n               dnn_hidden_units=None,\n               dnn_activation_fn=tf.nn.relu,\n               dnn_dropout=None,\n               config=None,\n               batch_norm=False,\n               linear_sparse_combiner='sum'):\n    \"\"\"Initializes a DNNLinearCombinedEstimator instance.\n\n    Args:\n      head: A `Head` instance constructed with a method such as\n        `tf.estimator.MultiLabelHead`.\n      model_dir: Directory to save model parameters, graph and etc. This can\n        also be used to load checkpoints from the directory into an estimator to\n        continue training a previously saved model.\n      linear_feature_columns: An iterable containing all the feature columns\n        used by linear part of the model. All items in the set must be instances\n        of classes derived from `FeatureColumn`.\n      linear_optimizer: An instance of `tf.keras.optimizers.*` used to apply\n        gradients to the linear part of the model. Can also be a string (one of\n        'Adagrad', 'Adam', 'Ftrl', 'RMSProp', 'SGD'), or callable. Defaults to\n        FTRL optimizer.\n      dnn_feature_columns: An iterable containing all the feature columns used\n        by deep part of the model. All items in the set must be instances of\n        classes derived from `FeatureColumn`.\n      dnn_optimizer: An instance of `tf.keras.optimizers.*` used to apply\n        gradients to the deep part of the model. Can also be a string (one of\n        'Adagrad', 'Adam', 'Ftrl', 'RMSProp', 'SGD'), or callable. Defaults to\n        Adagrad optimizer.\n      dnn_hidden_units: List of hidden units per layer. All layers are fully\n        connected.\n      dnn_activation_fn: Activation function applied to each layer. If None,\n        will use `tf.nn.relu`.\n      dnn_dropout: When not None, the probability we will drop out a given\n        coordinate.\n      config: RunConfig object to configure the runtime settings.\n      batch_norm: Whether to use batch normalization after each hidden layer.\n      linear_sparse_combiner: A string specifying how to reduce the linear model\n        if a categorical column is multivalent.  One of \"mean\", \"sqrtn\", and\n        \"sum\" -- these are effectively different ways to do example-level\n        normalization, which can be useful for bag-of-words features.  For more\n        details, see `tf.feature_column.linear_model`.\n\n    Raises:\n      ValueError: If both linear_feature_columns and dnn_features_columns are\n        empty at the same time.\n    \"\"\"\n    self._feature_columns = _validate_feature_columns(\n        linear_feature_columns=linear_feature_columns,\n        dnn_feature_columns=dnn_feature_columns)\n    estimator._canned_estimator_api_gauge.get_cell('Estimator').set(\n        'DNNLinearCombined')  # pylint: disable=protected-access\n\n    def _model_fn(features, labels, mode, config):\n      \"\"\"Call the _dnn_linear_combined_model_fn.\"\"\"\n      return _dnn_linear_combined_model_fn_v2(\n          features=features,\n          labels=labels,\n          mode=mode,\n          head=head,\n          linear_feature_columns=linear_feature_columns,\n          linear_optimizer=linear_optimizer,\n          dnn_feature_columns=dnn_feature_columns,\n          dnn_optimizer=dnn_optimizer,\n          dnn_hidden_units=dnn_hidden_units,\n          dnn_activation_fn=dnn_activation_fn,\n          dnn_dropout=dnn_dropout,\n          config=config,\n          batch_norm=batch_norm,\n          linear_sparse_combiner=linear_sparse_combiner)\n\n    super(DNNLinearCombinedEstimatorV2, self).__init__(\n        model_fn=_model_fn, model_dir=model_dir, config=config)\n\n\n@estimator_export(v1=['estimator.DNNLinearCombinedEstimator'])  # pylint: disable=missing-docstring\nclass DNNLinearCombinedEstimator(estimator.Estimator):\n  __doc__ = DNNLinearCombinedEstimatorV2.__doc__\n\n  def __init__(self,\n               head,\n               model_dir=None,\n               linear_feature_columns=None,\n               linear_optimizer='Ftrl',\n               dnn_feature_columns=None,\n               dnn_optimizer='Adagrad',\n               dnn_hidden_units=None,\n               dnn_activation_fn=tf.nn.relu,\n               dnn_dropout=None,\n               input_layer_partitioner=None,\n               config=None,\n               batch_norm=False,\n               linear_sparse_combiner='sum'):\n    self._feature_columns = _validate_feature_columns(\n        linear_feature_columns=linear_feature_columns,\n        dnn_feature_columns=dnn_feature_columns)\n    estimator._canned_estimator_api_gauge.get_cell('Estimator').set(\n        'DNNLinearCombined')  # pylint: disable=protected-access\n\n    def _model_fn(features, labels, mode, config):\n      \"\"\"Call the _dnn_linear_combined_model_fn.\"\"\"\n      return _dnn_linear_combined_model_fn(\n          features=features,\n          labels=labels,\n          mode=mode,\n          head=head,\n          linear_feature_columns=linear_feature_columns,\n          linear_optimizer=linear_optimizer,\n          dnn_feature_columns=dnn_feature_columns,\n          dnn_optimizer=dnn_optimizer,\n          dnn_hidden_units=dnn_hidden_units,\n          dnn_activation_fn=dnn_activation_fn,\n          dnn_dropout=dnn_dropout,\n          input_layer_partitioner=input_layer_partitioner,\n          config=config,\n          batch_norm=batch_norm,\n          linear_sparse_combiner=linear_sparse_combiner)\n\n    super(DNNLinearCombinedEstimator, self).__init__(\n        model_fn=_model_fn, model_dir=model_dir, config=config)\n\n\n@estimator_export('estimator.DNNLinearCombinedRegressor', v1=[])\nclass DNNLinearCombinedRegressorV2(estimator.EstimatorV2):\n  \"\"\"An estimator for TensorFlow Linear and DNN joined models for regression.\n\n  Note: This estimator is also known as wide-n-deep.\n\n  Example:\n\n  ```python\n  numeric_feature = numeric_column(...)\n  categorical_column_a = categorical_column_with_hash_bucket(...)\n  categorical_column_b = categorical_column_with_hash_bucket(...)\n\n  categorical_feature_a_x_categorical_feature_b = crossed_column(...)\n  categorical_feature_a_emb = embedding_column(\n      categorical_column=categorical_feature_a, ...)\n  categorical_feature_b_emb = embedding_column(\n      categorical_column=categorical_feature_b, ...)\n\n  estimator = tf.estimator.DNNLinearCombinedRegressor(\n      # wide settings\n      linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],\n      linear_optimizer=tf.keras.optimizers.Ftrl(...),\n      # deep settings\n      dnn_feature_columns=[\n          categorical_feature_a_emb, categorical_feature_b_emb,\n          numeric_feature],\n      dnn_hidden_units=[1000, 500, 100],\n      dnn_optimizer=tf.keras.optimizers.Adagrad(...),\n      # warm-start settings\n      warm_start_from=\"/path/to/checkpoint/dir\")\n\n  # To apply L1 and L2 regularization, you can set dnn_optimizer to:\n  tf.compat.v1.train.ProximalAdagradOptimizer(\n      learning_rate=0.1,\n      l1_regularization_strength=0.001,\n      l2_regularization_strength=0.001)\n  # To apply learning rate decay, you can set dnn_optimizer to a callable:\n  lambda: tf.keras.optimizers.Adam(\n      learning_rate=tf.compat.v1.train.exponential_decay(\n          learning_rate=0.1,\n          global_step=tf.compat.v1.train.get_global_step(),\n          decay_steps=10000,\n          decay_rate=0.96)\n  # It is the same for linear_optimizer.\n\n  # Input builders\n  def input_fn_train:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_eval:\n    # Returns tf.data.Dataset of (x, y) tuple where y represents label's class\n    # index.\n    pass\n  def input_fn_predict:\n    # Returns tf.data.Dataset of (x, None) tuple.\n    pass\n  estimator.train(input_fn=input_fn_train, steps=100)\n  metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n  predictions = estimator.predict(input_fn=input_fn_predict)\n  ```\n\n  Input of `train` and `evaluate` should have following features,\n  otherwise there will be a `KeyError`:\n\n  * for each `column` in `dnn_feature_columns` + `linear_feature_columns`:\n    - if `column` is a `CategoricalColumn`, a feature with `key=column.name`\n      whose `value` is a `SparseTensor`.\n    - if `column` is a `WeightedCategoricalColumn`, two features: the first\n      with `key` the id column name, the second with `key` the weight column\n      name. Both features' `value` must be a `SparseTensor`.\n    - if `column` is a `DenseColumn`, a feature with `key=column.name`\n      whose `value` is a `Tensor`.\n\n  Loss is calculated by using mean squared error.\n\n  @compatibility(eager)\n  Estimators can be used while eager execution is enabled. Note that `input_fn`\n  and all hooks are executed inside a graph context, so they have to be written\n  to be compatible with graph mode. Note that `input_fn` code using `tf.data`\n  generally works in both graph and eager modes.\n  @end_compatibility\n  \"\"\"\n\n  def __init__(self,\n               model_dir=None,\n               linear_feature_columns=None,\n               linear_optimizer='Ftrl',\n               dnn_feature_columns=None,\n               dnn_optimizer='Adagrad',\n               dnn_hidden_units=None,\n               dnn_activation_fn=tf.nn.relu,\n               dnn_dropout=None,\n               label_dimension=1,\n               weight_column=None,\n               config=None,\n               warm_start_from=None,\n               loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n               batch_norm=False,\n               linear_sparse_combiner='sum'):\n    \"\"\"Initializes a DNNLinearCombinedRegressor instance.\n\n    Args:\n      model_dir: Directory to save model parameters, graph and etc. This can\n        also be used to load checkpoints from the directory into a estimator to\n        continue training a previously saved model.\n      linear_feature_columns: An iterable containing all the feature columns\n        used by linear part of the model. All items in the set must be instances\n        of classes derived from `FeatureColumn`.\n      linear_optimizer: An instance of `tf.keras.optimizers.*` used to apply\n        gradients to the linear part of the model. Can also be a string (one of\n        'Adagrad', 'Adam', 'Ftrl', 'RMSProp', 'SGD'), or callable. Defaults to\n        FTRL optimizer.\n      dnn_feature_columns: An iterable containing all the feature columns used\n        by deep part of the model. All items in the set must be instances of\n        classes derived from `FeatureColumn`.\n      dnn_optimizer: An instance of `tf.keras.optimizers.*` used to apply\n        gradients to the deep part of the model. Can also be a string (one of\n        'Adagrad', 'Adam', 'Ftrl', 'RMSProp', 'SGD'), or callable. Defaults to\n        Adagrad optimizer.\n      dnn_hidden_units: List of hidden units per layer. All layers are fully\n        connected.\n      dnn_activation_fn: Activation function applied to each layer. If None,\n        will use `tf.nn.relu`.\n      dnn_dropout: When not None, the probability we will drop out a given\n        coordinate.\n      label_dimension: Number of regression targets per example. This is the\n        size of the last dimension of the labels and logits `Tensor` objects\n        (typically, these have shape `[batch_size, label_dimension]`).\n      weight_column: A string or a `NumericColumn` created by\n        `tf.feature_column.numeric_column` defining feature column representing\n        weights. It is used to down weight or boost examples during training. It\n        will be multiplied by the loss of the example. If it is a string, it is\n        used as a key to fetch weight tensor from the `features`. If it is a\n        `_NumericColumn`, raw tensor is fetched by key `weight_column.key`, then\n        weight_column.normalizer_fn is applied on it to get weight tensor.\n      config: RunConfig object to configure the runtime settings.\n      warm_start_from: A string filepath to a checkpoint to warm-start from, or\n        a `WarmStartSettings` object to fully configure warm-starting.  If the\n        string filepath is provided instead of a `WarmStartSettings`, then all\n        weights are warm-started, and it is assumed that vocabularies and Tensor\n        names are unchanged.\n      loss_reduction: One of `tf.losses.Reduction` except `NONE`. Describes how\n        to reduce training loss over batch. Defaults to `SUM_OVER_BATCH_SIZE`.\n      batch_norm: Whether to use batch normalization after each hidden layer.\n      linear_sparse_combiner: A string specifying how to reduce the linear model\n        if a categorical column is multivalent.  One of \"mean\", \"sqrtn\", and\n        \"sum\" -- these are effectively different ways to do example-level\n        normalization, which can be useful for bag-of-words features.  For more\n        details, see `tf.feature_column.linear_model`.\n\n    Raises:\n      ValueError: If both linear_feature_columns and dnn_features_columns are\n        empty at the same time.\n    \"\"\"\n    self._feature_columns = _validate_feature_columns(\n        linear_feature_columns=linear_feature_columns,\n        dnn_feature_columns=dnn_feature_columns)\n\n    head = regression_head.RegressionHead(\n        label_dimension=label_dimension,\n        weight_column=weight_column,\n        loss_reduction=loss_reduction)\n    estimator._canned_estimator_api_gauge.get_cell('Regressor').set(\n        'DNNLinearCombined')  # pylint: disable=protected-access\n\n    def _model_fn(features, labels, mode, config):\n      \"\"\"Call the _dnn_linear_combined_model_fn.\"\"\"\n      return _dnn_linear_combined_model_fn_v2(\n          features=features,\n          labels=labels,\n          mode=mode,\n          head=head,\n          linear_feature_columns=linear_feature_columns,\n          linear_optimizer=linear_optimizer,\n          dnn_feature_columns=dnn_feature_columns,\n          dnn_optimizer=dnn_optimizer,\n          dnn_hidden_units=dnn_hidden_units,\n          dnn_activation_fn=dnn_activation_fn,\n          dnn_dropout=dnn_dropout,\n          config=config,\n          batch_norm=batch_norm,\n          linear_sparse_combiner=linear_sparse_combiner)\n\n    super(DNNLinearCombinedRegressorV2, self).__init__(\n        model_fn=_model_fn,\n        model_dir=model_dir,\n        config=config,\n        warm_start_from=warm_start_from)\n\n\n@estimator_export(v1=['estimator.DNNLinearCombinedRegressor'])  # pylint: disable=missing-docstring\nclass DNNLinearCombinedRegressor(estimator.Estimator):\n  __doc__ = DNNLinearCombinedRegressorV2.__doc__.replace(\n      'SUM_OVER_BATCH_SIZE', 'SUM')\n\n  def __init__(self,\n               model_dir=None,\n               linear_feature_columns=None,\n               linear_optimizer='Ftrl',\n               dnn_feature_columns=None,\n               dnn_optimizer='Adagrad',\n               dnn_hidden_units=None,\n               dnn_activation_fn=tf.nn.relu,\n               dnn_dropout=None,\n               label_dimension=1,\n               weight_column=None,\n               input_layer_partitioner=None,\n               config=None,\n               warm_start_from=None,\n               loss_reduction=tf.compat.v1.losses.Reduction.SUM,\n               batch_norm=False,\n               linear_sparse_combiner='sum'):\n    self._feature_columns = _validate_feature_columns(\n        linear_feature_columns=linear_feature_columns,\n        dnn_feature_columns=dnn_feature_columns)\n    estimator._canned_estimator_api_gauge.get_cell('Regressor').set(\n        'DNNLinearCombined')  # pylint: disable=protected-access\n\n    head = head_lib._regression_head(  # pylint: disable=protected-access\n        label_dimension=label_dimension,\n        weight_column=weight_column,\n        loss_reduction=loss_reduction)\n\n    def _model_fn(features, labels, mode, config):\n      \"\"\"Call the _dnn_linear_combined_model_fn.\"\"\"\n      return _dnn_linear_combined_model_fn(\n          features=features,\n          labels=labels,\n          mode=mode,\n          head=head,\n          linear_feature_columns=linear_feature_columns,\n          linear_optimizer=linear_optimizer,\n          dnn_feature_columns=dnn_feature_columns,\n          dnn_optimizer=dnn_optimizer,\n          dnn_hidden_units=dnn_hidden_units,\n          dnn_activation_fn=dnn_activation_fn,\n          dnn_dropout=dnn_dropout,\n          input_layer_partitioner=input_layer_partitioner,\n          config=config,\n          batch_norm=batch_norm,\n          linear_sparse_combiner=linear_sparse_combiner)\n\n    super(DNNLinearCombinedRegressor, self).__init__(\n        model_fn=_model_fn,\n        model_dir=model_dir,\n        config=config,\n        warm_start_from=warm_start_from)\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/estimator", "file_path": "tensorflow_estimator/python/estimator/canned/dnn_linear_combined_test.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for v2 version of dnn_linear_combined.py.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport shutil\nimport tempfile\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport six\nimport tensorflow as tf\nfrom tensorflow.core.example import example_pb2\nfrom tensorflow.core.example import feature_pb2\nfrom tensorflow.python.feature_column import feature_column\nfrom tensorflow.python.feature_column import feature_column_v2\nfrom tensorflow_estimator.python.estimator import estimator\nfrom tensorflow_estimator.python.estimator.canned import dnn_linear_combined\nfrom tensorflow_estimator.python.estimator.canned import dnn_testing_utils\nfrom tensorflow_estimator.python.estimator.canned import linear_testing_utils\nfrom tensorflow_estimator.python.estimator.canned import prediction_keys\nfrom tensorflow_estimator.python.estimator.export import export\nfrom tensorflow_estimator.python.estimator.inputs import numpy_io\nfrom tensorflow_estimator.python.estimator.inputs import pandas_io\n\ntry:\n  # pylint: disable=g-import-not-at-top\n  import pandas as pd\n  HAS_PANDAS = True\nexcept IOError:\n  # Pandas writes a temporary file during import. If it fails, don't use pandas.\n  HAS_PANDAS = False\nexcept ImportError:\n  HAS_PANDAS = False\n\n\nclass DNNOnlyModelFnTest(dnn_testing_utils.BaseDNNModelFnTest,\n                         tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    dnn_testing_utils.BaseDNNModelFnTest.__init__(self, self._dnn_only_model_fn)\n\n  def _dnn_only_model_fn(self,\n                         features,\n                         labels,\n                         mode,\n                         head,\n                         hidden_units,\n                         feature_columns,\n                         optimizer='Adagrad',\n                         activation_fn=tf.nn.relu,\n                         dropout=None,\n                         config=None):\n    return dnn_linear_combined._dnn_linear_combined_model_fn_v2(\n        features=features,\n        labels=labels,\n        mode=mode,\n        head=head,\n        linear_feature_columns=[],\n        dnn_hidden_units=hidden_units,\n        dnn_feature_columns=feature_columns,\n        dnn_optimizer=optimizer,\n        dnn_activation_fn=activation_fn,\n        dnn_dropout=dropout,\n        config=config)\n\n\n# A function to mimic linear-regressor init reuse same tests.\ndef _linear_regressor_fn(feature_columns,\n                         model_dir=None,\n                         label_dimension=1,\n                         weight_column=None,\n                         optimizer='Ftrl',\n                         config=None,\n                         sparse_combiner='sum'):\n  return dnn_linear_combined.DNNLinearCombinedRegressorV2(\n      model_dir=model_dir,\n      linear_feature_columns=feature_columns,\n      linear_optimizer=optimizer,\n      label_dimension=label_dimension,\n      weight_column=weight_column,\n      config=config,\n      linear_sparse_combiner=sparse_combiner)\n\n\nclass LinearOnlyRegressorEvaluationV2Test(\n    linear_testing_utils.BaseLinearRegressorEvaluationTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    linear_testing_utils.BaseLinearRegressorEvaluationTest.__init__(\n        self, _linear_regressor_fn, fc_lib=feature_column_v2)\n\n\nclass LinearOnlyRegressorPredictV2Test(\n    linear_testing_utils.BaseLinearRegressorPredictTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    linear_testing_utils.BaseLinearRegressorPredictTest.__init__(\n        self, _linear_regressor_fn, fc_lib=feature_column_v2)\n\n\nclass LinearOnlyRegressorIntegrationV2Test(\n    linear_testing_utils.BaseLinearRegressorIntegrationTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    linear_testing_utils.BaseLinearRegressorIntegrationTest.__init__(\n        self, _linear_regressor_fn, fc_lib=feature_column_v2)\n\n\nclass LinearOnlyRegressorTrainingV2Test(\n    linear_testing_utils.BaseLinearRegressorTrainingTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    linear_testing_utils.BaseLinearRegressorTrainingTest.__init__(\n        self, _linear_regressor_fn, fc_lib=feature_column_v2)\n\n\ndef _linear_classifier_fn(feature_columns,\n                          model_dir=None,\n                          n_classes=2,\n                          weight_column=None,\n                          label_vocabulary=None,\n                          optimizer='Ftrl',\n                          config=None,\n                          sparse_combiner='sum'):\n  return dnn_linear_combined.DNNLinearCombinedClassifierV2(\n      model_dir=model_dir,\n      linear_feature_columns=feature_columns,\n      linear_optimizer=optimizer,\n      n_classes=n_classes,\n      weight_column=weight_column,\n      label_vocabulary=label_vocabulary,\n      config=config,\n      linear_sparse_combiner=sparse_combiner)\n\n\nclass LinearOnlyClassifierTrainingV2Test(\n    linear_testing_utils.BaseLinearClassifierTrainingTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    linear_testing_utils.BaseLinearClassifierTrainingTest.__init__(\n        self,\n        linear_classifier_fn=_linear_classifier_fn,\n        fc_lib=feature_column_v2)\n\n\nclass LinearOnlyClassifierClassesEvaluationV2Test(\n    linear_testing_utils.BaseLinearClassifierEvaluationTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    linear_testing_utils.BaseLinearClassifierEvaluationTest.__init__(\n        self,\n        linear_classifier_fn=_linear_classifier_fn,\n        fc_lib=feature_column_v2)\n\n\nclass LinearOnlyClassifierPredictV2Test(\n    linear_testing_utils.BaseLinearClassifierPredictTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    linear_testing_utils.BaseLinearClassifierPredictTest.__init__(\n        self,\n        linear_classifier_fn=_linear_classifier_fn,\n        fc_lib=feature_column_v2)\n\n\nclass LinearOnlyClassifierIntegrationV2Test(\n    linear_testing_utils.BaseLinearClassifierIntegrationTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    linear_testing_utils.BaseLinearClassifierIntegrationTest.__init__(\n        self,\n        linear_classifier_fn=_linear_classifier_fn,\n        fc_lib=feature_column_v2)\n\n\n@parameterized.parameters((feature_column_v2,))\nclass DNNLinearCombinedRegressorIntegrationTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._model_dir = tempfile.mkdtemp()\n\n  def tearDown(self):\n    if self._model_dir:\n      tf.compat.v1.summary.FileWriterCache.clear()\n      shutil.rmtree(self._model_dir)\n\n  def _test_complete_flow_helper(self, linear_feature_columns,\n                                 dnn_feature_columns, feature_spec,\n                                 train_input_fn, eval_input_fn,\n                                 predict_input_fn, input_dimension,\n                                 label_dimension, batch_size):\n    est = dnn_linear_combined.DNNLinearCombinedRegressorV2(\n        linear_feature_columns=linear_feature_columns,\n        dnn_hidden_units=(2, 2),\n        dnn_feature_columns=dnn_feature_columns,\n        label_dimension=label_dimension,\n        model_dir=self._model_dir)\n\n    # TRAIN\n    num_steps = 10\n    est.train(train_input_fn, steps=num_steps)\n\n    # EVALUTE\n    scores = est.evaluate(eval_input_fn)\n    self.assertEqual(num_steps, scores[tf.compat.v1.GraphKeys.GLOBAL_STEP])\n    self.assertIn('loss', six.iterkeys(scores))\n\n    # PREDICT\n    predictions = np.array([\n        x[prediction_keys.PredictionKeys.PREDICTIONS]\n        for x in est.predict(predict_input_fn)\n    ])\n    self.assertAllEqual((batch_size, label_dimension), predictions.shape)\n\n    # EXPORT\n    serving_input_receiver_fn = export.build_parsing_serving_input_receiver_fn(\n        feature_spec)\n    export_dir = est.export_saved_model(tempfile.mkdtemp(),\n                                        serving_input_receiver_fn)\n    self.assertTrue(tf.compat.v1.gfile.Exists(export_dir))\n\n  def _test_complete_flow(self, train_input_fn, eval_input_fn, predict_input_fn,\n                          input_dimension, label_dimension, batch_size,\n                          fc_impl):\n    linear_feature_columns = [\n        fc_impl.numeric_column('x', shape=(input_dimension,))\n    ]\n    dnn_feature_columns = [\n        fc_impl.numeric_column('x', shape=(input_dimension,))\n    ]\n    feature_columns = linear_feature_columns + dnn_feature_columns\n    feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n    self._test_complete_flow_helper(linear_feature_columns, dnn_feature_columns,\n                                    feature_spec, train_input_fn, eval_input_fn,\n                                    predict_input_fn, input_dimension,\n                                    label_dimension, batch_size)\n\n  def _test_complete_flow_dnn_fc_v1(self, train_input_fn, eval_input_fn,\n                                    predict_input_fn, input_dimension,\n                                    label_dimension, batch_size, fc_impl):\n    del fc_impl\n    linear_feature_columns = [\n        tf.feature_column.numeric_column('x', shape=(input_dimension,))\n    ]\n    dnn_feature_columns = [\n        feature_column._numeric_column('x', shape=(input_dimension,))\n    ]\n    feature_columns = linear_feature_columns + dnn_feature_columns\n    feature_spec = tf.compat.v1.feature_column.make_parse_example_spec(\n        feature_columns)\n    self._test_complete_flow_helper(linear_feature_columns, dnn_feature_columns,\n                                    feature_spec, train_input_fn, eval_input_fn,\n                                    predict_input_fn, input_dimension,\n                                    label_dimension, batch_size)\n\n  def _test_complete_flow_linear_fc_v1(self, train_input_fn, eval_input_fn,\n                                       predict_input_fn, input_dimension,\n                                       label_dimension, batch_size, fc_impl):\n    del fc_impl\n    linear_feature_columns = [\n        feature_column._numeric_column('x', shape=(input_dimension,))\n    ]\n    dnn_feature_columns = [\n        tf.feature_column.numeric_column('x', shape=(input_dimension,))\n    ]\n    feature_columns = linear_feature_columns + dnn_feature_columns\n    feature_spec = tf.compat.v1.feature_column.make_parse_example_spec(\n        feature_columns)\n    self._test_complete_flow_helper(linear_feature_columns, dnn_feature_columns,\n                                    feature_spec, train_input_fn, eval_input_fn,\n                                    predict_input_fn, input_dimension,\n                                    label_dimension, batch_size)\n\n  def _test_numpy_input_fn_helper(self, fc_impl, fn_to_run):\n    \"\"\"Tests complete flow with numpy_input_fn.\"\"\"\n    label_dimension = 2\n    batch_size = 10\n    data = np.linspace(0., 2., batch_size * label_dimension, dtype=np.float32)\n    data = data.reshape(batch_size, label_dimension)\n    # learn y = x\n    train_input_fn = numpy_io.numpy_input_fn(\n        x={'x': data},\n        y=data,\n        batch_size=batch_size,\n        num_epochs=None,\n        shuffle=True)\n    eval_input_fn = numpy_io.numpy_input_fn(\n        x={'x': data}, y=data, batch_size=batch_size, shuffle=False)\n    predict_input_fn = numpy_io.numpy_input_fn(\n        x={'x': data}, batch_size=batch_size, shuffle=False)\n\n    fn_to_run(\n        train_input_fn=train_input_fn,\n        eval_input_fn=eval_input_fn,\n        predict_input_fn=predict_input_fn,\n        input_dimension=label_dimension,\n        label_dimension=label_dimension,\n        batch_size=batch_size,\n        fc_impl=fc_impl)\n\n  def test_numpy_input_fn_basic(self, fc_impl):\n    self._test_numpy_input_fn_helper(fc_impl, self._test_complete_flow)\n\n  def test_numpy_input_fn_dnn_fc_v1(self, fc_impl):\n    with self.assertRaisesRegexp(\n        ValueError, r'Received a feature column from TensorFlow v1'):\n      self._test_numpy_input_fn_helper(fc_impl,\n                                       self._test_complete_flow_dnn_fc_v1)\n\n  def test_numpy_input_fn_linear_fc_v1(self, fc_impl):\n    with self.assertRaisesRegexp(\n        ValueError, r'Received a feature column from TensorFlow v1'):\n      self._test_numpy_input_fn_helper(fc_impl,\n                                       self._test_complete_flow_linear_fc_v1)\n\n  def _test_pandas_input_fn_helper(self, fc_impl, fn_to_run):\n    \"\"\"Tests complete flow with pandas_input_fn.\"\"\"\n    if not HAS_PANDAS:\n      return\n    label_dimension = 1\n    batch_size = 10\n    data = np.linspace(0., 2., batch_size, dtype=np.float32)\n    x = pd.DataFrame({'x': data})\n    y = pd.Series(data)\n    train_input_fn = pandas_io.pandas_input_fn(\n        x=x, y=y, batch_size=batch_size, num_epochs=None, shuffle=True)\n    eval_input_fn = pandas_io.pandas_input_fn(\n        x=x, y=y, batch_size=batch_size, shuffle=False)\n    predict_input_fn = pandas_io.pandas_input_fn(\n        x=x, batch_size=batch_size, shuffle=False)\n\n    fn_to_run(\n        train_input_fn=train_input_fn,\n        eval_input_fn=eval_input_fn,\n        predict_input_fn=predict_input_fn,\n        input_dimension=label_dimension,\n        label_dimension=label_dimension,\n        batch_size=batch_size,\n        fc_impl=fc_impl)\n\n  def test_pandas_input_fn_basic(self, fc_impl):\n    self._test_pandas_input_fn_helper(fc_impl, self._test_complete_flow)\n\n  def test_pandas_input_fn_dnn_fc_v1(self, fc_impl):\n    with self.assertRaisesRegexp(\n        ValueError, r'Received a feature column from TensorFlow v1'):\n      self._test_pandas_input_fn_helper(fc_impl,\n                                        self._test_complete_flow_dnn_fc_v1)\n\n  def test_pandas_input_fn_linear_fc_v1(self, fc_impl):\n    with self.assertRaisesRegexp(\n        ValueError, r'Received a feature column from TensorFlow v1'):\n      self._test_pandas_input_fn_helper(fc_impl,\n                                        self._test_complete_flow_linear_fc_v1)\n\n  def _test_input_fn_from_parse_example_helper(self, fc_impl, fn_to_run):\n    \"\"\"Tests complete flow with input_fn constructed from parse_example.\"\"\"\n    label_dimension = 2\n    batch_size = 10\n    data = np.linspace(0., 2., batch_size * label_dimension, dtype=np.float32)\n    data = data.reshape(batch_size, label_dimension)\n\n    serialized_examples = []\n    for datum in data:\n      example = example_pb2.Example(\n          features=feature_pb2.Features(\n              feature={\n                  'x':\n                      feature_pb2.Feature(\n                          float_list=feature_pb2.FloatList(value=datum)),\n                  'y':\n                      feature_pb2.Feature(\n                          float_list=feature_pb2.FloatList(value=datum)),\n              }))\n      serialized_examples.append(example.SerializeToString())\n\n    feature_spec = {\n        'x': tf.io.FixedLenFeature([label_dimension], tf.dtypes.float32),\n        'y': tf.io.FixedLenFeature([label_dimension], tf.dtypes.float32),\n    }\n\n    def _train_input_fn():\n      feature_map = tf.compat.v1.io.parse_example(serialized_examples,\n                                                  feature_spec)\n      features = linear_testing_utils.queue_parsed_features(feature_map)\n      labels = features.pop('y')\n      return features, labels\n\n    def _eval_input_fn():\n      feature_map = tf.compat.v1.io.parse_example(\n          tf.compat.v1.train.limit_epochs(serialized_examples, num_epochs=1),\n          feature_spec)\n      features = linear_testing_utils.queue_parsed_features(feature_map)\n      labels = features.pop('y')\n      return features, labels\n\n    def _predict_input_fn():\n      feature_map = tf.compat.v1.io.parse_example(\n          tf.compat.v1.train.limit_epochs(serialized_examples, num_epochs=1),\n          feature_spec)\n      features = linear_testing_utils.queue_parsed_features(feature_map)\n      features.pop('y')\n      return features, None\n\n    fn_to_run(\n        train_input_fn=_train_input_fn,\n        eval_input_fn=_eval_input_fn,\n        predict_input_fn=_predict_input_fn,\n        input_dimension=label_dimension,\n        label_dimension=label_dimension,\n        batch_size=batch_size,\n        fc_impl=fc_impl)\n\n  def test_input_fn_from_parse_example_basic(self, fc_impl):\n    self._test_input_fn_from_parse_example_helper(fc_impl,\n                                                  self._test_complete_flow)\n\n  def test_input_fn_from_parse_example_dnn_fc_v1(self, fc_impl):\n    with self.assertRaisesRegexp(\n        ValueError, r'Received a feature column from TensorFlow v1'):\n      self._test_input_fn_from_parse_example_helper(\n          fc_impl, self._test_complete_flow_dnn_fc_v1)\n\n  def test_input_fn_from_parse_example_linear_fc_v1(self, fc_impl):\n    with self.assertRaisesRegexp(\n        ValueError, r'Received a feature column from TensorFlow v1'):\n      self._test_input_fn_from_parse_example_helper(\n          fc_impl, self._test_complete_flow_linear_fc_v1)\n\n\n# A function to mimic dnn-classifier init reuse same tests.\ndef _dnn_classifier_fn(hidden_units,\n                       feature_columns,\n                       model_dir=None,\n                       n_classes=2,\n                       weight_column=None,\n                       label_vocabulary=None,\n                       optimizer='Adagrad',\n                       config=None):\n  return dnn_linear_combined.DNNLinearCombinedClassifierV2(\n      model_dir=model_dir,\n      dnn_hidden_units=hidden_units,\n      dnn_feature_columns=feature_columns,\n      dnn_optimizer=optimizer,\n      n_classes=n_classes,\n      weight_column=weight_column,\n      label_vocabulary=label_vocabulary,\n      config=config)\n\n\nclass DNNOnlyClassifierEvaluateV2Test(\n    dnn_testing_utils.BaseDNNClassifierEvaluateTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    dnn_testing_utils.BaseDNNClassifierEvaluateTest.__init__(\n        self, _dnn_classifier_fn, fc_impl=feature_column_v2)\n\n\nclass DNNOnlyClassifierPredictV2Test(\n    dnn_testing_utils.BaseDNNClassifierPredictTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    dnn_testing_utils.BaseDNNClassifierPredictTest.__init__(\n        self, _dnn_classifier_fn, fc_impl=feature_column_v2)\n\n\nclass DNNOnlyClassifierTrainV2Test(dnn_testing_utils.BaseDNNClassifierTrainTest,\n                                   tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    dnn_testing_utils.BaseDNNClassifierTrainTest.__init__(\n        self, _dnn_classifier_fn, fc_impl=feature_column_v2)\n\n\n# A function to mimic dnn-regressor init reuse same tests.\ndef _dnn_regressor_fn(hidden_units,\n                      feature_columns,\n                      model_dir=None,\n                      label_dimension=1,\n                      weight_column=None,\n                      optimizer='Adagrad',\n                      config=None):\n  return dnn_linear_combined.DNNLinearCombinedRegressorV2(\n      model_dir=model_dir,\n      dnn_hidden_units=hidden_units,\n      dnn_feature_columns=feature_columns,\n      dnn_optimizer=optimizer,\n      label_dimension=label_dimension,\n      weight_column=weight_column,\n      config=config)\n\n\nclass DNNOnlyRegressorEvaluateV2Test(\n    dnn_testing_utils.BaseDNNRegressorEvaluateTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    dnn_testing_utils.BaseDNNRegressorEvaluateTest.__init__(\n        self, _dnn_regressor_fn, fc_impl=feature_column_v2)\n\n\nclass DNNOnlyRegressorPredictV2Test(\n    dnn_testing_utils.BaseDNNRegressorPredictTest, tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    dnn_testing_utils.BaseDNNRegressorPredictTest.__init__(\n        self, _dnn_regressor_fn, fc_impl=feature_column_v2)\n\n\nclass DNNOnlyRegressorTrainV2Test(dnn_testing_utils.BaseDNNRegressorTrainTest,\n                                  tf.test.TestCase):\n\n  def __init__(self, methodName='runTest'):  # pylint: disable=invalid-name\n    tf.test.TestCase.__init__(self, methodName)\n    dnn_testing_utils.BaseDNNRegressorTrainTest.__init__(\n        self, _dnn_regressor_fn, fc_impl=feature_column_v2)\n\n\n@parameterized.parameters((feature_column_v2,))\nclass DNNLinearCombinedClassifierIntegrationTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._model_dir = tempfile.mkdtemp()\n\n  def tearDown(self):\n    if self._model_dir:\n      tf.compat.v1.summary.FileWriterCache.clear()\n      shutil.rmtree(self._model_dir)\n\n  def _as_label(self, data_in_float):\n    return np.rint(data_in_float).astype(np.int64)\n\n  def _test_complete_flow(self, train_input_fn, eval_input_fn, predict_input_fn,\n                          input_dimension, n_classes, batch_size, fc_impl):\n    linear_feature_columns = [\n        fc_impl.numeric_column('x', shape=(input_dimension,))\n    ]\n    dnn_feature_columns = [\n        fc_impl.numeric_column('x', shape=(input_dimension,))\n    ]\n    feature_columns = linear_feature_columns + dnn_feature_columns\n    est = dnn_linear_combined.DNNLinearCombinedClassifierV2(\n        linear_feature_columns=linear_feature_columns,\n        dnn_hidden_units=(2, 2),\n        dnn_feature_columns=dnn_feature_columns,\n        n_classes=n_classes,\n        model_dir=self._model_dir)\n\n    # TRAIN\n    num_steps = 10\n    est.train(train_input_fn, steps=num_steps)\n\n    # EVALUTE\n    scores = est.evaluate(eval_input_fn)\n    self.assertEqual(num_steps, scores[tf.compat.v1.GraphKeys.GLOBAL_STEP])\n    self.assertIn('loss', six.iterkeys(scores))\n\n    # PREDICT\n    predicted_proba = np.array([\n        x[prediction_keys.PredictionKeys.PROBABILITIES]\n        for x in est.predict(predict_input_fn)\n    ])\n    self.assertAllEqual((batch_size, n_classes), predicted_proba.shape)\n\n    # EXPORT\n    feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n    serving_input_receiver_fn = export.build_parsing_serving_input_receiver_fn(\n        feature_spec)\n    export_dir = est.export_saved_model(tempfile.mkdtemp(),\n                                        serving_input_receiver_fn)\n    self.assertTrue(tf.compat.v1.gfile.Exists(export_dir))\n\n  def test_numpy_input_fn(self, fc_impl):\n    \"\"\"Tests complete flow with numpy_input_fn.\"\"\"\n    n_classes = 3\n    input_dimension = 2\n    batch_size = 10\n    data = np.linspace(\n        0., n_classes - 1., batch_size * input_dimension, dtype=np.float32)\n    x_data = data.reshape(batch_size, input_dimension)\n    y_data = self._as_label(np.reshape(data[:batch_size], (batch_size, 1)))\n    # learn y = x\n    train_input_fn = numpy_io.numpy_input_fn(\n        x={'x': x_data},\n        y=y_data,\n        batch_size=batch_size,\n        num_epochs=None,\n        shuffle=True)\n    eval_input_fn = numpy_io.numpy_input_fn(\n        x={'x': x_data}, y=y_data, batch_size=batch_size, shuffle=False)\n    predict_input_fn = numpy_io.numpy_input_fn(\n        x={'x': x_data}, batch_size=batch_size, shuffle=False)\n\n    self._test_complete_flow(\n        train_input_fn=train_input_fn,\n        eval_input_fn=eval_input_fn,\n        predict_input_fn=predict_input_fn,\n        input_dimension=input_dimension,\n        n_classes=n_classes,\n        batch_size=batch_size,\n        fc_impl=fc_impl)\n\n  def test_pandas_input_fn(self, fc_impl):\n    \"\"\"Tests complete flow with pandas_input_fn.\"\"\"\n    if not HAS_PANDAS:\n      return\n    input_dimension = 1\n    n_classes = 2\n    batch_size = 10\n    data = np.linspace(0., n_classes - 1., batch_size, dtype=np.float32)\n    x = pd.DataFrame({'x': data})\n    y = pd.Series(self._as_label(data))\n    train_input_fn = pandas_io.pandas_input_fn(\n        x=x, y=y, batch_size=batch_size, num_epochs=None, shuffle=True)\n    eval_input_fn = pandas_io.pandas_input_fn(\n        x=x, y=y, batch_size=batch_size, shuffle=False)\n    predict_input_fn = pandas_io.pandas_input_fn(\n        x=x, batch_size=batch_size, shuffle=False)\n\n    self._test_complete_flow(\n        train_input_fn=train_input_fn,\n        eval_input_fn=eval_input_fn,\n        predict_input_fn=predict_input_fn,\n        input_dimension=input_dimension,\n        n_classes=n_classes,\n        batch_size=batch_size,\n        fc_impl=fc_impl)\n\n  def test_input_fn_from_parse_example(self, fc_impl):\n    \"\"\"Tests complete flow with input_fn constructed from parse_example.\"\"\"\n    input_dimension = 2\n    n_classes = 3\n    batch_size = 10\n    data = np.linspace(\n        0., n_classes - 1., batch_size * input_dimension, dtype=np.float32)\n    data = data.reshape(batch_size, input_dimension)\n\n    serialized_examples = []\n    for datum in data:\n      example = example_pb2.Example(\n          features=feature_pb2.Features(\n              feature={\n                  'x':\n                      feature_pb2.Feature(\n                          float_list=feature_pb2.FloatList(value=datum)),\n                  'y':\n                      feature_pb2.Feature(\n                          int64_list=feature_pb2.Int64List(\n                              value=self._as_label(datum[:1]))),\n              }))\n      serialized_examples.append(example.SerializeToString())\n\n    feature_spec = {\n        'x': tf.io.FixedLenFeature([input_dimension], tf.dtypes.float32),\n        'y': tf.io.FixedLenFeature([1], tf.dtypes.int64),\n    }\n\n    def _train_input_fn():\n      feature_map = tf.compat.v1.io.parse_example(serialized_examples,\n                                                  feature_spec)\n      features = linear_testing_utils.queue_parsed_features(feature_map)\n      labels = features.pop('y')\n      return features, labels\n\n    def _eval_input_fn():\n      feature_map = tf.compat.v1.io.parse_example(\n          tf.compat.v1.train.limit_epochs(serialized_examples, num_epochs=1),\n          feature_spec)\n      features = linear_testing_utils.queue_parsed_features(feature_map)\n      labels = features.pop('y')\n      return features, labels\n\n    def _predict_input_fn():\n      feature_map = tf.compat.v1.io.parse_example(\n          tf.compat.v1.train.limit_epochs(serialized_examples, num_epochs=1),\n          feature_spec)\n      features = linear_testing_utils.queue_parsed_features(feature_map)\n      features.pop('y')\n      return features, None\n\n    self._test_complete_flow(\n        train_input_fn=_train_input_fn,\n        eval_input_fn=_eval_input_fn,\n        predict_input_fn=_predict_input_fn,\n        input_dimension=input_dimension,\n        n_classes=n_classes,\n        batch_size=batch_size,\n        fc_impl=fc_impl)\n\n\n@parameterized.parameters((feature_column_v2,))\nclass DNNLinearCombinedTests(tf.test.TestCase):\n\n  def setUp(self):\n    self._model_dir = tempfile.mkdtemp()\n\n  def tearDown(self):\n    if self._model_dir:\n      shutil.rmtree(self._model_dir)\n\n  def test_train_op_calls_both_dnn_and_linear(self, fc_impl):\n    dnn_opt = tf.keras.optimizers.legacy.SGD(1.)\n    linear_opt = tf.keras.optimizers.legacy.SGD(1.)\n    x_column = fc_impl.numeric_column('x')\n    input_fn = numpy_io.numpy_input_fn(\n        x={'x': np.array([[0.], [1.]])},\n        y=np.array([[0.], [1.]]),\n        batch_size=1,\n        shuffle=False)\n    est = dnn_linear_combined.DNNLinearCombinedClassifierV2(\n        linear_feature_columns=[x_column],\n        # verifies linear_optimizer is used only for linear part.\n        linear_optimizer=linear_opt,\n        dnn_hidden_units=(2, 2),\n        dnn_feature_columns=[x_column],\n        # verifies dnn_optimizer is used only for dnn part.\n        dnn_optimizer=dnn_opt,\n        model_dir=self._model_dir)\n    num_steps = 1\n    est.train(input_fn, steps=num_steps)\n    # verifies train_op fires linear minimize op\n    self.assertEqual(num_steps,\n                     est.get_variable_value(linear_opt.iterations.name))\n    # verifies train_op fires dnn optmizer\n    self.assertEqual(num_steps, est.get_variable_value(dnn_opt.iterations.name))\n\n  def test_dnn_and_linear_logits_are_added(self, fc_impl):\n    with tf.Graph().as_default():\n      tf.Variable([[1.0]], name='linear/linear_model/x/weights')\n      tf.Variable([2.0], name='linear/linear_model/bias_weights')\n      tf.Variable([[3.0]], name='dnn/hiddenlayer_0/kernel')\n      tf.Variable([4.0], name='dnn/hiddenlayer_0/bias')\n      tf.Variable([[5.0]], name='dnn/logits/kernel')\n      tf.Variable([6.0], name='dnn/logits/bias')\n      tf.Variable(1, name='global_step', dtype=tf.dtypes.int64)\n      linear_testing_utils.save_variables_to_ckpt(self._model_dir)\n\n    x_column = fc_impl.numeric_column('x')\n    est = dnn_linear_combined.DNNLinearCombinedRegressorV2(\n        linear_feature_columns=[x_column],\n        dnn_hidden_units=[1],\n        dnn_feature_columns=[x_column],\n        model_dir=self._model_dir)\n    input_fn = numpy_io.numpy_input_fn(\n        x={'x': np.array([[10.]])}, batch_size=1, shuffle=False)\n    # linear logits = 10*1 + 2 = 12\n    # dnn logits = (10*3 + 4)*5 + 6 = 176\n    # logits = dnn + linear = 176 + 12 = 188\n    self.assertAllClose({\n        prediction_keys.PredictionKeys.PREDICTIONS: [188.],\n    }, next(est.predict(input_fn=input_fn)))\n\n\n@parameterized.parameters((feature_column_v2,))\nclass DNNLinearCombinedWarmStartingTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create a directory to save our old checkpoint and vocabularies to.\n    self._ckpt_and_vocab_dir = tempfile.mkdtemp()\n\n    # Make a dummy input_fn.\n    def _input_fn():\n      features = {\n          'age': [[23.], [31.]],\n          'city': [['Palo Alto'], ['Mountain View']],\n      }\n      return features, [0, 1]\n\n    self._input_fn = _input_fn\n\n  def tearDown(self):\n    # Clean up checkpoint / vocab dir.\n    tf.compat.v1.summary.FileWriterCache.clear()\n    shutil.rmtree(self._ckpt_and_vocab_dir)\n\n  def test_classifier_basic_warm_starting(self, fc_impl):\n    \"\"\"Tests correctness of DNNLinearCombinedClassifier default warm-start.\"\"\"\n    age = fc_impl.numeric_column('age')\n    city = fc_impl.embedding_column(\n        fc_impl.categorical_column_with_vocabulary_list(\n            'city', vocabulary_list=['Mountain View', 'Palo Alto']),\n        dimension=5)\n\n    # Create a DNNLinearCombinedClassifier and train to save a checkpoint.\n    dnn_lc_classifier = dnn_linear_combined.DNNLinearCombinedClassifierV2(\n        linear_feature_columns=[age],\n        dnn_feature_columns=[city],\n        dnn_hidden_units=[256, 128],\n        model_dir=self._ckpt_and_vocab_dir,\n        n_classes=4,\n        linear_optimizer='SGD',\n        dnn_optimizer='SGD')\n    dnn_lc_classifier.train(input_fn=self._input_fn, max_steps=1)\n\n    # Create a second DNNLinearCombinedClassifier, warm-started from the first.\n    # Use a learning_rate = 0.0 optimizer to check values (use SGD so we don't\n    # have accumulator values that change).\n    # To avoid optimizer naming issue during warm start, when to create the\n    # optimizer instance, the dnn_optimizer needs to be created first\n    # before the linear_optimizer, since this is the order pre-defined\n    # in the model function.\n    # Create a default graph context to make sure the optimizer instance is\n    # created within Graph v1 to make it consistent with estimator Graph.\n    with tf.Graph().as_default():\n      warm_started_dnn_lc_classifier = (\n          dnn_linear_combined.DNNLinearCombinedClassifierV2(\n              linear_feature_columns=[age],\n              dnn_feature_columns=[city],\n              dnn_hidden_units=[256, 128],\n              n_classes=4,\n              dnn_optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=0.0),\n              linear_optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=0.0),\n              warm_start_from=dnn_lc_classifier.model_dir))\n\n    warm_started_dnn_lc_classifier.train(input_fn=self._input_fn, max_steps=1)\n    for variable_name in warm_started_dnn_lc_classifier.get_variable_names():\n      if 'learning_rate' in variable_name:\n        self.assertAllClose(\n            0.0,\n            warm_started_dnn_lc_classifier.get_variable_value(variable_name))\n      else:\n        self.assertAllClose(\n            dnn_lc_classifier.get_variable_value(variable_name),\n            warm_started_dnn_lc_classifier.get_variable_value(variable_name))\n\n  def test_regressor_basic_warm_starting(self, fc_impl):\n    \"\"\"Tests correctness of DNNLinearCombinedRegressor default warm-start.\"\"\"\n    age = fc_impl.numeric_column('age')\n    city = fc_impl.embedding_column(\n        fc_impl.categorical_column_with_vocabulary_list(\n            'city', vocabulary_list=['Mountain View', 'Palo Alto']),\n        dimension=5)\n\n    # Create a DNNLinearCombinedRegressor and train to save a checkpoint.\n    dnn_lc_regressor = dnn_linear_combined.DNNLinearCombinedRegressorV2(\n        linear_feature_columns=[age],\n        dnn_feature_columns=[city],\n        dnn_hidden_units=[256, 128],\n        model_dir=self._ckpt_and_vocab_dir,\n        linear_optimizer='SGD',\n        dnn_optimizer='SGD')\n    dnn_lc_regressor.train(input_fn=self._input_fn, max_steps=1)\n\n    # Create a second DNNLinearCombinedRegressor, warm-started from the first.\n    # Use a learning_rate = 0.0 optimizer to check values (use SGD so we don't\n    # have accumulator values that change).\n    # To avoid optimizer naming issue during warm start, when to create the\n    # optimizer instance, the dnn_optimizer needs to be created first\n    # before the linear_optimizer, since this is the order pre-defined\n    # in the model function.\n    # Create a default graph context to make sure the optimizer instance is\n    # created within Graph v1 to make it consistent with estimator Graph.\n    with tf.Graph().as_default():\n      warm_started_dnn_lc_regressor = (\n          dnn_linear_combined.DNNLinearCombinedRegressorV2(\n              linear_feature_columns=[age],\n              dnn_feature_columns=[city],\n              dnn_hidden_units=[256, 128],\n              dnn_optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=0.0),\n              linear_optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=0.0),\n              warm_start_from=dnn_lc_regressor.model_dir))\n\n    warm_started_dnn_lc_regressor.train(input_fn=self._input_fn, max_steps=1)\n    for variable_name in warm_started_dnn_lc_regressor.get_variable_names():\n      if 'learning_rate' in variable_name:\n        self.assertAllClose(\n            0.0,\n            warm_started_dnn_lc_regressor.get_variable_value(variable_name))\n      else:\n        self.assertAllClose(\n            dnn_lc_regressor.get_variable_value(variable_name),\n            warm_started_dnn_lc_regressor.get_variable_value(variable_name))\n\n  def test_warm_starting_selective_variables(self, fc_impl):\n    \"\"\"Tests selecting variables to warm-start.\"\"\"\n    age = fc_impl.numeric_column('age')\n    city = fc_impl.embedding_column(\n        fc_impl.categorical_column_with_vocabulary_list(\n            'city', vocabulary_list=['Mountain View', 'Palo Alto']),\n        dimension=5)\n\n    # Create a DNNLinearCombinedClassifier and train to save a checkpoint.\n    dnn_lc_classifier = dnn_linear_combined.DNNLinearCombinedClassifierV2(\n        linear_feature_columns=[age],\n        dnn_feature_columns=[city],\n        dnn_hidden_units=[256, 128],\n        model_dir=self._ckpt_and_vocab_dir,\n        n_classes=4,\n        linear_optimizer='SGD',\n        dnn_optimizer='SGD')\n    dnn_lc_classifier.train(input_fn=self._input_fn, max_steps=1)\n\n    # Create a second DNNLinearCombinedClassifier, warm-started from the first.\n    # Use a learning_rate = 0.0 optimizer to check values (use SGD so we don't\n    # have accumulator values that change).\n    warm_started_dnn_lc_classifier = (\n        dnn_linear_combined.DNNLinearCombinedClassifierV2(\n            linear_feature_columns=[age],\n            dnn_feature_columns=[city],\n            dnn_hidden_units=[256, 128],\n            n_classes=4,\n            linear_optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=0.0),\n            dnn_optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=0.0),\n            # The provided regular expression will only warm-start the deep\n            # portion of the model.\n            warm_start_from=estimator.WarmStartSettings(\n                ckpt_to_initialize_from=dnn_lc_classifier.model_dir,\n                vars_to_warm_start='.*(dnn).*')))\n\n    warm_started_dnn_lc_classifier.train(input_fn=self._input_fn, max_steps=1)\n    for variable_name in warm_started_dnn_lc_classifier.get_variable_names():\n      if 'dnn' in variable_name:\n        if 'learning_rate' in variable_name:\n          self.assertAllClose(\n              0.0,\n              warm_started_dnn_lc_classifier.get_variable_value(variable_name))\n        else:\n          self.assertAllClose(\n              dnn_lc_classifier.get_variable_value(variable_name),\n              warm_started_dnn_lc_classifier.get_variable_value(variable_name))\n      elif 'linear' in variable_name:\n        linear_values = warm_started_dnn_lc_classifier.get_variable_value(\n            variable_name)\n        # Since they're not warm-started, the linear weights will be\n        # zero-initialized.\n        self.assertAllClose(np.zeros_like(linear_values), linear_values)\n\n\nif __name__ == '__main__':\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/estimator", "file_path": "tensorflow_estimator/python/estimator/tpu/tpu_estimator.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===================================================================\n\"\"\"TPUEstimator class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport enum\nimport math\nimport os\nimport signal\nimport sys\nimport threading\nimport time\n\nimport tensorflow as tf\nimport numpy as np\nimport six\nfrom six.moves import queue as Queue  # pylint: disable=redefined-builtin\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.core.framework import variable_pb2\nfrom tensorflow.core.framework.summary_pb2 import Summary\nfrom tensorflow.core.protobuf.tpu import compilation_result_pb2 as tpu_compilation_result\nfrom tensorflow.python.data.util import nest as data_nest\nfrom tensorflow.python.distribute.cluster_resolver import tpu_cluster_resolver\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import summary_ops_v2\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.tpu import functional as tpu_functional\nfrom tensorflow.python.tpu import preempted_hook\nfrom tensorflow.python.tpu import session_support\nfrom tensorflow.python.tpu import tensor_tracer\nfrom tensorflow.python.tpu import tpu\nfrom tensorflow.python.tpu import tpu_embedding_gradient\nfrom tensorflow.python.tpu import tpu_feed\nfrom tensorflow.python.tpu import tpu_function\nfrom tensorflow.python.tpu import training_loop\nfrom tensorflow.python.tpu.ops import tpu_ops\nfrom tensorflow.python.training import evaluation\nfrom tensorflow.python.util import function_utils\nfrom tensorflow.python.util import tf_inspect\nfrom tensorflow.python.util.tf_export import estimator_export\nfrom tensorflow_estimator.python.estimator import estimator as estimator_lib\nfrom tensorflow_estimator.python.estimator import model_fn as model_fn_lib\nfrom tensorflow_estimator.python.estimator.export import export_output as export_output_lib\nfrom tensorflow_estimator.python.estimator.tpu import _tpu_estimator_embedding\nfrom tensorflow_estimator.python.estimator.tpu import error_handling\nfrom tensorflow_estimator.python.estimator.tpu import iteration_count_estimator\nfrom tensorflow_estimator.python.estimator.tpu import tpu_config\nfrom tensorflow_estimator.python.estimator.tpu import tpu_context\nfrom tensorflow_estimator.python.estimator.tpu import util as util_lib\nfrom tensorflow_estimator.python.estimator.tpu._tpu_estimator_embedding import AdagradParameters  # pylint: disable=unused-import\nfrom tensorflow_estimator.python.estimator.tpu._tpu_estimator_embedding import AdamParameters  # pylint: disable=unused-import\nfrom tensorflow_estimator.python.estimator.tpu._tpu_estimator_embedding import EmbeddingConfigSpec  # pylint: disable=unused-import\nfrom tensorflow_estimator.python.estimator.tpu._tpu_estimator_embedding import StochasticGradientDescentParameters  # pylint: disable=unused-import\n\n_INITIAL_LOSS = 1e7\n_ZERO_LOSS = 0.\n_TPU_ESTIMATOR = 'tpu_estimator'\n_ITERATIONS_PER_LOOP_VAR = 'iterations_per_loop'\n_BATCH_SIZE_KEY = 'batch_size'\n_CTX_KEY = 'context'\n_USE_TPU_KEY = 'use_tpu'\n_CROSS_REPLICA_SUM_OP = 'CrossReplicaSum'\n_ONE_GIGABYTE = 1024 * 1024 * 1024\n_TPU_ENQUEUE_OPS = '_tpu_enqueue_ops'\n_TPU_TRAIN_OP = '_tpu_train_op'\n_INFERENCE_ON_TPU_MODE = '_inference_on_tpu'\n_KEY_WHEN_PREDICTIONS_IS_A_TENSOR = '_key_when_predictions_is_a_tensor'\n_TENSOR_PACKER_SMALL_FEATURE_DIM_SIZE = 1\n_TENSOR_PACKER_MINIMUM_NUM_SMALL_FEATURES_TO_GROUP = 5\n_TENSOR_PACKER_CONCATENATED_SMALL_FEATURES_KEY = '_concatenated_small_features'\n\n# Ideally _USE_TPU_KEY should be reserved as well. However there are already\n# models that make use of this key, thus it can not be reserved now to prevent\n# breakage. In the long run, we would like to mitigate this by migrating models\n# off of using _USE_TPU_KEY.\n_RESERVED_PARAMS_KEYS = [_BATCH_SIZE_KEY, _CTX_KEY]\n\n# TODO(b/65703635): Flip the value and remove all dead code. Currently, this is\n# only used for per-core based deployments. For per-host based pipelines, if a\n# user returns a Dataset instance it will be automatically wrapped in a\n# tf.while_loop (This can be disabled by returning features and labels\n# explicitly).\n_WRAP_INPUT_FN_INTO_WHILE_LOOP = False\n\n# Track the adoption of TPUEstimator\n_tpu_estimator_gauge = tf.compat.v2.__internal__.monitoring.BoolGauge(\n    '/tensorflow/api/tpu_estimator',\n    'Whether the program uses tpu estimator or not.')\n\nif ops.get_to_proto_function('{}_{}'.format(_TPU_ESTIMATOR,\n                                            _ITERATIONS_PER_LOOP_VAR)) is None:\n  ops.register_proto_function(\n      '{}_{}'.format(_TPU_ESTIMATOR, _ITERATIONS_PER_LOOP_VAR),\n      proto_type=variable_pb2.VariableDef,\n      to_proto=resource_variable_ops._to_proto_fn,  # pylint: disable=protected-access\n      from_proto=resource_variable_ops._from_proto_fn)  # pylint: disable=protected-access\n\n\ndef _is_iterable(obj):\n  \"\"\"A Python 2 and 3 compatible util to check whether `obj` is iterable.\"\"\"\n  try:\n    iter(obj)\n    return True\n  except TypeError:\n    return False\n\n\nclass CatchInvalidHostcallFunctions(control_flow_ops.XLAControlFlowContext):\n\n  def AddOp(self, op):\n    if op.type in [\n        'AudioSummary', 'AudioSummaryV2', 'HistogramSummary', 'ImageSummary',\n        'MergeSummary', 'ScalarSummary', 'TensorSummary', 'TensorSummaryV2'\n    ]:\n      raise ValueError('Please use tf.contrib.summary instead of tf.summary '\n                       'inside of host_calls.')\n\n\ndef _create_global_step(graph):\n  graph = graph or tf.compat.v1.get_default_graph()\n  if tf.compat.v1.train.get_global_step(graph) is not None:\n    raise ValueError('\"global_step\" already exists.')\n  # Create in proper graph and base name_scope.\n  with graph.as_default() as g, g.name_scope(None):\n    return tf.compat.v1.get_variable(\n        tf.compat.v1.GraphKeys.GLOBAL_STEP,\n        shape=[],\n        dtype=tf.dtypes.int64,\n        initializer=tf.compat.v1.initializers.zeros(),\n        trainable=False,\n        use_resource=True,\n        collections=[\n            tf.compat.v1.GraphKeys.GLOBAL_VARIABLES,\n            tf.compat.v1.GraphKeys.GLOBAL_STEP\n        ])\n\n\ndef _create_or_get_iterations_per_loop():\n  \"\"\"Creates or gets the iterations_per_loop variable.\n\n  In TPUEstimator, the user provided computation, the model_fn, is wrapped\n  inside a tf.while_loop for peak performance. The iterations of the loop are\n  specified by this variable, which adjusts its value on the CPU after each TPU\n  program execution and before the next TPU execution.\n\n  The purpose of using a variable, rather then a constant, is to allow\n  TPUEstimator adapt the TPU training iterations according to the final steps\n  specified by users. For example, if the user sets the iterations_per_loop as 4\n  in TPUConfig and steps as 10 in TPUEstimator.train(), the iterations_per_loop\n  variable will have the following value before each TPU training.\n\n      - 1-th TPU execution: iterations_per_loop = 4\n      - 2-th TPU execution: iterations_per_loop = 4\n      - 3-th TPU execution: iterations_per_loop = 2\n\n  As model_fn increases the global step once per train_op invocation, the global\n  step is 10 after all TPU executions, matching the steps=10 inputs passed in by\n  users.\n\n  Returns:\n    A TF non-trainable resource variable.\n\n  Raises:\n    RuntimeError: If multi iterations_per_loop variables were found.\n  \"\"\"\n  graph = tf.compat.v1.get_default_graph()\n  collection_name = '{}_{}'.format(_TPU_ESTIMATOR, _ITERATIONS_PER_LOOP_VAR)\n  iter_vars = graph.get_collection(collection_name)\n  if len(iter_vars) == 1:\n    return iter_vars[0]\n  elif len(iter_vars) > 1:\n    raise RuntimeError('Multiple iterations_per_loop_var in collection.')\n\n  with ops.colocate_with(tf.compat.v1.train.get_global_step()):\n    with tf.compat.v1.variable_scope(\n        _TPU_ESTIMATOR, reuse=tf.compat.v1.AUTO_REUSE):\n      return tf.compat.v1.get_variable(\n          _ITERATIONS_PER_LOOP_VAR,\n          initializer=tf.compat.v1.initializers.zeros(),\n          shape=[],\n          dtype=tf.dtypes.int32,\n          trainable=False,\n          collections=[collection_name, tf.compat.v1.GraphKeys.LOCAL_VARIABLES],\n          use_resource=True)\n\n\ndef _sync_variables_ops(ctx):\n  \"\"\"Create varriables synchronization ops.\n\n  Gets the variables back from TPU nodes. This means the variables updated\n  by TPU will now be *synced* to host memory.\n  In BROADCAST mode, we skip this sync since the variables are ususally too\n  big to transmit via RPC.\n\n  Args:\n    ctx: A `_InternalTPUContext` instance with mode.\n\n  Returns:\n    A list of sync ops.\n  \"\"\"\n\n  if not ctx.is_input_broadcast_with_iterators():\n    return [\n        tf.debugging.check_numerics(v.read_value(),\n                                    'Gradient for %s is NaN' % v.name).op\n        for v in tf.compat.v1.trainable_variables()\n    ]\n  else:\n    return [tf.no_op()]\n\n\ndef _increase_eval_step_op(iterations_per_loop):\n  \"\"\"Returns an op to increase the eval step for TPU evaluation.\n\n  Args:\n    iterations_per_loop: Tensor. The number of eval steps running in TPU system\n      before returning to CPU host for each `Session.run`.\n\n  Returns:\n    An operation\n  \"\"\"\n  eval_step = evaluation._get_or_create_eval_step()  # pylint: disable=protected-access\n  # Estimator evaluate increases 1 by default. So, we increase the difference.\n  return tf.compat.v1.assign_add(\n      eval_step,\n      tf.cast(iterations_per_loop - 1, dtype=eval_step.dtype),\n      use_locking=True)\n\n\ndef _extract_key_names(tensor_or_dict):\n  if isinstance(tensor_or_dict, dict):\n    return sorted(tensor_or_dict.keys())\n  return []\n\n\nclass PeriodicLogger(object):\n\n  def __init__(self, seconds):\n    self._log_every_n_seconds = seconds\n    self._last_log_time = 0\n\n  def log(self, msg, *args, **kw):\n    if time.time() - self._last_log_time > self._log_every_n_seconds:\n      self._last_log_time = time.time()\n      tf.compat.v1.logging.info(msg, *args, **kw)\n\n\nclass _SIGNAL(object):\n  \"\"\"Signal used to control the thread of infeed/outfeed.\n\n  All preserved signals must be negative numbers. Positive numbers are used to\n  indicate the number of iterations for next training/evaluation loop.\n  \"\"\"\n  NEXT_BATCH = -1\n  STOP = -2\n\n\n@estimator_export(v1=['estimator.tpu.TPUEstimatorSpec'])\nclass TPUEstimatorSpec(model_fn_lib._TPUEstimatorSpec):  # pylint: disable=protected-access\n  \"\"\"Ops and objects returned from a `model_fn` and passed to `TPUEstimator`.\n\n  See `EstimatorSpec` for `mode`, `predictions`, `loss`, `train_op`, and\n  `export_outputs`.\n\n  For evaluation, `eval_metrics `is a tuple of `metric_fn` and `tensors`, where\n  `metric_fn` runs on CPU to generate metrics and `tensors` represents the\n  `Tensor`s transferred from TPU system to CPU host and passed to `metric_fn`.\n  To be precise, TPU evaluation expects a slightly different signature from the\n  `tf.estimator.Estimator`. While `EstimatorSpec.eval_metric_ops` expects a\n  dict, `TPUEstimatorSpec.eval_metrics` is a tuple of `metric_fn` and `tensors`.\n  The `tensors` could be a list of `Tensor`s or dict of names to `Tensor`s. The\n  `tensors` usually specify the model logits, which are transferred back from\n  TPU system to CPU host. All tensors must have be batch-major, i.e., the batch\n  size is the first dimension. Once all tensors are available at CPU host from\n  all shards, they are concatenated (on CPU) and passed as positional arguments\n  to the `metric_fn` if `tensors` is list or keyword arguments if `tensors` is\n  a dict. `metric_fn` takes the `tensors` and returns a dict from metric string\n  name to the result of calling a metric function, namely a `(metric_tensor,\n  update_op)` tuple. See `TPUEstimator` for MNIST example how to specify the\n  `eval_metrics`.\n\n  `scaffold_fn` is a function running on CPU to generate the `Scaffold`. This\n  function should not capture any Tensors in `model_fn`.\n\n  `host_call` is a tuple of a `function` and a list or dictionary of `tensors`\n  to pass to that function and returns a list of Tensors. `host_call` currently\n  works for train() and evaluate(). The Tensors returned by the function is\n  executed on the CPU on every step, so there is communication overhead when\n  sending tensors from TPU to CPU. To reduce the overhead, try reducing the\n  size of the tensors. The `tensors` are concatenated along their major (batch)\n  dimension, and so must be >= rank 1. The `host_call` is useful for writing\n  summaries with `tf.contrib.summary.create_file_writer`.\n\n  @compatibility(TF2)\n  TPU Estimator manages its own TensorFlow graph and session, so it is not\n  compatible with TF2 behaviors. We recommend that you migrate to the newer\n  `tf.distribute.TPUStrategy`. See the\n  [TPU guide](https://www.tensorflow.org/guide/tpu) for details.\n  @end_compatibility\n  \"\"\"\n\n  def __new__(cls,\n              mode,\n              predictions=None,\n              loss=None,\n              train_op=None,\n              eval_metrics=None,\n              export_outputs=None,\n              scaffold_fn=None,\n              host_call=None,\n              training_hooks=None,\n              evaluation_hooks=None,\n              prediction_hooks=None):\n    \"\"\"Creates a validated `TPUEstimatorSpec` instance.\"\"\"\n    cls._host_calls = {}\n    if eval_metrics is not None:\n      cls._host_calls['eval_metrics'] = eval_metrics\n    if host_call is not None:\n      cls._host_calls['host_call'] = host_call\n    _OutfeedHostCall.validate(cls._host_calls)\n\n    training_hooks = tuple(training_hooks or [])\n    evaluation_hooks = tuple(evaluation_hooks or [])\n    prediction_hooks = tuple(prediction_hooks or [])\n\n    for hook in training_hooks + evaluation_hooks + prediction_hooks:\n      if not isinstance(hook, tf.compat.v1.train.SessionRunHook):\n        raise TypeError(\n            'All hooks must be SessionRunHook instances, given: {}'.format(\n                hook))\n\n    return super(TPUEstimatorSpec, cls).__new__(\n        cls,\n        mode=mode,\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op,\n        eval_metrics=eval_metrics,\n        export_outputs=export_outputs,\n        scaffold_fn=scaffold_fn,\n        host_call=host_call,\n        training_hooks=training_hooks,\n        evaluation_hooks=evaluation_hooks,\n        prediction_hooks=prediction_hooks)\n\n  def as_estimator_spec(self):\n    \"\"\"Creates an equivalent `EstimatorSpec` used by CPU train/eval.\"\"\"\n    host_call_ret = _OutfeedHostCall.create_cpu_hostcall(self._host_calls)\n    eval_metric_ops = None\n    if self.eval_metrics is not None:\n      eval_metric_ops = host_call_ret['eval_metrics']\n    hooks = None\n    if self.host_call is not None:\n      hooks = [_OutfeedHostCallHook(host_call_ret['host_call'])]\n    loss = self.loss\n    if tensor_tracer.TensorTracer.is_enabled() \\\n       and self.train_op is not None:\n      tt = tensor_tracer.TensorTracer()\n      loss = tt.trace_cpu(tf.compat.v1.get_default_graph(), loss, self.train_op)\n\n    hooks = tuple(hooks or [])\n    scaffold = self.scaffold_fn() if self.scaffold_fn else None\n    return model_fn_lib.EstimatorSpec(\n        mode=self.mode,\n        predictions=self.predictions,\n        loss=loss,\n        train_op=self.train_op,\n        eval_metric_ops=eval_metric_ops,\n        export_outputs=self.export_outputs,\n        scaffold=scaffold,\n        training_hooks=self.training_hooks + hooks,\n        evaluation_hooks=self.evaluation_hooks + hooks,\n        prediction_hooks=self.prediction_hooks + hooks)\n\n\nclass _OpQueueContext(object):\n  \"\"\"Manages work queue and thread for a infeed/outfeed thread.\"\"\"\n\n  def __init__(self, name, target, args):\n    self._name = name\n    self._queue = Queue.Queue()\n    args = (self,) + args\n    self._thread = threading.Thread(name=name, target=target, args=args)\n    self._thread.daemon = True\n    self._thread.start()\n\n  def stop(self):\n    self._queue.put(_SIGNAL.STOP)\n\n  def send_next_batch_signal(self, iterations):\n    self._queue.put(iterations)\n\n  def read_iteration_counts(self):\n    while True:\n      iterations = self._queue.get(block=True)\n      tf.compat.v1.logging.debug('%s read iterations %s', self._name,\n                                 iterations)\n      if iterations == _SIGNAL.STOP:\n        tf.compat.v1.logging.info('%s received shutdown signal, stopping.',\n                                  self._name)\n        return\n      yield iterations\n\n  def join(self):\n    tf.compat.v1.logging.info('Shutting down %s thread.', self._name)\n    self.stop()\n    self._thread.join()\n\n\nclass _OpSignalOnceQueueContext(_OpQueueContext):\n  \"\"\"Manages work queue and thread for a infeed/outfeed thread.\n\n  This subclass only signals once.\n  \"\"\"\n\n  def __init__(self, name, target, args):\n    super(_OpSignalOnceQueueContext, self).__init__(name, target, args)\n    self._has_signaled = False\n\n  def send_next_batch_signal(self, iterations):\n    if not self._has_signaled:\n      self._queue.put(iterations)\n      self._has_signaled = True\n\n\nclass TPUInfeedOutfeedSessionHook(tf.compat.v1.train.SessionRunHook):\n  \"\"\"A Session hook setting up the TPU initialization, infeed, and outfeed.\n\n  This hook does two major things:\n  1. initialize and shutdown TPU system.\n  2. launch and join the threads for infeed enqueue and (optional) outfeed\n     dequeue.\n  \"\"\"\n\n  def __init__(self,\n               ctx,\n               enqueue_ops,\n               dequeue_ops,\n               tpu_compile_op,\n               run_infeed_loop_on_coordinator=True,\n               rendezvous=None,\n               master=None,\n               session_config=None,\n               tpu_init_ops=None,\n               outfeed_every_n_steps=1):\n    self._master_job = ctx.master_job\n    self._enqueue_ops = enqueue_ops\n    self._dequeue_ops = dequeue_ops\n    self._rendezvous = rendezvous\n    self._master = master\n    self._session_config = session_config\n    self._init_ops = list(tpu_init_ops or [])\n    if ctx.embedding_config is None:\n      self._embedding_layer_config = None\n    else:\n      self._embedding_layer_config = (\n          ctx.embedding_config.tpu_embedding.config_proto)\n    self._run_infeed_loop_on_coordinator = run_infeed_loop_on_coordinator\n    self._initial_infeed_sleep_secs = (\n        ctx.config.tpu_config.initial_infeed_sleep_secs)\n    self._tpu_compile_op = tpu_compile_op\n\n    # When using model parallelism, the TPU is pre-initialized at startup to\n    # fetch mesh information. We skip re-initializing it here for\n    # MeshTensorFlow since it places variables on TPU directly. Reinitialize tpu\n    # is causing the variable corruption since the previous allocated memory\n    # might be overwritten for other purpose.\n    if (ctx.model_parallelism_enabled and\n        (ctx.config.tpu_config.per_host_input_for_training is\n         tpu_config.InputPipelineConfig.BROADCAST)):\n      self._should_initialize_tpu = False\n    else:\n      self._should_initialize_tpu = True\n    self._outfeed_every_n_steps = outfeed_every_n_steps\n\n  def begin(self):\n    tf.compat.v1.logging.info('TPU job name %s', self._master_job)\n    self._iterations_per_loop_var = _create_or_get_iterations_per_loop()\n    if self._should_initialize_tpu:\n      self._finalize_ops = [\n          tf.compat.v1.tpu.shutdown_system(job=self._master_job)\n      ]\n    else:\n      self._finalize_ops = []\n\n    summary_writer_init_ops = summary_ops_v2.summary_writer_initializer_op()\n    self._init_ops.extend(summary_writer_init_ops)\n    # Get all the writer resources from the initializer, so we know what to\n    # flush.\n    for op in summary_writer_init_ops:\n      self._finalize_ops.append(\n        summary_ops_v2.legacy_raw_flush(writer=op.inputs[0]))\n\n  def _run_infeed(self, queue_ctx, session):\n    tf.compat.v1.logging.info('Starting infeed thread controller.')\n    if self._initial_infeed_sleep_secs:\n      tf.compat.v1.logging.info('Infeed thread sleeping for %d seconds.',\n                                self._initial_infeed_sleep_secs)\n      time.sleep(self._initial_infeed_sleep_secs)\n      tf.compat.v1.logging.info('Infeed thread starting after sleep')\n\n    with self._rendezvous.catch_errors(source='infeed', session=session):\n      if self._run_infeed_loop_on_coordinator:\n        for count, steps in enumerate(queue_ctx.read_iteration_counts()):\n          for i in xrange(steps):\n            tf.compat.v1.logging.debug('Infeed enqueue for iteration (%d, %d)',\n                                       count, i)\n            session.run(self._enqueue_ops)\n      else:\n        for _ in queue_ctx.read_iteration_counts():\n          session.run(self._enqueue_ops)\n      tf.compat.v1.logging.info('Infeed thread finished, shutting down.')\n\n  def _run_outfeed(self, queue_ctx, session):\n    tf.compat.v1.logging.info('Starting outfeed thread controller.')\n    status_logger = PeriodicLogger(seconds=60)\n    with self._rendezvous.catch_errors(source='outfeed', session=session):\n      for count, steps in enumerate(queue_ctx.read_iteration_counts()):\n        step_counter = 0\n        for i in xrange(steps):\n          tf.compat.v1.logging.debug('Outfeed dequeue for iteration (%d, %d)',\n                                     count, i)\n          if step_counter % self._outfeed_every_n_steps == 0:\n            session.run(self._dequeue_ops)\n          step_counter += 1\n          status_logger.log('Outfeed finished for iteration (%d, %d)', count, i)\n      tf.compat.v1.logging.info('Outfeed thread finished, shutting down.')\n\n  def _create_infeed_controller(self, name, target, args):\n    return _OpQueueContext(name=name, target=target, args=args)\n\n  def _assertCompilationSucceeded(self, result, coord):\n    proto = tpu_compilation_result.CompilationResultProto()\n    proto.ParseFromString(result)\n    if proto.status_error_message:\n      tf.compat.v1.logging.error('Compilation failed: {}'.format(\n          proto.status_error_message))\n      coord.request_stop()\n    else:\n      tf.compat.v1.logging.info('Compilation succeeded')\n\n  def after_create_session(self, session, coord):\n    if self._should_initialize_tpu:\n      tf.compat.v1.logging.info('Init TPU system')\n      start = time.time()\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session(\n            self._master, config=self._session_config) as sess:\n          sess.run(\n              tf.compat.v1.tpu.initialize_system(\n                  job=self._master_job,\n                  embedding_config=self._embedding_layer_config))\n      tf.compat.v1.logging.info('Initialized TPU in %d seconds',\n                                time.time() - start)\n\n    session.run(\n        self._init_ops,\n        options=tf.compat.v1.RunOptions(timeout_in_ms=30 * 60 * 1000))\n\n    if os.environ.get('TPU_SPLIT_COMPILE_AND_EXECUTE', '') == '1':\n      tf.compat.v1.logging.info(\n          'Compiling user program: this may take a while...')\n      self._assertCompilationSucceeded(session.run(self._tpu_compile_op), coord)\n\n    self._infeed_controller = self._create_infeed_controller(\n        name='InfeedController', target=self._run_infeed, args=(session,))\n\n    self._outfeed_controller = _OpQueueContext(\n        name='OutfeedController', target=self._run_outfeed, args=(session,))\n\n    # Enable the worker watchdog to terminate workers on coordinator exit.\n    watchdog_timeout = int(os.environ.get('TF_TPU_WATCHDOG_TIMEOUT', '0'))\n    if watchdog_timeout > 0:\n      session_support.start_worker_watchdog(\n          session, shutdown_timeout=watchdog_timeout)\n\n  def before_run(self, run_context):\n    iterations = run_context.session.run(self._iterations_per_loop_var)\n\n    tf.compat.v1.logging.info('Enqueue next (%d) batch(es) of data to infeed.',\n                              iterations)\n    self._infeed_controller.send_next_batch_signal(iterations)\n\n    tf.compat.v1.logging.info(\n        'Dequeue next (%d) batch(es) of data from outfeed.', iterations)\n    self._outfeed_controller.send_next_batch_signal(iterations)\n\n  def end(self, session):\n    tf.compat.v1.logging.info('Stop infeed thread controller')\n    self._infeed_controller.join()\n    self._rendezvous.record_done('infeed')\n\n    tf.compat.v1.logging.info('Stop output thread controller')\n    self._outfeed_controller.join()\n    self._rendezvous.record_done('outfeed')\n\n    tf.compat.v1.logging.info('Shutdown TPU system.')\n    session.run(self._finalize_ops)\n\n\nclass TPUInfeedOutfeedSessionHookForPrediction(TPUInfeedOutfeedSessionHook):\n\n  def __init__(self,\n               ctx,\n               enqueue_ops,\n               dequeue_ops,\n               tpu_compile_op,\n               rendezvous=None,\n               master=None,\n               session_config=None):\n    super(TPUInfeedOutfeedSessionHookForPrediction, self).__init__(\n        ctx,\n        enqueue_ops,\n        dequeue_ops,\n        tpu_compile_op=tpu_compile_op,\n        run_infeed_loop_on_coordinator=False,\n        rendezvous=rendezvous,\n        master=master,\n        session_config=session_config)\n\n  def _create_infeed_controller(self, name, target, args):\n    return _OpSignalOnceQueueContext(name=name, target=target, args=args)\n\n\nclass _TPUStopAtStepHook(tf.compat.v1.train.SessionRunHook):\n  \"\"\"Hook that requests stop at a specified step.\n\n  This hook is similar to the `session_run_hook._StopAfterNEvalsHook` with\n  following differences for TPU training:\n\n  1. This hook sets the variable for `iterations_per_loop`, which is used by\n     `TPUInfeedOutfeedSessionHook` to control the iterations for infeed/outfeed.\n     If the `iterations_per_loop` value is specified as time in seconds, the\n     number of iterations per `Session.run` will be estimated automatically\n     based on per iteration runtime.\n\n     As the hook execution order is not guaranteed, the variable update is\n     handled in `after_create_session` and `after_run` as\n     `TPUInfeedOutfeedSessionHook` reads the variable value in `before_run`.\n\n  2. For each training loop (session.run), the global step could be increased\n     multiple times on TPU. The global step tensor value will be explicitly read\n     again in `after_run` to ensure the latest value is retrieved to avoid race\n     condition.\n  \"\"\"\n\n  def __init__(self,\n               iterations_per_loop_counter,\n               num_steps=None,\n               final_step=None):\n    \"\"\"Initializes a `TPUStopAtStepHook`.\n\n    Args:\n      iterations_per_loop_counter: A namedtuple of [`value',`unit`] that\n        represents the number of 'iterations count' or 'time in seconds' to run\n        optimizer per loop, based on the `unit` specified, `count` or `seconds`\n        respectively.\n      num_steps: Number of steps to execute.\n      final_step: Step after which to stop.\n\n    Raises:\n      ValueError: If one of the arguments is invalid.\n    \"\"\"\n    if num_steps is None and final_step is None:\n      raise ValueError('One of `num_steps` or `final_step` must be specified.')\n    if num_steps is not None and final_step is not None:\n      raise ValueError(\n          'Only one of `num_steps` or `final_step` can be specified.')\n    self._iterations_per_loop_counter = iterations_per_loop_counter\n    if self._iterations_per_loop_counter.unit not in ['seconds', 'count']:\n      raise ValueError('Only `count` or `seconds` are accepted as the '\n                       '`iterations_per_loop_counter.unit')\n    self._num_steps = num_steps\n    self._final_step = final_step\n    self._next_iteration_count = 1\n    self._iteration_count_estimator = None\n    if self._iterations_per_loop_counter.unit == 'seconds':\n      self._iteration_count_estimator = (\n          iteration_count_estimator.IterationCountEstimator())\n    self._start_time = time.time()\n\n  def _next_iterations(self, global_step, final_step):\n    \"\"\"Computes the next iterations count.\n\n    The next iterations count is computed by choosing the smaller of the\n    remaining step count (`final_step` - `global_step`) and the estimated\n    iterations count returned by the estimator.\n\n    Args:\n      global_step: The current step.\n      final_step: Step after which to stop.\n\n    Returns:\n      The number of iterations count to run per loop.\n    \"\"\"\n    remaining_steps = final_step - global_step\n\n    if self._iteration_count_estimator is not None:\n      estimated_iterations = self._iteration_count_estimator.get(\n          self._iterations_per_loop_counter.value)\n    else:\n      estimated_iterations = self._iterations_per_loop_counter.value\n\n    self._next_iteration_count = min(remaining_steps, estimated_iterations)\n    return self._next_iteration_count\n\n  def begin(self):\n    \"\"\"Initializes variables.\n\n    Initializes the global step and iterations per loop variables.\n\n    Raises:\n      RuntimeError: An error occurred if global step variable does not exist.\n    \"\"\"\n    self._global_step_tensor = tf.compat.v1.train.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError('Global step should be created.')\n\n    self._iterations_per_loop_var = _create_or_get_iterations_per_loop()\n\n  def after_create_session(self, session, coord):\n    \"\"\"Computes and updates the first time iterations count.\n\n    The iterations are computed by choosing the smaller of the (`final step` -\n    `global step`), and the initial estimated iterations returned by the\n    estimator (by default is 1).\n\n    Args:\n      session: A TensorFlow Session that has been created.\n      coord: A Coordinator object which keeps track of all threads.\n    \"\"\"\n    global_step = session.run(self._global_step_tensor)\n    if self._final_step is None:\n      self._final_step = global_step + self._num_steps\n\n    iterations = self._next_iterations(global_step, self._final_step)\n    self._iterations_per_loop_var.load(iterations, session=session)\n\n  def before_run(self, run_context):\n    \"\"\"Reset the timer.\"\"\"\n    if self._iteration_count_estimator is not None:\n      self._start_time = time.time()\n\n  def after_run(self, run_context, run_values):\n    \"\"\"Computes the next iterations per loop value or terminates.\n\n    Computes the elapsed time to run the last optimizer loop and if the\n    `IterationCountEstimator` is used, records the elapsed time and iterations\n    count. If the final step count has been reached, terminates. Otherwise,\n    computes and updates the number of iterations to run the optimizer per loop.\n\n    Args:\n      run_context: A `SessionRunContext` object.\n      run_values: A SessionRunValues object.\n    \"\"\"\n    if self._iteration_count_estimator is not None:\n      elapsed_time = time.time() - self._start_time\n      tf.compat.v1.logging.info('ElapsedTime: %.3f', elapsed_time)\n      self._iteration_count_estimator.update(elapsed_time,\n                                             self._next_iteration_count)\n\n    # Global step cannot be retrieved via SessionRunArgs and before_run due to\n    # race condition.\n    global_step = run_context.session.run(self._global_step_tensor)\n    if global_step >= self._final_step:\n      run_context.request_stop()\n    else:\n      iterations = self._next_iterations(global_step, self._final_step)\n      self._iterations_per_loop_var.load(\n          iterations, session=run_context.session)\n\n\nclass _SetEvalIterationsHook(tf.compat.v1.train.SessionRunHook):\n  \"\"\"Hook that requests stop at a specified step.\"\"\"\n\n  def __init__(self, num_steps):\n    \"\"\"Initializes a `_SetEvalIterationsHook`.\n\n    Args:\n      num_steps: Number of steps to execute.\n    \"\"\"\n    self._num_steps = num_steps\n\n  def begin(self):\n    self._iterations_per_loop_var = _create_or_get_iterations_per_loop()\n\n  def after_create_session(self, session, coord):\n    self._iterations_per_loop_var.load(self._num_steps, session=session)\n\n\nclass _StoppingPredictHook(tf.compat.v1.train.SessionRunHook):\n  \"\"\"Hook that requests stop according to the stopping signal in prediction.\"\"\"\n\n  def __init__(self, scalar_stopping_signal):\n    self._scalar_stopping_signal = scalar_stopping_signal\n\n  def begin(self):\n    self._iterations_per_loop_var = _create_or_get_iterations_per_loop()\n\n  def after_create_session(self, session, coord):\n    # This is not necessary as we do not run infeed enqueue and outfeed dequeue\n    # in side threads for prediction model. But it makes the\n    # TPUInfeedOutfeedSessionHook prints nice message.\n    self._iterations_per_loop_var.load(1, session=session)\n\n  def before_run(self, run_context):\n    return tf.compat.v1.train.SessionRunArgs(self._scalar_stopping_signal)\n\n  def after_run(self, run_context, run_values):\n    _ = run_context\n    scalar_stopping_signal = run_values.results\n    if _StopSignals.should_stop(scalar_stopping_signal):\n      # NOTE(xiejw): In prediction, stopping signals are inserted for each\n      # batch. And we append one more batch to signal the system it should stop.\n      # The data flow might look like\n      #\n      #  batch   0: images, labels, stop = 0  (user provided)\n      #  batch   1: images, labels, stop = 0  (user provided)\n      #  ...\n      #  batch  99: images, labels, stop = 0  (user provided)\n      #  batch 100: images, labels, stop = 1  (TPUEstimator appended)\n      #\n      # where the final batch (id = 100) is appended by TPUEstimator, so we\n      # should drop it before returning the predictions to user.\n      # To achieve that, we throw the OutOfRangeError in after_run. Once\n      # Monitored Session sees this error in SessionRunHook.after_run, the\n      # \"current\" prediction, i.e., batch with id=100, will be discarded\n      # immediately\n      raise tf.errors.OutOfRangeError(None, None, 'Stopped by stopping signal.')\n\n\ndef generate_per_core_enqueue_ops_fn_for_host(ctx, input_fn,\n                                              inputs_structure_recorder,\n                                              host_device, host_id):\n  \"\"\"Generates infeed enqueue ops for per-core input_fn on a single host.\"\"\"\n  captured_infeed_queue = _CapturedObject()\n  tpu_ordinal_function_impl = ctx.tpu_ordinal_function(host_id)\n\n  def enqueue_ops_fn():\n    \"\"\"A fn returns enqueue_ops.\"\"\"\n    num_cores_per_host = ctx.num_of_cores_per_host\n    per_host_sharded_inputs = []\n    for core_ordinal in range(num_cores_per_host):\n      with ops.name_scope('ordinal_%d' % (core_ordinal)):\n        user_context = tpu_context.TPUContext(\n            internal_ctx=ctx,\n            input_device=host_device,\n            invocation_index=host_id * ctx.num_of_cores_per_host + core_ordinal,\n            host_id=host_id)\n        inputs = _Inputs.from_input_fn(input_fn(user_context))\n        if inputs.is_dataset:\n          raise TypeError(\n              '`input_fn` returning `Dataset`  is not yet supported in '\n              'per-Core input pipeline deployment yet. Please set '\n              'TPUConfig.per_host_input_for_training to True or return '\n              '`features` and `labels` from `input_fn`')\n        features, labels = inputs.features_and_labels()\n\n        inputs_structure_recorder.validate_and_record_structure(\n            features, labels)\n        flattened_inputs = (\n            inputs_structure_recorder.flatten_features_and_labels(\n                features, labels))\n        per_host_sharded_inputs.append(flattened_inputs)\n\n    infeed_queue = tpu_feed.InfeedQueue(\n        number_of_tuple_elements=len(per_host_sharded_inputs[0]))\n    captured_infeed_queue.capture(infeed_queue)\n\n    per_host_enqueue_ops = infeed_queue.generate_enqueue_ops(\n        per_host_sharded_inputs, tpu_ordinal_function=tpu_ordinal_function_impl)\n    return per_host_enqueue_ops\n\n  return enqueue_ops_fn, captured_infeed_queue\n\n\ndef generate_per_host_enqueue_ops_fn_for_host(ctx, input_fn,\n                                              inputs_structure_recorder,\n                                              batch_axis, device, host_id):\n  \"\"\"Generates infeed enqueue ops for per-host input_fn on a single host.\"\"\"\n  captured_infeed_queue = _CapturedObject()\n\n  dataset_initializer = None\n\n  with tf.compat.v1.device(device):\n    user_context = tpu_context.TPUContext(\n        internal_ctx=ctx,\n        input_device=device,\n        invocation_index=host_id,\n        host_id=host_id)\n    inputs = _Inputs.from_input_fn(input_fn(user_context))\n\n    is_dataset = inputs.is_dataset\n    if ctx.mode == model_fn_lib.ModeKeys.PREDICT:\n      if not is_dataset:\n        raise TypeError(\n            'For mode PREDICT, `input_fn` must return `Dataset` instead of '\n            '`features` and `labels`.')\n      if batch_axis is not None:\n        raise TypeError('For mode PREDICT, batch_axis is not supported yet.')\n      inputs = _InputsWithStoppingSignals(\n          dataset=inputs.dataset,\n          batch_size=ctx.batch_size_for_input_fn,\n          add_padding=True)\n\n    if is_dataset:\n      dataset_initializer = inputs.dataset_initializer()\n\n    tpu_ordinal_function_impl = ctx.tpu_ordinal_function(host_id)\n\n  def enqueue_ops_fn():\n    \"\"\"A Fn returning the TPU infeed enqueue ops.\n\n    By providing as a Fn, it can be invoked inside the tf.while_loop such that\n    the input pipeline for multiple iterations can be executed by one\n    Session.run call.\n\n    Returns:\n      list of dict of ops.\n    \"\"\"\n    with tf.compat.v1.device(device):\n      num_of_replicas_per_host = ctx.num_of_replicas_per_host\n      # Convert user input to features and labels.  If the user returns a\n      # dataset, it is initialized and the features and labels extracted via\n      # `dataset.iterator.get_next()`\n      features, labels = inputs.features_and_labels()\n      signals = inputs.signals()\n\n      features, labels, enqueue_datas_list = (\n          _tpu_estimator_embedding.split_inputs(\n              ctx,\n              features,\n              labels,\n              num_cores_per_batch=num_of_replicas_per_host))\n\n      inputs_structure_recorder.validate_and_record_structure(features, labels)\n      unsharded_tensor_list = (\n          inputs_structure_recorder.flatten_features_and_labels(\n              features, labels, signals))\n\n      infeed_queue = tpu_feed.InfeedQueue(\n          tuple_types=[t.dtype for t in unsharded_tensor_list],\n          tuple_shapes=[t.shape for t in unsharded_tensor_list],\n          shard_dimensions=batch_axis)\n      captured_infeed_queue.capture(infeed_queue)\n      infeed_queue.set_number_of_shards(num_of_replicas_per_host)\n      per_host_enqueue_ops = (\n          infeed_queue.split_inputs_and_generate_enqueue_ops(\n              unsharded_tensor_list,\n              placement_function=lambda x: device,\n              tpu_ordinal_function=tpu_ordinal_function_impl))\n\n      if ctx.embedding_config:\n        per_host_enqueue_ops.extend(\n            ctx.embedding_config.tpu_embedding.generate_enqueue_ops(\n                enqueue_datas_list))\n\n      if signals is None:\n        return per_host_enqueue_ops\n      else:\n        return {\n            'ops': per_host_enqueue_ops,\n            'signals': signals,\n        }\n\n  return enqueue_ops_fn, captured_infeed_queue, dataset_initializer\n\n\ndef generate_per_host_v2_enqueue_ops_fn_for_host(ctx, input_fn,\n                                                 inputs_structure_recorder,\n                                                 device, host_id,\n                                                 invocation_index):\n  \"\"\"Generates infeed enqueue ops for per-host input_fn on a single host.\"\"\"\n  captured_infeed_queue = _CapturedObject()\n  dataset_initializer = None\n\n  with tf.compat.v1.device(device):\n    user_context = tpu_context.TPUContext(\n        internal_ctx=ctx,\n        input_device=device,\n        invocation_index=invocation_index,\n        host_id=host_id)\n    inputs = _Inputs.from_input_fn(input_fn(user_context))\n\n    is_dataset = inputs.is_dataset\n    if not is_dataset:\n      raise TypeError('`input_fn` must return a `Dataset` for the PER_HOST_V2 '\n                      'input pipeline configuration.')\n\n    # Be aware that when num_cores_per_replica > num_cores_per_host,\n    # ctx.num_of_replicas_per_host is 0.\n    if ctx.mode == model_fn_lib.ModeKeys.PREDICT:\n      inputs = _InputsWithStoppingSignals(\n          dataset=inputs.dataset,\n          batch_size=ctx.batch_size_for_input_fn,\n          add_padding=True,\n          num_invocations_per_step=max(1, ctx.num_of_replicas_per_host))\n\n    dataset_initializer = inputs.dataset_initializer()\n\n    tpu_ordinal_function_impl = ctx.tpu_ordinal_function(host_id)\n\n    def device_function_impl(shard_id):\n      if ctx.device_assignment is not None:\n        # Find the replica_id of the host's logical core 0.\n        # The current host_id is guaranteed to contain the logical core 0,\n        # even when num_cores_per_replica > num_cores_per_host -- the function\n        # caller makes sure that this host_id will must be receiving data (calls\n        # input_fn).\n        replica_id = ctx.device_assignment.lookup_replicas(\n            task_id=host_id, logical_core=0)[shard_id]\n        return ctx.tpu_host_placement_function(replica_id=replica_id)\n      else:\n        return None\n\n  def enqueue_ops_fn():\n    \"\"\"Generates the per_host enqueue ops.\"\"\"\n    control_deps = []\n    per_host_sharded_inputs = []\n    enqueue_datas_list = []\n    # Be aware that when num_cores_per_replica > num_cores_per_host,\n    # ctx.num_of_replicas_per_host is 0.\n    num_replicas_per_host = max(1, ctx.num_of_replicas_per_host)\n    cached_signals = None\n    with tf.compat.v1.device(device):\n      if not inputs.is_dataset:\n        raise TypeError('`input_fn` must return a `Dataset` for this mode.')\n      for host in range(num_replicas_per_host):\n        # Use control dependencies to ensure a deterministic ordering.\n        if ctx.allow_per_host_v2_parallel_get_next:\n          features, labels = inputs.features_and_labels()  # Calls get_next()\n        with tf.control_dependencies(control_deps):\n          if not ctx.allow_per_host_v2_parallel_get_next:\n            features, labels = inputs.features_and_labels()  # Calls get_next()\n          signals = inputs.signals()\n\n          # All the replicas share the replica 0's stopping signal.\n          # This avoids inconsistent state among different model replcias.\n          if cached_signals:\n            signals['stopping'] = cached_signals['stopping']\n          else:\n            cached_signals = signals\n\n        features, labels, enqueue_data = (\n            _tpu_estimator_embedding.split_inputs(ctx, features, labels))\n        if len(enqueue_data) != 1:\n          raise RuntimeError(('Missing or extra enqueue_data for host {}. '\n                              'len(enqueue_data) = {}.').format(\n                                 host, len(enqueue_data)))\n        enqueue_datas_list.append(enqueue_data[0])\n\n        inputs_structure_recorder.validate_and_record_structure(\n            features, labels)\n        flattened_inputs = (\n            inputs_structure_recorder.flatten_features_and_labels(\n                features, labels, signals))\n        control_deps.extend(flattened_inputs)\n        per_host_sharded_inputs.append(flattened_inputs)\n\n      if inputs_structure_recorder.flattened_input_dims:\n        input_partition_dims = inputs_structure_recorder.flattened_input_dims\n        if signals:\n          input_partition_dims += [None] * len(signals)\n        # pylint: disable=protected-access\n        infeed_queue = tpu_feed._PartitionedInfeedQueue(\n            number_of_tuple_elements=len(per_host_sharded_inputs[0]),\n            host_id=host_id,\n            input_partition_dims=input_partition_dims,\n            device_assignment=ctx.device_assignment)\n        per_host_enqueue_ops = infeed_queue.generate_enqueue_ops(\n            per_host_sharded_inputs)\n      else:\n        infeed_queue = tpu_feed.InfeedQueue(\n            number_of_tuple_elements=len(per_host_sharded_inputs[0]))\n        per_host_enqueue_ops = infeed_queue.generate_enqueue_ops(\n            per_host_sharded_inputs,\n            tpu_ordinal_function=tpu_ordinal_function_impl,\n            placement_function=device_function_impl)\n\n      captured_infeed_queue.capture(infeed_queue)\n\n    if ctx.embedding_config:\n      per_host_enqueue_ops.extend(\n          ctx.embedding_config.tpu_embedding.generate_enqueue_ops(\n              enqueue_datas_list))\n\n    if signals is None:\n      return per_host_enqueue_ops\n    else:\n      return {\n          'ops': per_host_enqueue_ops,\n          'signals': signals,\n      }\n\n  return enqueue_ops_fn, captured_infeed_queue, dataset_initializer\n\n\ndef generate_broadcast_enqueue_ops_fn(ctx, input_fn, inputs_structure_recorder,\n                                      num_hosts):\n  \"\"\"Generates infeed enqueue ops for one input_fn on all the hosts.\"\"\"\n  captured_infeed_queue = _CapturedObject()\n  dataset_initializer = None\n  device_0 = ctx.tpu_host_placement_function(host_id=0)\n  with tf.compat.v1.device(device_0):\n    user_context = tpu_context.TPUContext(\n        internal_ctx=ctx, input_device=device_0, invocation_index=0, host_id=0)\n    inputs = _Inputs.from_input_fn(input_fn(user_context))\n\n    is_dataset = inputs.is_dataset\n    if ctx.mode == model_fn_lib.ModeKeys.PREDICT:\n      if not is_dataset:\n        raise TypeError(\n            'For mode PREDICT, `input_fn` must return `Dataset` instead of '\n            '`features` and `labels`.')\n\n      inputs = _InputsWithStoppingSignals(\n          dataset=inputs.dataset,\n          batch_size=ctx.batch_size_for_input_fn,\n          add_padding=True)\n\n    if is_dataset:\n      dataset_initializer = inputs.dataset_initializer()\n    num_replicas_per_host = ctx.num_of_replicas_per_host\n\n  def tpu_ordinal_function_impl(shard_id):\n    if ctx.device_assignment:\n      return ctx.device_assignment.tpu_ordinal(replica=shard_id)\n    else:\n      return shard_id % num_replicas_per_host\n\n  def device_function_impl(shard_id):\n    # shard_id ranges from 0 to num_of_replicas_per_host - 1.\n    # A shard is a replica inside a host.\n    # In broadcast mode (generate_broadcast_enqueue_ops_fn), the enqueue ops\n    # are always executed on the first host. Thus shard_id equals to replica_id.\n    return ctx.tpu_host_placement_function(replica_id=shard_id)\n\n  def enqueue_ops_fn():\n    \"\"\"Generates enqueue ops for all the hosts.\"\"\"\n    broadcasted_inputs = []\n    flattened_inputs = None  # Cache result from input_fn.\n    signals = None\n    num_replicas = ctx.num_replicas\n    core_id = 0\n    for host_id in xrange(num_hosts):\n      with tf.compat.v1.device(\n          ctx.tpu_host_placement_function(host_id=host_id)):\n        for _ in xrange(ctx.num_of_replicas_per_host):\n          # Note: input_fn is only called once at host 0 for the first replica.\n          # The features and labels returned from that invocation are\n          # broadcasted to other replicas(including the replicas on other\n          # hosts).\n          if flattened_inputs is None:\n            features, labels = inputs.features_and_labels()  # Calls get_next()\n            signals = inputs.signals()\n\n            inputs_structure_recorder.validate_and_record_structure(\n                features, labels)\n            flattened_inputs = (\n                inputs_structure_recorder.flatten_features_and_labels(\n                    features, labels, signals))\n            if (ctx.config.tpu_config.eval_training_input_configuration is\n                tpu_config.InputPipelineConfig.SLICED):\n              input_slices = [\n                  tf.split(x, num_replicas) for x in flattened_inputs\n              ]\n          if (ctx.config.tpu_config.eval_training_input_configuration is\n              tpu_config.InputPipelineConfig.SLICED):\n            # for each core, slice out the flattened_inputs for each core.\n            broadcasted_inputs.append([x[core_id] for x in input_slices])\n            core_id += 1\n          else:\n            broadcasted_inputs.append(flattened_inputs)\n\n    infeed_queue = tpu_feed.InfeedQueue(\n        number_of_tuple_elements=len(broadcasted_inputs[0]))\n    captured_infeed_queue.capture(infeed_queue)\n    enqueue_ops = infeed_queue.generate_enqueue_ops(\n        broadcasted_inputs,\n        tpu_ordinal_function=tpu_ordinal_function_impl,\n        placement_function=device_function_impl)\n\n    if signals is None:\n      return enqueue_ops\n    else:\n      return {\n          'ops': enqueue_ops,\n          'signals': signals,\n      }\n\n  return enqueue_ops_fn, captured_infeed_queue, dataset_initializer\n\n\nclass TensorPacker(object):\n  \"\"\"Pack and unpack small tensors into a big one for efficiency.\"\"\"\n\n  def __init__(self, small_feature_dim_size,\n               minimum_num_small_features_to_group):\n    self._small_feature_dim_size = small_feature_dim_size\n    self._minimum_num_small_features_to_group = (\n        minimum_num_small_features_to_group)\n\n  def maybe_concatenate_features(self, features):\n    \"\"\"If there are enough small tensors, concat them for performance.\"\"\"\n    self._small_feature_names = {}\n    self._small_feature_sizes = {}\n    feature_names = _extract_key_names(features)\n    if feature_names:  # Not a single tensor.\n      # First pass: see if it is worth concatenating the small features.\n      for name in feature_names:\n        tensor = features[name]\n        # We do not handle nested inputs here.\n        if not isinstance(tensor, tf.Tensor):\n          return\n        shape = tensor.get_shape().as_list()\n        dtype = tensor.dtype\n        if (len(shape) == 2 and shape[1] is not None and\n            shape[1] <= self._small_feature_dim_size):\n          tf.compat.v1.logging.log_first_n(\n              tf.compat.v1.logging.INFO,\n              'Found small feature: %s %s', 1, name, shape)\n          if tensor.dtype not in self._small_feature_names:\n            self._small_feature_names[dtype] = []\n            self._small_feature_sizes[dtype] = []\n          self._small_feature_names[dtype].append(name)\n          self._small_feature_sizes[dtype].append(shape[1])\n\n      dtypes_ = list(self._small_feature_names.keys())\n      for dtype in dtypes_:\n        # If we could find 5 (or more) [batch_size, 1] dense features,\n        # we will group them.\n        if (len(self._small_feature_names[dtype]) <\n            self._minimum_num_small_features_to_group):\n          self._small_feature_names.pop(dtype)  # reset\n          self._small_feature_sizes.pop(dtype)  # reset\n\n      # Second pass: separate small features out\n      small_feature_tensors = {}\n      for dtype in self._small_feature_names:\n        small_feature_tensors[dtype] = []\n        for name in self._small_feature_names[dtype]:\n          small_feature_tensors[dtype].append(features.pop(name))\n\n      # Add the concat Tensor to features with a special key.\n      for dtype in self._small_feature_names:\n        key = self._get_small_feature_key(dtype)\n        if key in features:\n          raise ValueError('{} is reserved as feature key for concatenated'\n                           'small features.')\n        features[key] = (tf.concat(small_feature_tensors[dtype], axis=1))\n\n  def maybe_split_features(self, maybe_concatenated_features):\n    for dtype in self._small_feature_names:\n      key = self._get_small_feature_key(dtype)\n      concatenated_small_features = maybe_concatenated_features.pop(key)\n      splits = tf.split(\n          concatenated_small_features, self._small_feature_sizes[dtype], axis=1)\n      for name, split in zip(self._small_feature_names[dtype], splits):\n        maybe_concatenated_features[name] = split\n\n  def _get_small_feature_key(self, dtype):\n    return _TENSOR_PACKER_CONCATENATED_SMALL_FEATURES_KEY + '_' + str(dtype)\n\n\nclass _InputPipeline(object):\n  \"\"\"`_InputPipeline` handles invoking `input_fn` and piping to infeed queue.\n\n  `_InputPipeline` abstracts the per-core/per-host `input_fn` invocation from\n  call site.  To be precise, based on the configuration in\n  `_InternalTPUContext`,  it invokes `input_fn` for all cores (usually\n  multi-host TPU training) or for one host (usually for single-host TPU\n  evaluation), and sends all `features` and `labels` returned by `input_fn` to\n  TPU infeed. For per-core invocation, `features` and `labels` are piped to\n  infeed directly, one tuple for each core. For per-host invocation,  `features`\n  and `labels` are split at host (with respect to `batch_axis`) and piped to all\n  cores accordingly.\n\n  In addition, flatten/unflatten are handled by `_InputPipeline` also.  Model\n  inputs returned by the `input_fn` can have one of the following forms:\n  1. features\n  2. (features, labels)\n  3. ((arbitrarily nested structure of features), labels)\n\n  Internally, form 1 is reformed to `(features, None)` as features and labels\n  are passed separately to underlying methods. For TPU training, TPUEstimator\n  may expect multiple `features` and `labels` tuples one for each core.\n\n  TPUEstimator allows various different structures for inputs (namely `features`\n  and `labels`).  Both `features` and `labels` can be any nested sturcture\n  supported by TF nest (namely, dict, tuples, namedtuples or any nested\n  structure of such of Tensors).  `labels` could be `None` as well.\n\n  These are flattened before they are passed to the infeed/outfeed library\n  as that expectes flattend lists.\n  \"\"\"\n\n  class InputsStructureRecorder(object):\n    \"\"\"The recorder to record inputs structure.\"\"\"\n\n    def __init__(self, input_partition_dims=None):\n      # Holds the structure of inputs\n      self._feature_structure = {}\n      self._flattened_input_dims = None\n\n      if input_partition_dims:\n        # This should have been validated in TPUConfig.\n        assert len(input_partition_dims) <= 2, 'must have 1 or 2 elements.'\n        if len(input_partition_dims) == 2:\n          self._feature_dims, self._label_dims = input_partition_dims\n        else:\n          self._feature_dims = input_partition_dims[0]\n          self._label_dims = None\n\n        assert self._feature_dims is not None, ('input_partition_dims[0] must '\n                                                'not be None')\n      else:\n        self._feature_dims = None\n        self._label_dims = None\n\n      # Internal state.\n      self._initialized = False\n\n    @property\n    def flattened_input_dims(self):\n      assert self._initialized, 'InputsStructureRecorder is not initialized.'\n      return self._flattened_input_dims\n\n    def has_labels(self):\n      return 'labels' in self._feature_structure\n\n    def _flatten_input_dims(self, features, labels, feature_dims, label_dims):\n      \"\"\"Flatten input dims with the same order as flattened input tensors.\"\"\"\n\n      try:\n        flattened_input_dims = data_nest.flatten_up_to(features, feature_dims)\n      except TypeError as e:\n        raise ValueError(\n            'TPUConfig.input_partition_dims[0] mismatched the structure of'\n            ' features. input_partition_dims[0]: {}, features {}. {}'.format(\n                feature_dims, features, e))\n\n      if labels is not None:\n        if label_dims is not None:\n          try:\n            flattened_input_dims.extend(\n                data_nest.flatten_up_to(labels, self._label_dims))\n          except TypeError as e:\n            raise ValueError(\n                'TPUConfig.input_partition_dims[1] mismatched the structure of'\n                ' labels. input_partition_dims[1]: {}, labels: {}. {}'.format(\n                    label_dims, labels, e))\n        else:\n          num_label_tensors = len(data_nest.flatten(labels))\n          flattened_input_dims.extend([None] * num_label_tensors)\n      return flattened_input_dims\n\n    def validate_and_record_structure(self, features, labels):\n      \"\"\"Validates and records the structure of `features` and `labels`.\"\"\"\n      # Extract structure.\n      feature_names = _extract_key_names(features)\n      label_names = _extract_key_names(labels)\n\n      if not self._initialized:\n        # Record structure.\n        self._initialized = True\n        if self._feature_dims is not None:\n          feature_dims_names = _extract_key_names(self._feature_dims)\n          if feature_dims_names != feature_names:\n            raise ValueError(\n                'TPUConfig.input_partition_dims[0] mismatched feature'\n                ' keys. Expected {}, got {}'.format(feature_names,\n                                                    feature_dims_names))\n          label_dims_names = _extract_key_names(self._label_dims)\n          if self._label_dims is not None and label_dims_names != label_names:\n            raise ValueError(\n                'TPUConfig.input_partition_dims[1] mismatched label'\n                ' keys. Expected {}, got {}'.format(label_names,\n                                                    label_dims_names))\n          self._flattened_input_dims = self._flatten_input_dims(\n              features, labels, self._feature_dims, self._label_dims)\n\n    def flatten_features_and_labels(self, features, labels, signals=None):\n      \"\"\"Flattens the `features` and `labels` to a single tensor list.\"\"\"\n      self.tensor_packer = TensorPacker(\n          _TENSOR_PACKER_SMALL_FEATURE_DIM_SIZE,\n          _TENSOR_PACKER_MINIMUM_NUM_SMALL_FEATURES_TO_GROUP)\n      self.tensor_packer.maybe_concatenate_features(features)\n      self._feature_structure['features'] = features\n      if labels is not None:\n        self._feature_structure['labels'] = labels\n      if signals is not None:\n        self._feature_structure['signals'] = signals\n      return data_nest.flatten(self._feature_structure)\n\n    def unflatten_features_and_labels(self, flattened_inputs):\n      \"\"\"Restores the flattened inputs to original features and labels form.\n\n      Args:\n        flattened_inputs: Flattened inputs for each shard.\n\n      Returns:\n        A tuple of (`features`, `labels`), where `labels` could be None.\n        Each one, if present, should have identical structure (single tensor vs\n        dict) as the one returned by input_fn.\n\n      Raises:\n        ValueError: If the number of expected tensors from `flattened_inputs`\n          mismatches the recorded structure.\n      \"\"\"\n\n      unflattened_inputs = data_nest.pack_sequence_as(self._feature_structure,\n                                                      flattened_inputs)\n      features = unflattened_inputs['features']\n      self.tensor_packer.maybe_split_features(features)\n      return _Inputs(\n          features,\n          unflattened_inputs.get('labels'),\n          signals=unflattened_inputs.get('signals'))\n\n  def __init__(self, input_fn, batch_axis, ctx):\n    \"\"\"Constructor.\n\n    Args:\n      input_fn: input fn for train or eval.\n      batch_axis: A python tuple of int values describing how each tensor\n        produced by the Estimator `input_fn` should be split across the TPU\n        compute shards.\n      ctx: A `_InternalTPUContext` instance with mode.\n\n    Raises:\n      ValueError: If both `sharded_features` and `num_cores` are `None`.\n    \"\"\"\n    self._inputs_structure_recorder = _InputPipeline.InputsStructureRecorder(\n        ctx.input_partition_dims)\n\n    self._sharded_per_core = ctx.is_input_sharded_per_core()\n    self._input_fn = input_fn\n    self._infeed_queue = None\n    self._ctx = ctx\n    self._batch_axis = batch_axis\n\n  def generate_infeed_enqueue_ops_and_dequeue_fn(self):\n    \"\"\"Generates infeed enqueue ops and dequeue_fn.\"\"\"\n    # While tf.while_loop is called, the body function, which invokes\n    # `enqueue_fn` passed in, is called to construct the graph. So, input_fn\n    # structure is recorded.\n    enqueue_ops, all_hooks, run_infeed_loop_on_coordinator = (\n        self._invoke_input_fn_and_record_structure())\n\n    self._validate_input_pipeline()\n\n    def dequeue_fn():\n      \"\"\"dequeue_fn is used by TPU to retrieve the tensors.\"\"\"\n      # In the model-parallel case, both the host-side and device-side\n      # computations must agree on the core on which infeed takes place. We\n      # choose to perform infeed on logical core 0 of each replica.\n      values = self._infeed_queue.generate_dequeue_op(tpu_device=0)\n      # The unflatten process uses the structure information recorded above.\n      return self._inputs_structure_recorder.unflatten_features_and_labels(\n          values)\n\n    return (enqueue_ops, dequeue_fn, all_hooks, run_infeed_loop_on_coordinator)\n\n  def _invoke_input_fn_and_record_structure(self):\n    \"\"\"Deploys the input pipeline and record input structure.\"\"\"\n    enqueue_ops = []\n    infeed_queues = []\n    all_dataset_initializers = []\n    num_hosts = self._ctx.num_hosts\n    tpu_host_placement_fn = self._ctx.tpu_host_placement_function\n\n    run_infeed_loop_on_coordinator = True\n\n    if self._sharded_per_core:\n      # Per-Core input pipeline deployment.\n      # Invoke input pipeline for each core and placed on the corresponding\n      # host.\n      for host_id in range(num_hosts):\n        host_device = tpu_host_placement_fn(host_id=host_id)\n        with tf.compat.v1.device(host_device):\n          with ops.name_scope('input_pipeline_task%d' % (host_id)):\n            enqueue_ops_fn, captured_infeed_queue = (\n                generate_per_core_enqueue_ops_fn_for_host(\n                    self._ctx, self._input_fn, self._inputs_structure_recorder,\n                    host_device, host_id))\n\n            if _WRAP_INPUT_FN_INTO_WHILE_LOOP:\n              run_infeed_loop_on_coordinator = False\n              enqueue_ops.append(\n                  _wrap_computation_in_while_loop(\n                      device=host_device, op_fn=enqueue_ops_fn))\n            else:\n              enqueue_ops.append(enqueue_ops_fn())\n            # Infeed_queue_getter must be called after enqueue_ops_fn is called.\n            infeed_queues.append(captured_infeed_queue.get())\n\n    elif self._ctx.is_input_broadcast_with_iterators():\n      # Only calls input_fn in host 0.\n      host_device = tpu_host_placement_fn(host_id=0)\n      enqueue_ops_fn, captured_infeed_queue, dataset_initializer = (\n          generate_broadcast_enqueue_ops_fn(self._ctx, self._input_fn,\n                                            self._inputs_structure_recorder,\n                                            num_hosts))\n      if dataset_initializer:\n        all_dataset_initializers.append(dataset_initializer)\n        run_infeed_loop_on_coordinator = False\n        wrap_fn = (\n            _wrap_computation_in_while_loop\n            if self._ctx.mode != model_fn_lib.ModeKeys.PREDICT else\n            _wrap_computation_in_while_loop_with_stopping_signals)\n        enqueue_ops.append(wrap_fn(device=host_device, op_fn=enqueue_ops_fn))\n      else:\n        enqueue_ops.append(enqueue_ops_fn())\n      infeed_queues.append(captured_infeed_queue.get())\n\n    else:\n      # This branch handles two senarios:\n      #       num_cores_per_replica > num_cores_per_host\n      #   and num_cores_per_replica <= num_cores_per_host\n      # First, get the set of host_ids, by iterating replicas.\n      # We only want and will get the set of *unique* host_ids\n      # *that will call input_fn*. For each replica, we only call the input_fn\n      # from the CPU host that contains logical core 0.\n\n      # Use a list here to ensure deterministic order.\n      host_id_with_invocation_id_pair = []\n\n      if not self._ctx.is_replica_across_hosts():\n        for host_id in range(num_hosts):\n          invocation_index = host_id\n          host_id_with_invocation_id_pair.append((host_id, invocation_index))\n      else:\n        for replica_id in xrange(self._ctx.num_replicas):\n          invocation_index = replica_id\n          host_device, _ = self._ctx.device_for_replica(replica_id)\n          # TODO(lehou): Get host_id in a better way.\n          host_id = int(host_device.split('/task:')[1].split('/device:')[0])\n          host_id_with_invocation_id_pair.append((host_id, invocation_index))\n\n      for (host_id, invocation_index) in host_id_with_invocation_id_pair:\n        host_device = tpu_host_placement_fn(host_id=host_id)\n        with tf.compat.v1.device(host_device):\n          with ops.name_scope('input_pipeline_task%d' % (host_id)):\n            if self._ctx.is_input_per_host_with_iterators():\n              enqueue_ops_fn, captured_infeed_queue, dataset_initializer = (\n                  generate_per_host_v2_enqueue_ops_fn_for_host(\n                      self._ctx, self._input_fn,\n                      self._inputs_structure_recorder, host_device, host_id,\n                      invocation_index))\n            else:\n              enqueue_ops_fn, captured_infeed_queue, dataset_initializer = (\n                  generate_per_host_enqueue_ops_fn_for_host(\n                      self._ctx, self._input_fn,\n                      self._inputs_structure_recorder, self._batch_axis,\n                      host_device, host_id))\n\n            # NOTE(xiejw): We dispatch here based on the return type of the\n            # users `input_fn`.\n            #\n            # 1. If input_fn returns a Dataset instance, we initialize the\n            # iterator outside of tf.while_loop, and call the iterator.get_next\n            # inside tf.while_loop.  This should be always safe.\n            #\n            # 2. If input_fn returns (features, labels), it is too late to wrap\n            # them inside tf.while_loop, as resource initialization cannot be\n            # handled in TF control flow properly. In this case, we will use\n            # python loop to enqueue the data into TPU system.  This may be\n            # slow compared to the previous case.\n            if dataset_initializer:\n              all_dataset_initializers.append(dataset_initializer)\n              run_infeed_loop_on_coordinator = False\n              wrap_fn = (\n                  _wrap_computation_in_while_loop\n                  if self._ctx.mode != model_fn_lib.ModeKeys.PREDICT else\n                  _wrap_computation_in_while_loop_with_stopping_signals)\n              enqueue_ops.append(\n                  wrap_fn(device=host_device, op_fn=enqueue_ops_fn))\n            else:\n              enqueue_ops.append(enqueue_ops_fn())\n            infeed_queues.append(captured_infeed_queue.get())\n\n    # infeed_queue is used to generate dequeue ops. The only thing it uses for\n    # dequeue is dtypes and types. So, any one can be used. Here, grab the\n    # first one.\n    self._infeed_queue = infeed_queues[0]\n    return enqueue_ops, [\n        util_lib.MultiHostDatasetInitializerHook(all_dataset_initializers)\n    ], run_infeed_loop_on_coordinator\n\n  def _validate_input_pipeline(self):\n    \"\"\"Validates the input pipeline.\n\n    Perform some sanity checks to log user friendly information. We should\n    error out to give users better error message. But, if\n    _WRAP_INPUT_FN_INTO_WHILE_LOOP is False (legacy behavior), we cannot break\n    user code, so, log a warning.\n\n    Raises:\n      RuntimeError: If the validation failed.\n    \"\"\"\n    if tf.compat.v1.get_default_graph().get_collection(\n        tf.compat.v1.GraphKeys.QUEUE_RUNNERS):\n      err_msg = ('Input pipeline contains one or more QueueRunners. '\n                 'It could be slow and not scalable. Please consider '\n                 'converting your input pipeline to use `tf.data` instead (see '\n                 'https://www.tensorflow.org/guide/datasets for '\n                 'instructions.')\n      if _WRAP_INPUT_FN_INTO_WHILE_LOOP:\n        raise RuntimeError(err_msg)\n      else:\n        logging.warn(err_msg)\n\n\ndef call_computation(computation_inputs, computation, batch_config=None):\n  \"\"\"Call computation.\n\n  Args:\n    computation_inputs: A tensor or dict of tensors, the inputs to the\n      computation.\n    computation: A Python function that takes no inputs and builds computation\n      graph. If `computation` returns m outputs, this function will return a\n      list of m Tensors.\n    batch_config: A BatchConfig named tuple specifying the batching\n      configuration to use for inference batching.\n\n  Returns:\n    A list of output tensors.\n  \"\"\"\n\n  # Using `TPUPartitionedCall` makes it possible to target a different\n  # TPU core with every `Session.run()` call. Note that the entire inference\n  # graph executes on a single core, and that invocations of this graph\n  # will round-robin among the cores attached to a host.\n  def tpu_partitioned_call(partition_inputs):\n\n    # capture_resource_var_by_value enables variables to be mirrored on TPU\n    # to avoid fetching from CPU, since variables do not change during\n    # inference.\n    @function.Defun(capture_resource_var_by_value=False)\n    def tpu_subgraph():\n      return computation(partition_inputs)\n\n    return tpu_functional.TPUPartitionedCall(\n        args=tpu_subgraph.captured_inputs,\n        device_ordinal=tpu_ops.tpu_ordinal_selector(),\n        Tout=[o.type for o in tpu_subgraph.definition.signature.output_arg],\n        f=tpu_subgraph)\n\n  # Not using Batching Function but use TPUPartitionedCall/all cores.\n  if not batch_config:\n    return tpu_partitioned_call(computation_inputs)\n\n  # Use Batching Function and TPUPartitionedCall/all cores.\n  # Note that BatchingFunction requires a list of tensors and doesn't support\n  # a dict of tensors. So we preserve the structure by deterministically\n  # flattening the dict before batching and then recomposing it after batching\n  # to feed into the computation.\n  ordered_inputs_list = tf.nest.flatten(computation_inputs)\n\n  @tf.nondifferentiable_batch_function(\n      num_batch_threads=batch_config.num_batch_threads,\n      max_batch_size=batch_config.max_batch_size,\n      batch_timeout_micros=batch_config.batch_timeout_micros,\n      allowed_batch_sizes=batch_config.allowed_batch_sizes,\n      max_enqueued_batches=batch_config.max_enqueued_batches,\n      autograph=False)\n  def batched_tpu_computation(*tensor_args):\n    \"\"\"Recompose the input feature dict and calls the TPU computation.\"\"\"\n    computation_feature_input = tf.nest.pack_sequence_as(\n        computation_inputs, tensor_args)\n    return tpu_partitioned_call(computation_feature_input)\n\n  return batched_tpu_computation(*ordered_inputs_list)\n\n\nclass _ModelFnWrapper(object):\n  \"\"\"A `model_fn` wrapper.\n\n  This makes calling model_fn on CPU and TPU easier and more consistent and\n  performs necessary check and mutation required by TPU training and evaluation.\n\n  In addition, this wrapper manages converting the `model_fn` to a single TPU\n  train and eval step.\n  \"\"\"\n\n  def __init__(self, model_fn, config, params, ctx):\n    self._model_fn = model_fn\n    self._config = config\n    self._params = params\n    self._ctx = ctx\n\n  def call_without_tpu(self, features, labels, is_export_mode):\n    return self._call_model_fn(features, labels, is_export_mode=is_export_mode)\n\n  def _add_embedding_features(self, features, hook_dummy_table_variables):\n    \"\"\"Add embedding features, optionally add hook to intercept gradient.\"\"\"\n    if self._ctx.embedding_config:\n      tpu_embedding_ = self._ctx.embedding_config.tpu_embedding\n      embedding_activations = tpu_embedding_.get_activations()\n      if hook_dummy_table_variables:\n        new_embedding_activations = (\n            tpu_embedding_gradient.hook_dummy_table_variables_to_activations(\n                tpu_embedding_, embedding_activations,\n                self._ctx.embedding_config.dummy_table_variables))\n        features.update(new_embedding_activations)\n      else:\n        features.update(embedding_activations)\n\n  def convert_to_single_tpu_train_step(self, dequeue_fn):\n    \"\"\"Converts user provided model_fn` as a single train step on TPU.\n\n    The user provided `model_fn` takes input tuple\n    (features, labels) and produces the EstimatorSpec with train_op and loss for\n    train `mode`. This usually represents a single train computation on CPU.\n\n    For TPU training, a train (computation) step is first wrapped in a\n    tf.while_loop control flow to repeat for many times and then replicated to\n    all TPU shards. Besides the input should be taken from TPU infeed rather\n    than input pipeline (input_fn) directly. To fit TPU loop and replicate\n    pattern, the original train computation should be reformed, which is the\n    returned `train_step`.\n\n    Args:\n      dequeue_fn: The function to retrieve inputs, features and labels, from TPU\n        infeed dequeue channel.\n\n    Returns:\n      A tuple of train_fn, host_calls, and captured scaffold_fn. The train_fn\n      representing the train step for TPU.\n    \"\"\"\n\n    host_call = _OutfeedHostCall(\n        self._ctx,\n        outfeed_every_n_steps=self._config.tpu_config\n        .experimental_host_call_every_n_steps)\n    captured_scaffold_fn = _CapturedObject()\n    captured_training_hooks = _CapturedObject()\n\n    def train_step(step):\n      \"\"\"Training step function for use inside a while loop.\"\"\"\n      inputs = dequeue_fn()\n      features, labels = inputs.features_and_labels()\n      self._add_embedding_features(features, True)\n\n      estimator_spec = self._verify_estimator_spec(\n          self._call_model_fn(features, labels))\n      loss, train_op = estimator_spec.loss, estimator_spec.train_op\n\n      if tensor_tracer.TensorTracer.is_enabled():\n        tt = tensor_tracer.TensorTracer()\n        loss = tt.trace_tpu(tf.compat.v1.get_default_graph(), loss, train_op,\n                            self._ctx.num_replicas)\n        tracer_host_call = tt.host_call_deps_and_fn()\n      else:\n        tracer_host_call = {}\n\n      if isinstance(estimator_spec, model_fn_lib._TPUEstimatorSpec):  # pylint: disable=protected-access\n        captured_scaffold_fn.capture(estimator_spec.scaffold_fn)\n      else:\n        captured_scaffold_fn.capture(None)\n\n      captured_training_hooks.capture(estimator_spec.training_hooks)\n\n      if self._ctx.embedding_config is None:\n        apply_sparse_grads = []\n      else:\n        tpu_embedding_ = self._ctx.embedding_config.tpu_embedding\n        gradients = (\n            tpu_embedding_gradient.get_gradients_through_dummy_table_variables(\n                tpu_embedding_))\n        grad_multiplier = self._ctx.embedding_config.get_grad_multiplier()\n        if grad_multiplier is not None:\n          scaled_gradients = collections.OrderedDict(\n              (k, v * grad_multiplier) for k, v in six.iteritems(gradients))\n        else:\n          scaled_gradients = gradients\n        apply_sparse_grads = [\n            tpu_embedding_.generate_send_gradients_op(\n                scaled_gradients, tf.compat.v1.train.get_global_step())\n        ]\n\n      stopping_signals = None\n      user_provided_stopping_signals_name = None\n      if self._ctx.feed_hook is not None:\n        stopping_signals, user_provided_stopping_signals_name = \\\n          self._ctx.feed_hook.get_stopping_signals_and_name(features)\n\n      # We must run train_op to update the variables prior to running the\n      # outfeed.\n      with tf.control_dependencies([train_op] + apply_sparse_grads):\n        host_call_outfeed_ops = []\n        host_call_fn, host_call_args = None, []\n\n        if (isinstance(estimator_spec, model_fn_lib._TPUEstimatorSpec)  # pylint: disable=protected-access\n            and estimator_spec.host_call is not None):\n          host_call_fn, host_call_args = estimator_spec.host_call\n\n        if stopping_signals is not None:\n          identity_fn = lambda **kwargs: kwargs\n          tracer_host_call[user_provided_stopping_signals_name] = [\n              identity_fn, stopping_signals\n          ]\n\n        if host_call_fn:\n          # Ignore dummy hostcalls (no arguments)\n          if host_call_args:\n            tracer_host_call.update({'host_call': estimator_spec.host_call})\n            host_call.record(tracer_host_call)\n            host_call_outfeed_ops = host_call.create_enqueue_op(step)\n          elif tracer_host_call:\n            host_call.record(tracer_host_call)\n            host_call_outfeed_ops = host_call.create_enqueue_op(step)\n        else:\n          # Create a host call for the loss to track execution progress\n          # Without this, we don't have any indication of the state of the\n          # TPU program.\n          tracer_host_call.update(\n              {'host_call': (lambda loss_t: loss_t, [tf.reshape(loss, [1])])})\n          host_call.record(tracer_host_call)\n          host_call_outfeed_ops = host_call.create_enqueue_op(step)\n\n        with tf.control_dependencies(host_call_outfeed_ops):\n          return tf.identity(loss)\n\n    return (train_step, host_call, captured_scaffold_fn,\n            captured_training_hooks)\n\n  def convert_to_single_tpu_eval_step(self, dequeue_fn):\n    \"\"\"Converts user provided model_fn` as a single eval step on TPU.\n\n    Similar to training, the user provided `model_fn` takes input tuple\n    (features, labels) and produces the TPUEstimatorSpec with eval_metrics for\n    eval `mode`. This usually represents a single evaluation computation on CPU.\n\n    For TPU evaluation, a eval (computation) step is first wrapped in a\n    tf.while_loop control flow to repeat for many times and then replicated to\n    all TPU shards. Besides the input and output are slightly different. Input,\n    features and labels, should be taken from TPU infeed rather than input\n    pipeline (input_fn) directly. Output is managed in two stages.  First, the\n    model outputs as the result of evaluation computation, usually model logits,\n    should be transferred from TPU system to CPU. Then, all model outputs are\n    concatenated first on CPU and sent to the metric_fn for metrics computation.\n    To fit TPU evaluation pattern, the original eval computation should be\n    reformed, which is the returned `eval_step`.\n\n    Args:\n      dequeue_fn: The function to retrieve inputs, features and labels, from TPU\n        infeed dequeue channel.\n\n    Returns:\n      A tuple of eval_fn, host_calls, and captured scaffold_fn. The eval_fn\n      representing the eval step for TPU.\n    \"\"\"\n    host_calls = _OutfeedHostCall(self._ctx)\n    captured_scaffold_fn = _CapturedObject()\n    captured_eval_hooks = _CapturedObject()\n\n    def eval_step(total_loss):\n      \"\"\"Evaluation step function for use inside a while loop.\"\"\"\n      inputs = dequeue_fn()\n      features, labels = inputs.features_and_labels()\n      self._add_embedding_features(features, False)\n\n      tpu_estimator_spec = self._call_model_fn(features, labels)\n      if not isinstance(tpu_estimator_spec, model_fn_lib._TPUEstimatorSpec):  # pylint: disable=protected-access\n        raise RuntimeError(\n            'estimator_spec used by TPU evaluation must have type'\n            '`TPUEstimatorSpec`. Got {}'.format(type(tpu_estimator_spec)))\n\n      loss = tpu_estimator_spec.loss\n      captured_scaffold_fn.capture(tpu_estimator_spec.scaffold_fn)\n      captured_eval_hooks.capture(tpu_estimator_spec.evaluation_hooks)\n\n      to_record = {}\n      if tpu_estimator_spec.eval_metrics:\n        to_record['eval_metrics'] = tpu_estimator_spec.eval_metrics\n      if tpu_estimator_spec.host_call is not None:\n        # We assume that evaluate won't update global step, so we don't wrap\n        # this host_call.\n        to_record['host_call'] = tpu_estimator_spec.host_call\n      host_calls.record(to_record)\n\n      with tf.control_dependencies(host_calls.create_enqueue_op()):\n        return tf.math.add(total_loss, loss)\n\n    return eval_step, host_calls, captured_scaffold_fn, captured_eval_hooks\n\n  def convert_to_single_tpu_predict_step(self, dequeue_fn):\n    \"\"\"Converts user provided model_fn` as a single predict step on TPU.\n\n    Args:\n      dequeue_fn: The function to retrieve inputs, features and labels, from TPU\n        infeed dequeue channel.\n\n    Returns:\n      A tuple of predict_fn, host_calls, and captured scaffold_fn. The\n      predict_fn representing the predict step for TPU.\n    \"\"\"\n    host_calls = _OutfeedHostCall(self._ctx)\n    captured_scaffold_fn = _CapturedObject()\n    captured_predict_hooks = _CapturedObject()\n\n    def predict_step(unused_scalar_stopping_signal):\n      \"\"\"Evaluation step function for use inside a while loop.\"\"\"\n      inputs = dequeue_fn()\n      features, labels = inputs.features_and_labels()\n      stopping_signals = inputs.signals()\n\n      assert stopping_signals is not None, (\n          'Internal Error: `signals` is missing.')\n\n      tpu_estimator_spec = self._call_model_fn(\n          features, labels, is_export_mode=False)\n      if not isinstance(tpu_estimator_spec, model_fn_lib._TPUEstimatorSpec):  # pylint: disable=protected-access\n        raise RuntimeError(\n            'estimator_spec used by TPU prediction must have type'\n            '`TPUEstimatorSpec`. Got {}'.format(type(tpu_estimator_spec)))\n\n      self._verify_tpu_spec_predictions(tpu_estimator_spec.predictions)\n\n      captured_scaffold_fn.capture(tpu_estimator_spec.scaffold_fn)\n      captured_predict_hooks.capture(tpu_estimator_spec.prediction_hooks)\n      to_record = {}\n      identity_fn = lambda **kwargs: kwargs\n      to_record['predictions'] = [identity_fn, tpu_estimator_spec.predictions]\n      to_record['signals'] = [identity_fn, stopping_signals]\n      if tpu_estimator_spec.host_call is not None:\n        to_record['host_call'] = tpu_estimator_spec.host_call\n      host_calls.record(to_record)\n\n      with tf.control_dependencies(host_calls.create_enqueue_op()):\n        return _StopSignals.as_scalar_stopping_signal(stopping_signals)\n\n    return (predict_step, host_calls, captured_scaffold_fn,\n            captured_predict_hooks)\n\n  def _verify_tpu_spec_predictions(self, predictions):\n    \"\"\"Validates TPUEstimatorSpec.predictions dict.\"\"\"\n    # TODO(xiejw): Adds validation for prediction dictionrary.\n    # TODO(xiejw): Adds support for single tensor as predictions.\n    if not isinstance(predictions, dict):\n      raise TypeError('TPUEstimatorSpec.predictions must be dict of Tensors.')\n\n    for (key, tensor) in predictions.items():\n      if tensor.shape.dims[0].value is None:\n        raise ValueError(\n            'The tensor with key ({}) in TPUEstimatorSpec.predictions has '\n            'dynamic shape (should be static). Tensor: {}'.format(key, tensor))\n    return predictions\n\n  def _validate_model_features_and_labels(self, features, labels,\n                                          is_export_mode):\n    \"\"\"Validates that the features and labels for the model function are valid.\n\n    A valid features/labels object is the one with:\n    - Type: A tensor or any nested structure of tensors supported by TF nest,\n        namely nested dictionary, tuple, namedtuple, or sequence of tensors.\n    - Static shape if is_export_mode is False.\n\n    Args:\n      features: the features that would be input to the model function.\n      labels: the labels that would be input to the model function.\n      is_export_mode: boolean value specifying if in export mode.\n\n    Raises:\n      TypeError: If features/labels are not of the correct type.\n      ValueError: If features/labels have dynamic shape.\n    \"\"\"\n\n    def validate(obj, obj_name):\n      \"\"\"Helper validate function.\"\"\"\n      if is_export_mode or self._ctx.is_running_on_cpu(is_export_mode):\n        return\n      if isinstance(obj, tf.Tensor):\n        if not obj.get_shape().is_fully_defined():\n          raise ValueError(\n              'The {} to the model returned by input_fn must have static shape.'\n              ' Tensor: {}'.format(obj_name, obj))\n      else:\n        for tensor in data_nest.flatten(obj):\n          if not tensor.get_shape().is_fully_defined():\n            raise ValueError(\n                ('The {} to the model returned by input_fn must have static '\n                 'shape. Tensor: {}').format(obj_name, tensor))\n\n    validate(features, 'features')\n    if labels is not None:\n      validate(labels, 'labels')\n\n  def _call_model_fn(self, features, labels, is_export_mode=False):\n    \"\"\"Calls the model_fn with required parameters.\"\"\"\n    self._validate_model_features_and_labels(features, labels, is_export_mode)\n    model_fn_args = function_utils.fn_args(self._model_fn)\n    kwargs = {}\n\n    # Makes deep copy with `config` and params` in case user mutates them.\n    config = copy.deepcopy(self._config)\n    params = copy.deepcopy(self._params)\n\n    if 'labels' in model_fn_args:\n      kwargs['labels'] = labels\n    elif labels is not None:\n      raise ValueError(\n          'model_fn does not take labels, but input_fn returns labels.')\n    if 'mode' in model_fn_args:\n      kwargs['mode'] = self._ctx.mode\n    if 'config' in model_fn_args:\n      kwargs['config'] = config\n    if 'params' in model_fn_args:\n      kwargs['params'] = params\n\n    if 'params' not in model_fn_args:\n      raise ValueError('model_fn ({}) does not include params argument, '\n                       'required by TPUEstimator to pass batch size as '\n                       'params[\\'batch_size\\']'.format(self._model_fn))\n\n    if is_export_mode:\n      batch_size_for_model_fn = None\n    else:\n      batch_size_for_model_fn = self._ctx.batch_size_for_model_fn\n\n    if batch_size_for_model_fn is not None:\n      _add_item_to_params(params, _BATCH_SIZE_KEY, batch_size_for_model_fn)\n\n    running_on_cpu = self._ctx.is_running_on_cpu(is_export_mode)\n    # In export mode, params['use_tpu'] has already been set based on mode\n    # (i.e. True for _REWRITE_FOR_INFERENCE_MODE, False otherwise).\n    if not is_export_mode:\n      _add_item_to_params(params, _USE_TPU_KEY, not running_on_cpu)\n\n    if not running_on_cpu:\n      user_context = tpu_context.TPUContext(\n          internal_ctx=self._ctx, call_from_input_fn=False)\n      _add_item_to_params(params, _CTX_KEY, user_context)\n\n    estimator_spec = self._model_fn(features=features, **kwargs)\n    if (running_on_cpu and\n        isinstance(estimator_spec, model_fn_lib._TPUEstimatorSpec)):  # pylint: disable=protected-access\n      # The estimator_spec will be passed to `Estimator` directly, which expects\n      # type `EstimatorSpec`. As we are running on the CPU, escape\n      # the TPUInferenceContext.\n      graph_context = tf.compat.v1.get_default_graph(\n      )._get_control_flow_context()\n      try:\n        if isinstance(graph_context, tpu._TPUInferenceContext):\n          tf.compat.v1.get_default_graph()._set_control_flow_context(\n              graph_context.outer_context)\n        return estimator_spec.as_estimator_spec()\n      finally:\n        tf.compat.v1.get_default_graph()._set_control_flow_context(\n            graph_context)\n    else:\n      return estimator_spec\n\n  def _verify_estimator_spec(self, estimator_spec):\n    \"\"\"Validates the estimator_spec.\"\"\"\n    if isinstance(estimator_spec, model_fn_lib._TPUEstimatorSpec):  # pylint: disable=protected-access\n      return estimator_spec\n\n    err_msg = '{} returned by EstimatorSpec is not supported in TPUEstimator.'\n    if estimator_spec.training_chief_hooks:\n      raise ValueError(\n          err_msg.format('training_chief_hooks') + 'If you want' +\n          ' to pass training hooks, please pass via training_hooks.')\n\n    if estimator_spec.scaffold:\n      tf.compat.v1.logging.warn(\n          'EstimatorSpec.Scaffold is ignored by TPU train/eval. '\n          'Please use TPUEstimatorSpec.')\n    return estimator_spec\n\n\nclass _OutfeedHostCall(object):\n  \"\"\"Support for `eval_metrics` and `host_call` in TPUEstimatorSpec.\"\"\"\n\n  def __init__(self, ctx, outfeed_every_n_steps=1):\n    self._ctx = ctx\n    self._names = []\n    # All of these are dictionaries of lists keyed on the name.\n    self._host_fns = {}\n    self._tensor_keys = collections.defaultdict(list)\n    self._tensors = collections.defaultdict(list)\n    self._tensor_dtypes = collections.defaultdict(list)\n    self._tensor_shapes = collections.defaultdict(list)\n    self._outfeed_every_n_steps = outfeed_every_n_steps\n\n  @staticmethod\n  def validate(host_calls):\n    \"\"\"Validates the `eval_metrics` and `host_call` in `TPUEstimatorSpec`.\"\"\"\n\n    for name, host_call in host_calls.items():\n      if not isinstance(host_call, (tuple, list)):\n        raise ValueError('{} should be tuple or list'.format(name))\n      if len(host_call) != 2:\n        raise ValueError('{} should have two elements.'.format(name))\n      if not callable(host_call[0]):\n        raise TypeError('{}[0] should be callable.'.format(name))\n      if not isinstance(host_call[1], (tuple, list, dict)):\n        raise ValueError('{}[1] should be tuple or list, or dict.'.format(name))\n\n      if isinstance(host_call[1], (tuple, list)):\n        fullargspec = tf_inspect.getfullargspec(host_call[0])\n        fn_args = function_utils.fn_args(host_call[0])\n        # wrapped_hostcall_with_global_step uses varargs, so we allow that.\n        if fullargspec.varargs is None and len(host_call[1]) != len(fn_args):\n          raise RuntimeError(\n              'In TPUEstimatorSpec.{}, length of tensors {} does not match '\n              'method args of the function, which takes {}.'.format(\n                  name, len(host_call[1]), len(fn_args)))\n\n  @staticmethod\n  def create_cpu_hostcall(host_calls):\n    \"\"\"Runs on the host_call on CPU instead of TPU when use_tpu=False.\"\"\"\n\n    _OutfeedHostCall.validate(host_calls)\n    ret = {}\n    for name, host_call in host_calls.items():\n      host_fn, tensors = host_call\n      if isinstance(tensors, (tuple, list)):\n        ret[name] = host_fn(*tensors)\n      else:\n        # Must be dict.\n        try:\n          ret[name] = host_fn(**tensors)\n        except TypeError as e:\n          tf.compat.v1.logging.warn(\n              'Exception while calling %s: %s. It is likely the tensors '\n              '(%s[1]) do not match the '\n              'function\\'s arguments', name, e, name)\n          raise\n    return ret\n\n  def record(self, host_calls):\n    \"\"\"Records the host_call structure.\"\"\"\n\n    for name, host_call in host_calls.items():\n      host_fn, tensor_list_or_dict = host_call\n      self._names.append(name)\n      self._host_fns[name] = host_fn\n\n      if isinstance(tensor_list_or_dict, dict):\n        for (key, tensor) in six.iteritems(tensor_list_or_dict):\n          self._tensor_keys[name].append(key)\n          self._tensors[name].append(tensor)\n          self._tensor_dtypes[name].append(tensor.dtype)\n          self._tensor_shapes[name].append(tensor.shape)\n      else:\n        # List or tuple.\n        self._tensor_keys[name] = None\n        for tensor in tensor_list_or_dict:\n          self._tensors[name].append(tensor)\n          self._tensor_dtypes[name].append(tensor.dtype)\n          self._tensor_shapes[name].append(tensor.shape)\n\n  def create_enqueue_op(self, step=None):\n    \"\"\"Create the op to enqueue the recorded host_calls.\n\n    Returns:\n      A list of enqueue ops, which is empty if there are no host calls.\n    \"\"\"\n    if not self._names:\n      return []\n\n    tensors = []\n    # TODO(jhseu): Consider deduping tensors.\n    for name in self._names:\n      tensors.extend(self._tensors[name])\n\n    if self._outfeed_every_n_steps > 1 and step is None:\n      raise ValueError('If outfeed is requested every n steps, you must pass '\n                       'a tensor whose value is the step number within the '\n                       'current training loop.')\n    with tf.compat.v1.device(tf.compat.v1.tpu.core(0)):\n      if self._outfeed_every_n_steps == 1:\n        return [tpu_ops.outfeed_enqueue_tuple(tensors)]\n      else:\n        return [\n            tf.compat.v1.cond(\n                tf.math.equal(\n                    tf.math.floormod(step, self._outfeed_every_n_steps),\n                    0), lambda: tpu_ops.outfeed_enqueue_tuple(tensors),\n                lambda: tf.no_op())\n        ]\n\n  def create_tpu_hostcall(self):\n    \"\"\"Sends the tensors through outfeed and runs the host_fn on CPU.\n\n    The tensors are concatenated along dimension 0 to form a global tensor\n    across all shards. The concatenated function is passed to the host_fn and\n    executed on the first host.\n\n    Returns:\n      A dictionary mapping name to the return type of the host_call by that\n      name.\n\n    Raises:\n      RuntimeError: If outfeed tensor is scalar.\n    \"\"\"\n    if not self._names:\n      return {}\n\n    ret = {}\n    # For each i, dequeue_ops[i] is a list containing the tensors from all\n    # shards. This list is concatenated later.\n    dequeue_ops = []\n    tensor_dtypes = []\n    tensor_shapes = []\n    for name in self._names:\n      for _ in self._tensors[name]:\n        dequeue_ops.append([])\n      for dtype in self._tensor_dtypes[name]:\n        tensor_dtypes.append(dtype)\n      for shape in self._tensor_shapes[name]:\n        tensor_shapes.append(shape)\n\n    # Outfeed ops execute on each replica's first logical core. Note: we must\n    # constraint it such that we have at most one outfeed dequeue and enqueue\n    # per replica.\n    for i in xrange(self._ctx.num_replicas):\n      host_device, ordinal_id = self._ctx.device_for_replica(i)\n      with tf.compat.v1.device(host_device):\n        outfeed_tensors = tpu_ops.outfeed_dequeue_tuple(\n            dtypes=tensor_dtypes,\n            shapes=tensor_shapes,\n            device_ordinal=ordinal_id)\n        for j, item in enumerate(outfeed_tensors):\n          dequeue_ops[j].append(item)\n\n    # Deconstruct dequeue ops.\n    flat_dequeue_ops = []\n    for l in dequeue_ops:\n      flat_dequeue_ops.extend(l)\n\n    dequeue_ops_by_name = {}\n    pos = 0\n    for name in self._names:\n      dequeue_ops_by_name[name] = dequeue_ops[pos:pos +\n                                              len(self._tensors[name])]\n      pos += len(self._tensors[name])\n\n    def _call_host_fn(fn, *args, **kw):\n      context = CatchInvalidHostcallFunctions()\n      context.Enter()\n      result = fn(*args, **kw)\n      context.Exit()\n      context.ExitResult(result)\n      return result\n\n    # It is assumed evaluation always happens on single host TPU system. So,\n    # place all ops on tpu host if possible.\n    #\n    # TODO(jhseu): Evaluate whether this is right for summaries.\n    with tf.compat.v1.device(\n        self._ctx.tpu_host_placement_function(replica_id=0)):\n      for name in self._names:\n        dequeue_ops = dequeue_ops_by_name[name]\n        for i, item in enumerate(dequeue_ops):\n          # TODO(xiejw): Make the specification of the outfeed combinaton\n          # function more explicit and well-documented.  We may want to give the\n          # user the option of concatenating along any axis.\n          if (self._ctx.config.tpu_config.per_host_input_for_training is\n              tpu_config.InputPipelineConfig.BROADCAST):\n            # If the infeed is in BROADCAST mode (each core recieving the same\n            # input), then we assume that the cores also produce identical\n            # copies of the same output, and we simply take the output from\n            # the first core.  This mode is used by Mesh-TensorFlow.\n            with tf.control_dependencies(dequeue_ops[i]):\n              dequeue_ops[i] = tf.identity(dequeue_ops[i][0])\n          else:\n            if dequeue_ops[i][0].shape.ndims == 0:\n              raise RuntimeError(\n                  'All tensors outfed from TPU should preserve batch size '\n                  'dimension, but got scalar {}'.format(dequeue_ops[i][0]))\n            # Assume that the input has been batch-split and that axis 0 of the\n            # output tensors represents the batch size.  Concatenate along\n            # the axis 0 to re-combine the batch.\n            dequeue_ops[i] = tf.concat(dequeue_ops[i], axis=0)\n\n        if self._tensor_keys[name] is not None:\n          # The user-provided eval_metrics[1] is a dict.\n          dequeue_ops = dict(zip(self._tensor_keys[name], dequeue_ops))\n          try:\n            ret[name] = _call_host_fn(self._host_fns[name], **dequeue_ops)\n          except TypeError as e:\n            tf.compat.v1.logging.warn(\n                'Exception while calling %s: %s. It is likely the tensors '\n                '(%s[1]) do not match the '\n                'function\\'s arguments', name, e, name)\n            raise\n        else:\n          ret[name] = _call_host_fn(self._host_fns[name], *dequeue_ops)\n\n    # force all dequeue operations to be run if not consumed by the host calls\n    ret['__force_dequeue'] = tf.group(*flat_dequeue_ops)\n    return ret\n\n\nclass _OutfeedHostCallHook(tf.compat.v1.train.SessionRunHook):\n  \"\"\"Hook to run host calls when use_tpu=False.\"\"\"\n\n  def __init__(self, tensors):\n    self._tensors = tensors\n\n  def begin(self):\n    # We duplicate this code from the TPUInfeedOutfeedSessionHook rather than\n    # create a separate hook to guarantee execution order, because summaries\n    # need to be initialized before the outfeed thread starts.\n    # TODO(jhseu): Make a wrapper hook instead?\n    self._init_ops = summary_ops_v2.summary_writer_initializer_op()\n    # Get all the writer resources from the initializer, so we know what to\n    # flush.\n    self._finalize_ops = []\n    for op in self._init_ops:\n      self._finalize_ops.append(\n        summary_ops_v2.legacy_raw_flush(writer=op.inputs[0]))\n\n  def after_create_session(self, session, coord):\n    session.run(self._init_ops)\n\n  def before_run(self, run_context):\n    return tf.compat.v1.train.SessionRunArgs(self._tensors)\n\n  def end(self, session):\n    session.run(self._finalize_ops)\n\n\nclass _NotSaver(object):\n  \"\"\"What to pass instead of a saver object if you don't want saving.\"\"\"\n\n  def __init__(self, message):\n    self._message = message\n\n  def save(self, *args, **kwargs):\n    del args, kwargs\n    tf.compat.v1.logging.info(self._message)\n\n\nclass ExamplesPerSecondHook(tf.compat.v1.train.StepCounterHook):\n  \"\"\"Calculate and report global_step/sec and examples/sec during runtime.\"\"\"\n\n  def __init__(self,\n               batch_size,\n               every_n_steps=100,\n               every_n_secs=None,\n               output_dir=None,\n               summary_writer=None):\n    self._batch_size = batch_size\n    super(ExamplesPerSecondHook, self).__init__(\n        every_n_steps=every_n_steps,\n        every_n_secs=every_n_secs,\n        output_dir=output_dir,\n        summary_writer=summary_writer)\n\n  def _log_and_record(self, elapsed_steps, elapsed_time, global_step):\n    global_step_per_sec = elapsed_steps / elapsed_time\n    examples_per_sec = self._batch_size * global_step_per_sec\n    if self._summary_writer is not None:\n      global_step_summary = Summary(value=[\n          Summary.Value(\n              tag='global_step/sec', simple_value=global_step_per_sec)\n      ])\n      example_summary = Summary(value=[\n          Summary.Value(tag='examples/sec', simple_value=examples_per_sec)\n      ])\n      self._summary_writer.add_summary(global_step_summary, global_step)\n      self._summary_writer.add_summary(example_summary, global_step)\n    tf.compat.v1.logging.info('global_step/sec: %g', global_step_per_sec)\n    tf.compat.v1.logging.info('examples/sec: %g', examples_per_sec)\n\n\nclass InstallSignalHandlerHook(tf.compat.v1.train.SessionRunHook):\n  \"\"\"Change SIGINT (CTRL^C) handler to force quit the process.\n\n  The default behavior often results in hanging processes.\n  The original handler is restored after training/evaluation.\n  \"\"\"\n\n  def __init__(self):\n    self._signal_fn = signal.getsignal(signal.SIGINT)\n\n  def before_run(self, run_context):\n    signal.signal(signal.SIGINT, signal.SIG_DFL)\n\n  def end(self, session):\n    signal.signal(signal.SIGINT, self._signal_fn)\n\n\nclass ExportSavedModelApiVersion(enum.Enum):\n  V1 = 1\n  V2 = 2\n\n\nclass BatchConfig(\n    collections.namedtuple('BatchConfig', [\n        'num_batch_threads', 'max_batch_size', 'batch_timeout_micros',\n        'allowed_batch_sizes', 'max_enqueued_batches'\n    ])):\n  \"\"\"Class to handle config inputs into the batching function.\"\"\"\n\n  def __new__(cls,\n              num_batch_threads,\n              max_batch_size,\n              batch_timeout_micros,\n              allowed_batch_sizes,\n              max_enqueued_batches=100):\n    \"\"\"Creates an BatchConfig instance.\n\n    Args:\n     num_batch_threads: Number of scheduling threads for processing batches of\n       work. Determines the number of batches processed in parallel.\n      max_batch_size: Batch sizes will never be bigger than this.\n      batch_timeout_micros: Maximum number of microseconds to wait before\n        outputting an incomplete batch.\n      allowed_batch_sizes: Optional list of allowed batch sizes. If left empty,\n        does nothing. Otherwise, supplies a list of batch sizes, causing the op\n        to pad batches up to one of those sizes. The entries must increase\n        monotonically, and the final entry must equal max_batch_size.\n      max_enqueued_batches: The maximum depth of the batch queue. Defaults to\n        100.\n\n    Returns:\n      An BatchConfig instance.\n    \"\"\"\n    return super(BatchConfig, cls).__new__(\n        cls,\n        num_batch_threads=num_batch_threads,\n        max_batch_size=max_batch_size,\n        batch_timeout_micros=batch_timeout_micros,\n        allowed_batch_sizes=allowed_batch_sizes,\n        max_enqueued_batches=max_enqueued_batches)\n\n\n@estimator_export(v1=['estimator.tpu.TPUEstimator'])\nclass TPUEstimator(estimator_lib.Estimator):\n  \"\"\"Estimator with TPU support.\n\n  TPUEstimator also supports training on CPU and GPU. You don't need to define\n  a separate `tf.estimator.Estimator`.\n\n  TPUEstimator handles many of the details of running on TPU devices, such as\n  replicating inputs and models for each core, and returning to host\n  periodically to run hooks.\n\n  TPUEstimator transforms a global batch size in params to a per-shard batch\n  size when calling the `input_fn` and `model_fn`. Users should specify\n  global batch size in constructor, and then get the batch size for each shard\n  in `input_fn` and `model_fn` by `params['batch_size']`.\n\n  - For training, `model_fn` gets per-core batch size; `input_fn` may get\n    per-core or per-host batch size depending on `per_host_input_for_training`\n    in `TPUConfig` (See docstring for TPUConfig for details).\n\n  - For evaluation and prediction, `model_fn` gets per-core batch size and\n    `input_fn` get per-host batch size.\n\n  Evaluation\n  ==========\n\n  `model_fn` should return `TPUEstimatorSpec`, which expects the `eval_metrics`\n  for TPU evaluation. If eval_on_tpu is False, the evaluation will execute on\n  CPU or GPU; in this case the following discussion on TPU evaluation does not\n  apply.\n\n  `TPUEstimatorSpec.eval_metrics` is a tuple of `metric_fn` and `tensors`, where\n  `tensors` could be a list of any nested structure of `Tensor`s (See\n  `TPUEstimatorSpec` for details).  `metric_fn` takes the `tensors` and returns\n  a dict from metric string name to the result of calling a metric function,\n  namely a `(metric_tensor, update_op)` tuple.\n\n  One can set `use_tpu` to `False` for testing. All training, evaluation, and\n  predict will be executed on CPU. `input_fn` and `model_fn` will receive\n  `train_batch_size` or `eval_batch_size` unmodified as `params['batch_size']`.\n\n  Current limitations:\n  --------------------\n\n  1. TPU evaluation only works on a single host (one TPU worker) except\n     BROADCAST mode.\n\n  2. `input_fn` for evaluation should **NOT** raise an end-of-input exception\n     (`OutOfRangeError` or `StopIteration`). And all evaluation steps and all\n     batches should have the same size.\n\n  Example (MNIST):\n  ----------------\n\n  ```\n  # The metric Fn which runs on CPU.\n  def metric_fn(labels, logits):\n    predictions = tf.argmax(logits, 1)\n    return {\n      'accuracy': tf.compat.v1.metrics.precision(\n          labels=labels, predictions=predictions),\n    }\n\n  # Your model Fn which runs on TPU (eval_metrics is list in this example)\n  def model_fn(features, labels, mode, config, params):\n    ...\n    logits = ...\n\n    if mode = tf.estimator.ModeKeys.EVAL:\n      return tpu_estimator.TPUEstimatorSpec(\n          mode=mode,\n          loss=loss,\n          eval_metrics=(metric_fn, [labels, logits]))\n\n  # or specify the eval_metrics tensors as dict.\n  def model_fn(features, labels, mode, config, params):\n    ...\n    final_layer_output = ...\n\n    if mode = tf.estimator.ModeKeys.EVAL:\n      return tpu_estimator.TPUEstimatorSpec(\n          mode=mode,\n          loss=loss,\n          eval_metrics=(metric_fn, {\n              'labels': labels,\n              'logits': final_layer_output,\n          }))\n  ```\n\n  Prediction\n  ==========\n\n  Prediction on TPU is an experimental feature to support large batch inference.\n  It is not designed for latency-critical system. In addition, due to some\n  usability issues, for prediction with small dataset, CPU `.predict`, i.e.,\n  creating a new `TPUEstimator` instance with `use_tpu=False`, might be more\n  convenient.\n\n  Note: In contrast to TPU training/evaluation, the `input_fn` for prediction\n  *should* raise an end-of-input exception (`OutOfRangeError` or\n  `StopIteration`), which serves as the stopping signal to `TPUEstimator`. To be\n  precise, the ops created by `input_fn` produce one batch of the data.\n  The `predict()` API processes one batch at a time. When reaching the end of\n  the data source, an end-of-input exception should be raised by one of these\n  operations. The user usually does not need to do this manually. As long as the\n  dataset is not repeated forever, the `tf.data` API will raise an end-of-input\n  exception automatically after the last batch has been produced.\n\n  Note: Estimator.predict returns a Python generator. Please consume all the\n  data from the generator so that TPUEstimator can shutdown the TPU system\n  properly for user.\n\n  Current limitations:\n  --------------------\n  1. TPU prediction only works on a single host (one TPU worker).\n\n  2. `input_fn` must return a `Dataset` instance rather than `features`. In\n  fact, .train() and .evaluate() also support Dataset as return value.\n\n  Example (MNIST):\n  ----------------\n  ```\n  height = 32\n  width = 32\n  total_examples = 100\n\n  def predict_input_fn(params):\n    batch_size = params['batch_size']\n\n    images = tf.random.uniform(\n        [total_examples, height, width, 3], minval=-1, maxval=1)\n\n    dataset = tf.data.Dataset.from_tensor_slices(images)\n    dataset = dataset.map(lambda images: {'image': images})\n\n    dataset = dataset.batch(batch_size)\n    return dataset\n\n  def model_fn(features, labels, params, mode):\n     # Generate predictions, called 'output', from features['image']\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      return tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          predictions={\n              'predictions': output,\n              'is_padding': features['is_padding']\n          })\n\n  tpu_est = TPUEstimator(\n      model_fn=model_fn,\n      ...,\n      predict_batch_size=16)\n\n  # Fully consume the generator so that TPUEstimator can shutdown the TPU\n  # system.\n  for item in tpu_est.predict(input_fn=input_fn):\n    # Filter out item if the `is_padding` is 1.\n    # Process the 'predictions'\n  ```\n\n  Exporting\n  =========\n\n  `export_saved_model` exports 2 metagraphs, one with `saved_model.SERVING`, and\n  another with `saved_model.SERVING` and `saved_model.TPU` tags. At serving\n  time, these tags are used to select the appropriate metagraph to load.\n\n  Before running the graph on TPU, the TPU system needs to be initialized. If\n  TensorFlow Serving model-server is used, this is done automatically. If not,\n  please use `session.run(tpu.initialize_system())`.\n\n  There are two versions of the API: 1 or 2.\n\n  In V1, the exported CPU graph is `model_fn` as it is. The exported TPU graph\n  wraps `tpu.rewrite()` and `TPUPartitionedCallOp` around `model_fn` so\n  `model_fn` is on TPU by default. To place ops on CPU,\n  `tpu.outside_compilation(host_call, logits)` can be used.\n\n  Example:\n  ----------------\n\n  ```\n  def model_fn(features, labels, mode, config, params):\n    ...\n    logits = ...\n    export_outputs = {\n      'logits': export_output_lib.PredictOutput(\n        {'logits': logits})\n    }\n\n    def host_call(logits):\n      class_ids = math_ops.argmax(logits)\n      classes = string_ops.as_string(class_ids)\n      export_outputs['classes'] =\n        export_output_lib.ClassificationOutput(classes=classes)\n\n    tpu.outside_compilation(host_call, logits)\n\n    ...\n  ```\n\n  In V2, `export_saved_model()` sets up `params['use_tpu']` flag to let the user\n  know if the code is exporting to TPU (or not). When `params['use_tpu']` is\n  `True`, users need to call `tpu.rewrite()`, `TPUPartitionedCallOp` and/or\n  `batch_function()`.\n\n  TIP: V2 is recommended as it is more flexible (eg: batching, etc).\n\n  @compatibility(TF2)\n  TPU Estimator manages its own TensorFlow graph and session, so it is not\n  compatible with TF2 behaviors. We recommend that you migrate to the newer\n  `tf.distribute.TPUStrategy`. See the\n  [TPU guide](https://www.tensorflow.org/guide/tpu) for details.\n  @end_compatibility\n  \"\"\"\n\n  def __init__(self,\n               model_fn=None,\n               model_dir=None,\n               config=None,\n               params=None,\n               use_tpu=True,\n               train_batch_size=None,\n               eval_batch_size=None,\n               predict_batch_size=None,\n               batch_axis=None,\n               eval_on_tpu=True,\n               export_to_tpu=True,\n               export_to_cpu=True,\n               warm_start_from=None,\n               embedding_config_spec=None,\n               export_saved_model_api_version=ExportSavedModelApiVersion.V1):\n    \"\"\"Constructs an `TPUEstimator` instance.\n\n    Args:\n      model_fn: Model function as required by `Estimator` which returns\n        EstimatorSpec or TPUEstimatorSpec. `training_hooks`, 'evaluation_hooks',\n        and `prediction_hooks` must not capure any TPU Tensor inside the\n        model_fn.\n      model_dir: Directory to save model parameters, graph and etc. This can\n        also be used to load checkpoints from the directory into a estimator to\n        continue training a previously saved model. If `None`, the model_dir in\n        `config` will be used if set. If both are set, they must be same. If\n        both are `None`, a temporary directory will be used.\n      config: An `tpu_config.RunConfig` configuration object. Cannot be `None`.\n      params: An optional `dict` of hyper parameters that will be passed into\n        `input_fn` and `model_fn`.  Keys are names of parameters, values are\n        basic python types. There are reserved keys for `TPUEstimator`,\n        including 'batch_size'.\n      use_tpu: A bool indicating whether TPU support is enabled. Currently, -\n        TPU training and evaluation respect this bit, but eval_on_tpu can\n        override execution of eval. See below.\n      train_batch_size: An int representing the global training batch size.\n        TPUEstimator transforms this global batch size to a per-shard batch\n        size, as params['batch_size'], when calling `input_fn` and `model_fn`.\n        Cannot be `None` if `use_tpu` is `True`. Must be divisible by total\n        number of replicas.\n      eval_batch_size: An int representing evaluation batch size. Must be\n        divisible by total number of replicas.\n      predict_batch_size: An int representing the prediction batch size. Must be\n        divisible by total number of replicas.\n      batch_axis: A python tuple of int values describing how each tensor\n        produced by the Estimator `input_fn` should be split across the TPU\n        compute shards. For example, if your input_fn produced (images, labels)\n        where the images tensor is in `HWCN` format, your shard dimensions would\n        be [3, 0], where 3 corresponds to the `N` dimension of your images\n        Tensor, and 0 corresponds to the dimension along which to split the\n        labels to match up with the corresponding images. If None is supplied,\n        and per_host_input_for_training is True, batches will be sharded based\n        on the major dimension. If tpu_config.per_host_input_for_training is\n        False or `PER_HOST_V2`, batch_axis is ignored.\n      eval_on_tpu: If False, evaluation runs on CPU or GPU. In this case, the\n        model_fn must return `EstimatorSpec` when called with `mode` as `EVAL`.\n      export_to_tpu: If True, `export_saved_model()` exports a metagraph for\n        serving on TPU. Note that unsupported export modes such as EVAL will be\n        ignored. For those modes, only a CPU model will be exported. Currently,\n        export_to_tpu only supports PREDICT.\n      export_to_cpu: If True, `export_saved_model()` exports a metagraph for\n        serving on CPU.\n      warm_start_from: Optional string filepath to a checkpoint or SavedModel to\n        warm-start from, or a `tf.estimator.WarmStartSettings` object to fully\n        configure warm-starting.  If the string filepath is provided instead of\n        a `WarmStartSettings`, then all variables are warm-started, and it is\n        assumed that vocabularies and Tensor names are unchanged.\n      embedding_config_spec: Optional EmbeddingConfigSpec instance to support\n        using TPU embedding.\n      export_saved_model_api_version: an integer: 1 or 2. 1 corresponds to V1,\n        2 corresponds to V2. (Defaults to V1). With\n        V1, `export_saved_model()` adds rewrite() and TPUPartitionedCallOp() for\n        user; while in v2, user is expected to add rewrite(),\n        TPUPartitionedCallOp() etc in their model_fn.\n\n    Raises:\n      ValueError: `params` has reserved keys already.\n    \"\"\"\n    if config is None or not isinstance(config, tpu_config.RunConfig):\n      raise ValueError(\n          '`config` must be provided with type `tpu_config.RunConfig`')\n\n    if params is not None and any(k in params for k in _RESERVED_PARAMS_KEYS):\n      raise ValueError('{} are reserved keys but existed in params {}.'.format(\n          _RESERVED_PARAMS_KEYS, params))\n\n    if use_tpu:\n      # Perform some very basic validations. More validations will be found in\n      # _InternalTPUContext.\n      if train_batch_size is None:\n        raise ValueError('`train_batch_size` cannot be `None`')\n      util_lib.check_positive_integer(train_batch_size, 'train_batch_size')\n\n      if (config.tpu_config.per_host_input_for_training is\n          tpu_config.InputPipelineConfig.PER_SHARD_V1 and\n          config.tpu_config.num_cores_per_replica):\n        raise ValueError(\n            'Model parallelism only supports per host input for training. '\n            'Please adjust TPURunconfig.per_host_input_for_training.')\n\n      if eval_batch_size is not None:\n        util_lib.check_positive_integer(eval_batch_size, 'eval_batch_size')\n\n      if predict_batch_size is not None:\n        util_lib.check_positive_integer(predict_batch_size,\n                                        'predict_batch_size')\n\n      if embedding_config_spec:\n        if (config.tpu_config.per_host_input_for_training not in (\n            tpu_config.InputPipelineConfig.PER_HOST_V1,\n            tpu_config.InputPipelineConfig.PER_HOST_V2)):\n          raise ValueError('Only PER_HOST_V1 and PER_HOST_V2 is supported when '\n                           'using TPU Embedding; got {}.'.format(\n                               config.tpu_config.per_host_input_for_training))\n        self._embedding_from_feature_columns = (\n            embedding_config_spec.feature_columns is not None)\n\n    if (not (use_tpu and eval_on_tpu) and embedding_config_spec and\n        embedding_config_spec.partition_strategy == 'mod'):\n      raise ValueError('Mod sharding of embedding tables not supported on '\n                       'CPU.')\n    _tpu_estimator_gauge.get_cell().set(True)\n    # Verifies the model_fn signature according to Estimator framework.\n    estimator_lib._verify_model_fn_args(model_fn, params)  # pylint: disable=protected-access\n    # We cannot store config and params in this constructor as parent\n    # constructor might change them, such as assigning a temp dir for\n    # config.model_dir.\n    model_function = self._augment_model_fn(model_fn, batch_axis)\n\n    # Overwrite log_step_count_steps to disable TensorLoggingHook and\n    # StepCounterHook from being created in Estimator. TPUEstimator already\n    # added equivalent hooks in _augment_model_fn above.\n    self._log_every_n_steps = config.log_step_count_steps\n    config = config.replace(log_step_count_steps=None)\n\n    # Passing non-None params as wrapped model_fn has it.\n    params = params or {}\n    super(TPUEstimator, self).__init__(\n        model_fn=model_function,\n        model_dir=model_dir,\n        config=config,\n        params=params,\n        warm_start_from=warm_start_from)\n    self._iterations_per_training_loop = util_lib.parse_iterations_per_loop(\n        self._config.tpu_config.iterations_per_loop)\n    # In absence of an explicit `log_every_n_secs` config, if the\n    # `iterations_per_loop` value is specified as time in seconds, enable\n    # logging every n secs based on the `iterations_per_loop` value. A trade-off\n    # avoiding API change on the current release.\n    # TODO(henrytan): add `log_every_n_secs` to RunConfig.\n    if self._iterations_per_training_loop.unit == 'seconds':\n      self._log_every_n_secs = self._iterations_per_training_loop.value\n      self._log_every_n_steps = None\n    elif self._iterations_per_training_loop.unit == 'count':\n      if self._log_every_n_steps is not None:\n        # Each session.run() lasts for iterations_per_loop. We can't log\n        # in-between a session.run(), and we can only log after the\n        # `iterations_per_loop` steps, so we can only approximate. If a user\n        # requests to log every N steps, we actually want to roughly log every\n        # N / `iterations_per_loop` steps to match the original intention.\n        self._log_every_n_steps = (\n            int(\n                math.ceil(\n                    float(self._log_every_n_steps) /\n                    self._iterations_per_training_loop.value)))\n      self._log_every_n_secs = None\n    else:\n      assert False, ('Invalid TPUConfig `iterations_per_loop` value. '\n                     'Indicates a bug in `iterations_per_loop` '\n                     'parsing.')\n\n    # All properties passed to _InternalTPUContext are immutable.\n    # pylint: disable=protected-access\n    self._ctx = tpu_context._get_tpu_context(self._config, train_batch_size,\n                                             eval_batch_size,\n                                             predict_batch_size, use_tpu,\n                                             eval_on_tpu, embedding_config_spec)\n\n    self._export_to_cpu = export_to_cpu\n    self._export_to_tpu = export_to_tpu\n\n    if not (isinstance(export_saved_model_api_version,\n                       ExportSavedModelApiVersion)\n            or export_saved_model_api_version == 1\n            or export_saved_model_api_version == 2):\n      raise ValueError('export_saved_model_api_version should be 1 or 2; '\n                       'got {}.'.format(\n                           export_saved_model_api_version))\n    self._export_saved_model_api_version = export_saved_model_api_version\n    self._is_input_fn_invoked = None\n\n    self._rendezvous = {}\n\n  def _add_meta_graph_for_mode(self,\n                               builder,\n                               input_receiver_fn_map,\n                               checkpoint_path,\n                               save_variables=True,\n                               mode=model_fn_lib.ModeKeys.PREDICT,\n                               export_tags=None,\n                               check_variables=True,\n                               strip_default_attrs=True):\n    if self._export_to_tpu and mode != model_fn_lib.ModeKeys.PREDICT:\n      tf.compat.v1.logging.warn(\n          'TPUEstimator only handles mode PREDICT for exporting '\n          'when `export_to_tpu` is `True`; Mode {} will be ignored '\n          'for TPU.'.format(mode))\n\n    if not self._export_to_cpu and not self._export_to_tpu:\n      raise ValueError('One of export_to_cpu and export_to_tpu must be true.')\n\n    if self._export_to_cpu:\n      (super(TPUEstimator, self)._add_meta_graph_for_mode(\n          builder,\n          input_receiver_fn_map,\n          checkpoint_path,\n          save_variables,\n          mode=mode,\n          export_tags=export_tags,\n          check_variables=check_variables,\n          strip_default_attrs=strip_default_attrs))\n\n    if self._export_to_tpu and mode == model_fn_lib.ModeKeys.PREDICT:\n      input_receiver_fn_map = {\n          _INFERENCE_ON_TPU_MODE: input_receiver_fn_map[mode]\n      }\n      export_tags = [tf.saved_model.SERVING, tf.saved_model.TPU]\n      mode = _INFERENCE_ON_TPU_MODE\n\n      # See b/110052256 for why `check_variables` is `False`.\n      if not self._export_to_cpu:\n        check_variables = save_variables = True\n      else:\n        check_variables = save_variables = False\n      (super(TPUEstimator, self)._add_meta_graph_for_mode(\n          builder,\n          input_receiver_fn_map,\n          checkpoint_path,\n          save_variables=save_variables,\n          mode=mode,\n          export_tags=export_tags,\n          check_variables=check_variables,\n          strip_default_attrs=strip_default_attrs))\n\n  def _call_model_fn(self, features, labels, mode, config):\n    if mode == _INFERENCE_ON_TPU_MODE:\n      context = tpu._TPUInferenceContext('tpu_inference', check_ops=False)\n      try:\n        context.Enter()\n        if (\n            (self._export_saved_model_api_version ==\n             ExportSavedModelApiVersion.V1)\n            or self._export_saved_model_api_version == 1):\n          result = self._call_model_fn_for_inference(features, labels, mode,\n                                                     config)\n        else:\n          result = super(TPUEstimator,\n                         self)._call_model_fn(features, labels, mode, config)\n      finally:\n        context.Exit()\n      return result\n    else:\n      return super(TPUEstimator, self)._call_model_fn(features, labels, mode,\n                                                      config)\n\n  def _call_model_fn_for_inference(self, features, labels, mode, config):\n    \"\"\"Wraps `_call_model_fn` for `export_saved_model`.\"\"\"\n    if mode != _INFERENCE_ON_TPU_MODE:\n      raise ValueError('mode must be {}; '\n                       'got {}.'.format(_INFERENCE_ON_TPU_MODE, mode))\n    return model_fn_inference_on_tpu(\n        self._model_fn,\n        features,\n        labels,\n        config,\n        self._params,\n        batch_config=None)\n\n  def _create_global_step(self, graph):\n    \"\"\"Creates a global step suitable for TPUs.\n\n    Args:\n      graph: The graph in which to create the global step.\n\n    Returns:\n      A global step `Tensor`.\n\n    Raises:\n      ValueError: if the global step tensor is already defined.\n    \"\"\"\n    return _create_global_step(graph)\n\n  def _convert_train_steps_to_hooks(self, steps, max_steps):\n    with self._ctx.with_mode(model_fn_lib.ModeKeys.TRAIN) as ctx:\n      if ctx.is_running_on_cpu():\n        return super(TPUEstimator,\n                     self)._convert_train_steps_to_hooks(steps, max_steps)\n\n    # On TPU.\n    if steps is None and max_steps is None:\n      raise ValueError(\n          'For TPU training, one of `steps` or `max_steps` must be set. '\n          'Cannot be both `None`.')\n\n    # Estimator.train has explicit positiveness check.\n    if steps is not None:\n      util_lib.check_positive_integer(steps, 'Train steps')\n    if max_steps is not None:\n      util_lib.check_positive_integer(max_steps, 'Train max_steps')\n\n    return [\n        _TPUStopAtStepHook(self._iterations_per_training_loop, steps, max_steps)\n    ]\n\n  def _convert_eval_steps_to_hooks(self, steps):\n    with self._ctx.with_mode(model_fn_lib.ModeKeys.EVAL) as ctx:\n      if ctx.is_running_on_cpu():\n        return super(TPUEstimator, self)._convert_eval_steps_to_hooks(steps)\n\n    if steps is None:\n      raise ValueError('Evaluate `steps` must be set on TPU. Cannot be `None`.')\n\n    util_lib.check_positive_integer(steps, 'Eval steps')\n\n    return [\n        evaluation._StopAfterNEvalsHook(  # pylint: disable=protected-access\n            num_evals=steps),\n        _SetEvalIterationsHook(steps)\n    ]\n\n  def _call_input_fn(self, input_fn, mode, input_context=None):\n    \"\"\"Calls the input function.\n\n    Args:\n      input_fn: The input function.\n      mode: ModeKeys\n      input_context: Optional instance of `tf.distribute.InputContext`.\n\n    Returns:\n      In TPU mode, returns an input_fn to be called later in model_fn.\n      Otherwise, calls the input_fn and returns either fatures or\n        (features, labels).\n\n    Raises:\n      ValueError: if input_fn takes invalid arguments or does not have `params`.\n    \"\"\"\n    input_fn_args = function_utils.fn_args(input_fn)\n    config = self.config  # a deep copy.\n    kwargs = {}\n    if 'params' in input_fn_args:\n      kwargs['params'] = self.params  # a deep copy.\n    else:\n      raise ValueError('input_fn ({}) does not include params argument, '\n                       'required by TPUEstimator to pass batch size as '\n                       'params[\"batch_size\"]'.format(input_fn))\n    if 'config' in input_fn_args:\n      kwargs['config'] = config\n\n    if 'mode' in input_fn_args:\n      kwargs['mode'] = mode\n\n    if 'input_context' in input_fn_args:\n      kwargs['input_context'] = input_context\n\n    # Records the fact input_fn has been invoked.\n    self._is_input_fn_invoked = True\n\n    with self._ctx.with_mode(mode) as ctx:\n      if (ctx.is_running_on_cpu() and\n          ctx.is_input_slice_broadcast_to_all_cores()):\n        raise ValueError('Invalid TPUConfig `eval_training_input_configuration`'\n                         ' value. SLICED mode only works on use_tpu = True.')\n      # Setting the batch size in params first. This helps user to have same\n      # input_fn for use_tpu=True/False.\n      batch_size_for_input_fn = ctx.batch_size_for_input_fn\n      if batch_size_for_input_fn is not None:\n        _add_item_to_params(kwargs['params'], _BATCH_SIZE_KEY,\n                            batch_size_for_input_fn)\n\n      # For export_saved_model, input_fn is never passed to Estimator. So,\n      # `is_export_mode` must be False.\n      if ctx.is_running_on_cpu(is_export_mode=False):\n        with tf.compat.v1.device('/device:CPU:0'):\n          return input_fn(**kwargs)\n\n      # For TPU computation, input_fn should be invoked in a tf.while_loop for\n      # performance. While constructing the tf.while_loop, the structure of\n      # inputs returned by the `input_fn` needs to be recorded. The structure\n      # includes whether features or labels is dict or single Tensor, dict keys,\n      # tensor shapes, and dtypes. The recorded structure is used to create the\n      # infeed dequeue ops, which must be wrapped and passed as a Fn, called\n      # inside the TPU computation, as the TPU computation is wrapped inside a\n      # tf.while_loop also. So, we either pass input_fn to model_fn or pass\n      # dequeue_fn to model_fn. Here, `input_fn` is passed directly as\n      # `features` in `model_fn` signature.\n      def _input_fn(ctx):\n        _add_item_to_params(kwargs['params'], _CTX_KEY, ctx)\n        return input_fn(**kwargs)\n\n      return _input_fn\n\n  def _validate_features_in_predict_input(self, result):\n    \"\"\"Skip the validation.\n\n    For TPUEstimator, we do not need to check the result type. `_InputPipeline`\n    has stronger check. Parent class's check generates confusing warning msg.\n\n    Args:\n      result: `features` returned by input_fn.\n    \"\"\"\n    pass\n\n  def train(self,\n            input_fn,\n            hooks=None,\n            steps=None,\n            max_steps=None,\n            saving_listeners=None):\n    rendezvous = error_handling.ErrorRendezvous(num_sources=3)\n    self._rendezvous[model_fn_lib.ModeKeys.TRAIN] = rendezvous\n    try:\n      return super(TPUEstimator, self).train(\n          input_fn=input_fn,\n          hooks=hooks,\n          steps=steps,\n          max_steps=max_steps,\n          saving_listeners=saving_listeners)\n    except Exception:  # pylint: disable=broad-except\n      rendezvous.record_error('training_loop', sys.exc_info())\n    finally:\n      rendezvous.record_done('training_loop')\n      rendezvous.raise_errors()\n\n  def evaluate(self,\n               input_fn,\n               steps=None,\n               hooks=None,\n               checkpoint_path=None,\n               name=None):\n    rendezvous = error_handling.ErrorRendezvous(num_sources=3)\n    self._rendezvous[model_fn_lib.ModeKeys.EVAL] = rendezvous\n    try:\n      return super(TPUEstimator, self).evaluate(\n          input_fn,\n          steps=steps,\n          hooks=hooks,\n          checkpoint_path=checkpoint_path,\n          name=name)\n    except Exception:  # pylint: disable=broad-except\n      rendezvous.record_error('evaluation_loop', sys.exc_info())\n    finally:\n      rendezvous.record_done('evaluation_loop')\n      rendezvous.raise_errors()\n\n  def predict(self,\n              input_fn,\n              predict_keys=None,\n              hooks=None,\n              checkpoint_path=None,\n              yield_single_examples=True):\n    rendezvous = error_handling.ErrorRendezvous(num_sources=3)\n    self._rendezvous[model_fn_lib.ModeKeys.PREDICT] = rendezvous\n    try:\n      for result in super(TPUEstimator, self).predict(\n          input_fn=input_fn,\n          predict_keys=predict_keys,\n          hooks=hooks,\n          checkpoint_path=checkpoint_path,\n          yield_single_examples=yield_single_examples):\n        yield result\n    except Exception:  # pylint: disable=broad-except\n      rendezvous.record_error('prediction_loop', sys.exc_info())\n    finally:\n      rendezvous.record_done('prediction_loop')\n      rendezvous.raise_errors()\n\n    rendezvous.record_done('prediction_loop')\n    rendezvous.raise_errors()\n\n  def _augment_model_fn(self, model_fn, batch_axis):\n    \"\"\"Returns a new model_fn, which wraps the TPU support.\"\"\"\n\n    def _model_fn(features, labels, mode, config, params):\n      \"\"\"A Estimator `model_fn` for TPUEstimator.\"\"\"\n\n      # `input_fn` is called in `train()`, `evaluate()`, and `predict()`,\n      # but not in `export_saved_model()`.\n      if self._is_input_fn_invoked:\n        is_export_mode = False\n      else:\n        is_export_mode = True\n\n      # Clear the bit.\n      self._is_input_fn_invoked = None\n\n      if is_export_mode:\n        if mode == _INFERENCE_ON_TPU_MODE:\n          _add_item_to_params(params, _USE_TPU_KEY, True)\n          mode = model_fn_lib.ModeKeys.PREDICT\n        else:\n          _add_item_to_params(params, _USE_TPU_KEY, False)\n\n      with self._ctx.with_mode(mode) as ctx:\n        model_fn_wrapper = _ModelFnWrapper(model_fn, config, params, ctx)\n\n        # examples_hook is added to training_hooks for both CPU and TPU\n        # execution.\n        if (self._log_every_n_steps is not None or\n            self._log_every_n_secs is not None):\n          examples_hook = ExamplesPerSecondHook(\n              ctx.global_batch_size,\n              # pylint:disable=g-long-ternary\n              output_dir=(self.model_dir\n                          if not config or config.save_summary_steps else None),\n              # pylint:enable=g-long-ternary\n              every_n_steps=self._log_every_n_steps,\n              every_n_secs=self._log_every_n_secs)\n\n        if ctx.is_running_on_cpu(is_export_mode=is_export_mode):\n          tf.compat.v1.logging.info('Running %s on CPU/GPU', mode)\n          estimator_spec = model_fn_wrapper.call_without_tpu(\n              features, labels, is_export_mode=is_export_mode)\n          if (self._log_every_n_steps is not None or\n              self._log_every_n_secs is not None):\n            estimator_spec = estimator_spec._replace(\n                training_hooks=estimator_spec.training_hooks + (examples_hook,))\n          return estimator_spec\n\n        assert labels is None, '`labels` passed to `model_fn` must be `None`.'\n        # TPUEstimator._call_input_fn passes `input_fn` as features to here.\n        assert callable(features), '`input_fn` is not callable.'\n        input_fn = features\n\n        tpu_init_ops = []\n        if ctx.embedding_config and mode == model_fn_lib.ModeKeys.TRAIN:\n          dummy_table_variables, dummy_table_variables_init = (\n              tpu_embedding_gradient.create_dummy_table_variables(\n                  ctx.embedding_config.tpu_embedding))\n          ctx.embedding_config.dummy_table_variables = dummy_table_variables\n          tpu_init_ops.append(dummy_table_variables_init)\n\n        input_holders = _InputPipeline(input_fn, batch_axis, ctx)\n        enqueue_ops, dequeue_fn, input_hooks, run_infeed_loop_on_coordinator = (\n            input_holders.generate_infeed_enqueue_ops_and_dequeue_fn())\n\n        graph = tf.compat.v1.get_default_graph()\n        for enqueue_op in enqueue_ops:\n          if isinstance(enqueue_op, list):\n            graph.get_collection_ref(_TPU_ENQUEUE_OPS).extend(enqueue_op)\n          else:\n            graph.add_to_collection(_TPU_ENQUEUE_OPS, enqueue_op)\n\n        if mode == model_fn_lib.ModeKeys.TRAIN:\n          compile_op, loss, host_call, scaffold_fn, training_hooks = (\n              _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\n          has_saver_hook = training_hooks and any(\n              isinstance(hook, tf.compat.v1.train.CheckpointSaverHook)\n              for hook in training_hooks)\n          if ctx.embedding_config:\n            g = tf.compat.v1.get_default_graph()\n            table_to_config_dict = (\n                ctx.embedding_config.tpu_embedding.table_to_config_dict)\n            optimization_parameters = (\n                ctx.embedding_config.tpu_embedding.optimization_parameters)\n            if self._embedding_from_feature_columns:\n              embedding_variable_name_by_table, slot_variable_names_by_table = (\n                  _tpu_estimator_embedding.get_full_variable_names(\n                      g, table_to_config_dict, optimization_parameters))\n            else:\n              embedding_variable_name_by_table = None\n              slot_variable_names_by_table = None\n            embedding_variables_and_ops = (\n                ctx.embedding_config.tpu_embedding.create_variables_and_ops(\n                    embedding_variable_name_by_table,\n                    slot_variable_names_by_table))\n            tpu_init_ops.extend(embedding_variables_and_ops.load_ops())\n          # scaffold_fn must be called after variables for TPU embedding has\n          # been created on CPU, as user might reinitialize those from some\n          # checkpoint within scaffold_fn.\n          scaffold = _get_scaffold(scaffold_fn)\n\n          host_ops = host_call.create_tpu_hostcall()\n\n          shutdown_hooks = []\n          shutdown_mode = os.environ.get('TF_TPU_GRACEFUL_SHUTDOWN_MODE',\n                                         'reset_computation')\n          if shutdown_mode:\n            if shutdown_mode == 'shutdown_worker':\n              finalizer_hooks = [\n                  session_support.ShutdownLameWorkers(),\n              ]\n            elif shutdown_mode == 'shutdown_all_workers':\n              finalizer_hooks = [\n                  session_support.ShutdownAllWorkers(),\n              ]\n            elif shutdown_mode == 'reset_computation':\n              finalizer_hooks = [\n                  session_support.ResetComputation(),\n              ]\n            elif not shutdown_mode:\n              finalizer_hooks = []\n            else:\n              raise ValueError('Unknown TF_TPU_GRACEFUL_SHUTDOWN_MODE \"%s\"' %\n                               shutdown_mode)\n\n            if finalizer_hooks:\n              if has_saver_hook:\n                saver = _NotSaver(\n                    'No save on shutdown when there are user-defined '\n                    'CheckpointSaverHooks')\n              else:\n                saver = None  # Yes automatic save on shutdown.\n              shutdown_hooks.append(\n                  session_support.GracefulShutdownHook(\n                      checkpoint_prefix=self.model_dir + '/model.ckpt',\n                      on_shutdown_hooks=finalizer_hooks,\n                      saver=saver))\n\n          with tf.control_dependencies([loss]):\n            global_step = tf.identity(tf.compat.v1.train.get_global_step())\n          hooks = input_hooks + shutdown_hooks\n\n          if ctx.feed_hook is not None:\n            tf.compat.v1.logging.info(\n                'Use user implemented tpu infeed outfeed session hook class.')\n            infeed_outfeed_session_hook_class = ctx.feed_hook\n          else:\n            infeed_outfeed_session_hook_class = TPUInfeedOutfeedSessionHook\n\n          hooks.extend([\n              infeed_outfeed_session_hook_class(\n                  ctx,\n                  enqueue_ops,\n                  host_ops,\n                  tpu_compile_op=compile_op,\n                  run_infeed_loop_on_coordinator=(\n                      run_infeed_loop_on_coordinator),\n                  rendezvous=self._rendezvous[mode],\n                  master=self._config.master,\n                  session_config=self._session_config,\n                  tpu_init_ops=tpu_init_ops,\n                  outfeed_every_n_steps=self._config.tpu_config\n                  .experimental_host_call_every_n_steps),\n              InstallSignalHandlerHook()\n          ])\n          if _check_add_preemption_hook(self._config.cluster):\n            hooks.extend(\n                [preempted_hook.CloudTPUPreemptedHook(self._config.cluster)])\n          if (self._log_every_n_steps is not None or\n              self._log_every_n_secs is not None):\n            if self._iterations_per_training_loop.unit == 'count':\n              examples_hook._set_steps_per_run(  # pylint: disable=protected-access\n                  self._iterations_per_training_loop.value)\n            hooks.append(\n                tf.compat.v1.train.LoggingTensorHook(\n                    {\n                        'loss': tf.identity(loss),\n                        'step': global_step,\n                    },\n                    every_n_iter=self._log_every_n_steps,\n                    every_n_secs=self._log_every_n_secs))\n            hooks.append(examples_hook)\n\n          if training_hooks:\n            hooks.extend(training_hooks)\n\n          chief_hooks = []\n          if (not has_saver_hook and\n              (self._config.save_checkpoints_secs or\n               self._config.save_checkpoints_steps)):\n            checkpoint_hook = tf.compat.v1.train.CheckpointSaverHook(\n                self.model_dir,\n                save_secs=self._config.save_checkpoints_secs,\n                save_steps=self._config.save_checkpoints_steps,\n                scaffold=scaffold,\n                save_graph_def=self._config.checkpoint_save_graph_def)\n            if self._iterations_per_training_loop.unit == 'count':\n              checkpoint_hook._set_steps_per_run(  # pylint: disable=protected-access\n                  self._iterations_per_training_loop.value)\n            chief_hooks.append(checkpoint_hook)\n          else:\n            tf.compat.v1.logging.info('Bypassing TPUEstimator hook')\n\n          tf.compat.v1.summary.scalar(model_fn_lib.LOSS_METRIC_KEY, loss)\n          with tf.control_dependencies([loss]):\n            update_ops = _sync_variables_ops(ctx)\n            if ctx.embedding_config:\n              update_ops.extend(embedding_variables_and_ops.retrieve_ops())\n\n          # Validate the TPU training graph to catch basic errors\n          _validate_tpu_training_graph(ctx)\n\n          train_op = tf.group(*update_ops)\n          graph.add_to_collection(_TPU_TRAIN_OP, train_op)\n\n          return model_fn_lib.EstimatorSpec(\n              mode,\n              loss=loss,\n              training_chief_hooks=chief_hooks,\n              training_hooks=hooks,\n              train_op=train_op,\n              scaffold=scaffold)\n\n        if mode == model_fn_lib.ModeKeys.EVAL:\n          compile_op, total_loss, host_calls, scaffold_fn, eval_hooks = (\n              _eval_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\n          if ctx.embedding_config:\n            g = tf.compat.v1.get_default_graph()\n            table_to_config_dict = (\n                ctx.embedding_config.tpu_embedding.table_to_config_dict)\n            if self._embedding_from_feature_columns:\n              embedding_variable_name_by_table, _ = (\n                  _tpu_estimator_embedding.get_full_variable_names(\n                      g, table_to_config_dict))\n            else:\n              embedding_variable_name_by_table = None\n            embedding_variables_and_ops = (\n                ctx.embedding_config.tpu_embedding.create_variables_and_ops(\n                    embedding_variable_name_by_table))\n            tpu_init_ops.extend(embedding_variables_and_ops.load_ops())\n          # scaffold_fn must be called after variables for TPU embedding has\n          # been created on CPU, as user might reinitialize those from some\n          # checkpoint within scaffold_fn.\n          scaffold = _get_scaffold(scaffold_fn)\n          iterations_per_loop_var = _create_or_get_iterations_per_loop()\n          mean_loss = tf.compat.v1.div(\n              total_loss,\n              tf.cast(iterations_per_loop_var, dtype=total_loss.dtype))\n\n          with tf.control_dependencies([mean_loss]):\n            # After TPU evaluation computation is done (the mean_loss tensor),\n            # reads all variables back from TPU and updates the eval step\n            # counter properly\n            internal_ops_to_run = _sync_variables_ops(ctx)\n            internal_ops_to_run.append(\n                _increase_eval_step_op(iterations_per_loop_var))\n\n          host_call_ret = host_calls.create_tpu_hostcall()\n          eval_metric_ops = {}\n          eval_update_ops = []\n\n          eval_metrics = host_call_ret.get('eval_metrics', {})\n          if eval_metrics:\n            # Creates a dummy metric update_op for all metrics. Estimator\n            # expects all metrics in `eval_metric_ops` have update_op and calls\n            # them one by one. The real metric update_ops are invoked in a\n            # separated thread. So, here give Estimator the dummy op for all\n            # metrics.\n            with tf.control_dependencies(internal_ops_to_run):\n              dummy_update_op = tf.no_op()\n\n            for k, v in eval_metrics.items():\n              eval_metric_ops[k] = (v[0], dummy_update_op)\n              eval_update_ops.append(v[1])\n          else:\n            # If no eval metrics are passed, create an identity node for the\n            # loss and add `internal_ops_to_run` to its dependencies. So\n            # `internal_ops_to_run` can be executed.\n            with tf.control_dependencies(internal_ops_to_run):\n              mean_loss = tf.identity(mean_loss)\n\n          if 'host_call' not in host_call_ret:\n            host_ops = []\n          else:\n            host_ops = host_call_ret['host_call']\n          hooks = [\n              TPUInfeedOutfeedSessionHook(\n                  ctx,\n                  enqueue_ops,\n                  eval_update_ops + host_ops,\n                  tpu_compile_op=compile_op,\n                  run_infeed_loop_on_coordinator=(\n                      run_infeed_loop_on_coordinator),\n                  rendezvous=self._rendezvous[mode],\n                  master=self._config.evaluation_master,\n                  session_config=self._session_config,\n                  tpu_init_ops=tpu_init_ops)\n          ] + input_hooks\n\n          if _check_add_preemption_hook(self._config.cluster):\n            hooks.extend(\n                [preempted_hook.CloudTPUPreemptedHook(self._config.cluster)])\n\n          if eval_hooks:\n            hooks.extend(eval_hooks)\n\n          return model_fn_lib.EstimatorSpec(\n              mode,\n              loss=mean_loss,\n              evaluation_hooks=hooks,\n              eval_metric_ops=eval_metric_ops,\n              scaffold=scaffold)\n\n        # Predict\n        assert mode == model_fn_lib.ModeKeys.PREDICT\n\n        (compile_op, dummy_predict_op, host_calls, scaffold_fn,\n         prediction_hooks) = _predict_on_tpu_system(ctx, model_fn_wrapper,\n                                                    dequeue_fn)\n        scaffold = _get_scaffold(scaffold_fn)\n        with tf.control_dependencies([dummy_predict_op]):\n          internal_ops_to_run = _sync_variables_ops(ctx)\n          with tf.control_dependencies(internal_ops_to_run):\n            dummy_predict_op = tf.no_op()\n\n        # In train and evaluation, the main TPU program is passed to monitored\n        # training session to run. Infeed enqueue and outfeed dequeue are\n        # executed in side threads. This is not the configuration for\n        # prediction mode.\n        #\n        # For prediction, the Estimator executes the EstimatorSpec.predictions\n        # directly and yield the element (via generator) to call site. So, the\n        # outfeed based prediction must be passed to MonitoredSession directly.\n        # Other parts of the TPU execution are organized as follows.\n        #\n        # 1. All outfeed based Tensors must be grouped with predictions Tensors\n        #    to form a single invocation. This avoid the issue we might trigger\n        #    multiple outfeeds incorrectly. To achieve this, `host_call` is\n        #    placed in control_dependencies of `stopping_signals`, and\n        #    `stopping_signals` is passed into _StoppingPredictHook, which sets\n        #    the `stopping_signals` as SessionRunArgs. MonitoredSession merges\n        #    all SessionRunArgs with the fetch in session.run together.\n        #\n        # 2. The TPU program (dummy_predict_op) and enqueue_ops (infeed Enqueue)\n        #    are grouped together. They will be launched once and only once in\n        #    side threads and they quit naturally according to the SAME stopping\n        #    condition.\n        enqueue_ops.append(dummy_predict_op)\n\n        host_call_ret = host_calls.create_tpu_hostcall()\n        if 'host_call' not in host_call_ret:\n          host_ops = []\n        else:\n          host_ops = host_call_ret['host_call']\n\n        predictions = host_call_ret['predictions']\n        _verify_cross_hosts_transfer_size(\n            predictions,\n            message=(\n                'The estimated size for TPUEstimatorSpec.predictions is too '\n                'large.'))\n        signals = host_call_ret['signals']\n\n        with tf.control_dependencies(host_ops):\n          host_ops = []  # Empty, we do do not need it anymore.\n          scalar_stopping_signal = _StopSignals.as_scalar_stopping_signal(\n              signals)\n          predictions = _PaddingSignals.slice_tensor_or_dict(\n              predictions, signals)\n\n        hooks = [\n            _StoppingPredictHook(scalar_stopping_signal),\n            TPUInfeedOutfeedSessionHookForPrediction(\n                ctx,\n                enqueue_ops,\n                host_ops,\n                rendezvous=self._rendezvous[mode],\n                tpu_compile_op=compile_op,\n                master=self._config.master,\n                session_config=self._session_config),\n        ] + input_hooks\n\n        if prediction_hooks:\n          hooks.extend(prediction_hooks)\n\n        return model_fn_lib.EstimatorSpec(\n            mode,\n            prediction_hooks=hooks,\n            predictions=predictions,\n            scaffold=scaffold)\n\n    return _model_fn\n\n\ndef _check_add_preemption_hook(cluster):\n  return (tpu_cluster_resolver.is_running_in_gce() and cluster and isinstance(\n      cluster, tf.distribute.cluster_resolver.TPUClusterResolver) and\n          cluster._cloud_tpu_client.api_available())\n\n\ndef _export_output_to_tensors(export_output):\n  \"\"\"Get a list of `Tensors` used in `export_output`.\n\n  Args:\n    export_output: an `ExportOutput` object such as `ClassificationOutput`,\n      `RegressionOutput`, or `PredictOutput`.\n\n  Returns:\n    a list of tensors used in export_output.\n\n  Raises:\n    ValueError: if `export_output` is not one of `ClassificationOutput`,\n        `RegressionOutput`, or `PredictOutput`.\n  \"\"\"\n  if isinstance(export_output, export_output_lib.ClassificationOutput):\n    return [export_output.scores, export_output.classes]\n  elif isinstance(export_output, export_output_lib.RegressionOutput):\n    return [export_output.value]\n  elif isinstance(export_output, export_output_lib.PredictOutput):\n    return list(export_output.outputs.values())\n  else:\n    raise ValueError(\n        '`export_output` must be have type `ClassificationOutput`, '\n        '`RegressionOutput`, or `PredictOutput`; got {}.'.format(export_output))\n\n\ndef _clone_export_output_with_tensors(export_output, tensors):\n  \"\"\"Clones `export_output` but with new `tensors`.\n\n  Args:\n    export_output: an `ExportOutput` object such as `ClassificationOutput`,\n      `RegressionOutput`, or `PredictOutput`.\n    tensors: a list of `Tensors` used to construct a new `export_output`.\n\n  Returns:\n    A dict similar to `export_output` but with `tensors`.\n\n  Raises:\n    ValueError: if `export_output` is not one of `ClassificationOutput`,\n        `RegressionOutput`, or `PredictOutput`.\n  \"\"\"\n  if isinstance(export_output, export_output_lib.ClassificationOutput):\n    if len(tensors) != 2:\n      raise ValueError('tensors must be of length 2; '\n                       'got {}.'.format(len(tensors)))\n    return export_output_lib.ClassificationOutput(*tensors)\n  elif isinstance(export_output, export_output_lib.RegressionOutput):\n    if len(tensors) != 1:\n      raise ValueError('tensors must be of length 1; '\n                       'got {}'.format(len(tensors)))\n    return export_output_lib.RegressionOutput(*tensors)\n  elif isinstance(export_output, export_output_lib.PredictOutput):\n    return export_output_lib.PredictOutput(\n        dict(zip(export_output.outputs.keys(), tensors)))\n  else:\n    raise ValueError(\n        '`export_output` must be have type `ClassificationOutput`, '\n        '`RegressionOutput`, or `PredictOutput`; got {}.'.format(export_output))\n\n\ndef _eval_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn):\n  \"\"\"Executes `model_fn_wrapper` multiple times on all TPU shards.\"\"\"\n  iterations_per_loop_var = _create_or_get_iterations_per_loop()\n\n  (single_tpu_eval_step, host_calls, captured_scaffold_fn, captured_eval_hooks\n  ) = model_fn_wrapper.convert_to_single_tpu_eval_step(dequeue_fn)\n\n  @tpu_function.on_device_training_loop\n  def multi_tpu_eval_steps_on_single_shard(replica_id):\n    # `tpu.split_compile_and_shard()` splits and passes input for each\n    # replica as an array. As so, correctly reshape the input to be a\n    # scalar.\n    replica_id = tf.reshape(replica_id, [])\n    with tpu_context._TPUEstimatorReplicaContext(replica_id):  # pylint: disable=protected-access\n      return training_loop.repeat(iterations_per_loop_var, single_tpu_eval_step,\n                                  [_ZERO_LOSS])\n\n  # Add input that represents id for each replica in sync so that\n  # _TPUEstimatorReplicaContext can be correctly entered during\n  # replicated computation.\n  replica_id_inputs = []\n  replica_id_inputs.append([tf.constant(i) for i in range(ctx.num_replicas)])\n\n  (\n      compile_op,\n      loss,\n  ) = tpu.split_compile_and_shard(\n      multi_tpu_eval_steps_on_single_shard,\n      inputs=replica_id_inputs,\n      num_shards=ctx.num_replicas,\n      outputs_from_all_shards=False,\n      device_assignment=ctx.device_assignment)\n\n  loss = loss[0]\n  return (compile_op, loss, host_calls, captured_scaffold_fn,\n          captured_eval_hooks.get())\n\n\ndef _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn):\n  \"\"\"Executes `model_fn_wrapper` multiple times on all TPU shards.\"\"\"\n  iterations_per_loop_var = _create_or_get_iterations_per_loop()\n\n  (single_tpu_train_step, host_call, captured_scaffold_fn,\n   captured_training_hooks) = (\n       model_fn_wrapper.convert_to_single_tpu_train_step(dequeue_fn))\n\n  @tpu_function.on_device_training_loop\n  def multi_tpu_train_steps_on_single_shard(replica_id):\n    # `tpu.split_compile_and_shard()` splits and passes input for each\n    # replica as an array. As so, correctly reshape the input to be a\n    # scalar.\n    replica_id = tf.reshape(replica_id, [])\n    with tpu_context._TPUEstimatorReplicaContext(replica_id):  # pylint: disable=protected-access\n      outputs = training_loop.while_loop(\n          lambda i, loss: i < iterations_per_loop_var,\n          lambda i, loss: [i + 1, single_tpu_train_step(i)],\n          inputs=[0, _INITIAL_LOSS])\n      return outputs[1:]\n\n  # Add input that represents id for each replica in sync so that\n  # _TPUEstimatorReplicaContext can be correctly entered during\n  # replicated computation.\n  replica_id_inputs = []\n  replica_id_inputs.append([tf.constant(i) for i in range(ctx.num_replicas)])\n\n  (compile_op, loss) = tpu.split_compile_and_shard(\n      multi_tpu_train_steps_on_single_shard,\n      inputs=replica_id_inputs,\n      num_shards=ctx.num_replicas,\n      outputs_from_all_shards=False,\n      device_assignment=ctx.device_assignment)\n\n  loss = loss[0]\n  return (compile_op, loss, host_call, captured_scaffold_fn,\n          captured_training_hooks.get())\n\n\ndef _predict_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn):\n  \"\"\"Executes `model_fn_wrapper` multiple times on all TPU shards.\"\"\"\n  (single_tpu_predict_step, host_calls, captured_scaffold_fn,\n   captured_predict_hooks\n  ) = model_fn_wrapper.convert_to_single_tpu_predict_step(dequeue_fn)\n\n  @tpu_function.on_device_training_loop\n  def multi_tpu_predict_steps_on_single_shard(replica_id):\n    # `tpu.split_compile_and_shard()` splits and passes input for each\n    # replica as an array. As so, correctly reshape the input to be a\n    # scalar.\n    replica_id = tf.reshape(replica_id, [])\n    with tpu_context._TPUEstimatorReplicaContext(replica_id):  # pylint: disable=protected-access\n\n      def cond(scalar_stopping_signal):\n        return tf.math.logical_not(\n            _StopSignals.should_stop(scalar_stopping_signal))\n\n      inputs = [_StopSignals.NON_STOPPING_SIGNAL]\n      outputs = training_loop.while_loop(\n          cond, single_tpu_predict_step, inputs=inputs, name=b'loop')\n      return outputs\n\n  # Add input that represents id for each replica in sync so that\n  # _TPUEstimatorReplicaContext can be correctly entered during\n  # replicated computation.\n  replica_id_inputs = []\n  replica_id_inputs.append([tf.constant(i) for i in range(ctx.num_replicas)])\n  (\n      compile_op,\n      dummy_predict_op,\n  ) = tpu.split_compile_and_shard(\n      multi_tpu_predict_steps_on_single_shard,\n      inputs=replica_id_inputs,\n      num_shards=ctx.num_replicas,\n      outputs_from_all_shards=False,\n      device_assignment=ctx.device_assignment)\n\n  dummy_predict_op = dummy_predict_op[0]\n  return (compile_op, dummy_predict_op, host_calls, captured_scaffold_fn,\n          captured_predict_hooks.get())\n\n\ndef _wrap_computation_in_while_loop(device, op_fn):\n  \"\"\"Wraps the ops generated by `op_fn` in tf.while_loop.\"\"\"\n\n  def computation(i):\n    with tf.control_dependencies(op_fn()):\n      return i + 1\n\n  iterations_per_loop_var = _create_or_get_iterations_per_loop()\n  # By setting parallel_iterations=1, the parallel execution in while_loop is\n  # basically turned off.\n  with tf.compat.v1.device(device):\n    iterations = tf.identity(iterations_per_loop_var)\n    return tf.compat.v1.while_loop(\n        lambda i: i < iterations,\n        computation, [tf.constant(0)],\n        parallel_iterations=1)\n\n\ndef _wrap_computation_in_while_loop_with_stopping_signals(device, op_fn):\n  \"\"\"Wraps the ops generated by `op_fn` in tf.while_loop.\"\"\"\n\n  def cond(scalar_stopping_signal):\n    return tf.math.logical_not(_StopSignals.should_stop(scalar_stopping_signal))\n\n  def computation(unused_scalar_stopping_signal):\n    return_value = op_fn()\n    execute_ops = return_value['ops']\n    signals = return_value['signals']\n    with tf.control_dependencies(execute_ops):\n      return _StopSignals.as_scalar_stopping_signal(signals)\n\n  # By setting parallel_iterations=1, the parallel execution in while_loop is\n  # basically turned off.\n  with tf.compat.v1.device(device):\n    return tf.compat.v1.while_loop(\n        cond,\n        computation, [_StopSignals.NON_STOPPING_SIGNAL],\n        parallel_iterations=1)\n\n\ndef _validate_tpu_training_graph(ctx):\n  \"\"\"Validate graph before running distributed training.\n\n  Args:\n    ctx: A `_InternalTPUContext` instance with mode.\n\n  Raises:\n    ValueError: If the graph seems invalid for running on device\n  \"\"\"\n  if control_flow_util.ENABLE_CONTROL_FLOW_V2:\n    return  # b/124241278\n\n  operations = tf.compat.v1.get_default_graph().get_operations()\n\n  # Check if there is atleast one CrossReplicaSum operation in the graph\n  # This should be introduced by using the CrossShardOptimizer wrapper\n  cross_replica_sum_ops = [\n      o for o in operations if o.type == _CROSS_REPLICA_SUM_OP\n  ]\n  if not cross_replica_sum_ops and ctx.num_replicas > 1:\n    raise ValueError(\n        'CrossShardOptimizer must be used for model training on TPUs.')\n\n\nclass _CapturedObject(object):\n  \"\"\"A placeholder to capture an object.\n\n  This is useful when we need to capture a Python object in the Tensorflow\n  control flow body function and use it outside the control flow.\n  \"\"\"\n\n  def __init__(self):\n    self._object = None\n    self._captured = False\n\n  def capture(self, o):\n    if self._captured:\n      raise RuntimeError(\n          'InternalError: Object can capture only once. Please file bug.')\n\n    self._captured = True\n    self._object = o\n\n  def get(self):\n    if not self._captured:\n      raise RuntimeError(\n          'InternalError: Object is not captured properly before `get`. '\n          'Please file bug.')\n    return self._object\n\n\ndef _get_scaffold(captured_scaffold_fn):\n  \"\"\"Retrieves the Scaffold from `captured_scaffold_fn`.\"\"\"\n  with _CapturingContext(message='Inside scaffold_fn'):\n    scaffold_fn = captured_scaffold_fn.get()\n    if scaffold_fn:\n      scaffold = scaffold_fn()\n      if scaffold is None:\n        raise ValueError(\n            'TPUEstimatorSpec.scaffold_fn returns None, which is not allowed')\n    else:\n      scaffold = None\n\n  if scaffold:\n    wrapped_finalize = scaffold.finalize\n\n    def _finalize():\n      with _CapturingContext('Inside Scaffold.finalize'):\n        wrapped_finalize()\n\n    scaffold.finalize = _finalize\n  return scaffold\n\n\nclass _CapturingContext(control_flow_ops.ControlFlowContext):\n  \"\"\"Tracks references to Tensors defined in TPU replication.\"\"\"\n\n  def __init__(self, message):\n    control_flow_ops.ControlFlowContext.__init__(self)\n    self._message = message\n\n  def to_control_flow_context_def(self, context_def, export_scope=None):\n    # pylint: disable=useless-super-delegation\n    # NOTE(slebedev): the method is required by `ControlFlowContext`.\n    super(_CapturingContext,\n          self).to_control_flow_context_def(context_def, export_scope)\n\n  def AddOp(self, op):  # pylint: disable=invalid-name\n    for c in op.inputs:\n      if tpu._TPU_REPLICATE_ATTR in c.op.node_def.attr:  # pylint: disable=protected-access\n        raise ValueError('{}: Op {} depends on TPU computation {}, '\n                         'which is not allowed.'.format(self._message, op, c))\n\n  def AddValue(self, value):\n    self.AddOp(value.op)\n    return value\n\n  def __enter__(self):\n    # pylint: disable=protected-access\n    self._g = tf.compat.v1.get_default_graph()\n    self._old = self._g._get_control_flow_context()\n    self._g._set_control_flow_context(self)\n    # pylint: enable=protected-access\n\n  def __exit__(self, _, __, ___):  # pylint: disable=invalid-name\n    self._g._set_control_flow_context(self._old)  # pylint: disable=protected-access\n\n\nclass _Inputs(object):\n  \"\"\"A data structure representing the input_fn returned values.\n\n  This also supports the returned value from input_fn as `Dataset`.\n  \"\"\"\n\n  def __init__(self, features=None, labels=None, dataset=None, signals=None):\n    if dataset is not None and (features is not None or labels is not None or\n                                signals is not None):\n      raise RuntimeError('Internal Error: Either (features and labels) or '\n                         'dataset should be provided, not both. Please file '\n                         'bug')\n\n    self._features = features\n    self._labels = labels\n    self._signals = signals\n\n    self._dataset = dataset\n    self._iterator = None\n\n  @staticmethod\n  def from_input_fn(return_values):\n    \"\"\"Returns an `_Inputs` instance according to `input_fn` return value.\"\"\"\n    if isinstance(return_values, tf.compat.v2.data.Dataset):\n      dataset = return_values\n      return _Inputs(dataset=dataset)\n\n    features, labels = _Inputs._parse_inputs(return_values)\n    return _Inputs(features, labels)\n\n  @staticmethod\n  def _parse_inputs(return_values):\n    if isinstance(return_values, tuple):\n      features, labels = return_values\n    else:\n      features, labels = return_values, None\n    return features, labels\n\n  @property\n  def is_dataset(self):\n    \"\"\"Returns True if the return value from input_fn is Dataset.\"\"\"\n    return self._dataset is not None\n\n  def dataset_initializer(self):\n    \"\"\"Returns the dataset's initializer.\n\n    The initializer must be run before calling `features_and_labels`.\n    \"\"\"\n    self._iterator = tf.compat.v1.data.make_initializable_iterator(\n        self._dataset)\n    return self._iterator.initializer\n\n  def features_and_labels(self):\n    \"\"\"Gets `features` and `labels`.\"\"\"\n    if self.is_dataset:\n      if self._iterator is None:\n        raise RuntimeError('Internal error: Must run dataset_initializer '\n                           'before calling features_and_labels(). Please file '\n                           'a bug!')\n      return _Inputs._parse_inputs(self._iterator.get_next())\n\n    return (self._features, self._labels)\n\n  def signals(self):\n    return self._signals\n\n  @property\n  def dataset(self):\n    return self._dataset\n\n\nclass _InputsWithStoppingSignals(_Inputs):\n  \"\"\"Inputs with `_StopSignals` inserted into the dataset.\"\"\"\n\n  def __init__(self,\n               dataset,\n               batch_size,\n               add_padding=False,\n               num_invocations_per_step=1):\n\n    assert dataset is not None\n    user_provided_dataset = dataset.map(\n        _InputsWithStoppingSignals.insert_stopping_signal(\n            stop=False, batch_size=batch_size, add_padding=add_padding))\n    if num_invocations_per_step == 1:\n      final_batch_dataset = dataset.take(1).map(\n          _InputsWithStoppingSignals.insert_stopping_signal(\n              stop=True, batch_size=batch_size, add_padding=add_padding))\n    else:\n      # We append (2 * num_invocations_per_step - 1) batches for exhausting the\n      # user_provided_dataset and stop properly.\n      # For example, if num_invocations_per_step is 2, we append 3 additional\n      # padding batches: b1, b2, b3.\n      # If user_provided_dataset contains two batches: a1, a2\n      # Step 1: [a1, a2]\n      # Step 2: [b1, b2] -> STOP\n      # If user_provided_dataset contains three batches: a1, a2, a3.\n      # The training loops:\n      # Step 1: [a1, a2]\n      # Step 2: [a3, b1]\n      # Step 3: [b2, b3] -> STOP.\n      final_batch_dataset = dataset.take(1).map(\n          _InputsWithStoppingSignals.insert_stopping_signal(\n              stop=True, batch_size=batch_size, add_padding=add_padding))\n      final_batch_dataset = final_batch_dataset.repeat(\n          2 * num_invocations_per_step - 1)\n\n      def _set_mask(data_dict):\n        signals = data_dict['signals']\n        signals['padding_mask'] = tf.compat.v1.ones_like(\n            signals['padding_mask'])\n        data_dict['signals'] = signals\n        return data_dict\n\n      # Mask out the extra batch.\n      final_batch_dataset = final_batch_dataset.map(_set_mask)\n\n    dataset = user_provided_dataset.concatenate(final_batch_dataset).prefetch(2)\n\n    super(_InputsWithStoppingSignals, self).__init__(dataset=dataset)\n    self._current_inputs = None\n\n  def features_and_labels(self):\n    if self._current_inputs is not None:\n      raise RuntimeError(\n          'Internal Error: The previous inputs have not been properly '\n          'consumed. First call features_and_labels, then call signals.')\n\n    inputs_with_signals = self._iterator.get_next()\n    features = inputs_with_signals['features']\n    labels = inputs_with_signals.get('labels')\n\n    self._current_inputs = inputs_with_signals\n    return features, labels\n\n  def signals(self):\n    \"\"\"Returns the `Signals` from `_Inputs`.\"\"\"\n    if self._current_inputs is None:\n      raise RuntimeError(\n          'Internal Error: The current inputs have not been properly '\n          'generated. First call features_and_labels, then call signals.')\n    signals = self._current_inputs['signals']\n    self._current_inputs = None\n    return signals\n\n  @staticmethod\n  def insert_stopping_signal(stop, batch_size, add_padding=False):\n    \"\"\"Inserts stopping_signal into dataset via _map_fn.\n\n    Here we change the data structure in the dataset, such that the return value\n    is a dictionary now and `features`, `labels`, and `signals` are three\n    distinguished keys in that dict. This provides a better structure, which\n    eases the process to decompose the inputs (see `features_and_labels`).\n\n    Args:\n      stop: bool, state of current stopping signals.\n      batch_size: int, batch size.\n      add_padding: bool, whether to pad the tensor to full batch size.\n\n    Returns:\n      A map_fn passed to dataset.map API.\n    \"\"\"\n\n    def _map_fn(*args):\n      \"\"\"The map fn to insert signals.\"\"\"\n      if len(args) == 1:\n        # Unpack the single Tensor/dict argument as features. This is required\n        # for the input_fn returns no labels.\n        args = args[0]\n      features, labels = _Inputs._parse_inputs(args)\n      new_input_dict = {}\n\n      if add_padding:\n        padding_mask, features, labels = (\n            _PaddingSignals.pad_features_and_labels(features, labels,\n                                                    batch_size))\n\n        new_input_dict['features'] = features\n        if labels is not None:\n          new_input_dict['labels'] = labels\n\n      else:\n        new_input_dict['features'] = features\n        if labels is not None:\n          new_input_dict['labels'] = labels\n        padding_mask = None\n\n      new_input_dict['signals'] = _StopSignals(\n          stop=stop, batch_size=batch_size,\n          padding_mask=padding_mask).as_dict()\n\n      return new_input_dict\n\n    return _map_fn\n\n\nclass _StopSignals(object):\n  \"\"\"Signals class holding all logic to handle TPU stopping condition.\"\"\"\n\n  NON_STOPPING_SIGNAL = False\n  STOPPING_SIGNAL = True\n\n  def __init__(self, stop, batch_size, padding_mask=None):\n    self._stop = stop\n    self._batch_size = batch_size\n    self._padding_mask = padding_mask\n\n  def as_dict(self):\n    \"\"\"Returns the signals as Python dict.\"\"\"\n    shape = [self._batch_size, 1]\n    dtype = tf.dtypes.bool\n\n    if self._stop:\n      stopping = tf.ones(shape=shape, dtype=dtype)\n    else:\n      stopping = tf.zeros(shape=shape, dtype=dtype)\n\n    signals = {'stopping': stopping}\n    if self._padding_mask is not None:\n      signals['padding_mask'] = self._padding_mask\n    return signals\n\n  @staticmethod\n  def as_scalar_stopping_signal(signals):\n    return tf.identity(signals['stopping'][0][0])\n\n  @staticmethod\n  def should_stop(scalar_stopping_signal):\n    \"\"\"Detects whether scalar_stopping_signal indicates stopping.\"\"\"\n    if isinstance(scalar_stopping_signal, tf.Tensor):\n      # STOPPING_SIGNAL is a constant True. Here, the logical_and is just the TF\n      # way to express the bool check whether scalar_stopping_signal is True.\n      return tf.math.logical_and(scalar_stopping_signal,\n                                 _StopSignals.STOPPING_SIGNAL)\n    else:\n      # For non Tensor case, it is used in SessionRunHook. So, we cannot modify\n      # the graph anymore. Here, we use pure Python.\n      return bool(scalar_stopping_signal)\n\n\nclass _PaddingSignals(object):\n  \"\"\"Signals class holding all logic to handle padding.\"\"\"\n\n  @staticmethod\n  def pad_features_and_labels(features, labels, batch_size):\n    \"\"\"Pads out the batch dimension of features and labels.\"\"\"\n    real_batch_size = tf.compat.v1.shape(\n        _PaddingSignals._find_any_tensor(features))[0]\n\n    batch_size_tensor = tf.constant(batch_size, tf.dtypes.int32)\n\n    check_greater = tf.compat.v1.debugging.assert_greater_equal(\n        batch_size_tensor,\n        real_batch_size,\n        data=(batch_size_tensor, real_batch_size),\n        message='The real batch size should not be greater than batch_size.')\n\n    with tf.control_dependencies([check_greater]):\n      missing_count = batch_size_tensor - real_batch_size\n\n    def pad_single_tensor(tensor):\n      \"\"\"Pads out the batch dimension of a tensor to the complete batch_size.\"\"\"\n      rank = len(tensor.shape)\n      assert rank > 0\n      padding = tf.stack([[0, missing_count]] + [[0, 0]] * (rank - 1))\n      padded_shape = (batch_size,) + tuple(tensor.shape[1:])\n      padded_tensor = tf.compat.v1.pad(tensor, padding)\n      padded_tensor.set_shape(padded_shape)\n      return padded_tensor\n\n    def nest_pad(tensor_or_dict):\n      return tf.nest.map_structure(pad_single_tensor, tensor_or_dict)\n\n    features = nest_pad(features)\n    if labels is not None:\n      labels = nest_pad(labels)\n\n    padding_mask = _PaddingSignals._padding_mask(real_batch_size, missing_count,\n                                                 batch_size)\n\n    return padding_mask, features, labels\n\n  @staticmethod\n  def slice_tensor_or_dict(tensor_or_dict, signals):\n    \"\"\"Slice the real Tensors according to padding mask in signals.\"\"\"\n\n    padding_mask = signals['padding_mask']\n    batch_size = tf.compat.v1.shape(padding_mask)[0]\n\n    def verify_batch_size(tensor):\n      check_batch_size = tf.math.equal(batch_size, tensor.shape[0])\n      with tf.control_dependencies([check_batch_size]):\n        return tf.identity(tensor)\n\n    def slice_single_tensor(tensor):\n      rank = len(tensor.shape)\n      assert rank > 0\n      real_batch_size = batch_size - tf.math.reduce_sum(padding_mask)\n      return verify_batch_size(tensor)[0:real_batch_size]\n\n    # As we split the Tensors to all TPU cores and concat them back, it is\n    # important to ensure the real data is placed before padded ones, i.e.,\n    # order is preserved. By that, the sliced padding mask should have all 0's.\n    # If this assertion failed, # the slice logic here would not hold.\n    sliced_padding_mask = slice_single_tensor(padding_mask)\n    assert_padding_mask = tf.math.equal(\n        tf.math.reduce_sum(sliced_padding_mask), 0)\n\n    with tf.control_dependencies([assert_padding_mask]):\n      should_stop = _StopSignals.should_stop(\n          _StopSignals.as_scalar_stopping_signal(signals))\n\n    is_full_batch = tf.math.equal(tf.math.reduce_sum(padding_mask), 0)\n\n    def slice_fn(tensor):\n      # If the current batch is full batch or part of stopping signals, we do\n      # not need to slice to save performance.\n      return tf.compat.v1.cond(\n          tf.math.logical_or(should_stop, is_full_batch),\n          (lambda: verify_batch_size(tensor)),\n          (lambda: slice_single_tensor(tensor)))\n\n    return tf.nest.map_structure(slice_fn, tensor_or_dict)\n\n  @staticmethod\n  def _find_any_tensor(batch_features):\n    tensors = [\n        x for x in tf.nest.flatten(batch_features) if isinstance(x, tf.Tensor)\n    ]\n    if not tensors:\n      raise ValueError('Cannot find any Tensor in features dict.')\n    return tensors[0]\n\n  @staticmethod\n  def _padding_mask(real_batch_size, missing_count, batch_size):\n    padding_mask = tf.concat([\n        tf.zeros((real_batch_size,), dtype=tf.dtypes.int32),\n        tf.ones((missing_count,), dtype=tf.dtypes.int32)\n    ],\n                             axis=0)\n    padding_mask.set_shape((batch_size,))\n    return padding_mask\n\n\ndef _verify_cross_hosts_transfer_size(tensor_dict, message):\n  total_size = 0\n  tensor_structure = {}\n  for key, tensor in tensor_dict.items():\n    shape = tensor.shape\n    size = np.product(shape) * tensor.dtype.size\n    tensor_structure[key] = shape\n    total_size += size\n  if total_size >= _ONE_GIGABYTE:\n    raise ValueError(\n        '{} The transfer size is larger than the protobuf limit. Please '\n        'consider to use Tensors with smaller shapes or reduce batch '\n        'size. Given:\\n'\n        '{}'.format(\n            message, '\\n'.join([\n                ' -- Key: {}, Shape: {}'.format(k, v)\n                for k, v in tensor_structure.items()\n            ])))\n\n\ndef _add_item_to_params(params, key, value):\n  \"\"\"Adds a new item into `params`.\"\"\"\n  if hasattr(params, 'set_hparam'):\n    # For HParams, we need to use special API.\n    if key in params:\n      params.set_hparam(key, value)\n    else:\n      params.add_hparam(key, value)\n  else:\n    # Now params is Python dict.\n    params[key] = value\n\n\ndef export_estimator_savedmodel(estimator,\n                                export_dir_base,\n                                serving_input_receiver_fn,\n                                assets_extra=None,\n                                as_text=False,\n                                checkpoint_path=None):\n  \"\"\"Export `Estimator` trained model for TPU inference.\n\n  Args:\n    estimator: `Estimator` with which model has been trained.\n    export_dir_base: A string containing a directory in which to create\n      timestamped subdirectories containing exported SavedModels.\n    serving_input_receiver_fn: A function that takes no argument and returns a\n      `ServingInputReceiver` or `TensorServingInputReceiver`.\n    assets_extra: A dict specifying how to populate the assets.extra directory\n      within the exported SavedModel, or `None` if no extra assets are needed.\n    as_text: whether to write the SavedModel proto in text format.\n    checkpoint_path: The checkpoint path to export.  If `None` (the default),\n      the most recent checkpoint found within the model directory is chosen.\n\n  Returns:\n    The string path to the exported directory.\n  \"\"\"\n  # `TPUEstimator` requires `tpu_config.RunConfig`, so we cannot use\n  # `estimator.config`.\n  config = tpu_config.RunConfig(model_dir=estimator.model_dir)\n  est = TPUEstimator(\n      estimator._model_fn,  # pylint: disable=protected-access\n      config=config,\n      params=estimator.params,\n      use_tpu=True,\n      train_batch_size=2048,  # Does not matter.\n      eval_batch_size=2048,  # Does not matter.\n  )\n  return est.export_saved_model(export_dir_base, serving_input_receiver_fn,\n                                assets_extra, as_text, checkpoint_path)\n\n\ndef model_fn_inference_on_tpu(model_fn,\n                              features,\n                              labels=None,\n                              config=None,\n                              params=None,\n                              batch_config=None):\n  \"\"\"Convenience wrapper for export_saved_model API v2 for a model_fn.\n  WARNING:THIS METHOD IS DEPRECATED AND NOT PART OF THE APIS.\n\n  Make sure to set\n  `export_saved_model_api_version=tpu_estimator.ExportSavedModelApiVersion.V2`\n  when initializing TPUEstimator (default API version is V1). This is because\n  1) `tpu.rewrite` (or `tpu.compile`) shouldn't be called in a nested way\n      (otherwise validation will throw error like\n      \"NotImplementedError: tpu_shard_context cannot be nested.\")\n  2) When using V1 API, Estimator calls `tpu.rewrite` so\n     using `model_fn_inference_on_tpu` will trigger a nested call.\n     When using V2 API, users of Estimator needs to call `tpu.rewrite` (which\n     the wrapper does).\n\n  It attempts to execute the entire model function on the TPU for prediction.\n  Note that this does not support features which are SparseTensors. If you have\n  SparseTensor features, consider partitioning your model function further and\n  use inference_on_tpu.\n\n  Args:\n    model_fn: the model_fn for which we want to inference on TPU.\n    features: a tensor or dict of tensors, serves as the feature inputs to the\n      model.\n    labels: a tensor or dict of tensors, serves as the labels inputs to the\n      model.\n    config: auxiliary config to the Estimator.\n    params: hparams that we want to pass to the model_fn.\n    batch_config: a named tuple to wrap the inference batching configuration\n      inputs.\n\n  Returns:\n    An EstimatorSpec containing the outputs in export_outputs and predictions.\n  \"\"\"\n  computation, capture = _build_computation_for_inference(\n      model_fn, labels, config, params)\n  tensors = call_computation(features, computation, batch_config=batch_config)\n  estimator_spec, export_outputs_dict, predictions_dict, none_indices = (\n      capture.get())\n  predictions_list = tensors[:len(predictions_dict)]\n  export_outputs_list_without_none = tensors[len(predictions_dict):]\n\n  # Reinsert `None`s which we've taken out in\n  # `_build_computation_for_inference()`.\n  export_outputs_list = []\n  while none_indices or export_outputs_list_without_none:\n    if none_indices and none_indices[0] == len(export_outputs_list):\n      export_outputs_list.append(None)\n      none_indices.pop(0)\n    else:\n      export_outputs_list.append(export_outputs_list_without_none.pop(0))\n\n  # Reconstruct `export_outputs` with updated tensors.\n  new_export_outputs_dict = tf.nest.pack_sequence_as(export_outputs_dict,\n                                                     export_outputs_list)\n  export_outputs = estimator_spec.export_outputs\n  new_export_outputs = collections.OrderedDict(\n      (k, _clone_export_output_with_tensors(export_outputs[k], v))\n      for k, v in six.iteritems(new_export_outputs_dict))\n  # Reconstruct `predictions` with updated tensors.\n  new_predictions = tf.nest.pack_sequence_as(predictions_dict, predictions_list)\n  if (len(new_predictions) == 1 and\n      _KEY_WHEN_PREDICTIONS_IS_A_TENSOR in new_predictions):\n    new_predictions = new_predictions[_KEY_WHEN_PREDICTIONS_IS_A_TENSOR]\n\n  return estimator_spec._replace(\n      export_outputs=new_export_outputs, predictions=new_predictions)\n\n\ndef _build_computation_for_inference(model_fn, labels, config, params):\n  \"\"\"Builds the computation with calls the model_fn for inference.\"\"\"\n  capture = _CapturedObject()\n\n  def computation(computation_input):\n    \"\"\"Computation to be passed to `TPUPartitionedCall()`.\"\"\"\n    tpu_computation, tpu_capture = _build_tpu_computation_for_inference(\n        model_fn, computation_input, labels, config, params)\n\n    tensors_on_cpu = tf.compat.v1.tpu.rewrite(tpu_computation)\n    tpu.prune_unconnected_ops_from_xla(tf.compat.v1.get_default_graph())\n\n    (estimator_spec, export_outputs_dict, export_outputs_list,\n     predictions_dict) = (\n         tpu_capture.get())\n    predictions_list = tensors_on_cpu[:len(predictions_dict)]\n    export_outputs_tpu_on_cpu_list = tensors_on_cpu[len(predictions_dict):]\n\n    # Reconstruct tensors used in export_outputs, with TPU tensors replaced\n    # with their CPU counterpart returned from `rewrite_for_inference()`.\n    # `function.Defun()` does not like `None`s in return values, so we leave\n    # `None`s out but record their positions for later reconstruction.\n    export_outputs_list_without_none = []\n    none_indices = []\n    for i, t in enumerate(export_outputs_list):\n      if t is None:\n        none_indices.append(i)\n      else:\n        export_outputs_list_without_none.append(\n            export_outputs_tpu_on_cpu_list.pop(0))\n\n    capture.capture(\n        (estimator_spec, export_outputs_dict, predictions_dict, none_indices))\n    return predictions_list + export_outputs_list_without_none\n\n  return computation, capture\n\n\ndef _build_tpu_computation_for_inference(model_fn, features, labels, config,\n                                         params):\n  \"\"\"Builds the TPU computation for inference on TPU.\"\"\"\n  capture = _CapturedObject()\n\n  def computation():\n    \"\"\"Compute tpu tensors used in export_outputs.\n\n    Passed to rewrite_for_inference so that model_fn will be called under\n    the rewriting contexts. Only tpu tensors are returned, but export_outputs\n    and scaffold are captured.\n\n    Returns:\n       A list of Tensors used in export_outputs and not marked for\n       outside_compilation.\n    \"\"\"\n    # We should only call model fn once and it should be inside `computation`\n    # so that building the graph will happen under `rewrite_for_inference`.\n\n    model_fn_args = function_utils.fn_args(model_fn)\n    kwargs = {}\n    # Makes deep copy with `config` and params` in case user mutates them.\n    if 'labels' in model_fn_args:\n      kwargs['labels'] = labels\n    if 'mode' in model_fn_args:\n      kwargs['mode'] = model_fn_lib.ModeKeys.PREDICT\n    if 'config' in model_fn_args:\n      kwargs['config'] = config\n    if 'params' in model_fn_args:\n      kwargs['params'] = params\n    estimator_spec = model_fn(features, **kwargs)\n\n    # We pick the TPU tensors out from `export_output` and later return them\n    # from `computation` for rewriting.\n    export_outputs_dict = collections.OrderedDict(\n        (k, _export_output_to_tensors(v))\n        for k, v in six.iteritems(estimator_spec.export_outputs))\n    export_outputs_list = tf.nest.flatten(export_outputs_dict)\n    export_outputs_tpu_list = [t for t in export_outputs_list if t is not None]\n\n    if isinstance(estimator_spec.predictions, dict):\n      predictions_dict = collections.OrderedDict(\n          (k, v) for k, v in six.iteritems(estimator_spec.predictions))\n    else:\n      predictions_dict = {\n          _KEY_WHEN_PREDICTIONS_IS_A_TENSOR: estimator_spec.predictions\n      }\n    predictions_list = tf.nest.flatten(predictions_dict)\n\n    # We cannot return everything we want through the return values, so\n    # capture the rest here for later use.\n    capture.capture((estimator_spec, export_outputs_dict, export_outputs_list,\n                     predictions_dict))\n    return predictions_list + export_outputs_tpu_list\n\n  return computation, capture\n\n\ndef inference_on_tpu(computation,\n                     inputs_to_tpu,\n                     num_batch_threads,\n                     max_batch_size,\n                     batch_timeout_micros,\n                     allowed_batch_sizes=None,\n                     max_enqueued_batches=100):\n  \"\"\"Convenient wrapper for export_saved_model API v2 to wrap TPU computation.\n\n  WARNING: THIS METHOD IS DEPRECATED AND NOT PART OF THE APIS.\n\n  Make sure to set\n  `export_saved_model_api_version=tpu_estimator.ExportSavedModelApiVersion.V2`\n  when initializing TPUEstimator (default API version is V1). This is because\n  1) `tpu.rewrite` (or `tpu.compile`) shouldn't be called in a nested way\n      (otherwise validation will throw error like\n      \"NotImplementedError: tpu_shard_context cannot be nested.\")\n  2) When using V1 API, Estimator calls `tpu.rewrite` so\n     using `model_fn_inference_on_tpu` will trigger a nested call.\n     When using V2 API, users of Estimator needs to call `tpu.rewrite` (which\n     the wrapper does).\n\n  It puts computation on TPU, add batching around it and round robin computation\n  between TPU cores.\n\n  See tpu_estimator_test.py for an example.\n\n  Args:\n    computation: computation to be put on TPU, which takes inputs_to_tpu as\n      arguments.\n    inputs_to_tpu: a list of tensors as input to computation.\n    num_batch_threads: Number of scheduling threads for processing batches of\n      work. Determines the number of batches processed in parallel.\n    max_batch_size: Batch sizes will never be bigger than this. If None or 0,\n      no batching will done.\n    batch_timeout_micros: Maximum number of microseconds to wait before\n      outputting an incomplete batch.\n    allowed_batch_sizes: Optional list of allowed batch sizes. If left empty,\n      does nothing. Otherwise, supplies a list of batch sizes, causing the op to\n      pad batches up to one of those sizes. The entries must increase\n      monotonically, and the final entry must equal max_batch_size.\n    max_enqueued_batches: The maximum depth of the batch queue. Defaults to 100.\n\n  Returns:\n    The unbatched computation output Tensors.\n  \"\"\"\n\n  def _tpu_call(args):\n    \"\"\"Function to either call or feed into BatchFunction.\"\"\"\n\n    @function.Defun(capture_resource_var_by_value=False)\n    def tpu_computation():\n      \"\"\"Function to feed into the TPUPartitionedCallOp.\"\"\"\n      tensors_on_cpu = tf.compat.v1.tpu.rewrite(computation, args)\n      tpu.prune_unconnected_ops_from_xla(tf.compat.v1.get_default_graph())\n      return tensors_on_cpu\n\n    return tpu_functional.TPUPartitionedCall(\n        args=tpu_computation.captured_inputs,\n        device_ordinal=tpu_ops.tpu_ordinal_selector(),\n        Tout=[o.type for o in tpu_computation.definition.signature.output_arg],\n        f=tpu_computation)\n\n  if not max_batch_size:\n    return _tpu_call(inputs_to_tpu)\n\n  @tf.nondifferentiable_batch_function(num_batch_threads, max_batch_size,\n                                       batch_timeout_micros,\n                                       allowed_batch_sizes,\n                                       max_enqueued_batches)\n  def batched_tpu_computation(*args):\n    \"\"\"Function to feed into the BatchOp.\"\"\"\n    return _tpu_call(args)\n\n  return batched_tpu_computation(*inputs_to_tpu)\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/examples", "file_path": "tensorflow_examples/lite/model_maker/core/data_util/dataloader_test.py", "content": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core import test_util\nfrom tensorflow_examples.lite.model_maker.core.data_util import dataloader\n\n\nclass DataLoaderTest(tf.test.TestCase):\n\n  def test_split(self):\n    ds = tf.data.Dataset.from_tensor_slices([[0, 1], [1, 1], [0, 0], [1, 0]])\n    data = dataloader.DataLoader(ds, 4)\n    train_data, test_data = data.split(0.5)\n\n    self.assertEqual(len(train_data), 2)\n    self.assertIsInstance(train_data, dataloader.DataLoader)\n    self.assertIsInstance(test_data, dataloader.DataLoader)\n    for i, elem in enumerate(train_data.gen_dataset()):\n      self.assertTrue((elem.numpy() == np.array([i, 1])).all())\n\n    self.assertEqual(len(test_data), 2)\n    for i, elem in enumerate(test_data.gen_dataset()):\n      self.assertTrue((elem.numpy() == np.array([i, 0])).all())\n\n  def test_len(self):\n    size = 4\n    ds = tf.data.Dataset.from_tensor_slices([[0, 1], [1, 1], [0, 0], [1, 0]])\n    data = dataloader.DataLoader(ds, size)\n    self.assertEqual(len(data), size)\n\n  def test_gen_dataset(self):\n    input_dim = 8\n    data = test_util.get_dataloader(\n        data_size=2, input_shape=[input_dim], num_classes=2)\n\n    ds = data.gen_dataset()\n    self.assertEqual(len(ds), 2)\n    for (feature, label) in ds:\n      self.assertTrue((tf.shape(feature).numpy() == np.array([1, 8])).all())\n      self.assertTrue((tf.shape(label).numpy() == np.array([1])).all())\n\n    ds2 = data.gen_dataset(batch_size=2)\n    self.assertEqual(len(ds2), 1)\n    for (feature, label) in ds2:\n      self.assertTrue((tf.shape(feature).numpy() == np.array([2, 8])).all())\n      self.assertTrue((tf.shape(label).numpy() == np.array([2])).all())\n\n    ds3 = data.gen_dataset(batch_size=2, is_training=True, shuffle=True)\n    self.assertEqual(ds3.cardinality(), 1)\n    for (feature, label) in ds3.take(10):\n      self.assertTrue((tf.shape(feature).numpy() == np.array([2, 8])).all())\n      self.assertTrue((tf.shape(label).numpy() == np.array([2])).all())\n\n\nclass ClassificationDataLoaderTest(tf.test.TestCase):\n\n  def test_split(self):\n\n    class MagicClassificationDataLoader(dataloader.ClassificationDataLoader):\n\n      def __init__(self, dataset, size, index_to_label, value):\n        super(MagicClassificationDataLoader,\n              self).__init__(dataset, size, index_to_label)\n        self.value = value\n\n      def split(self, fraction):\n        return self._split(fraction, self.index_to_label, self.value)\n\n    # Some dummy inputs.\n    magic_value = 42\n    num_classes = 2\n    index_to_label = (False, True)\n\n    # Create data loader from sample data.\n    ds = tf.data.Dataset.from_tensor_slices([[0, 1], [1, 1], [0, 0], [1, 0]])\n    data = MagicClassificationDataLoader(ds, len(ds), index_to_label,\n                                         magic_value)\n\n    # Train/Test data split.\n    fraction = .25\n    train_data, test_data = data.split(fraction)\n\n    # `split` should return instances of child DataLoader.\n    self.assertIsInstance(train_data, MagicClassificationDataLoader)\n    self.assertIsInstance(test_data, MagicClassificationDataLoader)\n\n    # Make sure number of entries are right.\n    self.assertEqual(len(train_data.gen_dataset()), len(train_data))\n    self.assertEqual(len(train_data), fraction * len(ds))\n    self.assertEqual(len(test_data), len(ds) - len(train_data))\n\n    # Make sure attributes propagated correctly.\n    self.assertEqual(train_data.num_classes, num_classes)\n    self.assertEqual(test_data.index_to_label, index_to_label)\n    self.assertEqual(train_data.value, magic_value)\n    self.assertEqual(test_data.value, magic_value)\n\n\nif __name__ == '__main__':\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/examples", "file_path": "tensorflow_examples/lite/model_maker/core/task/model_spec/audio_spec_test.py", "content": "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for audio specs.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport unittest\n\nimport numpy as np\nfrom packaging import version\nimport tensorflow.compat.v2 as tf\nfrom tensorflow_examples.lite.model_maker.core.task import configs\nfrom tensorflow_examples.lite.model_maker.core.task import model_util\nfrom tensorflow_examples.lite.model_maker.core.task.model_spec import audio_spec\n\n\ndef _gen_dataset(spec, total_samples, num_classes, batch_size, seed):\n\n  def fill_shape(new_shape):\n\n    @tf.function\n    def fn(value):\n      return tf.cast(tf.fill(dims=new_shape, value=value), tf.float32)\n\n    return fn\n\n  wav_ds = tf.data.experimental.RandomDataset(seed=seed).take(total_samples)\n  wav_ds = wav_ds.map(fill_shape([\n      spec.target_sample_rate,\n  ]))\n\n  labels = tf.data.Dataset.from_tensor_slices(\n      np.random.randint(low=0, high=num_classes,\n                        size=total_samples).astype('int32'))\n  dataset = tf.data.Dataset.zip((wav_ds, labels))\n  dataset = spec.preprocess_ds(dataset)\n\n  @tf.function\n  def _one_hot_encoding_label(wav, label):\n    return wav, tf.one_hot(label, num_classes)\n\n  dataset = dataset.map(_one_hot_encoding_label)\n\n  dataset = dataset.batch(batch_size)\n\n  return dataset\n\n\nclass BaseSpecTest(tf.test.TestCase):\n\n  def testEnsureVersion(self):\n    valid_versions = ['2.5.0', '2.5.0rc1', '2.6']\n    invalid_versions = [\n        '2.4.1',\n    ]\n    specs = [audio_spec.YAMNetSpec, audio_spec.BrowserFFTSpec]\n\n    tmp_version_fn = audio_spec._get_tf_version\n    for spec in specs:\n      for valid_version in valid_versions:\n        audio_spec._get_tf_version = lambda: valid_version  # pylint: disable=cell-var-from-loop\n        spec()\n\n      for valid_version in invalid_versions:\n        audio_spec._get_tf_version = lambda: valid_version  # pylint: disable=cell-var-from-loop\n        with self.assertRaisesRegexp(RuntimeError, '2.5.0'):\n          spec()\n\n    audio_spec._get_tf_version = tmp_version_fn\n\n\nclass BaseTest(tf.test.TestCase):\n\n  def _train_and_export(self,\n                        spec,\n                        num_classes,\n                        filename,\n                        expected_model_size,\n                        quantization_config=None,\n                        training=True):\n    dataset = _gen_dataset(\n        spec, total_samples=10, num_classes=num_classes, batch_size=2, seed=100)\n    model = spec.create_model(num_classes)\n    epochs = 1 if training else 0\n    spec.run_classifier(\n        model, epochs=epochs, train_ds=dataset, validation_ds=None)\n\n    tflite_filepath = os.path.join(self.get_temp_dir(), filename)\n    spec.export_tflite(\n        model,\n        tflite_filepath,\n        index_to_label=['label_{}'.format(i) for i in range(num_classes)],\n        quantization_config=quantization_config)\n\n    self.assertNear(\n        os.path.getsize(tflite_filepath), expected_model_size, 1000 * 1000)\n\n    return tflite_filepath\n\n\n@unittest.skipIf(\n    version.parse(tf.__version__) < version.parse('2.5'),\n    'Audio Classification requires TF 2.5 or later')\nclass YAMNetSpecTest(BaseTest):\n\n  def _test_preprocess(self, input_shape, input_count, output_shape,\n                       output_count):\n    spec = audio_spec.YAMNetSpec()\n    wav_ds = tf.data.Dataset.from_tensor_slices([tf.ones(input_shape)] *\n                                                input_count)\n    label_ds = tf.data.Dataset.range(input_count).map(\n        lambda x: tf.cast(x, tf.int32))\n\n    ds = tf.data.Dataset.zip((wav_ds, label_ds))\n    ds = spec.preprocess_ds(ds)\n\n    chunks = output_count // input_count\n\n    cnt = 0\n    for item, label in ds:\n      cnt += 1\n    self.assertEqual(cnt, output_count)\n\n    # More thorough checks.\n    cnt = 0\n    for item, label in ds:\n      self.assertEqual(output_shape, item.shape)\n      self.assertEqual(label, cnt // chunks)\n      cnt += 1\n\n  def test_preprocess(self):\n    # No padding on the input.\n    self._test_preprocess(\n        input_shape=(10,), input_count=2, output_shape=(1024,), output_count=0)\n    # Split the input data into trunks\n    self._test_preprocess(\n        input_shape=(16000 * 2,),\n        input_count=2,\n        output_shape=(1024,),\n        output_count=6)\n    self._test_preprocess(\n        input_shape=(15600,),\n        input_count=1,\n        output_shape=(1024,),\n        output_count=1)\n\n  def test_create_model(self):\n    # Make sure that there is no naming conflicts in the graph.\n    spec = audio_spec.YAMNetSpec()\n    model = spec.create_model(10)\n    model = spec.create_model(10)\n    model = spec.create_model(10)\n    self.assertEqual(model.input_shape, (None, 1024))\n    self.assertEqual(model.output_shape, (None, 10))\n\n  def test_yamnet_two_heads(self):\n    tflite_path = self._train_and_export(\n        audio_spec.YAMNetSpec(keep_yamnet_and_custom_heads=True),\n        num_classes=2,\n        filename='two_heads.tflite',\n        expected_model_size=15 * 1000 * 1000)\n    self.assertEqual(\n        2, len(model_util.get_lite_runner(tflite_path).output_details))\n    self.assertAllEqual(\n        [1, 521],\n        model_util.get_lite_runner(tflite_path).output_details[0]['shape'])\n    self.assertAllEqual(\n        [1, 2],\n        model_util.get_lite_runner(tflite_path).output_details[1]['shape'])\n    self.assertEqual(\n        model_util.extract_tflite_metadata_json(tflite_path), \"\"\"{\n  \"name\": \"yamnet/classification\",\n  \"description\": \"Recognizes sound events\",\n  \"version\": \"v1\",\n  \"subgraph_metadata\": [\n    {\n      \"input_tensor_metadata\": [\n        {\n          \"name\": \"audio_clip\",\n          \"description\": \"Input audio clip to be classified.\",\n          \"content\": {\n            \"content_properties_type\": \"AudioProperties\",\n            \"content_properties\": {\n              \"sample_rate\": 16000,\n              \"channels\": 1\n            }\n          },\n          \"stats\": {\n          }\n        }\n      ],\n      \"output_tensor_metadata\": [\n        {\n          \"name\": \"yamnet\",\n          \"description\": \"Scores in range 0..1.0 for each of the 521 output classes.\",\n          \"content\": {\n            \"content_properties_type\": \"FeatureProperties\",\n            \"content_properties\": {\n            }\n          },\n          \"stats\": {\n            \"max\": [\n              1.0\n            ],\n            \"min\": [\n              0.0\n            ]\n          },\n          \"associated_files\": [\n            {\n              \"name\": \"yamnet_labels.txt\",\n              \"description\": \"Labels for categories that the model can recognize.\",\n              \"type\": \"TENSOR_AXIS_LABELS\"\n            }\n          ]\n        },\n        {\n          \"name\": \"custom\",\n          \"description\": \"Scores in range 0..1.0 for each output classes.\",\n          \"content\": {\n            \"content_properties_type\": \"FeatureProperties\",\n            \"content_properties\": {\n            }\n          },\n          \"stats\": {\n            \"max\": [\n              1.0\n            ],\n            \"min\": [\n              0.0\n            ]\n          },\n          \"associated_files\": [\n            {\n              \"name\": \"custom_labels.txt\",\n              \"description\": \"Labels for categories that the model can recognize.\",\n              \"type\": \"TENSOR_AXIS_LABELS\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"author\": \"TensorFlow Lite Model Maker\",\n  \"license\": \"Apache License. Version 2.0 http://www.apache.org/licenses/LICENSE-2.0.\",\n  \"min_parser_version\": \"1.3.0\"\n}\n\"\"\")\n\n  def test_yamnet_single_head(self):\n    tflite_path = self._train_and_export(\n        audio_spec.YAMNetSpec(keep_yamnet_and_custom_heads=False),\n        num_classes=2,\n        filename='single_head.tflite',\n        expected_model_size=13 * 1000 * 1000)\n    self.assertEqual(\n        1, len(model_util.get_lite_runner(tflite_path).output_details))\n    self.assertAllEqual(\n        [1, 2],\n        model_util.get_lite_runner(tflite_path).output_details[0]['shape'])\n    self.assertEqual(\n        model_util.extract_tflite_metadata_json(tflite_path), \"\"\"{\n  \"name\": \"yamnet/classification\",\n  \"description\": \"Recognizes sound events\",\n  \"version\": \"v1\",\n  \"subgraph_metadata\": [\n    {\n      \"input_tensor_metadata\": [\n        {\n          \"name\": \"audio_clip\",\n          \"description\": \"Input audio clip to be classified.\",\n          \"content\": {\n            \"content_properties_type\": \"AudioProperties\",\n            \"content_properties\": {\n              \"sample_rate\": 16000,\n              \"channels\": 1\n            }\n          },\n          \"stats\": {\n          }\n        }\n      ],\n      \"output_tensor_metadata\": [\n        {\n          \"name\": \"custom\",\n          \"description\": \"Scores in range 0..1.0 for each output classes.\",\n          \"content\": {\n            \"content_properties_type\": \"FeatureProperties\",\n            \"content_properties\": {\n            }\n          },\n          \"stats\": {\n            \"max\": [\n              1.0\n            ],\n            \"min\": [\n              0.0\n            ]\n          },\n          \"associated_files\": [\n            {\n              \"name\": \"custom_labels.txt\",\n              \"description\": \"Labels for categories that the model can recognize.\",\n              \"type\": \"TENSOR_AXIS_LABELS\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"author\": \"TensorFlow Lite Model Maker\",\n  \"license\": \"Apache License. Version 2.0 http://www.apache.org/licenses/LICENSE-2.0.\",\n  \"min_parser_version\": \"1.3.0\"\n}\n\"\"\")\n\n  def test_no_metadata(self):\n    audio_spec.ENABLE_METADATA = False\n    tflite_path = self._train_and_export(\n        audio_spec.YAMNetSpec(keep_yamnet_and_custom_heads=True),\n        num_classes=2,\n        filename='two_heads.tflite',\n        expected_model_size=15 * 1000 * 1000)\n    self.assertEqual(\n        2, len(model_util.get_lite_runner(tflite_path).output_details))\n    with self.assertRaisesRegex(ValueError, 'The model does not have metadata'):\n      model_util.extract_tflite_metadata_json(tflite_path)\n    audio_spec.ENABLE_METADATA = True\n\n  def test_binary_classification(self):\n    self._train_and_export(\n        audio_spec.YAMNetSpec(keep_yamnet_and_custom_heads=True),\n        num_classes=2,\n        filename='binary_classification.tflite',\n        expected_model_size=15 * 1000 * 1000)\n\n  def test_dynamic_range_quantization(self):\n    self._train_and_export(\n        audio_spec.YAMNetSpec(keep_yamnet_and_custom_heads=True),\n        num_classes=5,\n        filename='basic_5_classes_training.tflite',\n        expected_model_size=4 * 1000 * 1000,\n        quantization_config=configs.QuantizationConfig.for_dynamic())\n\n\n@unittest.skipIf(\n    version.parse(tf.__version__) < version.parse('2.5'),\n    'Audio Classification requires TF 2.5 or later')\nclass BrowserFFTSpecTest(BaseTest):\n\n  @classmethod\n  def setUpClass(cls):\n    super(BrowserFFTSpecTest, cls).setUpClass()\n    cls._spec = audio_spec.BrowserFFTSpec()\n\n  def test_model_initialization(self):\n    model = self._spec.create_model(10)\n\n    self.assertEqual(self._spec._preprocess_model.input_shape,\n                     (None, self._spec.EXPECTED_WAVEFORM_LENGTH))\n    self.assertEqual(self._spec._preprocess_model.output_shape,\n                     (None, None, 232, 1))\n    self.assertEqual(self._spec._tfjs_sc_model.input_shape, (None, 43, 232, 1))\n    self.assertEqual(self._spec._tfjs_sc_model.output_shape, (None, 20))\n    self.assertEqual(model.input_shape, (None, 43, 232, 1))\n    self.assertEqual(model.output_shape, (None, 10))\n\n  def test_create_model(self):\n    # Make sure that there is no naming conflicts.\n    self._spec.create_model(100)\n    self._spec.create_model(100)\n    self._spec.create_model(100)\n\n    tf.keras.backend.clear_session()\n    # Binary classification is not supported yet.\n    with self.assertRaises(ValueError):\n      self._spec.create_model(0)\n    tf.keras.backend.clear_session()\n    with self.assertRaises(ValueError):\n      self._spec.create_model(1)\n    tf.keras.backend.clear_session()\n    # It's more efficient to use BinaryClassification when num_classes=2, but\n    # this is still supported (slightly less efficient).\n    self._spec.create_model(20)\n    tf.keras.backend.clear_session()\n\n  def test_dynamic_range_quantization(self):\n    self._train_and_export(\n        audio_spec.BrowserFFTSpec(),\n        num_classes=2,\n        filename='binary_classification.tflite',\n        expected_model_size=1 * 1000 * 1000,\n        quantization_config=configs.QuantizationConfig.for_dynamic(),\n        training=False)  # Training results Nan values with the current scheme.\n\n  def test_binary_classification(self):\n    self._train_and_export(\n        audio_spec.BrowserFFTSpec(),\n        num_classes=2,\n        filename='binary_classification.tflite',\n        expected_model_size=6 * 1000 * 1000)\n\n  def test_basic_training(self):\n    tflite_path = self._train_and_export(\n        audio_spec.BrowserFFTSpec(),\n        num_classes=5,\n        filename='basic_5_classes_training.tflite',\n        expected_model_size=6 * 1000 * 1000)\n    self.assertEqual(\n        model_util.extract_tflite_metadata_json(tflite_path), \"\"\"{\n  \"name\": \"AudioClassifier\",\n  \"description\": \"Identify the most prominent type in the audio clip from a known set of categories.\",\n  \"version\": \"v1\",\n  \"subgraph_metadata\": [\n    {\n      \"input_tensor_metadata\": [\n        {\n          \"name\": \"audio_clip\",\n          \"description\": \"Input audio clip to be classified.\",\n          \"content\": {\n            \"content_properties_type\": \"AudioProperties\",\n            \"content_properties\": {\n              \"sample_rate\": 44100,\n              \"channels\": 1\n            }\n          },\n          \"stats\": {\n          }\n        }\n      ],\n      \"output_tensor_metadata\": [\n        {\n          \"name\": \"probability\",\n          \"description\": \"Scores of the labels respectively.\",\n          \"content\": {\n            \"content_properties_type\": \"FeatureProperties\",\n            \"content_properties\": {\n            }\n          },\n          \"stats\": {\n            \"max\": [\n              1.0\n            ],\n            \"min\": [\n              0.0\n            ]\n          },\n          \"associated_files\": [\n            {\n              \"name\": \"probability_labels.txt\",\n              \"description\": \"Labels for categories that the model can recognize.\",\n              \"type\": \"TENSOR_AXIS_LABELS\"\n            }\n          ]\n        }\n      ]\n    }\n  ],\n  \"author\": \"TensorFlow Lite Model Maker\",\n  \"license\": \"Apache License. Version 2.0 http://www.apache.org/licenses/LICENSE-2.0.\",\n  \"min_parser_version\": \"1.3.0\"\n}\n\"\"\")\n\n\nif __name__ == '__main__':\n  # Load compressed models from tensorflow_hub\n  os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/federated", "file_path": "tensorflow_federated/python/aggregators/distributed_dp.py", "content": "# Copyright 2021, Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"A wrapper tff.aggregator for distributed DP with secure aggregation.\"\"\"\n\nimport collections\nimport math\nfrom typing import Optional\nimport warnings\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_privacy as tfp\n\nfrom tensorflow_federated.python.aggregators import concat\nfrom tensorflow_federated.python.aggregators import differential_privacy\nfrom tensorflow_federated.python.aggregators import discretization\nfrom tensorflow_federated.python.aggregators import factory\nfrom tensorflow_federated.python.aggregators import modular_clipping\nfrom tensorflow_federated.python.aggregators import quantile_estimation\nfrom tensorflow_federated.python.aggregators import robust\nfrom tensorflow_federated.python.aggregators import rotation\nfrom tensorflow_federated.python.aggregators import secure\nfrom tensorflow_federated.python.core.impl.federated_context import federated_computation\nfrom tensorflow_federated.python.core.impl.federated_context import intrinsics\nfrom tensorflow_federated.python.core.impl.tensorflow_context import tensorflow_computation\nfrom tensorflow_federated.python.core.impl.types import computation_types\nfrom tensorflow_federated.python.core.impl.types import placements\nfrom tensorflow_federated.python.core.impl.types import type_analysis\nfrom tensorflow_federated.python.core.impl.types import type_conversions\nfrom tensorflow_federated.python.core.templates import aggregation_process\nfrom tensorflow_federated.python.core.templates import measured_process\n\n# Supported DP mechanisms.\nDP_MECHANISMS = [\n    # Distributed Discrete Gaussian (https://arxiv.org/abs/2102.06387).\n    'distributed_dgauss',\n    # Distributed Skellam (https://arxiv.org/abs/2110.04995).\n    'distributed_skellam',\n]\n\n# Supported random rotation operations.\nROTATION_TYPES = [\n    'hd',  # Randomized Fast Walsh-Hadamard Transform.\n    'dft',  # Randomized Discrete Fourier Transform.\n]\n\n# The maximum possible scaling factor before rounding is applied. This is needed\n# because when the number of clients per round is small (<10) and/or the noise\n# multiplier is very small (say < 0.001) and/or the number of bits is large (say\n# > 18), the scale factor computed by `_heuristic_scale_factor(...)` becomes\n# very large. For example, for a noise multiplier = 0, number of bits = 20, and\n# 10 clients per round, the scaling factor is on the order of 1e8. Such a large\n# scaling factor leads to overflows when computing the inflated and scaled l1/l2\n# norm bounds using int32 bit representations. In practice, however, such very\n# scaling factors do not offer any added value (in minimizing rounding errors\n# and/or minimizing the inflation of the l1/l2 norms upon rounding). Capping the\n# scaling factor to 1e6 avoids these overflow issues without compromising\n# utility.\n# TODO(b/223427213): Adapt the bitwidth whenever the scale is too high.\nMAX_SCALE_FACTOR = 1e6\n\n\nclass DistributedDpSumFactory(factory.UnweightedAggregationFactory):\n  \"\"\"An `UnweightedAggregationFactory` for distributed DP with SecAgg.\n\n  The created `tff.templates.AggregationProcess` serves as a wrapper that\n  encapsulates several component tff.aggregators into an implementation of\n  a distributed differential privacy (DP) algorithm with secure aggregation.\n  Distributed DP algorithms aim to \"distribute trust\" away from the central\n  server by allowing clients to add their own noise for differential privacy\n  while allowing comparable privacy/utility to be achieved compared to the\n  central DP model.\n\n  Currently, two specific distributed DP algorithms are supported: the\n  distributed discrete Gaussian mechanism (Algorithms 1 and 2 in\n  https://arxiv.org/abs/2102.06387) and the distributed\n  Skellam mechanism (Algorithm 1 in https://arxiv.org/abs/2110.04995).\n\n  This wrapper aggregator manages the nesting/composition of the components,\n  coordinate their states, surfaces relevant states/metrics, and integrates\n  auto-tuning algorithms to update relevant states/hyperparameters.\n\n  This aggregator accepts tensors or structures of tensors with integer or\n  floating dtypes, and keeps the value type/structure after aggregation.\n\n  TODO(b/221465205): Add exact privacy accounting for DDP using new accounting\n  APIs.\n\n  To obtain concrete (epsilon, delta) guarantees, one could use the analysis\n  tools provided in tensorflow_privacy on the metrics generated in each round.\n  \"\"\"\n\n  def __init__(self,\n               noise_multiplier: float,\n               expected_clients_per_round: int,\n               bits: int,\n               l2_clip: float,\n               modclip_prob: float = 1e-4,\n               beta: float = math.exp(-0.5),\n               mechanism: str = 'distributed_skellam',\n               rotation_type: str = 'dft',\n               auto_l2_clip: bool = False,\n               auto_l2_target_quantile: float = 0.5,\n               auto_l2_lr: float = 0.2,\n               auto_l2_clip_count_stddev: Optional[float] = None):\n    \"\"\"Initializes the `DistributedDpSumFactory`.\n\n    Note that the `create` method of this factory needs to be executed in TF\n    eager mode. Please see the note in the docstring of the private function\n    `_clip_prob_to_num_stddevs` below.\n\n    Args:\n      noise_multiplier: A float specifying the noise multiplier (central noise\n        stddev / L2 clip norm) for model updates. Note that this is with respect\n        to the initial L2 clip norm, and the quantization procedure as part of\n        the DDP algorithm may inflated the L2 sensitivity. The specified noise\n        will be split into `expected_clients_per_round` noise shares to be added\n        locally on the clients. A value of 1.0 or higher may be needed for\n        strong privacy. Must be nonnegative. A value of 0.0 means no noise will\n        be added.\n      expected_clients_per_round: An integer specifying the expected number of\n        clients to participate in this round. This number dictates how much\n        noise is added locally. In particular, local noise stddev = central\n        noise stddev / `expected_clients_per_round`. Must be a positive integer.\n      bits: A positive integer specifying the communication bit-width B (where\n        2^B will be the field size for SecAgg operations). Note that this is for\n        the noisy quantized aggregate at the server and thus should account for\n        the number of clients. Must be in the inclusive range [1, 22], and\n        should be at least as large as log_2(expected_clients_per_round).\n      l2_clip: A float specifying the value of the L2 clipping norm. Must be\n        positive. If `auto_l2_clip` is set to True, a reasonable default is 0.1.\n      modclip_prob: (Optional) A float in the exclusive range (0, 1) specifying\n        the target probability for modular wrapping due to SecAgg's modulo\n        operations. Default to 0.01% (roughly 3.9 standard deviations of the\n        mean assuming roughly normally distributed aggregates at the server).\n      beta: (Optional) The conditional randomized rounding bias. Must be a float\n        in the range [0, 1). The larger the value, the less post-rounding L2\n        sensitivity inflation. Defaults to exp(-0.5). Please see Sections 4 of\n          https://arxiv.org/pdf/2102.06387.pdf for a detailed explanation of\n            conditional randomized rounding.\n      mechanism: (Optional) The distributed DP mechanism to use. Possible\n        options are 'distributed_dgauss' (distributed discrete Gaussian\n        mechanism) or 'distributed_skellam' (distributed Skellam mechanism; the\n        default).\n      rotation_type: (Optional) The rotation operation used to spread out input\n        values across vector dimensions. Possible options are 'hd' (randomized\n        Hadamard transform) or 'dft' (discrete Fourier transform; the defaut).\n      auto_l2_clip: (Optional) A bool indicating whether to adaptively adjust\n        the L2 norm clipping (i.e., `l2_clip`) after each round. Note that this\n        involves private quantile estimation which would result in a larger\n        effective `noise_multiplier` for the actual client values. The algorithm\n        used is based on https://arxiv.org/pdf/1905.03871.pdf with the geometric\n        update method. Defaults to `False`.\n      auto_l2_target_quantile: (Optional) A float in the inclusive range [0, 1]\n        indicating the target quantile to which the L2 clipping norm should\n        adapt. A value of 0.8 means a clipping norm should be chosen such that\n        80% of the client values have norm below it. Defaults to 0.5. Ignored if\n        `auto_l2_clip` is `False`.\n      auto_l2_lr: (Optional) A float specifying the the learning rate for the\n        adaptive L2 clipping process. Default to 0.2. Ignored if `auto_l2_clip`\n        is `False`.\n      auto_l2_clip_count_stddev: (Optional) The stddev of the noise added to the\n        clipped counts in the adaptive clipping algorithm. If None, defaults to\n        `0.05 * expected_clients_per_round` (unless `noise_multiplier` is 0, in\n        which case it is also 0). Ignored if `auto_l2_clip` is `False`.\n\n    Raises:\n      TypeError: If arguments have the wrong type(s).\n      ValueError: If arguments have invalid value(s).\n    \"\"\"\n    _check_nonnegative(noise_multiplier, 'noise_multiplier')\n    _check_positive(expected_clients_per_round, 'expected_clients_per_round')\n    _check_integer(expected_clients_per_round, 'expected_clients_per_round')\n    # While larger bits are possible, much of the DDP implementation relies on\n    # float32 which only represents ints <= 2^24. We cap at bits <= 22.\n    _check_in_range(bits, 'bits', 1, 22, True, True)\n    _check_integer(bits, 'bits')\n    _check_positive(l2_clip, 'l2_clip')\n    _check_in_range(modclip_prob, 'modclip_prob', 0, 1, False, False)\n    _check_in_range(beta, 'beta', 0, 1, True, False)\n    _check_str(mechanism, 'mechanism', DP_MECHANISMS)\n    _check_str(rotation_type, 'rotation_type', ROTATION_TYPES)\n    _check_bool(auto_l2_clip, 'auto_l2_clip')\n\n    if auto_l2_clip:\n      _check_in_range(auto_l2_target_quantile, 'auto_l2_target_quantile', 0, 1,\n                      True, True)\n      _check_positive(auto_l2_lr, 'auto_l2_lr')\n      if auto_l2_clip_count_stddev is not None:\n        _check_nonnegative(auto_l2_clip_count_stddev,\n                           'auto_l2_clip_count_stddev')\n\n    self._initial_l2_clip = l2_clip\n    self._noise_multiplier = noise_multiplier\n    self._num_clients = expected_clients_per_round\n    self._bits = bits\n    self._modclip_prob = modclip_prob\n    self._k_stddevs = _clip_prob_to_num_stddevs(modclip_prob)\n    self._beta = beta\n    self._mechanism = mechanism\n    self._rotation_type = rotation_type\n    self._auto_l2_clip = auto_l2_clip\n\n    # Value range checks based on the client count and the clip probability.\n    if bits < math.log2(expected_clients_per_round):\n      raise ValueError('bits should be >= log2(expected_clients_per_round). '\n                       f'Found 2^b = 2^{bits} < {expected_clients_per_round}.')\n    if 2**(2 * bits) < expected_clients_per_round * self._k_stddevs**2:\n      raise ValueError(f'The selected bit-width ({bits}) is too small for the '\n                       f'given parameters (expected_clients_per_round = '\n                       f'{expected_clients_per_round}, modclip_prob = '\n                       f'{modclip_prob}). You must decrease the '\n                       f'`expected_clients_per_round`, increase `bits`, or '\n                       f'increase `modclip_prob`.')\n\n    if auto_l2_clip:\n      self._l2_clip, self._value_noise_mult = self._build_auto_l2_clip_process(\n          auto_l2_target_quantile, auto_l2_lr, auto_l2_clip_count_stddev)\n    else:\n      self._l2_clip = self._initial_l2_clip\n      self._value_noise_mult = self._noise_multiplier\n\n  def _build_auto_l2_clip_process(self, target_quantile, learning_rate,\n                                  clip_count_stddev):\n    \"\"\"Builds a `tff.templates.EstimationProcess` for adaptive L2 clipping.\n\n    Specifically, we use the private quantile estimation algorithm described in\n    https://arxiv.org/abs/1905.03871 for choosing the adaptive L2 clip norm.\n    The default noise level for the procedure follows the paper and the\n    implementation of `tff.aggregators.DifferentiallyPrivateFactory`.\n\n    Note that for consistency with the use of secure aggregation for the client\n    values, the binary flags as part of the quantile estimation procedure\n    indicating whether client L2 norms are below the current estimate are also\n    securely aggregated.\n\n    Args:\n      target_quantile: See `auto_l2_target_quantile` at __init__ docstring.\n      learning_rate: See `auto_l2_lr` at __init__ docstring.\n      clip_count_stddev: See `auto_l2_clip_count_stddev` at __init__ docstring.\n\n    Returns:\n      The `EstimationProcess` for adaptive L2 clipping and the required noise\n      multiplier for the record aggregation.\n    \"\"\"\n    value_noise_mult, clip_count_stddev = (\n        differential_privacy.adaptive_clip_noise_params(self._noise_multiplier,\n                                                        self._num_clients,\n                                                        clip_count_stddev))\n\n    estimator_query = tfp.QuantileEstimatorQuery(\n        initial_estimate=self._initial_l2_clip,\n        target_quantile=target_quantile,\n        learning_rate=learning_rate,\n        below_estimate_stddev=clip_count_stddev,\n        expected_num_records=self._num_clients,\n        geometric_update=True)\n    # Note also that according to https://arxiv.org/abs/1905.03871, the binary\n    # flags for quantile estimation are shifted from [0, 1] to [-0.5, 0.5], so\n    # we set the SecAgg input bounds accordingly.\n    estimator_process = quantile_estimation.PrivateQuantileEstimationProcess(\n        quantile_estimator_query=estimator_query,\n        record_aggregation_factory=secure.SecureSumFactory(\n            upper_bound_threshold=0.5, lower_bound_threshold=-0.5))\n\n    return estimator_process, value_noise_mult\n\n  def _build_aggregation_factory(self):\n    central_stddev = self._value_noise_mult * self._initial_l2_clip\n    local_stddev = central_stddev / math.sqrt(self._num_clients)\n\n    # Ensure dim is at least 1 only for computing DDP parameters.\n    self._client_dim = max(1, self._client_dim)\n    if self._rotation_type == 'hd':\n      # Hadamard transform requires dimension to be powers of 2.\n      self._padded_dim = 2**math.ceil(math.log2(self._client_dim))\n      rotation_factory = rotation.HadamardTransformFactory\n    else:\n      # DFT pads at most 1 zero.\n      self._padded_dim = math.ceil(self._client_dim / 2.0) * 2\n      rotation_factory = rotation.DiscreteFourierTransformFactory\n\n    scale = _heuristic_scale_factor(local_stddev, self._initial_l2_clip,\n                                    self._bits, self._num_clients,\n                                    self._padded_dim, self._k_stddevs).numpy()\n\n    # Very large scales could lead to overflows and are not as helpful for\n    # utility. See comment above for more details.\n    scale = min(scale, MAX_SCALE_FACTOR)\n\n    if scale <= 1:\n      warnings.warn(f'The selected scale_factor {scale} <= 1. This may lead to'\n                    f'substantial quantization errors. Consider increasing'\n                    f'the bit-width (currently {self._bits}) or decreasing the'\n                    f'expected number of clients per round (currently '\n                    f'{self._num_clients}).')\n\n    # The procedure for obtaining inflated L2 bound assumes eager TF execution\n    # and can be rewritten with NumPy if needed.\n    inflated_l2 = discretization.inflated_l2_norm_bound(\n        l2_norm_bound=self._initial_l2_clip,\n        gamma=1.0 / scale,\n        beta=self._beta,\n        dim=self._padded_dim).numpy()\n\n    # Add small leeway on norm bounds to gracefully allow numerical errors.\n    # Specifically, the norm thresholds are computed directly from the specified\n    # parameters in Python and will be checked right before noising; on the\n    # other hand, the actual norm of the record (to be measured at noising time)\n    # can possibly be (negligibly) higher due to the float32 arithmetic after\n    # the conditional rounding (thus failing the check). While we have mitigated\n    # this by sharing the computation for the inflated norm bound from\n    # quantization, adding a leeway makes the execution more robust (it does not\n    # need to abort should any precision issues happen) while not affecting the\n    # correctness if privacy accounting is done based on the norm bounds at the\n    # DPQuery/DPFactory (which incorporates the leeway).\n    scaled_inflated_l2 = (inflated_l2 + 1e-5) * scale\n    # Since values are scaled and rounded to integers, we have L1 <= L2^2\n    # on top of the general of L1 <= sqrt(d) * L2.\n    scaled_l1 = math.ceil(scaled_inflated_l2 *\n                          min(math.sqrt(self._padded_dim), scaled_inflated_l2))\n\n    # Build nested aggregtion factory.\n    # 1. Secure Aggregation. In particular, we have 4 modular clips from\n    #    nesting two modular clip aggregators:\n    #    #1. outer-client: clips to [-2^(b-1), 2^(b-1)]\n    #        Bounds the client values (with limited effect as scaling was\n    #        chosen such that `num_clients` is taken into account).\n    #    #2. inner-client: clips to [0, 2^b]\n    #        Similar to applying a two's complement to the values such that\n    #        frequent values (post-rotation) are now near 0 (representing small\n    #        positives) and 2^b (small negatives). 0 also always map to 0, and\n    #        we do not require another explicit value range shift from\n    #        [-2^(b-1), 2^(b-1)] to [0, 2^b] to make sure that values are\n    #        compatible with SecAgg's mod m = 2^b. This can be reverted at #4.\n    #    #3. inner-server: clips to [0, 2^b]\n    #        Ensures the aggregated value range does not grow by log_2(n).\n    #        NOTE: If underlying SecAgg is implemented using the new\n    #        `tff.federated_secure_modular_sum()` operator with the same\n    #        modular clipping range, then this would correspond to a no-op.\n    #    #4. outer-server: clips to [-2^(b-1), 2^(b-1)]\n    #        Keeps aggregated values centered near 0 out of the logical SecAgg\n    #        black box for outer aggregators.\n    #    Note that the scaling factor and the bit-width are chosen such that\n    #    the number of clients to aggregate is taken into account.\n    nested_factory = secure.SecureSumFactory(\n        upper_bound_threshold=2**self._bits - 1, lower_bound_threshold=0)\n    nested_factory = modular_clipping.ModularClippingSumFactory(\n        clip_range_lower=0,\n        clip_range_upper=2**self._bits,\n        inner_agg_factory=nested_factory)\n    nested_factory = modular_clipping.ModularClippingSumFactory(\n        clip_range_lower=-(2**(self._bits - 1)),\n        clip_range_upper=2**(self._bits - 1),\n        inner_agg_factory=nested_factory)\n\n    # 2. DP operations. DP params are in the scaled domain (post-quantization).\n    if self._mechanism == 'distributed_dgauss':\n      dp_query = tfp.DistributedDiscreteGaussianSumQuery(\n          l2_norm_bound=scaled_inflated_l2, local_stddev=local_stddev * scale)\n    else:\n      dp_query = tfp.DistributedSkellamSumQuery(\n          l1_norm_bound=scaled_l1,\n          l2_norm_bound=scaled_inflated_l2,\n          local_stddev=local_stddev * scale)\n\n    nested_factory = differential_privacy.DifferentiallyPrivateFactory(\n        query=dp_query, record_aggregation_factory=nested_factory)\n\n    # 3. Discretization operations. This appropriately quantizes the inputs.\n    nested_factory = discretization.DiscretizationFactory(\n        inner_agg_factory=nested_factory,\n        scale_factor=scale,\n        stochastic=True,\n        beta=self._beta,\n        prior_norm_bound=self._initial_l2_clip)\n\n    # 4. L2 clip, possibly adaptively with a `tff.templates.EstimationProcess`.\n    nested_factory = robust.clipping_factory(\n        clipping_norm=self._l2_clip,\n        inner_agg_factory=nested_factory,\n        clipped_count_sum_factory=secure.SecureSumFactory(\n            upper_bound_threshold=1, lower_bound_threshold=0))\n\n    # 5. Flattening to improve quantization and reduce modular wrapping.\n    nested_factory = rotation_factory(inner_agg_factory=nested_factory)\n\n    # 6. Concat the input structure into a single vector.\n    nested_factory = concat.concat_factory(inner_agg_factory=nested_factory)\n    return nested_factory\n\n  def _unpack_state(self, agg_state):\n    # Note: `agg_state` has a nested structure similar to the composed\n    # aggregator. Please print it to figure out how to correctly unpack the\n    # needed states. This is especially needed when you add, remove, or change\n    # any of the core composed aggregators.\n    # TODO(b/222162205): Simplify how we compose states of nested aggregators.\n    rotation_state = agg_state  # Concat has no states.\n    l2_clip_state, _ = rotation_state\n    discrete_state = l2_clip_state['inner_agg']\n    dp_state = discrete_state['inner_agg_process']\n    return l2_clip_state, discrete_state, dp_state\n\n  def _unpack_measurements(self, agg_measurements):\n    rotate_metrics = agg_measurements  # Concat has no measurements.\n    l2_clip_metrics = rotate_metrics[self._rotation_type]\n    discrete_metrics = l2_clip_metrics['clipping']\n    dp_metrics = discrete_metrics['discretize']\n    return l2_clip_metrics, discrete_metrics, dp_metrics\n\n  def _autotune_component_states(self, agg_state):\n    \"\"\"Updates the nested aggregator state in-place.\n\n    This procedure makes the following assumptions: (1) this wrapper aggregator\n    has knowledge about the states of the component aggregators and their\n    Python containers, and can thus make in-place modifications directly; (2)\n    this aggregator has knowledge about the state of the `DPQuery` objects\n    (types and members) that are used by the `DifferentiallyPrivateFactory`, and\n    can thus update the members directly. Both assumptions should be revisited.\n\n    Args:\n      agg_state: The state of this aggregator, which is a nested object\n        containing the states of the component aggregators.\n\n    Returns:\n      The updated agg_state.\n    \"\"\"\n\n    @tensorflow_computation.tf_computation\n    def _update_scale(agg_state, new_l2_clip):\n      _, discrete_state, _ = self._unpack_state(agg_state)\n      new_central_stddev = new_l2_clip * self._value_noise_mult\n      new_local_stddev = new_central_stddev / math.sqrt(self._num_clients)\n      new_scale = _heuristic_scale_factor(new_local_stddev, new_l2_clip,\n                                          self._bits, self._num_clients,\n                                          self._padded_dim, self._k_stddevs)\n\n      # Very large scales could lead to overflows and are not as helpful for\n      # utility. See comment above for more details.\n      new_scale = tf.math.minimum(\n          new_scale, tf.constant(MAX_SCALE_FACTOR, dtype=tf.float64))\n\n      discrete_state['scale_factor'] = tf.cast(new_scale, tf.float32)\n      return agg_state\n\n    @tensorflow_computation.tf_computation\n    def _update_dp_params(agg_state, new_l2_clip):\n      _, discrete_state, dp_state = self._unpack_state(agg_state)\n      new_scale = discrete_state['scale_factor']\n      new_inflated_l2 = discretization.inflated_l2_norm_bound(\n          l2_norm_bound=new_l2_clip,\n          gamma=1.0 / new_scale,\n          beta=self._beta,\n          dim=self._padded_dim)\n      # Similarly include a norm bound leeway. See inline comment in\n      # `_build_aggregation_factory()` for more details.\n      new_scaled_inflated_l2 = (new_inflated_l2 + 1e-5) * new_scale\n      l1_fac = tf.minimum(math.sqrt(self._padded_dim), new_scaled_inflated_l2)\n      new_scaled_l1 = tf.math.ceil(new_scaled_inflated_l2 * l1_fac)\n      new_scaled_l1 = tf.cast(new_scaled_l1, tf.int32)\n      # Recompute noise stddevs.\n      new_central_stddev = new_l2_clip * self._value_noise_mult\n      new_local_stddev = new_central_stddev / math.sqrt(self._num_clients)\n      # Update DP params: norm bounds (uninflated/inflated) and local stddev.\n      dp_query_state = dp_state.query_state\n      if self._mechanism == 'distributed_dgauss':\n        new_dp_query_state = dp_query_state._replace(\n            l2_norm_bound=new_scaled_inflated_l2,\n            local_stddev=new_local_stddev * new_scale)\n      else:\n        new_dp_query_state = dp_query_state._replace(\n            l1_norm_bound=new_scaled_l1,\n            l2_norm_bound=new_scaled_inflated_l2,\n            local_stddev=new_local_stddev * new_scale)\n      new_dp_state = differential_privacy.DPAggregatorState(\n          new_dp_query_state, dp_state.agg_state, dp_state.dp_event,\n          dp_state.is_init_state)\n      discrete_state['inner_agg_process'] = new_dp_state\n      discrete_state['prior_norm_bound'] = new_l2_clip\n      return agg_state\n\n    l2_clip_state, _, _ = self._unpack_state(agg_state)\n    # NOTE(b/170893510): Explicitly declaring Union[float, EstimationProcess]\n    # for _l2_clip or doing isinstance() check still triggers attribute-error.\n    new_l2_clip = self._l2_clip.report(l2_clip_state['clipping_norm'])  # pytype: disable=attribute-error\n    agg_state = intrinsics.federated_map(_update_scale,\n                                         (agg_state, new_l2_clip))\n    agg_state = intrinsics.federated_map(_update_dp_params,\n                                         (agg_state, new_l2_clip))\n    return agg_state\n\n  def _derive_measurements(self, agg_state, agg_measurements):\n    _, discrete_state, dp_state = self._unpack_state(agg_state)\n    l2_clip_metrics, _, dp_metrics = self._unpack_measurements(agg_measurements)\n    dp_query_state, _, _, _ = dp_state\n\n    actual_num_clients = intrinsics.federated_secure_sum_bitwidth(\n        intrinsics.federated_value(1, placements.CLIENTS), bitwidth=1)\n    padded_dim = intrinsics.federated_value(\n        int(self._padded_dim), placements.SERVER)\n\n    measurements = collections.OrderedDict(\n        l2_clip=l2_clip_metrics['clipping_norm'],\n        scale_factor=discrete_state['scale_factor'],\n        scaled_inflated_l2=dp_query_state.l2_norm_bound,\n        scaled_local_stddev=dp_query_state.local_stddev,\n        actual_num_clients=actual_num_clients,\n        padded_dim=padded_dim,\n        dp_query_metrics=dp_metrics['dp_query_metrics'])\n\n    return intrinsics.federated_zip(measurements)\n\n  def create(self, value_type):\n    # Checks value_type and compute client data dimension.\n    if (value_type.is_struct_with_python() and\n        type_analysis.is_structure_of_tensors(value_type)):\n      num_elements_struct = type_conversions.structure_from_tensor_type_tree(\n          lambda x: x.shape.num_elements(), value_type)\n      self._client_dim = sum(tf.nest.flatten(num_elements_struct))\n    elif value_type.is_tensor():\n      self._client_dim = value_type.shape.num_elements()\n    else:\n      raise TypeError('Expected `value_type` to be `TensorType` or '\n                      '`StructWithPythonType` containing only `TensorType`. '\n                      f'Found type: {repr(value_type)}')\n    # Checks that all values are integers or floats.\n    if not (type_analysis.is_structure_of_floats(value_type) or\n            type_analysis.is_structure_of_integers(value_type)):\n      raise TypeError('Component dtypes of `value_type` must all be integers '\n                      f'or floats. Found {repr(value_type)}.')\n\n    ddp_agg_process = self._build_aggregation_factory().create(value_type)\n    init_fn = ddp_agg_process.initialize\n\n    @federated_computation.federated_computation(\n        init_fn.type_signature.result, computation_types.at_clients(value_type))\n    def next_fn(state, value):\n      agg_output = ddp_agg_process.next(state, value)\n      new_measurements = self._derive_measurements(agg_output.state,\n                                                   agg_output.measurements)\n      new_state = agg_output.state\n      if self._auto_l2_clip:\n        new_state = self._autotune_component_states(agg_output.state)\n\n      return measured_process.MeasuredProcessOutput(\n          state=new_state,\n          result=agg_output.result,\n          measurements=new_measurements)\n\n    return aggregation_process.AggregationProcess(init_fn, next_fn)\n\n\ndef _clip_prob_to_num_stddevs(clip_prob):\n  \"\"\"Computes the number of stddevs for the target clipping probability.\n\n  This function assumes (approximately) normal distributions. It is implemented\n  using TensorFlow to avoid depending on SciPy's `stats.norm.ppf` and it thus\n  assumes eager TF execution. This can be replaced with the `statistics` package\n  from Python >= 3.8.\n\n  Args:\n    clip_prob: A float for clipping probability in the exclusive range (0, 1).\n\n  Returns:\n    The number of standard deviations corresponding to the clip prob.\n  \"\"\"\n  return math.sqrt(2) * tf.math.erfcinv(clip_prob).numpy()\n\n\ndef _heuristic_scale_factor(local_stddev,\n                            l2_clip,\n                            bits,\n                            num_clients,\n                            dim,\n                            k_stddevs,\n                            rho=1.0):\n  \"\"\"Selects a scaling factor by assuming subgaussian aggregates.\n\n  Selects scale_factor = 1 / gamma such that k stddevs of the noisy, quantized,\n  aggregated client values are bounded within the bit-width. The aggregate at\n  the server is assumed to follow a subgaussian distribution. Note that the\n  DDP algorithm is correct for any reasonable scaling factor, thus even if the\n  subgaussian assumption does not hold (e.g. in the case of distributed Skellam\n  which has sub-exponential tails), this function still provides a useful\n  heuristic. See Section 4.2 and 4.4 of https://arxiv.org/pdf/2102.06387.pdf\n  for more details.\n\n  Specifically, the implementation is solving for gamma using the following\n  expression:\n\n    2^b = 2k * sqrt(rho / dim * (cn)^2 + (gamma^2 / 4 + sigma^2) * n) / gamma.\n\n  Args:\n    local_stddev: The local noise standard deviation.\n    l2_clip: The initial L2 clip norm. See the __init__ docstring.\n    bits: The bit-width. See the __init__ docstring.\n    num_clients: The expected number of clients. See the __init__ docstring.\n    dim: The dimension of the client vector that includes any necessary padding.\n    k_stddevs: The number of standard deviations of the noisy and quantized\n      aggregate values to bound within the bit-width.\n    rho: (Optional) The subgaussian flatness parameter of the random orthogonal\n      transform as part of the DDP procedure. See Section 4.2 of the above paper\n      for more details.\n\n  Returns:\n    The selected scaling factor in tf.float64.\n  \"\"\"\n  bits = tf.cast(bits, tf.float64)\n  c = tf.cast(l2_clip, tf.float64)\n  dim = tf.cast(dim, tf.float64)\n  k_stddevs = tf.cast(k_stddevs, tf.float64)\n  n = tf.cast(num_clients, tf.float64)\n  sigma = tf.cast(local_stddev, tf.float64)\n\n  numer = tf.sqrt(2.0**(2.0 * bits) - n * k_stddevs**2)\n  denom = 2.0 * k_stddevs * tf.sqrt(rho / dim * c**2 * n**2 + n * sigma**2)\n  scale_factor = numer / denom\n  return scale_factor\n\n\ndef _check_scalar(value, label):\n  is_bool = isinstance(value, bool)\n  is_py_scalar = isinstance(value, (int, float))\n  is_np_scalar = np.isscalar(value)\n  if is_bool or not (is_py_scalar or is_np_scalar):\n    raise TypeError(f'{label} must be a scalar. Found {repr(value)}.')\n\n\ndef _check_positive(value, label):\n  _check_scalar(value, label)\n  if value <= 0:\n    raise ValueError(f'{label} must be positive. Found {repr(value)}.')\n\n\ndef _check_nonnegative(value, label):\n  _check_scalar(value, label)\n  if value < 0:\n    raise ValueError(f'{label} must be nonnegative. Found {repr(value)}.')\n\n\ndef _check_integer(value, label):\n  _check_scalar(value, label)\n  if not isinstance(value, (int, np.integer)):\n    raise TypeError(f'{label} must be an integer. Found {repr(value)}.')\n\n\ndef _check_float(value, label):\n  _check_scalar(value, label)\n  if not isinstance(value, (float, np.floating)):\n    raise TypeError(f'{label} must be a float. Found {repr(value)}.')\n\n\ndef _check_bool(value, label):\n  if not isinstance(value, bool):\n    raise TypeError(f'{label} must be a bool. Found {repr(value)}.')\n\n\ndef _check_str(value, label, options):\n  error_msg = f'`{label}` must be a string and one of {options}. Found {value}.'\n  if not isinstance(value, str):\n    raise TypeError(error_msg)\n  if value not in options:\n    raise ValueError(error_msg)\n\n\ndef _check_in_range(value, label, left, right, left_inclusive, right_inclusive):\n  \"\"\"Checks that a scalar value is in specified range.\"\"\"\n  _check_scalar(value, label)\n  _check_bool(left_inclusive, 'left_inclusive')\n  _check_bool(right_inclusive, 'right_inclusive')\n  if left > right:\n    raise ValueError(f'left must be smaller than right; found {left}, {right}.')\n  left_cond = value >= left if left_inclusive else value > left\n  right_cond = value <= right if right_inclusive else value < right\n  if not left_cond or not right_cond:\n    raise ValueError(f'{label} should be between {left} and {right} (with '\n                     f'left_inclusive={left_inclusive} and right_inclusive='\n                     f'{right_inclusive}). Found {value}.')\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/federated", "file_path": "tensorflow_federated/python/learning/metrics/aggregation_factory_test.py", "content": "# Copyright 2022, The TensorFlow Federated Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\n\nfrom absl.testing import parameterized\nimport tensorflow as tf\n\nfrom tensorflow_federated.python.aggregators import factory\nfrom tensorflow_federated.python.aggregators import quantile_estimation\nfrom tensorflow_federated.python.core.backends.test import execution_contexts\nfrom tensorflow_federated.python.core.impl.types import computation_types\nfrom tensorflow_federated.python.core.impl.types import placements\nfrom tensorflow_federated.python.core.impl.types import type_conversions\nfrom tensorflow_federated.python.core.templates import aggregation_process\nfrom tensorflow_federated.python.core.templates import estimation_process\nfrom tensorflow_federated.python.core.templates import measured_process\nfrom tensorflow_federated.python.core.test import static_assert\nfrom tensorflow_federated.python.learning.metrics import aggregation_factory\n\n# Convenience aliases.\nTensorType = computation_types.TensorType\nMetricRange = aggregation_factory._MetricRange\n\n\ndef _get_finalized_metrics_type(metric_finalizers, unfinalized_metrics):\n  if callable(metric_finalizers):\n    finalized_metrics = metric_finalizers(unfinalized_metrics)\n  else:\n    finalized_metrics = collections.OrderedDict(\n        (metric, finalizer(unfinalized_metrics[metric]))\n        for metric, finalizer in metric_finalizers.items())\n  return type_conversions.type_from_tensors(finalized_metrics)\n\n\n@tf.function\ndef _tf_mean(x):\n  return tf.math.divide_no_nan(x[0], x[1])\n\n\n_DEFAULT_FLOAT_FACTORY_KEY = 'None/default_estimation_process/1'\n_DEFAULT_INT_FACTORY_KEY = '0/1048575/3'\n\n_DEFAULT_FIXED_FLOAT_RANGE = (\n    float(aggregation_factory.DEFAULT_FIXED_SECURE_LOWER_BOUND),\n    float(aggregation_factory.DEFAULT_FIXED_SECURE_UPPER_BOUND))\n_DEFAULT_AUTO_TUNED_FLOAT_RANGE = (\n    None,\n    quantile_estimation.PrivateQuantileEstimationProcess.no_noise(\n        initial_estimate=50.0,\n        target_quantile=0.95,\n        learning_rate=1.0,\n        multiplier=2.0,\n        secure_estimation=True))\n_DEFAULT_INT_RANGE = (int(aggregation_factory.DEFAULT_FIXED_SECURE_LOWER_BOUND),\n                      int(aggregation_factory.DEFAULT_FIXED_SECURE_UPPER_BOUND))\n\n\nclass SumThenFinalizeFactoryComputationTest(tf.test.TestCase,\n                                            parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      ('scalar_metric',\n       collections.OrderedDict(num_examples=tf.function(func=lambda x: x)),\n       collections.OrderedDict(num_examples=1.0)),\n      ('non_scalar_metric', collections.OrderedDict(loss=_tf_mean),\n       collections.OrderedDict(loss=[2.0, 1.0])),\n      ('callable',\n       tf.function(\n           lambda x: collections.OrderedDict(mean_loss=_tf_mean(x['loss']))),\n       collections.OrderedDict(loss=[1.0, 2.0])))\n  def test_type_properties(self, metric_finalizers, unfinalized_metrics):\n    aggregate_factory = aggregation_factory.SumThenFinalizeFactory(\n        metric_finalizers)\n    self.assertIsInstance(aggregate_factory,\n                          factory.UnweightedAggregationFactory)\n    local_unfinalized_metrics_type = type_conversions.type_from_tensors(\n        unfinalized_metrics)\n    process = aggregate_factory.create(local_unfinalized_metrics_type)\n    self.assertIsInstance(process, aggregation_process.AggregationProcess)\n\n    expected_state_type = computation_types.FederatedType(\n        ((), local_unfinalized_metrics_type), placements.SERVER)\n    expected_initialize_type = computation_types.FunctionType(\n        parameter=None, result=expected_state_type)\n    self.assertTrue(\n        process.initialize.type_signature.is_equivalent_to(\n            expected_initialize_type))\n\n    finalized_metrics_type = _get_finalized_metrics_type(\n        metric_finalizers, unfinalized_metrics)\n    result_value_type = computation_types.FederatedType(\n        (finalized_metrics_type, finalized_metrics_type), placements.SERVER)\n    measurements_type = computation_types.FederatedType((), placements.SERVER)\n    expected_next_type = computation_types.FunctionType(\n        parameter=collections.OrderedDict(\n            state=expected_state_type,\n            unfinalized_metrics=computation_types.FederatedType(\n                local_unfinalized_metrics_type, placements.CLIENTS)),\n        result=measured_process.MeasuredProcessOutput(expected_state_type,\n                                                      result_value_type,\n                                                      measurements_type))\n    self.assertTrue(\n        process.next.type_signature.is_equivalent_to(expected_next_type))\n\n  @parameterized.named_parameters(\n      ('default_metric_value_ranges', None),\n      ('custom_metric_value_ranges',\n       collections.OrderedDict(\n           num_examples=(0, 100), loss=[\n               None,\n               (0.0, 100.0),\n           ])))\n  def test_type_properties_with_inner_secure_sum_process(\n      self, metric_value_ranges):\n    if metric_value_ranges is None:\n      secure_summation_factory = aggregation_factory.SecureSumFactory()\n    else:\n      secure_summation_factory = aggregation_factory.SecureSumFactory(\n          metric_value_ranges)\n    metric_finalizers = collections.OrderedDict(\n        num_examples=tf.function(func=lambda x: x),\n        loss=tf.function(func=lambda x: tf.math.divide_no_nan(x[0], x[1])),\n        custom_sum=tf.function(\n            func=lambda x: tf.add_n(map(tf.math.reduce_sum, x))))\n    local_unfinalized_metrics = collections.OrderedDict(\n        num_examples=1,\n        loss=[2.0, 1.0],\n        custom_sum=[tf.constant(1.0), tf.constant([1.0, 1.0])])\n    local_unfinalized_metrics_type = type_conversions.type_from_tensors(\n        local_unfinalized_metrics)\n\n    aggregate_factory = aggregation_factory.SumThenFinalizeFactory(\n        metric_finalizers, inner_summation_factory=secure_summation_factory)\n    process = aggregate_factory.create(local_unfinalized_metrics_type)\n\n    self.assertIsInstance(process, aggregation_process.AggregationProcess)\n\n    secure_summation_process = secure_summation_factory.create(\n        local_unfinalized_metrics_type)\n    expected_state_type = computation_types.FederatedType(\n        (secure_summation_process.initialize.type_signature.result.member,\n         local_unfinalized_metrics_type), placements.SERVER)\n    expected_initialize_type = computation_types.FunctionType(\n        parameter=None, result=expected_state_type)\n    self.assertTrue(\n        process.initialize.type_signature.is_equivalent_to(\n            expected_initialize_type))\n\n    finalized_metrics_type = _get_finalized_metrics_type(\n        metric_finalizers, local_unfinalized_metrics)\n    result_value_type = computation_types.FederatedType(\n        (finalized_metrics_type, finalized_metrics_type), placements.SERVER)\n    measurements_type = computation_types.FederatedType(\n        secure_summation_process.next.type_signature.result.measurements.member,\n        placements.SERVER)\n    expected_next_type = computation_types.FunctionType(\n        parameter=collections.OrderedDict(\n            state=expected_state_type,\n            unfinalized_metrics=computation_types.FederatedType(\n                local_unfinalized_metrics_type, placements.CLIENTS)),\n        result=measured_process.MeasuredProcessOutput(expected_state_type,\n                                                      result_value_type,\n                                                      measurements_type))\n    self.assertTrue(\n        process.next.type_signature.is_equivalent_to(expected_next_type))\n\n    try:\n      static_assert.assert_not_contains_unsecure_aggregation(process.next)\n    except:  # pylint: disable=bare-except\n      self.fail('Metric aggregation contains non-secure summation aggregation')\n\n  @parameterized.named_parameters(\n      ('float', 1.0),\n      ('list',\n       [tf.function(func=lambda x: x),\n        tf.function(func=lambda x: x + 1)]))\n  def test_incorrect_finalizers_type_raises(self, bad_finalizers):\n    with self.assertRaises(TypeError):\n      aggregation_factory.SumThenFinalizeFactory(bad_finalizers)\n\n  @parameterized.named_parameters(\n      ('federated_type',\n       computation_types.FederatedType(tf.float32, placements.SERVER)),\n      ('function_type', computation_types.FunctionType(None, ())),\n      ('sequence_type', computation_types.SequenceType(tf.float32)))\n  def test_incorrect_unfinalized_metrics_type_raises(\n      self, bad_unfinalized_metrics_type):\n    metric_finalizers = collections.OrderedDict(\n        num_examples=tf.function(func=lambda x: x))\n    aggregate_factory = aggregation_factory.SumThenFinalizeFactory(\n        metric_finalizers)\n    with self.assertRaisesRegex(TypeError,\n                                'Expected .*`tff.types.StructWithPythonType`'):\n      aggregate_factory.create(bad_unfinalized_metrics_type)\n\n  def test_finalizers_and_unfinalized_metrics_type_mismatch_raises(self):\n    metric_finalizers = collections.OrderedDict(\n        num_examples=tf.function(func=lambda x: x))\n    aggregate_factory = aggregation_factory.SumThenFinalizeFactory(\n        metric_finalizers)\n    local_unfinalized_metrics_type = computation_types.StructWithPythonType(\n        collections.OrderedDict(\n            x=tf.TensorSpec(shape=[None, 2], dtype=tf.float32),\n            y=tf.TensorSpec(shape=[None, 1], dtype=tf.float32)),\n        container_type=collections.OrderedDict)\n    with self.assertRaisesRegex(\n        ValueError,\n        'The metric names in `metric_finalizers` do not match those'):\n      aggregate_factory.create(local_unfinalized_metrics_type)\n\n  def test_unfinalized_metrics_type_and_initial_values_mismatch_raises(self):\n    metric_finalizers = collections.OrderedDict(\n        num_examples=tf.function(func=lambda x: x))\n    local_unfinalized_metrics_type = type_conversions.type_from_tensors(\n        collections.OrderedDict(num_examples=1.0))\n    initial_unfinalized_metrics = collections.OrderedDict(num_examples=[1.0])\n    aggregate_factory = aggregation_factory.SumThenFinalizeFactory(\n        metric_finalizers,\n        initial_unfinalized_metrics=initial_unfinalized_metrics)\n    with self.assertRaisesRegex(TypeError, 'initial unfinalized metrics type'):\n      aggregate_factory.create(local_unfinalized_metrics_type)\n\n\nclass SumThenFinalizeFactoryExecutionTest(tf.test.TestCase):\n\n  def test_sum_then_finalize_metrics(self):\n    metric_finalizers = collections.OrderedDict(\n        num_examples=tf.function(func=lambda x: x),\n        loss=tf.function(func=lambda x: tf.math.divide_no_nan(x[0], x[1])),\n        custom_sum=tf.function(\n            func=lambda x: tf.add_n(map(tf.math.reduce_sum, x))))\n    local_unfinalized_metrics = collections.OrderedDict(\n        num_examples=1.0,\n        loss=[2.0, 1.0],\n        custom_sum=[tf.constant(1.0), tf.constant([1.0, 1.0])])\n    local_unfinalized_metrics_type = type_conversions.type_from_tensors(\n        local_unfinalized_metrics)\n    aggregate_factory = aggregation_factory.SumThenFinalizeFactory(\n        metric_finalizers)\n    process = aggregate_factory.create(local_unfinalized_metrics_type)\n\n    state = process.initialize()\n    _, unfinalized_metrics_accumulators = state\n    expected_unfinalized_metrics_accumulators = collections.OrderedDict(\n        num_examples=0.0,\n        loss=[0.0, 0.0],\n        custom_sum=[tf.constant(0.0), tf.constant([0.0, 0.0])])\n    tf.nest.map_structure(self.assertAllEqual, unfinalized_metrics_accumulators,\n                          expected_unfinalized_metrics_accumulators)\n\n    client_data = [local_unfinalized_metrics, local_unfinalized_metrics]\n    output = process.next(state, client_data)\n    _, unfinalized_metrics_accumulators = output.state\n    current_round_metrics, total_rounds_metrics = output.result\n    expected_unfinalized_metrics_accumulators = collections.OrderedDict(\n        num_examples=2.0,\n        loss=[4.0, 2.0],\n        custom_sum=[tf.constant(2.0), tf.constant([2.0, 2.0])])\n    tf.nest.map_structure(self.assertAllEqual, unfinalized_metrics_accumulators,\n                          expected_unfinalized_metrics_accumulators)\n    self.assertEqual(\n        current_round_metrics,\n        collections.OrderedDict(\n            num_examples=2.0, loss=2.0, custom_sum=tf.constant(6.0)))\n    self.assertEqual(\n        total_rounds_metrics,\n        collections.OrderedDict(\n            num_examples=2.0, loss=2.0, custom_sum=tf.constant(6.0)))\n\n  def test_sum_then_finalize_metrics_with_initial_values(self):\n    metric_finalizers = collections.OrderedDict(\n        num_examples=tf.function(func=lambda x: x),\n        loss=tf.function(func=lambda x: tf.math.divide_no_nan(x[0], x[1])))\n    local_unfinalized_metrics = collections.OrderedDict(\n        num_examples=1.0, loss=[2.0, 1.0])\n    local_unfinalized_metrics_type = type_conversions.type_from_tensors(\n        local_unfinalized_metrics)\n    initial_unfinalized_metrics = collections.OrderedDict(\n        num_examples=2.0, loss=[3.0, 2.0])\n    aggregate_factory = aggregation_factory.SumThenFinalizeFactory(\n        metric_finalizers,\n        initial_unfinalized_metrics=initial_unfinalized_metrics)\n    process = aggregate_factory.create(local_unfinalized_metrics_type)\n\n    state = process.initialize()\n    _, unfinalized_metrics_accumulators = state\n    self.assertEqual(unfinalized_metrics_accumulators,\n                     initial_unfinalized_metrics)\n\n    client_data = [local_unfinalized_metrics, local_unfinalized_metrics]\n    output = process.next(state, client_data)\n    _, unfinalized_metrics_accumulators = output.state\n    current_round_metrics, total_rounds_metrics = output.result\n    self.assertEqual(unfinalized_metrics_accumulators,\n                     collections.OrderedDict(num_examples=4.0, loss=[7.0, 4.0]))\n    self.assertEqual(current_round_metrics,\n                     collections.OrderedDict(num_examples=2.0, loss=2.0))\n    self.assertEqual(total_rounds_metrics,\n                     collections.OrderedDict(num_examples=4.0, loss=1.75))\n\n  def test_secure_sum_then_finalize_metrics(self):\n    metric_finalizers = collections.OrderedDict(\n        num_examples=tf.function(func=lambda x: x),\n        loss=tf.function(func=lambda x: tf.math.divide_no_nan(x[0], x[1])),\n        custom_sum=tf.function(\n            func=lambda x: tf.add_n(map(tf.math.reduce_sum, x))))\n    local_unfinalized_metrics = collections.OrderedDict(\n        num_examples=1,\n        loss=[2.0, 1.0],\n        custom_sum=[tf.constant(101.0),\n                    tf.constant([1.0, 1.0])])\n    local_unfinalized_metrics_type = type_conversions.type_from_tensors(\n        local_unfinalized_metrics)\n    secure_sum_factory = aggregation_factory.SecureSumFactory()\n\n    aggregate_factory = aggregation_factory.SumThenFinalizeFactory(\n        metric_finalizers, inner_summation_factory=secure_sum_factory)\n    process = aggregate_factory.create(local_unfinalized_metrics_type)\n    state = process.initialize()\n    init_inner_summation_process_state, unfinalized_metrics_accumulators = state\n    expected_init_unfinalized_metrics_accumulators = collections.OrderedDict(\n        num_examples=0.0,\n        loss=[0.0, 0.0],\n        custom_sum=[tf.constant(0.0), tf.constant([0.0, 0.0])])\n    secure_sum_process = secure_sum_factory.create(\n        local_unfinalized_metrics_type)\n    tf.nest.map_structure(self.assertAllEqual, unfinalized_metrics_accumulators,\n                          expected_init_unfinalized_metrics_accumulators)\n    tf.nest.map_structure(self.assertAllEqual,\n                          init_inner_summation_process_state,\n                          secure_sum_process.initialize())\n\n    client_data = [local_unfinalized_metrics, local_unfinalized_metrics]\n    output = process.next(state, client_data)\n    try:\n      static_assert.assert_not_contains_unsecure_aggregation(process.next)\n    except:  # pylint: disable=bare-except\n      self.fail('Metric aggregation contains non-secure summation aggregation')\n\n    _, unfinalized_metrics_accumulators = output.state\n    # Inital clippling bounds for float values are [-100.0, 100.0], metric\n    # values fall outside the ranges will be clipped to within the range (e.g.,\n    # 101.0 in `custom_sum` will be clipped to 100.0).\n    expected_unfinalized_metrics_accumulators = collections.OrderedDict(\n        num_examples=2.0,\n        loss=[4.0, 2.0],\n        custom_sum=[tf.constant(200.0),\n                    tf.constant([2.0, 2.0])])\n    tf.nest.map_structure(self.assertAllEqual, unfinalized_metrics_accumulators,\n                          expected_unfinalized_metrics_accumulators)\n\n    current_round_metrics, total_rounds_metrics = output.result\n    self.assertEqual(\n        current_round_metrics,\n        collections.OrderedDict(\n            num_examples=2.0, loss=2.0, custom_sum=tf.constant(204.0)))\n    self.assertEqual(\n        total_rounds_metrics,\n        collections.OrderedDict(\n            num_examples=2.0, loss=2.0, custom_sum=tf.constant(204.0)))\n\n    self.assertEqual(\n        output.measurements,\n        collections.OrderedDict([(_DEFAULT_FLOAT_FACTORY_KEY,\n                                  collections.OrderedDict(\n                                      secure_upper_clipped_count=2,\n                                      secure_lower_clipped_count=0,\n                                      secure_upper_threshold=100.0,\n                                      secure_lower_threshold=-100.0)),\n                                 (_DEFAULT_INT_FACTORY_KEY,\n                                  collections.OrderedDict(\n                                      secure_upper_clipped_count=0,\n                                      secure_lower_clipped_count=0,\n                                      secure_upper_threshold=1048575,\n                                      secure_lower_threshold=0))]))\n\n\nclass SecureSumFactoryTest(tf.test.TestCase, parameterized.TestCase):\n\n  def test_default_value_ranges_returns_correct_results(self):\n    aggregate_factory = aggregation_factory.SecureSumFactory()\n    local_unfinalized_metrics = collections.OrderedDict(\n        num_examples=1,\n        loss=[2.0, 1.0],\n        custom_sum=[tf.constant(1.0), tf.constant([1.0, 1.0])])\n    local_unfinalized_metrics_type = type_conversions.type_from_tensors(\n        local_unfinalized_metrics)\n    process = aggregate_factory.create(local_unfinalized_metrics_type)\n    try:\n      static_assert.assert_not_contains_unsecure_aggregation(process.next)\n    except:  # pylint: disable=bare-except\n      self.fail('Metric aggregation contains non-secure summation aggregation')\n\n    state = process.initialize()\n\n    expected_factory_keys = set(\n        [_DEFAULT_FLOAT_FACTORY_KEY, _DEFAULT_INT_FACTORY_KEY])\n    self.assertEqual(state.keys(), expected_factory_keys)\n\n    client_data = [local_unfinalized_metrics, local_unfinalized_metrics]\n    output = process.next(state, client_data)\n    self.assertEqual(output.state.keys(), expected_factory_keys)\n    # Assert only default float bounds are updated.\n    self.assertNotAllEqual(output.state[_DEFAULT_FLOAT_FACTORY_KEY],\n                           state[_DEFAULT_FLOAT_FACTORY_KEY])\n    self.assertAllEqual(output.state[_DEFAULT_INT_FACTORY_KEY],\n                        state[_DEFAULT_INT_FACTORY_KEY])\n\n    tf.nest.map_structure(\n        self.assertAllEqual, output.result,\n        collections.OrderedDict(\n            num_examples=2,\n            loss=[4.0, 2.0],\n            custom_sum=[tf.constant(2.0),\n                        tf.constant([2.0, 2.0])]))\n\n    self.assertEqual(\n        output.measurements,\n        collections.OrderedDict([(_DEFAULT_FLOAT_FACTORY_KEY,\n                                  collections.OrderedDict(\n                                      secure_upper_clipped_count=0,\n                                      secure_lower_clipped_count=0,\n                                      secure_upper_threshold=100.0,\n                                      secure_lower_threshold=-100.0)),\n                                 (_DEFAULT_INT_FACTORY_KEY,\n                                  collections.OrderedDict(\n                                      secure_upper_clipped_count=0,\n                                      secure_lower_clipped_count=0,\n                                      secure_upper_threshold=aggregation_factory\n                                      .DEFAULT_FIXED_SECURE_UPPER_BOUND,\n                                      secure_lower_threshold=aggregation_factory\n                                      .DEFAULT_FIXED_SECURE_LOWER_BOUND))]))\n\n  def test_user_value_ranges_returns_correct_results(self):\n    local_unfinalized_metrics = collections.OrderedDict(\n        num_examples=150,\n        loss=[2.0, 1.0],\n        custom_sum=[tf.constant(101.0),\n                    tf.constant([1.0, 1.0])])\n    local_unfinalized_metrics_type = type_conversions.type_from_tensors(\n        local_unfinalized_metrics)\n    metric_value_ranges = collections.OrderedDict(\n        num_examples=(0, 100), loss=[\n            None,\n            (0.0, 100.0),\n        ])\n    aggregate_factory = aggregation_factory.SecureSumFactory(\n        metric_value_ranges)\n    process = aggregate_factory.create(local_unfinalized_metrics_type)\n    try:\n      static_assert.assert_not_contains_unsecure_aggregation(process.next)\n    except:  # pylint: disable=bare-except\n      self.fail('Metric aggregation contains non-secure summation aggregation')\n\n    state = process.initialize()\n    custom_float_factory_key = aggregation_factory.create_factory_key(\n        0.0, 100.0, tf.float32)\n    custom_int_factory_key = aggregation_factory.create_factory_key(\n        0, 100, tf.int32)\n    expected_factory_keys = set([\n        _DEFAULT_FLOAT_FACTORY_KEY, custom_float_factory_key,\n        custom_int_factory_key\n    ])\n    self.assertEqual(state.keys(), expected_factory_keys)\n\n    client_data = [local_unfinalized_metrics, local_unfinalized_metrics]\n    output = process.next(state, client_data)\n    self.assertEqual(output.state.keys(), expected_factory_keys)\n    # Assert only default float bounds are updated.\n    self.assertNotAllEqual(output.state[_DEFAULT_FLOAT_FACTORY_KEY],\n                           state[_DEFAULT_FLOAT_FACTORY_KEY])\n    self.assertAllEqual(output.state[custom_float_factory_key],\n                        state[custom_float_factory_key])\n    self.assertAllEqual(output.state[custom_int_factory_key],\n                        state[custom_int_factory_key])\n\n    tf.nest.map_structure(\n        self.assertAllEqual, output.result,\n        collections.OrderedDict(\n            num_examples=200,\n            loss=[4.0, 2.0],\n            custom_sum=[tf.constant(200.0),\n                        tf.constant([2.0, 2.0])]))\n\n    self.assertEqual(\n        output.measurements,\n        collections.OrderedDict([(_DEFAULT_FLOAT_FACTORY_KEY,\n                                  collections.OrderedDict(\n                                      secure_upper_clipped_count=2,\n                                      secure_lower_clipped_count=0,\n                                      secure_upper_threshold=100.0,\n                                      secure_lower_threshold=-100.0)),\n                                 (custom_float_factory_key,\n                                  collections.OrderedDict(\n                                      secure_upper_clipped_count=0,\n                                      secure_lower_clipped_count=0,\n                                      secure_upper_threshold=100.0,\n                                      secure_lower_threshold=0.0)),\n                                 (custom_int_factory_key,\n                                  collections.OrderedDict(\n                                      secure_upper_clipped_count=2,\n                                      secure_lower_clipped_count=0,\n                                      secure_upper_threshold=100,\n                                      secure_lower_threshold=0))]))\n\n  @parameterized.named_parameters(\n      ('federated_type',\n       computation_types.FederatedType(tf.float32, placements.SERVER)),\n      ('function_type', computation_types.FunctionType(None, ())),\n      ('sequence_type', computation_types.SequenceType(tf.float32)))\n  def test_incorrect_unfinalized_metrics_type_raises(\n      self, bad_unfinalized_metrics_type):\n    secure_sum_factory = aggregation_factory.SecureSumFactory()\n    with self.assertRaisesRegex(TypeError,\n                                'Expected .*`tff.types.StructWithPythonType`'):\n      secure_sum_factory.create(bad_unfinalized_metrics_type)\n\n  def test_user_value_ranges_fails_invalid_dtype(self):\n    local_unfinalized_metrics = collections.OrderedDict(\n        custom_sum=[tf.constant('abc')])\n    secure_sum_factory = aggregation_factory.SecureSumFactory()\n    with self.assertRaises(aggregation_factory.UnquantizableDTypeError):\n      secure_sum_factory.create(\n          local_unfinalized_metrics_type=type_conversions.type_from_tensors(\n              local_unfinalized_metrics))\n\n  def test_user_value_ranges_fails_not_2_tuple(self):\n    local_unfinalized_metrics_type = type_conversions.type_from_tensors(\n        collections.OrderedDict(\n            accuracy=[tf.constant(1.0), tf.constant(2.0)]))\n    metric_value_ranges = collections.OrderedDict(accuracy=[\n        # Invalid specification\n        (0.0, 1.0, 2.0),\n        None\n    ])\n    secure_sum_factory = aggregation_factory.SecureSumFactory(\n        metric_value_ranges)\n    with self.assertRaisesRegex(ValueError, 'must be defined as a 2-tuple'):\n      secure_sum_factory.create(local_unfinalized_metrics_type)\n\n\nclass CreateDefaultSecureSumQuantizationRangesTest(parameterized.TestCase,\n                                                   tf.test.TestCase):\n\n  # The auto-tuned bound of float values is a `tff.templates.EstimationProcess`,\n  # simply check two bounds have the same type.\n  def assertAutoTunedBoundEqual(self, a, b, msg=None):\n    if isinstance(a, estimation_process.EstimationProcess):\n      return self.assertIsInstance(b, estimation_process.EstimationProcess, msg)\n\n  @parameterized.named_parameters(\n      ('float32', TensorType(tf.float32, [3]), _DEFAULT_AUTO_TUNED_FLOAT_RANGE),\n      ('float64', TensorType(tf.float64, [1]), _DEFAULT_AUTO_TUNED_FLOAT_RANGE),\n      ('int32', TensorType(tf.int32, [1]), _DEFAULT_INT_RANGE),\n      ('int64', TensorType(tf.int64, [3]), _DEFAULT_INT_RANGE),\n      ('<int64,float32>', computation_types.to_type([tf.int64, tf.float32]),\n       [_DEFAULT_INT_RANGE, _DEFAULT_AUTO_TUNED_FLOAT_RANGE]),\n      ('<a=int64,b=<c=float32,d=[int32,int32]>>',\n       computation_types.to_type(\n           collections.OrderedDict(\n               a=tf.int64,\n               b=collections.OrderedDict(c=tf.float32, d=[tf.int32, tf.int32\n                                                         ]))),\n       collections.OrderedDict(\n           a=_DEFAULT_INT_RANGE,\n           b=collections.OrderedDict(\n               c=_DEFAULT_AUTO_TUNED_FLOAT_RANGE,\n               d=[_DEFAULT_INT_RANGE, _DEFAULT_INT_RANGE]))),\n  )\n  def test_default_auto_tuned_range_construction(self, type_spec,\n                                                 expected_range):\n\n    self.addTypeEqualityFunc(estimation_process.EstimationProcess,\n                             self.assertAutoTunedBoundEqual)\n    tf.nest.map_structure(\n        self.assertEqual,\n        aggregation_factory.create_default_secure_sum_quantization_ranges(\n            type_spec), expected_range)\n\n  @parameterized.named_parameters(\n      ('float32', TensorType(tf.float32, [3]), _DEFAULT_FIXED_FLOAT_RANGE),\n      ('float64', TensorType(tf.float64, [1]), _DEFAULT_FIXED_FLOAT_RANGE),\n      ('int32', TensorType(tf.int32, [1]), _DEFAULT_INT_RANGE),\n      ('int64', TensorType(tf.int64, [3]), _DEFAULT_INT_RANGE),\n      ('<int64,float32>', computation_types.to_type([tf.int64, tf.float32]),\n       [_DEFAULT_INT_RANGE, _DEFAULT_FIXED_FLOAT_RANGE]),\n      ('<a=int64,b=<c=float32,d=[int32,int32]>>',\n       computation_types.to_type(\n           collections.OrderedDict(\n               a=tf.int64,\n               b=collections.OrderedDict(c=tf.float32, d=[tf.int32, tf.int32\n                                                         ]))),\n       collections.OrderedDict(\n           a=_DEFAULT_INT_RANGE,\n           b=collections.OrderedDict(\n               c=_DEFAULT_FIXED_FLOAT_RANGE,\n               d=[_DEFAULT_INT_RANGE, _DEFAULT_INT_RANGE]))),\n  )\n  def test_default_fixed_range_construction(self, type_spec, expected_range):\n    self.assertAllEqual(\n        aggregation_factory.create_default_secure_sum_quantization_ranges(\n            type_spec, use_auto_tuned_bounds_for_float_values=False),\n        expected_range)\n\n  @parameterized.named_parameters(\n      ('float32_float_range', TensorType(\n          tf.float32, [3]), 0.1, 0.5, _DEFAULT_AUTO_TUNED_FLOAT_RANGE),\n      ('float32_int_range', TensorType(\n          tf.float32, [3]), 1, 5, _DEFAULT_AUTO_TUNED_FLOAT_RANGE),\n      ('int32_int_range', TensorType(tf.int32, [1]), 1, 5, (1, 5)),\n      ('int32_float_range', TensorType(tf.int32, [1]), 1., 5., (1, 5)),\n      ('int32_float_range_truncated', TensorType(tf.int32, [1]), 1.5, 5.5,\n       (2, 5)),\n      ('<int64,float32>', computation_types.to_type([tf.int64, tf.float32]), 1,\n       5, [(1, 5), _DEFAULT_AUTO_TUNED_FLOAT_RANGE]),\n      ('<a=int64,b=<c=float32,d=[int32,int32]>>',\n       computation_types.to_type(\n           collections.OrderedDict(\n               a=tf.int64,\n               b=collections.OrderedDict(c=tf.float32, d=[tf.int32, tf.int32\n                                                         ]))), 1, 5,\n       collections.OrderedDict(\n           a=(1, 5),\n           b=collections.OrderedDict(\n               c=_DEFAULT_AUTO_TUNED_FLOAT_RANGE, d=[(1, 5), (1, 5)]))),\n  )\n  def test_user_supplied_range_using_default_auto_tuned_range(\n      self, type_spec, lower_bound, upper_bound, expected_range):\n    self.addTypeEqualityFunc(estimation_process.EstimationProcess,\n                             self.assertAutoTunedBoundEqual)\n    tf.nest.map_structure(\n        self.assertEqual,\n        aggregation_factory.create_default_secure_sum_quantization_ranges(\n            type_spec, lower_bound, upper_bound), expected_range)\n\n  @parameterized.named_parameters(\n      ('float32_float_range', TensorType(tf.float32, [3]), 0.1, 0.5,\n       (0.1, 0.5)),\n      ('float32_int_range', TensorType(tf.float32, [3]), 1, 5, (1., 5.)),\n      ('int32_int_range', TensorType(tf.int32, [1]), 1, 5, (1, 5)),\n      ('int32_float_range', TensorType(tf.int32, [1]), 1., 5., (1, 5)),\n      ('int32_float_range_truncated', TensorType(tf.int32, [1]), 1.5, 5.5,\n       (2, 5)),\n      ('<int64,float32>', computation_types.to_type(\n          [tf.int64, tf.float32]), 1, 5, [(1, 5), (1., 5.)]),\n      ('<a=int64,b=<c=float32,d=[int32,int32]>>',\n       computation_types.to_type(\n           collections.OrderedDict(\n               a=tf.int64,\n               b=collections.OrderedDict(c=tf.float32, d=[tf.int32, tf.int32\n                                                         ]))), 1, 5,\n       collections.OrderedDict(\n           a=(1, 5), b=collections.OrderedDict(c=(1., 5.), d=[(1, 5),\n                                                              (1, 5)]))),\n  )\n  def test_user_supplied_range_using_default_fixed_range(\n      self, type_spec, lower_bound, upper_bound, expected_range):\n    self.assertAllEqual(\n        aggregation_factory.create_default_secure_sum_quantization_ranges(\n            type_spec,\n            lower_bound,\n            upper_bound,\n            use_auto_tuned_bounds_for_float_values=False), expected_range)\n\n  def test_invalid_dtype(self):\n    with self.assertRaises(aggregation_factory.UnquantizableDTypeError):\n      aggregation_factory.create_default_secure_sum_quantization_ranges(\n          TensorType(tf.string))\n\n  def test_too_narrow_integer_range(self):\n    with self.assertRaisesRegex(ValueError, 'not wide enough'):\n      aggregation_factory.create_default_secure_sum_quantization_ranges(\n          TensorType(tf.int32), lower_bound=0.7, upper_bound=1.3)\n\n  def test_range_reversed(self):\n    with self.assertRaisesRegex(ValueError, 'must be greater than'):\n      aggregation_factory.create_default_secure_sum_quantization_ranges(\n          TensorType(tf.int32), lower_bound=10, upper_bound=5)\n    with self.assertRaisesRegex(ValueError, 'must be greater than'):\n      aggregation_factory.create_default_secure_sum_quantization_ranges(\n          TensorType(tf.int32), lower_bound=10., upper_bound=5.)\n\n\nclass FillMissingMetricValueRangesTest(parameterized.TestCase,\n                                       tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      ('none_user_ranges', None,\n       collections.OrderedDict(\n           num_example=MetricRange(0, 100),\n           loss=[MetricRange(0.0, 100.0),\n                 MetricRange(0.0, 100.0)])),\n      ('partial_user_ranges', collections.OrderedDict(loss=[None, (0.0,\n                                                                   400.0)]),\n       collections.OrderedDict(\n           num_example=MetricRange(0, 100),\n           loss=[MetricRange(0.0, 100.0),\n                 MetricRange(0.0, 400.0)])),\n      ('full_tuple_user_ranges',\n       collections.OrderedDict(\n           num_example=(10, 200), loss=[(-100.0, 300.0), (0.0, 400.0)]),\n       collections.OrderedDict(\n           num_example=MetricRange(10, 200),\n           loss=[MetricRange(-100.0, 300.0),\n                 MetricRange(0.0, 400.0)])),\n      ('full_metric_range_user_ranges',\n       collections.OrderedDict(\n           num_example=MetricRange(10, 200),\n           loss=[MetricRange(-100.0, 300.0),\n                 MetricRange(0.0, 400.0)]),\n       collections.OrderedDict(\n           num_example=MetricRange(10, 200),\n           loss=[MetricRange(-100.0, 300.0),\n                 MetricRange(0.0, 400.0)])))\n  def test_fill_user_ranges_returns_correct_results(self, user_ranges,\n                                                    expected_filled_ranges):\n    default_ranges = collections.OrderedDict(\n        num_example=(0, 100), loss=[(0.0, 100.0), (0.0, 100.0)])\n    filled_ranges = aggregation_factory.fill_missing_values_with_defaults(\n        default_ranges, user_ranges)\n    tf.nest.map_structure(self.assertAllEqual, filled_ranges,\n                          expected_filled_ranges)\n\n  @parameterized.named_parameters(\n      ('range_as_list',\n       collections.OrderedDict(\n           num_example=[10, 200], loss=[None, [0.0, 400.0]]), 'range'),\n      ('invalid_bound_type',\n       collections.OrderedDict(num_example=('lower', 'upper')), 'lower bound'),\n      ('bounds_not_match', collections.OrderedDict(num_example=(1.0, 100)),\n       'same type'))\n  def test_invalid_user_ranges_type_raises(self, user_ranges, expected_regex):\n    default_ranges = collections.OrderedDict(\n        num_example=(0, 100), loss=[(0.0, 100.0), (0.0, 100.0)])\n    with self.assertRaisesRegex(TypeError, expected_regex):\n      aggregation_factory.fill_missing_values_with_defaults(\n          default_ranges, user_ranges)\n\n  @parameterized.named_parameters(\n      ('1_tuple', collections.OrderedDict(num_example=(10,))),\n      ('3_tuple', collections.OrderedDict(num_example=(10, 50, 100))))\n  def test_invalid_user_ranges_value_raises(self, user_ranges):\n    default_ranges = collections.OrderedDict(\n        num_example=(0, 100), loss=[(0.0, 100.0), (0.0, 100.0)])\n    with self.assertRaisesRegex(ValueError, '2-tuple'):\n      aggregation_factory.fill_missing_values_with_defaults(\n          default_ranges, user_ranges)\n\n\nif __name__ == '__main__':\n  execution_contexts.set_test_python_execution_context()\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/federated", "file_path": "tensorflow_federated/python/learning/templates/model_delta_client_work_test.py", "content": "# Copyright 2021, The TensorFlow Federated Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\nfrom unittest import mock\n\nfrom absl.testing import parameterized\nimport attr\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_federated.python.core.backends.native import execution_contexts\nfrom tensorflow_federated.python.core.impl.federated_context import federated_computation\nfrom tensorflow_federated.python.core.impl.federated_context import intrinsics\nfrom tensorflow_federated.python.core.impl.tensorflow_context import tensorflow_computation\nfrom tensorflow_federated.python.core.impl.types import computation_types\nfrom tensorflow_federated.python.core.impl.types import type_test_utils\nfrom tensorflow_federated.python.core.templates import measured_process\nfrom tensorflow_federated.python.learning import client_weight_lib\nfrom tensorflow_federated.python.learning import keras_utils\nfrom tensorflow_federated.python.learning import model_examples\nfrom tensorflow_federated.python.learning.framework import dataset_reduce\nfrom tensorflow_federated.python.learning.metrics import counters\nfrom tensorflow_federated.python.learning.models import functional\nfrom tensorflow_federated.python.learning.models import model_weights as model_weights_lib\nfrom tensorflow_federated.python.learning.optimizers import sgdm\nfrom tensorflow_federated.python.learning.templates import client_works\nfrom tensorflow_federated.python.learning.templates import model_delta_client_work\n\n\nclass ModelDeltaClientWorkComputationTest(tf.test.TestCase,\n                                          parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_initialize_has_expected_type_signature_with_keras_optimizer(\n      self, weighting):\n    optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n    model_fn = model_examples.LinearRegression\n\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        model_fn, optimizer_fn, weighting)\n\n    expected_state_type = computation_types.at_server(())\n    expected_initialize_type = computation_types.FunctionType(\n        parameter=None, result=expected_state_type)\n    expected_initialize_type.check_equivalent_to(\n        client_work_process.initialize.type_signature)\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_next_has_expected_type_signature_with_keras_optimizer(\n      self, weighting):\n    optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n    model_fn = model_examples.LinearRegression\n\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        model_fn, optimizer_fn, weighting)\n\n    mw_type = model_weights_lib.ModelWeights(\n        trainable=computation_types.to_type([(tf.float32, (2, 1)), tf.float32]),\n        non_trainable=computation_types.to_type([tf.float32]))\n    expected_param_model_weights_type = computation_types.at_clients(mw_type)\n    expected_param_data_type = computation_types.at_clients(\n        computation_types.SequenceType(\n            computation_types.to_type(model_fn().input_spec)))\n    expected_result_type = computation_types.at_clients(\n        client_works.ClientResult(\n            update=mw_type.trainable,\n            update_weight=computation_types.TensorType(tf.float32)))\n    expected_state_type = computation_types.at_server(())\n    expected_measurements_type = computation_types.at_server(\n        collections.OrderedDict(\n            train=collections.OrderedDict(\n                loss=tf.float32, num_examples=tf.int32)))\n    expected_next_type = computation_types.FunctionType(\n        parameter=collections.OrderedDict(\n            state=expected_state_type,\n            weights=expected_param_model_weights_type,\n            client_data=expected_param_data_type),\n        result=measured_process.MeasuredProcessOutput(\n            expected_state_type, expected_result_type,\n            expected_measurements_type))\n    type_test_utils.assert_types_equivalent(\n        client_work_process.next.type_signature, expected_next_type)\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_initialize_has_expected_type_signature_with_tff_optimizer(\n      self, weighting):\n    optimizer = sgdm.build_sgdm(learning_rate=1.0)\n    model_fn = model_examples.LinearRegression\n\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        model_fn, optimizer, weighting)\n\n    expected_state_type = computation_types.at_server(\n        collections.OrderedDict(learning_rate=tf.float32))\n    expected_initialize_type = computation_types.FunctionType(\n        parameter=None, result=expected_state_type)\n    type_test_utils.assert_types_equivalent(\n        client_work_process.initialize.type_signature, expected_initialize_type)\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_next_has_expected_type_signature_with_tff_optimizer(self, weighting):\n    optimizer = sgdm.build_sgdm(learning_rate=1.0)\n    model_fn = model_examples.LinearRegression\n\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        model_fn, optimizer, weighting)\n\n    mw_type = model_weights_lib.ModelWeights(\n        trainable=computation_types.to_type([(tf.float32, (2, 1)), tf.float32]),\n        non_trainable=computation_types.to_type([tf.float32]))\n    expected_param_model_weights_type = computation_types.at_clients(mw_type)\n    expected_param_data_type = computation_types.at_clients(\n        computation_types.SequenceType(\n            computation_types.to_type(model_fn().input_spec)))\n    expected_result_type = computation_types.at_clients(\n        client_works.ClientResult(\n            update=mw_type.trainable,\n            update_weight=computation_types.TensorType(tf.float32)))\n    expected_state_type = computation_types.at_server(\n        collections.OrderedDict(learning_rate=tf.float32))\n    expected_measurements_type = computation_types.at_server(\n        collections.OrderedDict(\n            train=collections.OrderedDict(\n                loss=tf.float32, num_examples=tf.int32)))\n    expected_next_type = computation_types.FunctionType(\n        parameter=collections.OrderedDict(\n            state=expected_state_type,\n            weights=expected_param_model_weights_type,\n            client_data=expected_param_data_type),\n        result=measured_process.MeasuredProcessOutput(\n            expected_state_type, expected_result_type,\n            expected_measurements_type))\n    type_test_utils.assert_types_equivalent(\n        client_work_process.next.type_signature, expected_next_type)\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_get_hparams_has_expected_type_signature_with_keras_optimizer(\n      self, weighting):\n    optimizer = lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n    model_fn = model_examples.LinearRegression\n\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        model_fn, optimizer, weighting)\n\n    expected_state_type = collections.OrderedDict()\n    expected_hparams_type = expected_state_type\n    expected_get_hparams_type = computation_types.FunctionType(\n        parameter=expected_state_type, result=expected_hparams_type)\n    type_test_utils.assert_types_equivalent(\n        client_work_process.get_hparams.type_signature,\n        expected_get_hparams_type)\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_set_hparams_has_expected_type_signature_with_keras_optimizer(\n      self, weighting):\n    optimizer = lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n    model_fn = model_examples.LinearRegression\n\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        model_fn, optimizer, weighting)\n\n    expected_state_type = collections.OrderedDict()\n    expected_hparams_type = expected_state_type\n    expected_parameter_type = computation_types.StructType([\n        ('state', expected_state_type), ('hparams', expected_hparams_type)\n    ])\n    expected_set_hparams_type = computation_types.FunctionType(\n        parameter=expected_parameter_type, result=expected_state_type)\n    type_test_utils.assert_types_equivalent(\n        client_work_process.set_hparams.type_signature,\n        expected_set_hparams_type)\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_get_hparams_has_expected_type_signature_with_tff_optimizer(\n      self, weighting):\n    optimizer = sgdm.build_sgdm(learning_rate=1.0)\n    model_fn = model_examples.LinearRegression\n\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        model_fn, optimizer, weighting)\n\n    expected_state_type = collections.OrderedDict(learning_rate=tf.float32)\n    expected_hparams_type = expected_state_type\n    expected_get_hparams_type = computation_types.FunctionType(\n        parameter=expected_state_type, result=expected_hparams_type)\n    type_test_utils.assert_types_equivalent(\n        client_work_process.get_hparams.type_signature,\n        expected_get_hparams_type)\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_set_hparams_has_expected_type_signature_with_tff_optimizer(\n      self, weighting):\n    optimizer = sgdm.build_sgdm(learning_rate=1.0)\n    model_fn = model_examples.LinearRegression\n\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        model_fn, optimizer, weighting)\n\n    expected_state_type = collections.OrderedDict(learning_rate=tf.float32)\n    expected_hparams_type = expected_state_type\n    expected_parameter_type = computation_types.StructType([\n        ('state', expected_state_type), ('hparams', expected_hparams_type)\n    ])\n    expected_set_hparams_type = computation_types.FunctionType(\n        parameter=expected_parameter_type, result=expected_state_type)\n    type_test_utils.assert_types_equivalent(\n        client_work_process.set_hparams.type_signature,\n        expected_set_hparams_type)\n\n  def test_raises_with_created_keras_optimizer(self):\n    with self.assertRaises(TypeError):\n      model_delta_client_work.build_model_delta_client_work(\n          model_examples.LinearRegression,\n          tf.keras.optimizers.SGD(learning_rate=1.0),\n          client_weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES)\n\n  def test_raises_with_created_model(self):\n    with self.assertRaises(TypeError):\n      model_delta_client_work.build_model_delta_client_work(\n          model_examples.LinearRegression(),\n          sgdm.build_sgdm(learning_rate=1.0),\n          client_weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES)\n\n\ndef create_test_dataset() -> tf.data.Dataset:\n  # Create a dataset with 4 examples:\n  dataset = tf.data.Dataset.from_tensor_slices(\n      collections.OrderedDict(\n          x=[[0.0, 0.0], [1.0, 0.0], [2.0, 0.0], [3.0, 0.0]],\n          y=[[0.0], [0.0], [1.0], [1.0]]))\n  # Repeat the dataset 2 times with batches of 3 examples,\n  # producing 3 minibatches (the last one with only 2 examples).\n  # Note that `batch` is required for this dataset to be useable,\n  # as it adds the batch dimension which is expected by the model.\n  return dataset.repeat(2).batch(3)\n\n\ndef create_test_initial_weights() -> model_weights_lib.ModelWeights:\n  return model_weights_lib.ModelWeights(\n      trainable=[tf.zeros((2, 1)), tf.constant(0.0)], non_trainable=[0.0])\n\n\ndef create_model():\n  return model_examples.LinearRegression(feature_dim=2)\n\n\nclass ModelDeltaClientWorkExecutionTest(tf.test.TestCase,\n                                        parameterized.TestCase):\n  \"\"\"Tests of the client work of FedAvg using a common model and data.\"\"\"\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_keras_tff_client_work_equal(self, weighting):\n    dataset = create_test_dataset()\n    client_update_keras = model_delta_client_work.build_model_delta_update_with_keras_optimizer(\n        model_fn=create_model, weighting=weighting)\n    client_update_tff = model_delta_client_work.build_model_delta_update_with_tff_optimizer(\n        model_fn=create_model, weighting=weighting)\n    keras_result = client_update_keras(\n        tf.keras.optimizers.SGD(learning_rate=0.1),\n        create_test_initial_weights(), dataset)\n    tff_result = client_update_tff(\n        sgdm.build_sgdm(learning_rate=0.1), create_test_initial_weights(),\n        dataset)\n    self.assertAllClose(keras_result[0].update, tff_result[0].update)\n    self.assertEqual(keras_result[0].update_weight, tff_result[0].update_weight)\n    self.assertAllClose(keras_result[1], tff_result[1])\n\n  @parameterized.named_parameters(\n      ('non-simulation_noclip_uniform', False, {}, 0.1,\n       client_weight_lib.ClientWeighting.UNIFORM),\n      ('non-simulation_noclip_num_examples', False, {}, 0.1,\n       client_weight_lib.ClientWeighting.NUM_EXAMPLES),\n      ('simulation_noclip_uniform', True, {}, 0.1,\n       client_weight_lib.ClientWeighting.UNIFORM),\n      ('simulation_noclip_num_examples', True, {}, 0.1,\n       client_weight_lib.ClientWeighting.NUM_EXAMPLES),\n      ('non-simulation_clipnorm', False, {\n          'clipnorm': 0.2\n      }, 0.05, client_weight_lib.ClientWeighting.NUM_EXAMPLES),\n      ('non-simulation_clipvalue', False, {\n          'clipvalue': 0.1\n      }, 0.02, client_weight_lib.ClientWeighting.NUM_EXAMPLES),\n  )\n  def test_client_tf(self, simulation, optimizer_kwargs, expected_norm,\n                     weighting):\n    client_tf = model_delta_client_work.build_model_delta_update_with_keras_optimizer(\n        model_fn=create_model,\n        weighting=weighting,\n        use_experimental_simulation_loop=simulation)\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, **optimizer_kwargs)\n    dataset = create_test_dataset()\n    client_result, model_output = self.evaluate(\n        client_tf(optimizer, create_test_initial_weights(), dataset))\n    # Both trainable parameters should have been updated, and we don't return\n    # the non-trainable variable.\n    self.assertAllGreater(\n        np.linalg.norm(client_result.update, axis=-1), expected_norm)\n    if weighting == client_weight_lib.ClientWeighting.UNIFORM:\n      self.assertEqual(client_result.update_weight, 1.0)\n    else:\n      self.assertEqual(client_result.update_weight, 8.0)\n    self.assertDictContainsSubset({\n        'num_examples': 8,\n    }, model_output)\n    self.assertBetween(model_output['loss'][0], np.finfo(np.float32).eps, 10.0)\n\n  @parameterized.named_parameters(('_inf', np.inf), ('_nan', np.nan))\n  def test_non_finite_aggregation(self, bad_value):\n    client_tf = model_delta_client_work.build_model_delta_update_with_keras_optimizer(\n        model_fn=create_model,\n        weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES)\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n    dataset = create_test_dataset()\n    init_weights = create_test_initial_weights()\n    init_weights.trainable[1] = bad_value\n    client_outputs = client_tf(optimizer, init_weights, dataset)\n    self.assertEqual(self.evaluate(client_outputs[0].update_weight), 0.0)\n    self.assertAllClose(\n        self.evaluate(client_outputs[0].update), [[[0.0], [0.0]], 0.0])\n\n  def test_custom_metrics_aggregator(self):\n\n    def sum_then_finalize_then_times_two(metric_finalizers,\n                                         local_unfinalized_metrics_type):\n\n      @federated_computation.federated_computation(\n          computation_types.at_clients(local_unfinalized_metrics_type))\n      def aggregation_computation(client_local_unfinalized_metrics):\n        unfinalized_metrics_sum = intrinsics.federated_sum(\n            client_local_unfinalized_metrics)\n\n        @tensorflow_computation.tf_computation(local_unfinalized_metrics_type)\n        def finalizer_computation(unfinalized_metrics):\n          finalized_metrics = collections.OrderedDict()\n          for metric_name, metric_finalizer in metric_finalizers.items():\n            finalized_metrics[metric_name] = metric_finalizer(\n                unfinalized_metrics[metric_name]) * 2\n          return finalized_metrics\n\n        return intrinsics.federated_map(finalizer_computation,\n                                        unfinalized_metrics_sum)\n\n      return aggregation_computation\n\n    process = model_delta_client_work.build_model_delta_client_work(\n        model_fn=create_model,\n        optimizer=sgdm.build_sgdm(learning_rate=1.0),\n        client_weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES,\n        metrics_aggregator=sum_then_finalize_then_times_two)\n    client_model_weights = [create_test_initial_weights()]\n    client_data = [create_test_dataset()]\n    output = process.next(process.initialize(), client_model_weights,\n                          client_data)\n    # Train metrics should be multiplied by two by the custom aggregator.\n    self.assertEqual(output.measurements['train']['num_examples'], 16)\n\n  @parameterized.named_parameters(('non-simulation', False),\n                                  ('simulation', True))\n  @mock.patch.object(\n      dataset_reduce,\n      '_dataset_reduce_fn',\n      wraps=dataset_reduce._dataset_reduce_fn)\n  def test_client_tf_dataset_reduce_fn(self, simulation, mock_method):\n    client_tf = model_delta_client_work.build_model_delta_update_with_keras_optimizer(\n        model_fn=create_model,\n        weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES,\n        use_experimental_simulation_loop=simulation)\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n    dataset = create_test_dataset()\n    client_tf(optimizer, create_test_initial_weights(), dataset)\n    if simulation:\n      mock_method.assert_not_called()\n    else:\n      mock_method.assert_called()\n\n  @parameterized.named_parameters(\n      ('tff_simple', sgdm.build_sgdm(learning_rate=1.0)),\n      ('tff_momentum', sgdm.build_sgdm(learning_rate=1.0, momentum=0.9)),\n      ('keras_simple', lambda: tf.keras.optimizers.SGD(learning_rate=1.0)),\n      ('keras_momentum',\n       lambda: tf.keras.optimizers.SGD(learning_rate=1.0, momentum=0.9)))\n  def test_execution_with_optimizer(self, optimizer):\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        create_model,\n        optimizer,\n        client_weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES)\n    client_data = [create_test_dataset()]\n    client_model_weights = [create_test_initial_weights()]\n\n    state = client_work_process.initialize()\n    output = client_work_process.next(state, client_model_weights, client_data)\n\n    self.assertCountEqual(output.measurements.keys(), ['train'])\n\n  def test_get_hparams_returns_expected_result_with_tff_optimizer(self):\n    optimizer = sgdm.build_sgdm(learning_rate=1.0, momentum=0.9)\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        create_model,\n        optimizer,\n        client_weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES)\n    state = client_work_process.initialize()\n\n    hparams = client_work_process.get_hparams(state)\n\n    expected_hparams = collections.OrderedDict(learning_rate=1.0, momentum=0.9)\n    self.assertDictEqual(hparams, expected_hparams)\n\n  def test_get_hparams_returns_expected_result_with_keras_optimizer(self):\n    optimizer = lambda: tf.keras.optimizers.SGD(learning_rate=1.0, momentum=0.9)\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        create_model,\n        optimizer,\n        client_weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES)\n    state = client_work_process.initialize()\n\n    hparams = client_work_process.get_hparams(state)\n\n    expected_hparams = collections.OrderedDict()\n    self.assertDictEqual(hparams, expected_hparams)\n\n  def test_set_hparams_returns_expected_result_with_tff_optimizer(self):\n    optimizer = sgdm.build_sgdm(learning_rate=1.0, momentum=0.9)\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        create_model,\n        optimizer,\n        client_weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES)\n    state = client_work_process.initialize()\n    hparams = collections.OrderedDict(learning_rate=0.5, momentum=0.3)\n\n    state = client_work_process.set_hparams(state, hparams)\n\n    self.assertDictEqual(state, hparams)\n\n  def test_set_hparams_returns_expected_result_with_keras_optimizer(self):\n    optimizer = lambda: tf.keras.optimizers.SGD(learning_rate=1.0, momentum=0.9)\n    client_work_process = model_delta_client_work.build_model_delta_client_work(\n        create_model,\n        optimizer,\n        client_weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES)\n    state = client_work_process.initialize()\n    hparams = collections.OrderedDict()\n\n    updated_state = client_work_process.set_hparams(state, hparams)\n\n    self.assertEqual(updated_state, state)\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_tff_client_work_uses_optimizer_hparams(self, weighting):\n    dataset = create_test_dataset()\n    optimizer1 = sgdm.build_sgdm(learning_rate=0.0)\n    optimizer2 = sgdm.build_sgdm(learning_rate=0.1)\n    optimizer_hparams = collections.OrderedDict(learning_rate=1.0)\n\n    client_update_tff = model_delta_client_work.build_model_delta_update_with_tff_optimizer(\n        model_fn=create_model, weighting=weighting)\n    result1 = client_update_tff(optimizer1, create_test_initial_weights(),\n                                dataset, optimizer_hparams)\n    client_update_tff = model_delta_client_work.build_model_delta_update_with_tff_optimizer(\n        model_fn=create_model, weighting=weighting)\n    result2 = client_update_tff(optimizer2, create_test_initial_weights(),\n                                dataset, optimizer_hparams)\n\n    self.assertAllClose(result1[0].update, result2[0].update)\n    self.assertEqual(result1[0].update_weight, result2[0].update_weight)\n    self.assertAllClose(result1[1], result2[1])\n\n\nclass FunctionalModelDeltaClientWorkExecutionTest(tf.test.TestCase,\n                                                  parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      ('uniform', client_weight_lib.ClientWeighting.UNIFORM),\n      ('num_examples', client_weight_lib.ClientWeighting.NUM_EXAMPLES))\n  def test_functional_model_matches_model_fn(self, weighting):\n    dataset = create_test_dataset()\n\n    # Build a FunctionalModel based client_model_update procedure. This will\n    # be compared to a model_fn based implementation built below.\n    keras_model = model_examples.build_linear_regression_keras_functional_model(\n        feature_dims=2)\n    loss_fn = tf.keras.losses.MeanSquaredError()\n    input_spec = dataset.element_spec\n    functional_model = functional.functional_model_from_keras(\n        keras_model, loss_fn=loss_fn, input_spec=input_spec)\n\n    # Note: we must wrap in a `tf_computation` for the correct graph-context\n    # processing of Keras models wrapped as FunctionalModel.\n    @tensorflow_computation.tf_computation\n    def client_update_functional_model(model_weights, dataset):\n      model_delta_fn = model_delta_client_work.build_functional_model_delta_update(\n          model=functional_model, weighting=weighting)\n      return model_delta_fn(\n          sgdm.build_sgdm(learning_rate=0.1), model_weights, dataset)\n\n    # Build a model_fn based client_model_update procedure. This will be\n    # comapred to the FunctionalModel variant built above to ensure they\n    # procduce the same results.\n    def model_fn():\n      keras_model = model_examples.build_linear_regression_keras_functional_model(\n          feature_dims=2)\n      loss_fn = tf.keras.losses.MeanSquaredError()\n      input_spec = dataset.element_spec\n      return keras_utils.from_keras_model(\n          keras_model, loss=loss_fn, input_spec=input_spec)\n\n    client_update_model_fn = model_delta_client_work.build_model_delta_update_with_tff_optimizer(\n        model_fn=model_fn, weighting=weighting)\n    model_fn_optimizer = sgdm.build_sgdm(learning_rate=0.1)\n    model_fn_weights = model_weights_lib.ModelWeights.from_model(model_fn())\n\n    functional_model_weights = functional_model.initial_weights\n    for _ in range(10):\n      # pylint: disable=cell-var-from-loop\n      model_fn_output, _ = client_update_model_fn(model_fn_optimizer,\n                                                  model_fn_weights, dataset)\n      functional_model_output, _ = client_update_functional_model(\n          functional_model_weights, dataset)\n      self.assertAllClose(model_fn_output.update,\n                          functional_model_output.update)\n      self.assertAllClose(model_fn_output.update_weight,\n                          functional_model_output.update_weight)\n      model_fn_weights = attr.evolve(\n          model_fn_weights,\n          trainable=tf.nest.map_structure(\n              lambda u, v: u + v * model_fn_output.update_weight,\n              model_fn_weights.trainable, model_fn_output.update))\n      functional_model_weights = (tf.nest.map_structure(\n          lambda u, v: u + v * functional_model_output.update_weight,\n          functional_model_weights[0],\n          functional_model_output.update), functional_model_weights[1])\n      # pylint: enable=cell-var-from-loop\n    self.assertAllClose(\n        attr.astuple(model_fn_weights), functional_model_weights)\n\n  def test_metrics_output(self):\n    keras_model = model_examples.build_linear_regression_keras_functional_model(\n        feature_dims=2)\n    loss_fn = tf.keras.losses.MeanSquaredError()\n    dataset = create_test_dataset()\n    input_spec = dataset.element_spec\n\n    def build_metrics_fn():\n      return collections.OrderedDict(num_examples=counters.NumExamplesCounter())\n\n    functional_model = functional.functional_model_from_keras(\n        keras_model,\n        loss_fn=loss_fn,\n        input_spec=input_spec,\n        metrics_constructor=build_metrics_fn)\n\n    process = model_delta_client_work.build_functional_model_delta_client_work(\n        model=functional_model,\n        optimizer=sgdm.build_sgdm(learning_rate=1.0),\n        client_weighting=client_weight_lib.ClientWeighting.NUM_EXAMPLES)\n    num_clients = 3\n    client_model_weights = [functional_model.initial_weights] * num_clients\n    client_datasets = [dataset] * num_clients\n    output = process.next(process.initialize(), client_model_weights,\n                          client_datasets)\n    self.assertEqual(output.measurements['train']['num_examples'],\n                     8 * num_clients)\n\n\nif __name__ == '__main__':\n  execution_contexts.set_localhost_cpp_execution_context()\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/model-optimization", "file_path": "tensorflow_model_optimization/python/core/clustering/keras/cluster_test.py", "content": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for keras clustering API.\"\"\"\n\nimport json\nimport os\nimport tempfile\n\nfrom absl.testing import parameterized\nimport tensorflow as tf\n\nfrom tensorflow_model_optimization.python.core.clustering.keras import cluster\nfrom tensorflow_model_optimization.python.core.clustering.keras import cluster_config\nfrom tensorflow_model_optimization.python.core.clustering.keras import cluster_wrapper\nfrom tensorflow_model_optimization.python.core.clustering.keras import clusterable_layer\nfrom tensorflow_model_optimization.python.core.clustering.keras import clustering_registry\nfrom tensorflow_model_optimization.python.core.clustering.keras.experimental import cluster as experimental_cluster\n\nkeras = tf.keras\nerrors_impl = tf.errors\nlayers = keras.layers\ntest = tf.test\n\n\nclass TestModel(keras.Model):\n  \"\"\"A model subclass.\"\"\"\n\n  def __init__(self):\n    \"\"\"A test subclass model with one dense layer.\"\"\"\n    super(TestModel, self).__init__(name='test_model')\n    self.layer1 = keras.layers.Dense(10, activation='relu')\n\n  def call(self, inputs):\n    return self.layer1(inputs)\n\n\nclass CustomClusterableLayer(layers.Dense, clusterable_layer.ClusterableLayer):\n\n  def get_clusterable_weights(self):\n    return [('kernel', self.kernel)]\n\n\nclass CustomNonClusterableLayer(layers.Dense):\n  pass\n\n\nclass KerasCustomLayer(keras.layers.Layer):\n\n  def __init__(self, units=32):\n    super(KerasCustomLayer, self).__init__()\n    self.units = units\n\n  def build(self, input_shape):\n    self.w = self.add_weight(\n        shape=(input_shape[-1], self.units),\n        initializer='random_normal',\n        trainable=True,\n    )\n    self.b = self.add_weight(\n        shape=(self.units,), initializer='random_normal', trainable=False)\n\n  def call(self, inputs):\n    return tf.matmul(inputs, self.w) + self.b\n\n\nclass MyClusterableLayer(keras.layers.Dense,\n                         clusterable_layer.ClusterableLayer):\n\n  def get_clusterable_weights(self):\n    # Cluster kernel and bias.\n    return [('kernel', self.kernel), ('bias', self.bias)]\n\n\nclass MyClusterableLayerInvalid(keras.layers.Dense,\n                                clusterable_layer.ClusterableLayer):\n  \"\"\"Invalid layer: it does not implement get_clusterable_weights(self).\"\"\"\n  pass\n\n\nclass TestCustomerableWeightsCA(clustering_registry.ClusteringAlgorithm\n                               ):\n  \"\"\"Dummy class derived from ClusteringAlgorithm.\"\"\"\n\n  def get_pulling_indices(self, weight):\n    return [1, 2, 3]\n\n\nclass KerasCustomLayerClusterable(keras.layers.Layer,\n                                  clusterable_layer.ClusterableLayer):\n  \"\"\"Custom Keras clusterable layer, providing its own clustering algorithm.\"\"\"\n\n  def __init__(self):\n    super().__init__()\n    self.kernel = None\n\n  def get_clusterable_weights(self):\n    return [('kernel', self.kernel)]\n\n  def get_clusterable_algorithm(self, weight_name):\n    return TestCustomerableWeightsCA\n\n\nclass ClusterTest(test.TestCase, parameterized.TestCase):\n  \"\"\"Unit tests for the cluster module.\"\"\"\n\n  def setUp(self):\n    super(ClusterTest, self).setUp()\n\n    self.keras_clusterable_layer = layers.Dense(10)\n    self.keras_non_clusterable_layer = layers.Dropout(0.4)\n    self.keras_unsupported_layer = layers.ConvLSTM2D(2, (5, 5))  # Unsupported\n    self.custom_clusterable_layer = CustomClusterableLayer(10)\n    self.custom_non_clusterable_layer = CustomNonClusterableLayer(10)\n    self.keras_depthwiseconv2d_layer = layers.DepthwiseConv2D((3, 3), (1, 1))\n    self.keras_dense_layer = layers.Dense(10)\n    self.keras_conv1d_layer = layers.Conv1D(filters=3, kernel_size=(5))\n    self.keras_conv1d_tr_layer = layers.Conv1DTranspose(\n        filters=3, kernel_size=(5))\n    self.keras_conv2d_layer = layers.Conv2D(filters=3, kernel_size=(4, 5))\n    self.keras_conv2d_tr_layer = layers.Conv2DTranspose(\n        filters=3, kernel_size=(4, 5))\n    self.keras_conv3d_layer = layers.Conv3D(filters=2, kernel_size=(3, 4, 5))\n    self.keras_custom_layer = KerasCustomLayer()\n    self.clusterable_layer = MyClusterableLayer(10)\n\n    self.model = keras.Sequential()\n    self.params = {\n        'number_of_clusters':\n            8,\n        'cluster_centroids_init':\n            cluster_config.CentroidInitialization.DENSITY_BASED\n    }\n\n  def _build_clustered_layer_model(self, layer, input_shape=(10, 1)):\n    self.model.add(keras.Input(shape=input_shape))\n    self.model.add(layer)\n    self.model.build()\n    wrapped_layer = cluster.cluster_weights(self.model.layers[0], **self.params)\n    return wrapped_layer\n\n  def _validate_clustered_layer(self, original_layer, wrapped_layer):\n    self.assertIsInstance(wrapped_layer, cluster_wrapper.ClusterWeights)\n    self.assertEqual(original_layer, wrapped_layer.layer)\n\n  def _count_clustered_layers(self, model):\n    count = 0\n    for layer in model.layers:\n      if isinstance(layer, cluster_wrapper.ClusterWeights):\n        count += 1\n    return count\n\n  def testClusterKerasClusterableLayer(self):\n    \"\"\"Verifies that a built-in keras layer marked as clusterable is being clustered correctly.\"\"\"\n    wrapped_layer = self._build_clustered_layer_model(\n        self.keras_clusterable_layer)\n\n    self._validate_clustered_layer(self.keras_clusterable_layer, wrapped_layer)\n\n  def testClusterKerasClusterableLayerWithSparsityPreservation(self):\n    \"\"\"Verifies that a built-in keras layer marked as clusterable is being clustered correctly when sparsity preservation is enabled.\"\"\"\n    preserve_sparsity_params = {'preserve_sparsity': True}\n    params = {**self.params, **preserve_sparsity_params}\n    wrapped_layer = experimental_cluster.cluster_weights(\n        self.keras_clusterable_layer, **params)\n\n    self._validate_clustered_layer(self.keras_clusterable_layer, wrapped_layer)\n\n  def testClusterKerasNonClusterableLayer(self):\n    \"\"\"Verifies that a built-in keras layer not marked as clusterable is not being clustered.\"\"\"\n    wrapped_layer = self._build_clustered_layer_model(\n        self.keras_non_clusterable_layer)\n\n    self._validate_clustered_layer(self.keras_non_clusterable_layer,\n                                   wrapped_layer)\n    self.assertEqual([], wrapped_layer.layer.get_clusterable_weights())\n\n  def testDepthwiseConv2DLayerNonClusterable(self):\n    \"\"\"Verifies that we don't cluster a DepthwiseConv2D layer, because clustering of this type of layer gives big unrecoverable accuracy loss.\"\"\"\n    wrapped_layer = self._build_clustered_layer_model(\n        self.keras_depthwiseconv2d_layer, input_shape=(10, 10, 10))\n\n    self._validate_clustered_layer(self.keras_depthwiseconv2d_layer,\n                                   wrapped_layer)\n    self.assertEqual([], wrapped_layer.layer.get_clusterable_weights())\n\n  def testDenseLayer(self):\n    \"\"\"Verifies that we can cluster a Dense layer.\"\"\"\n    input_shape = (28, 1)\n    wrapped_layer = self._build_clustered_layer_model(\n        self.keras_dense_layer,\n        input_shape=input_shape\n        )\n\n    self._validate_clustered_layer(self.keras_dense_layer,\n                                   wrapped_layer)\n    self.assertEqual([1, 10],\n                     wrapped_layer.layer.get_clusterable_weights()[0][1].shape)\n\n  def testConv1DLayer(self):\n    \"\"\"Verifies that we can cluster a Conv1D layer.\"\"\"\n    input_shape = (28, 1)\n    wrapped_layer = self._build_clustered_layer_model(\n        self.keras_conv1d_layer,\n        input_shape=input_shape)\n\n    self._validate_clustered_layer(self.keras_conv1d_layer,\n                                   wrapped_layer)\n    self.assertEqual([5, 1, 3],\n                     wrapped_layer.layer.get_clusterable_weights()[0][1].shape)\n\n  def testConv1DTransposeLayer(self):\n    \"\"\"Verifies that we can cluster a Conv1DTranspose layer.\"\"\"\n    input_shape = (28, 1)\n    wrapped_layer = self._build_clustered_layer_model(\n        self.keras_conv1d_tr_layer,\n        input_shape=input_shape)\n\n    self._validate_clustered_layer(self.keras_conv1d_tr_layer,\n                                   wrapped_layer)\n    self.assertEqual([5, 3, 1],\n                     wrapped_layer.layer.get_clusterable_weights()[0][1].shape)\n\n  def testConv2DLayer(self):\n    \"\"\"Verifies that we can cluster a Conv2D layer.\"\"\"\n    input_shape = (28, 28, 1)\n    wrapped_layer = self._build_clustered_layer_model(\n        self.keras_conv2d_layer,\n        input_shape=input_shape)\n\n    self._validate_clustered_layer(self.keras_conv2d_layer,\n                                   wrapped_layer)\n    self.assertEqual([4, 5, 1, 3],\n                     wrapped_layer.layer.get_clusterable_weights()[0][1].shape)\n\n  def testConv2DTransposeLayer(self):\n    \"\"\"Verifies that we can cluster a Conv2DTranspose layer.\"\"\"\n    input_shape = (28, 28, 1)\n    wrapped_layer = self._build_clustered_layer_model(\n        self.keras_conv2d_tr_layer,\n        input_shape=input_shape)\n\n    self._validate_clustered_layer(self.keras_conv2d_tr_layer,\n                                   wrapped_layer)\n    self.assertEqual([4, 5, 3, 1],\n                     wrapped_layer.layer.get_clusterable_weights()[0][1].shape)\n\n  def testConv3DLayer(self):\n    \"\"\"Verifies that we can cluster a Conv3D layer.\"\"\"\n    input_shape = (28, 28, 28, 1)\n    wrapped_layer = self._build_clustered_layer_model(\n        self.keras_conv3d_layer,\n        input_shape=input_shape)\n\n    self._validate_clustered_layer(self.keras_conv3d_layer,\n                                   wrapped_layer)\n    self.assertEqual([3, 4, 5, 1, 2],\n                     wrapped_layer.layer.get_clusterable_weights()[0][1].shape)\n\n  def testClusterKerasUnsupportedLayer(self):\n    \"\"\"Verifies that attempting to cluster an unsupported layer raises an exception.\"\"\"\n    keras_unsupported_layer = self.keras_unsupported_layer\n    # We need to build weights before check.\n    keras_unsupported_layer.build(input_shape=(10, 10))\n    with self.assertRaises(ValueError):\n      cluster.cluster_weights(keras_unsupported_layer, **self.params)\n\n  def testClusterCustomClusterableLayer(self):\n    \"\"\"Verifies that a custom clusterable layer is being clustered correctly.\"\"\"\n    wrapped_layer = self._build_clustered_layer_model(\n        self.custom_clusterable_layer)\n\n    self._validate_clustered_layer(self.custom_clusterable_layer, wrapped_layer)\n    self.assertEqual([('kernel', wrapped_layer.layer.kernel)],\n                     wrapped_layer.layer.get_clusterable_weights())\n\n  def testClusterCustomClusterableLayerWithSparsityPreservation(self):\n    \"\"\"Verifies that a custom clusterable layer is being clustered correctly when sparsity preservation is enabled.\"\"\"\n    preserve_sparsity_params = {'preserve_sparsity': True}\n    params = {**self.params, **preserve_sparsity_params}\n    wrapped_layer = experimental_cluster.cluster_weights(\n        self.custom_clusterable_layer, **params)\n    self.model.add(wrapped_layer)\n    self.model.build(input_shape=(10, 1))\n\n    self._validate_clustered_layer(self.custom_clusterable_layer, wrapped_layer)\n    self.assertEqual([('kernel', wrapped_layer.layer.kernel)],\n                     wrapped_layer.layer.get_clusterable_weights())\n\n  def testClusterCustomNonClusterableLayer(self):\n    \"\"\"Verifies that attempting to cluster a custom non-clusterable layer raises an exception.\"\"\"\n    custom_non_clusterable_layer = self.custom_non_clusterable_layer\n    # Once layer is empty with no weights allocated, clustering is supported.\n    cluster_wrapper.ClusterWeights(custom_non_clusterable_layer, **self.params)\n    # We need to build weights before check that clustering is not supported.\n    custom_non_clusterable_layer.build(input_shape=(10, 10))\n    with self.assertRaises(ValueError):\n      cluster_wrapper.ClusterWeights(custom_non_clusterable_layer,\n                                     **self.params)\n\n  def testClusterMyClusterableLayer(self):\n    # we have weights to cluster.\n    layer = self.clusterable_layer\n    layer.build(input_shape=(10, 10))\n\n    wrapped_layer = cluster_wrapper.ClusterWeights(layer, **self.params)\n\n    self.assertIsInstance(wrapped_layer, cluster_wrapper.ClusterWeights)\n\n  def testKerasCustomLayerClusterable(self):\n    \"\"\"Verifies that we can wrap keras custom layer that is customerable.\"\"\"\n    layer = KerasCustomLayerClusterable()\n    wrapped_layer = cluster_wrapper.ClusterWeights(layer, **self.params)\n\n    self.assertIsInstance(wrapped_layer, cluster_wrapper.ClusterWeights)\n\n  def testClusterMyClusterableLayerInvalid(self):\n    \"\"\"Verifies that assertion is thrown when function get_clusterable_weights is not provided.\"\"\"\n    with self.assertRaises(TypeError):\n      MyClusterableLayerInvalid(10)  # pylint: disable=abstract-class-instantiated\n\n  def testClusterKerasCustomLayer(self):\n    \"\"\"Verifies that attempting to cluster a keras custom layer raises an exception.\"\"\"\n    # If layer is not built, it has not weights, so\n    # we just skip it.\n    keras_custom_layer = self.keras_custom_layer\n    cluster_wrapper.ClusterWeights(keras_custom_layer, **self.params)\n    # We need to build weights before check that clustering is not supported.\n    keras_custom_layer.build(input_shape=(10, 10))\n    with self.assertRaises(ValueError):\n      cluster_wrapper.ClusterWeights(keras_custom_layer, **self.params)\n\n  def testStripClusteringSequentialModelWithKernelRegularizer(self):\n    \"\"\"Verifies that stripping the clustering wrappers from a sequential model produces the expected config.\"\"\"\n    model = keras.Sequential([\n        layers.Dense(10, input_shape=(10,)),\n        layers.Dense(10, kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n    ])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    stripped_model = cluster.strip_clustering(clustered_model)\n    # check that kernel regularizer is present in the second dense layer\n    self.assertIsNotNone(stripped_model.layers[1].kernel_regularizer)\n    with tempfile.TemporaryDirectory() as tmp_dir_name:\n      keras_file = os.path.join(tmp_dir_name, 'cluster_test')\n      stripped_model.save(keras_file, save_traces=True)\n\n  def testStripClusteringSequentialModelWithBiasRegularizer(self):\n    \"\"\"Verifies that stripping the clustering wrappers from a sequential model produces the expected config.\"\"\"\n    model = keras.Sequential([\n        layers.Dense(10, input_shape=(10,)),\n        layers.Dense(10, bias_regularizer=tf.keras.regularizers.L1(0.01)),\n    ])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    stripped_model = cluster.strip_clustering(clustered_model)\n    # check that kernel regularizer is present in the second dense layer\n    self.assertIsNotNone(stripped_model.layers[1].bias_regularizer)\n    with tempfile.TemporaryDirectory() as tmp_dir_name:\n      keras_file = os.path.join(tmp_dir_name, 'cluster_test')\n      stripped_model.save(keras_file, save_traces=True)\n\n  def testStripClusteringSequentialModelWithActivityRegularizer(self):\n    \"\"\"Verifies that stripping the clustering wrappers from a sequential model produces the expected config.\"\"\"\n    model = keras.Sequential([\n        layers.Dense(10, input_shape=(10,)),\n        layers.Dense(10, activity_regularizer=tf.keras.regularizers.L1(0.01)),\n    ])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    stripped_model = cluster.strip_clustering(clustered_model)\n    # check that kernel regularizer is present in the second dense layer\n    self.assertIsNotNone(stripped_model.layers[1].activity_regularizer)\n    with tempfile.TemporaryDirectory() as tmp_dir_name:\n      keras_file = os.path.join(tmp_dir_name, 'cluster_test')\n      stripped_model.save(keras_file, save_traces=True)\n\n  def testStripClusteringSequentialModelWithKernelConstraint(self):\n    \"\"\"Verifies that stripping the clustering wrappers from a sequential model produces the expected config.\"\"\"\n    model = keras.Sequential([\n        layers.Dense(10, input_shape=(10,)),\n        layers.Dense(10, kernel_constraint=tf.keras.constraints.max_norm(2.)),\n    ])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    stripped_model = cluster.strip_clustering(clustered_model)\n    # check that kernel regularizer is present in the second dense layer\n    self.assertIsNotNone(stripped_model.layers[1].kernel_constraint)\n    with tempfile.TemporaryDirectory() as tmp_dir_name:\n      keras_file = os.path.join(tmp_dir_name, 'cluster_test')\n      stripped_model.save(keras_file, save_traces=True)\n\n  def testStripClusteringSequentialModelWithBiasConstraint(self):\n    \"\"\"Verifies that stripping the clustering wrappers from a sequential model produces the expected config.\"\"\"\n    model = keras.Sequential([\n        layers.Dense(10, input_shape=(10,)),\n        layers.Dense(10, bias_constraint=tf.keras.constraints.max_norm(2.)),\n    ])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    stripped_model = cluster.strip_clustering(clustered_model)\n    # check that kernel regularizer is present in the second dense layer\n    self.assertIsNotNone(stripped_model.layers[1].bias_constraint)\n    with tempfile.TemporaryDirectory() as tmp_dir_name:\n      keras_file = os.path.join(tmp_dir_name, 'cluster_test')\n      stripped_model.save(keras_file, save_traces=True)\n\n  def testClusterSequentialModelSelectively(self):\n    clustered_model = keras.Sequential()\n    clustered_model.add(\n        cluster.cluster_weights(self.keras_clusterable_layer, **self.params))\n    clustered_model.add(self.keras_clusterable_layer)\n    clustered_model.build(input_shape=(1, 10))\n\n    self.assertIsInstance(clustered_model.layers[0],\n                          cluster_wrapper.ClusterWeights)\n    self.assertNotIsInstance(clustered_model.layers[1],\n                             cluster_wrapper.ClusterWeights)\n\n  def testClusterSequentialModelSelectivelyWithSparsityPreservation(self):\n    \"\"\"Verifies that layers within a sequential model can be clustered selectively when sparsity preservation is enabled.\"\"\"\n    preserve_sparsity_params = {'preserve_sparsity': True}\n    params = {**self.params, **preserve_sparsity_params}\n    clustered_model = keras.Sequential()\n    clustered_model.add(\n        experimental_cluster.cluster_weights(self.keras_clusterable_layer,\n                                             **params))\n    clustered_model.add(self.keras_clusterable_layer)\n    clustered_model.build(input_shape=(1, 10))\n\n    self.assertIsInstance(clustered_model.layers[0],\n                          cluster_wrapper.ClusterWeights)\n    self.assertNotIsInstance(clustered_model.layers[1],\n                             cluster_wrapper.ClusterWeights)\n\n  def testClusterFunctionalModelSelectively(self):\n    \"\"\"Verifies that layers within a functional model can be clustered selectively.\"\"\"\n    i1 = keras.Input(shape=(10,))\n    i2 = keras.Input(shape=(10,))\n    x1 = cluster.cluster_weights(layers.Dense(10), **self.params)(i1)\n    x2 = layers.Dense(10)(i2)\n    outputs = layers.Add()([x1, x2])\n    clustered_model = keras.Model(inputs=[i1, i2], outputs=outputs)\n\n    self.assertIsInstance(clustered_model.layers[2],\n                          cluster_wrapper.ClusterWeights)\n    self.assertNotIsInstance(clustered_model.layers[3],\n                             cluster_wrapper.ClusterWeights)\n\n  def testClusterFunctionalModelSelectivelyWithSparsityPreservation(self):\n    \"\"\"Verifies that layers within a functional model can be clustered selectively when sparsity preservation is enabled.\"\"\"\n    preserve_sparsity_params = {'preserve_sparsity': True}\n    params = {**self.params, **preserve_sparsity_params}\n    i1 = keras.Input(shape=(10,))\n    i2 = keras.Input(shape=(10,))\n    x1 = experimental_cluster.cluster_weights(layers.Dense(10), **params)(i1)\n    x2 = layers.Dense(10)(i2)\n    outputs = layers.Add()([x1, x2])\n    clustered_model = keras.Model(inputs=[i1, i2], outputs=outputs)\n\n    self.assertIsInstance(clustered_model.layers[2],\n                          cluster_wrapper.ClusterWeights)\n    self.assertNotIsInstance(clustered_model.layers[3],\n                             cluster_wrapper.ClusterWeights)\n\n  def testClusterModelValidLayersSuccessful(self):\n    \"\"\"Verifies that clustering a sequential model results in all clusterable layers within the model being clustered.\"\"\"\n    model = keras.Sequential([\n        self.keras_clusterable_layer, self.keras_non_clusterable_layer,\n        self.custom_clusterable_layer\n    ])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    clustered_model.build(input_shape=(1, 28, 28, 1))\n\n    self.assertEqual(len(model.layers), len(clustered_model.layers))\n    for layer, clustered_layer in zip(model.layers, clustered_model.layers):\n      self._validate_clustered_layer(layer, clustered_layer)\n\n  def testClusterModelValidLayersSuccessfulWithSparsityPreservation(self):\n    \"\"\"Verifies that clustering a sequential model results in all clusterable layers within the model being clustered when sparsity preservation is enabled.\"\"\"\n    preserve_sparsity_params = {'preserve_sparsity': True}\n    params = {**self.params, **preserve_sparsity_params}\n    model = keras.Sequential([\n        self.keras_clusterable_layer, self.keras_non_clusterable_layer,\n        self.custom_clusterable_layer\n    ])\n    clustered_model = experimental_cluster.cluster_weights(model, **params)\n    clustered_model.build(input_shape=(1, 28, 28, 1))\n\n    self.assertEqual(len(model.layers), len(clustered_model.layers))\n    for layer, clustered_layer in zip(model.layers, clustered_model.layers):\n      self._validate_clustered_layer(layer, clustered_layer)\n\n  def testClusterModelUnsupportedKerasLayerRaisesError(self):\n    \"\"\"Verifies that attempting to cluster a model that contains an unsupported layer raises an exception.\"\"\"\n    keras_unsupported_layer = self.keras_unsupported_layer\n    # We need to build weights before check.\n    keras_unsupported_layer.build(input_shape=(10, 10))\n    with self.assertRaises(ValueError):\n      cluster.cluster_weights(\n          keras.Sequential([\n              self.keras_clusterable_layer, self.keras_non_clusterable_layer,\n              self.custom_clusterable_layer, keras_unsupported_layer\n          ]), **self.params)\n\n  def testClusterModelCustomNonClusterableLayerRaisesError(self):\n    \"\"\"Verifies that attempting to cluster a model that contains a custom non-clusterable layer raises an exception.\"\"\"\n    with self.assertRaises(ValueError):\n      custom_non_clusterable_layer = self.custom_non_clusterable_layer\n      # We need to build weights before check.\n      custom_non_clusterable_layer.build(input_shape=(1, 2))\n      cluster.cluster_weights(\n          keras.Sequential([\n              self.keras_clusterable_layer, self.keras_non_clusterable_layer,\n              self.custom_clusterable_layer, custom_non_clusterable_layer\n          ]), **self.params)\n\n  def testClusterModelDoesNotWrapAlreadyWrappedLayer(self):\n    \"\"\"Verifies that clustering a model that contains an already clustered layer does not result in wrapping the clustered layer into another cluster_wrapper.\"\"\"\n    model = keras.Sequential([\n        layers.Flatten(),\n        cluster.cluster_weights(layers.Dense(10), **self.params),\n    ])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    clustered_model.build(input_shape=(10, 10, 1))\n\n    self.assertEqual(len(model.layers), len(clustered_model.layers))\n    self._validate_clustered_layer(model.layers[0], clustered_model.layers[0])\n    # Second layer is used as-is since it's already a clustered layer.\n    self.assertEqual(model.layers[1], clustered_model.layers[1])\n    self._validate_clustered_layer(model.layers[1].layer,\n                                   clustered_model.layers[1])\n\n  def testClusterValidLayersListSuccessful(self):\n    \"\"\"Verifies that clustering a list of layers results in all clusterable layers within the list being clustered.\"\"\"\n    model_layers = [\n        self.keras_clusterable_layer, self.keras_non_clusterable_layer,\n        self.custom_clusterable_layer\n    ]\n    clustered_list = cluster.cluster_weights(model_layers, **self.params)\n\n    self.assertEqual(len(model_layers), len(clustered_list))\n    for layer, clustered_layer in zip(model_layers, clustered_list):\n      self._validate_clustered_layer(layer, clustered_layer)\n\n  def testClusterSequentialModelNoInput(self):\n    \"\"\"Verifies that a sequential model without an input layer is being clustered correctly.\"\"\"\n    # No InputLayer\n    model = keras.Sequential([\n        layers.Dense(10),\n        layers.Dense(10),\n    ])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    self.assertEqual(self._count_clustered_layers(clustered_model), 2)\n\n  def testClusterSequentialModelWithInput(self):\n    \"\"\"Verifies that a sequential model with an input layer is being clustered correctly.\"\"\"\n    # With InputLayer\n    model = keras.Sequential([\n        layers.Dense(10, input_shape=(10,)),\n        layers.Dense(10),\n    ])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    self.assertEqual(self._count_clustered_layers(clustered_model), 2)\n\n  def testClusterSequentialModelPreservesBuiltStateNoInput(self):\n    \"\"\"Verifies that clustering a sequential model without an input layer preserves the built state of the model.\"\"\"\n    # No InputLayer\n    model = keras.Sequential([\n        layers.Dense(10),\n        layers.Dense(10),\n    ])\n    self.assertEqual(model.built, False)\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    self.assertEqual(model.built, False)\n\n    # Test built state is preserved across serialization\n    with cluster.cluster_scope():\n      loaded_model = keras.models.model_from_config(\n          json.loads(clustered_model.to_json()))\n      self.assertEqual(loaded_model.built, False)\n\n  def testClusterSequentialModelPreservesBuiltStateWithInput(self):\n    \"\"\"Verifies that clustering a sequential model with an input layer preserves the built state of the model.\"\"\"\n    # With InputLayer\n    model = keras.Sequential([\n        layers.Dense(10, input_shape=(10,)),\n        layers.Dense(10),\n    ])\n    self.assertEqual(model.built, True)\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    self.assertEqual(model.built, True)\n\n    # Test built state is preserved across serialization\n    with cluster.cluster_scope():\n      loaded_model = keras.models.model_from_config(\n          json.loads(clustered_model.to_json()))\n    self.assertEqual(loaded_model.built, True)\n\n  def testClusterFunctionalModelPreservesBuiltState(self):\n    \"\"\"Verifies that clustering a functional model preserves the built state of the model.\"\"\"\n    i1 = keras.Input(shape=(10,))\n    i2 = keras.Input(shape=(10,))\n    x1 = layers.Dense(10)(i1)\n    x2 = layers.Dense(10)(i2)\n    outputs = layers.Add()([x1, x2])\n    model = keras.Model(inputs=[i1, i2], outputs=outputs)\n    self.assertEqual(model.built, True)\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    self.assertEqual(model.built, True)\n\n    # Test built state preserves across serialization\n    with cluster.cluster_scope():\n      loaded_model = keras.models.model_from_config(\n          json.loads(clustered_model.to_json()))\n    self.assertEqual(loaded_model.built, True)\n\n  def testClusterFunctionalModel(self):\n    \"\"\"Verifies that a functional model is being clustered correctly.\"\"\"\n    i1 = keras.Input(shape=(10,))\n    i2 = keras.Input(shape=(10,))\n    x1 = layers.Dense(10)(i1)\n    x2 = layers.Dense(10)(i2)\n    outputs = layers.Add()([x1, x2])\n    model = keras.Model(inputs=[i1, i2], outputs=outputs)\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    self.assertEqual(self._count_clustered_layers(clustered_model), 3)\n\n  def testClusterFunctionalModelWithLayerReused(self):\n    \"\"\"Verifies that a layer reused within a functional model multiple times is only being clustered once.\"\"\"\n    # The model reuses the Dense() layer. Make sure it's only clustered once.\n    inp = keras.Input(shape=(10,))\n    dense_layer = layers.Dense(10)\n    x = dense_layer(inp)\n    x = dense_layer(x)\n    model = keras.Model(inputs=[inp], outputs=[x])\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    self.assertEqual(self._count_clustered_layers(clustered_model), 1)\n\n  def testClusterSubclassModel(self):\n    \"\"\"Verifies that attempting to cluster an instance of a subclass of keras.Model raises an exception.\"\"\"\n    model = TestModel()\n    with self.assertRaises(ValueError):\n      _ = cluster.cluster_weights(model, **self.params)\n\n  def testClusterSubclassModelAsSubmodel(self):\n    \"\"\"Verifies that attempting to cluster a model with submodel that is a subclass throws an exception.\"\"\"\n    model_subclass = TestModel()\n    model = keras.Sequential([layers.Dense(10), model_subclass])\n    with self.assertRaisesRegex(ValueError, 'Subclassed models.*'):\n      _ = cluster.cluster_weights(model, **self.params)\n\n  def testStripClusteringSequentialModel(self):\n    \"\"\"Verifies that stripping the clustering wrappers from a sequential model produces the expected config.\"\"\"\n    model = keras.Sequential([\n        layers.Dense(10, input_shape=(5,)),\n        layers.Dense(10),\n    ])\n\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    stripped_model = cluster.strip_clustering(clustered_model)\n\n    self.assertEqual(self._count_clustered_layers(stripped_model), 0)\n    self.assertEqual(model.get_config(), stripped_model.get_config())\n\n  def testClusterStrippingFunctionalModel(self):\n    \"\"\"Verifies that stripping the clustering wrappers from a functional model produces the expected config.\"\"\"\n    i1 = keras.Input(shape=(10,))\n    i2 = keras.Input(shape=(10,))\n    x1 = layers.Dense(10)(i1)\n    x2 = layers.Dense(10)(i2)\n    outputs = layers.Add()([x1, x2])\n    model = keras.Model(inputs=[i1, i2], outputs=outputs)\n\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    stripped_model = cluster.strip_clustering(clustered_model)\n\n    self.assertEqual(self._count_clustered_layers(stripped_model), 0)\n    self.assertEqual(model.get_config(), stripped_model.get_config())\n\n  def testClusterWeightsStrippedWeights(self):\n    \"\"\"Verifies that stripping the clustering wrappers from a functional model preserves the clustered weights.\"\"\"\n    i1 = keras.Input(shape=(10,))\n    x1 = layers.BatchNormalization()(i1)\n    outputs = x1\n    model = keras.Model(inputs=[i1], outputs=outputs)\n\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    cluster_weight_length = (len(clustered_model.get_weights()))\n    stripped_model = cluster.strip_clustering(clustered_model)\n\n    self.assertEqual(self._count_clustered_layers(stripped_model), 0)\n    self.assertLen(stripped_model.get_weights(), cluster_weight_length)\n\n  def testStrippedKernel(self):\n    \"\"\"Verifies that stripping the clustering wrappers from a functional model restores the layers kernel and the layers weight array to the new clustered weight value.\"\"\"\n    i1 = keras.Input(shape=(1, 1, 1))\n    x1 = layers.Conv2D(12, 1)(i1)\n    outputs = x1\n    model = keras.Model(inputs=[i1], outputs=outputs)\n\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    clustered_conv2d_layer = clustered_model.layers[1]\n    clustered_kernel = clustered_conv2d_layer.layer.kernel\n    stripped_model = cluster.strip_clustering(clustered_model)\n    stripped_conv2d_layer = stripped_model.layers[1]\n\n    self.assertEqual(self._count_clustered_layers(stripped_model), 0)\n    self.assertIsNot(stripped_conv2d_layer.kernel, clustered_kernel)\n    self.assertIn(stripped_conv2d_layer.kernel, stripped_conv2d_layer.weights)\n\n  def testStripSelectivelyClusteredFunctionalModel(self):\n    \"\"\"Verifies that invoking strip_clustering() on a selectively clustered functional model strips the clustering wrappers from the clustered layers.\"\"\"\n    i1 = keras.Input(shape=(10,))\n    i2 = keras.Input(shape=(10,))\n    x1 = cluster.cluster_weights(layers.Dense(10), **self.params)(i1)\n    x2 = layers.Dense(10)(i2)\n    outputs = layers.Add()([x1, x2])\n    clustered_model = keras.Model(inputs=[i1, i2], outputs=outputs)\n\n    stripped_model = cluster.strip_clustering(clustered_model)\n\n    self.assertEqual(self._count_clustered_layers(stripped_model), 0)\n    self.assertIsInstance(stripped_model.layers[2], layers.Dense)\n\n  def testStripSelectivelyClusteredSequentialModel(self):\n    \"\"\"Verifies that invoking strip_clustering() on a selectively clustered sequential model strips the clustering wrappers from the clustered layers.\"\"\"\n    clustered_model = keras.Sequential([\n        cluster.cluster_weights(layers.Dense(10), **self.params),\n        layers.Dense(10),\n    ])\n    clustered_model.build(input_shape=(1, 10))\n\n    stripped_model = cluster.strip_clustering(clustered_model)\n\n    self.assertEqual(self._count_clustered_layers(stripped_model), 0)\n    self.assertIsInstance(stripped_model.layers[0], layers.Dense)\n\n  def testStripClusteringAndSetOriginalWeightsBack(self):\n    \"\"\"Verifies that we can set_weights onto the stripped model.\"\"\"\n    model = keras.Sequential([\n        layers.Dense(10, input_shape=(5,)),\n        layers.Dense(10),\n    ])\n\n    # Save original weights\n    original_weights = model.get_weights()\n\n    # Cluster and strip\n    clustered_model = cluster.cluster_weights(model, **self.params)\n    stripped_model = cluster.strip_clustering(clustered_model)\n\n    # Set back original weights onto the strip model\n    stripped_model.set_weights(original_weights)\n\n\nif __name__ == '__main__':\n  test.main()\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/model-optimization", "file_path": "tensorflow_model_optimization/python/core/quantization/keras/experimental/default_n_bit/default_n_bit_quantize_registry_test.py", "content": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for QuantizeRegistry.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantizers\nfrom tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit import default_n_bit_quantize_registry as n_bit_registry\n\nkeras = tf.keras\nK = tf.keras.backend\nl = tf.keras.layers\n\ndeserialize_keras_object = tf.keras.utils.deserialize_keras_object\nserialize_keras_object = tf.keras.utils.serialize_keras_object\n\n\nclass _TestHelper(object):\n\n  def _convert_list(self, list_of_tuples):\n    \"\"\"Transforms a list of 2-tuples to a tuple of 2 lists.\n\n    `QuantizeConfig` methods return a list of 2-tuples in the form\n    [(weight1, quantizer1), (weight2, quantizer2)]. This function converts\n    it into a 2-tuple of lists. ([weight1, weight2]), (quantizer1, quantizer2).\n\n    Args:\n      list_of_tuples: List of 2-tuples.\n\n    Returns:\n      2-tuple of lists.\n    \"\"\"\n    list1 = []\n    list2 = []\n    for a, b in list_of_tuples:\n      list1.append(a)\n      list2.append(b)\n\n    return list1, list2\n\n  # TODO(pulkitb): Consider asserting on full equality for quantizers.\n\n  def _assert_weight_quantizers(self, quantizer_list):\n    for quantizer in quantizer_list:\n      self.assertIsInstance(quantizer, quantizers.LastValueQuantizer)\n\n  def _assert_activation_quantizers(self, quantizer_list):\n    for quantizer in quantizer_list:\n      self.assertIsInstance(quantizer, quantizers.MovingAverageQuantizer)\n\n  def _assert_kernel_equality(self, a, b):\n    self.assertAllEqual(a.numpy(), b.numpy())\n\n\nclass QuantizeRegistryTest(\n    tf.test.TestCase, parameterized.TestCase, _TestHelper):\n\n  def setUp(self):\n    super(QuantizeRegistryTest, self).setUp()\n    self.quantize_registry = n_bit_registry.DefaultNBitQuantizeRegistry(\n        num_bits_weight=4, num_bits_activation=4)\n\n  class CustomLayer(l.Layer):\n    pass\n\n  # supports() tests.\n\n  def testSupports_KerasLayer(self):\n    self.assertTrue(self.quantize_registry.supports(l.Dense(10)))\n    self.assertTrue(self.quantize_registry.supports(l.Conv2D(10, (2, 2))))\n\n  @unittest.skip\n  def testSupports_KerasRNNLayers(self):\n    self.assertTrue(self.quantize_registry.supports(l.LSTM(10)))\n    self.assertTrue(self.quantize_registry.supports(l.GRU(10)))\n\n  @unittest.skip\n  def testSupports_KerasRNNLayerWithKerasRNNCells(self):\n    self.assertTrue(self.quantize_registry.supports(l.RNN(cell=l.LSTMCell(10))))\n    self.assertTrue(\n        self.quantize_registry.supports(\n            l.RNN(cell=[l.LSTMCell(10), l.GRUCell(10)])))\n\n  def testDoesNotSupport_CustomLayer(self):\n    self.assertFalse(self.quantize_registry.supports(self.CustomLayer()))\n\n  @unittest.skip\n  def testDoesNotSupport_RNNLayerWithCustomRNNCell(self):\n\n    class MinimalRNNCell(l.Layer):\n\n      def __init__(self, units, **kwargs):\n        self.units = units\n        self.state_size = units\n        super(MinimalRNNCell, self).__init__(**kwargs)\n\n    self.assertFalse(\n        self.quantize_registry.supports(l.RNN(cell=MinimalRNNCell(10))))\n    self.assertFalse(\n        self.quantize_registry.supports(\n            l.RNN(cell=[l.LSTMCell(10), MinimalRNNCell(10)])))\n\n  # get_quantize_config() tests.\n  @parameterized.parameters([False, True])\n  def testGetsPerTensorIfPerAxisDisabled(self, disable_per_axis):\n    test_quantize_registry = (n_bit_registry.\n                              DefaultNBitQuantizeRegistry(\n                                  disable_per_axis,\n                                  num_bits_weight=4, num_bits_activation=4))\n    quantize_config = test_quantize_registry.get_quantize_config(\n        l.Conv2D(10, (2, 2)))\n    if disable_per_axis:\n      self.assertIsInstance(\n          quantize_config,\n          n_bit_registry.DefaultNBitQuantizeConfig)\n    else:\n      self.assertIsInstance(\n          quantize_config,\n          n_bit_registry.DefaultNBitConvQuantizeConfig)\n\n  def testRaisesError_UnsupportedLayer(self):\n    with self.assertRaises(ValueError):\n      self.quantize_registry.get_quantize_config(self.CustomLayer())\n\n  def testReturnsConfig_KerasLayer(self):\n    model = keras.Sequential([(\n        l.Dense(2, input_shape=(3,)))])\n    layer = model.layers[0]\n\n    quantize_config = self.quantize_registry.get_quantize_config(layer)\n\n    (weights, weight_quantizers) = self._convert_list(\n        quantize_config.get_weights_and_quantizers(layer))\n    (activations, activation_quantizers) = self._convert_list(\n        quantize_config.get_activations_and_quantizers(layer))\n\n    self._assert_weight_quantizers(weight_quantizers)\n    self.assertEqual([layer.kernel], weights)\n\n    self._assert_activation_quantizers(activation_quantizers)\n    self.assertEqual([layer.activation], activations)\n\n    quantize_kernel = keras.backend.variable(\n        np.ones(layer.kernel.shape.as_list()))\n    quantize_activation = keras.activations.relu\n    quantize_config.set_quantize_weights(layer, [quantize_kernel])\n    quantize_config.set_quantize_activations(layer, [quantize_activation])\n\n    self._assert_kernel_equality(layer.kernel, quantize_kernel)\n    self.assertEqual(layer.activation, quantize_activation)\n\n  def testReturnsConfig_LayerWithResultQuantizer(self):\n    layer = l.ReLU()\n    quantize_config = self.quantize_registry.get_quantize_config(layer)\n\n    output_quantizers = quantize_config.get_output_quantizers(layer)\n\n    self.assertLen(output_quantizers, 1)\n    self._assert_activation_quantizers(output_quantizers)\n\n  @unittest.skip\n  def testReturnsConfig_KerasRNNLayer(self):\n    model = keras.Sequential([(\n        l.LSTM(2, input_shape=(3, 2)))])\n    layer = model.layers[0]\n\n    quantize_config = self.quantize_registry.get_quantize_config(layer)\n\n    (weights, weight_quantizers) = self._convert_list(\n        quantize_config.get_weights_and_quantizers(layer))\n    (activations, activation_quantizers) = self._convert_list(\n        quantize_config.get_activations_and_quantizers(layer))\n\n    self._assert_weight_quantizers(weight_quantizers)\n    self.assertEqual([layer.cell.kernel, layer.cell.recurrent_kernel], weights)\n\n    self._assert_activation_quantizers(activation_quantizers)\n    self.assertEqual(\n        [layer.cell.activation, layer.cell.recurrent_activation], activations)\n\n  @unittest.skip\n  def testReturnsConfig_KerasRNNLayerWithKerasRNNCells(self):\n    lstm_cell = l.LSTMCell(3)\n    gru_cell = l.GRUCell(2)\n    model = keras.Sequential([l.RNN([lstm_cell, gru_cell], input_shape=(3, 2))])\n    layer = model.layers[0]\n\n    quantize_config = self.quantize_registry.get_quantize_config(layer)\n\n    (weights, weight_quantizers) = self._convert_list(\n        quantize_config.get_weights_and_quantizers(layer))\n    (activations, activation_quantizers) = self._convert_list(\n        quantize_config.get_activations_and_quantizers(layer))\n\n    self._assert_weight_quantizers(weight_quantizers)\n    self.assertEqual(\n        [lstm_cell.kernel, lstm_cell.recurrent_kernel,\n         gru_cell.kernel, gru_cell.recurrent_kernel],\n        weights)\n\n    self._assert_activation_quantizers(activation_quantizers)\n    self.assertEqual(\n        [lstm_cell.activation, lstm_cell.recurrent_activation,\n         gru_cell.activation, gru_cell.recurrent_activation],\n        activations)\n\n  def testReturnsActivationConfig_Activation(self):\n    activation_layer = keras.layers.Activation('relu')\n\n    quantize_config = self.quantize_registry.get_quantize_config(\n        activation_layer)\n\n    self.assertIsInstance(\n        quantize_config,\n        n_bit_registry.DefaultNBitActivationQuantizeConfig)\n    self._assert_activation_quantizers(\n        quantize_config.get_output_quantizers(activation_layer))\n\n\nclass DefaultNBitQuantizeConfigTest(tf.test.TestCase, _TestHelper):\n\n  def _simple_dense_layer(self):\n    layer = l.Dense(2)\n    layer.build(input_shape=(3,))\n    return layer\n\n  def testGetsQuantizeWeightsAndQuantizers(self):\n    layer = self._simple_dense_layer()\n\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        ['kernel'], ['activation'], False,\n        num_bits_weight=4, num_bits_activation=4)\n    (weights, weight_quantizers) = self._convert_list(\n        quantize_config.get_weights_and_quantizers(layer))\n\n    self._assert_weight_quantizers(weight_quantizers)\n    self.assertEqual([layer.kernel], weights)\n\n  def testGetsQuantizeActivationsAndQuantizers(self):\n    layer = self._simple_dense_layer()\n\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        ['kernel'], ['activation'], False,\n        num_bits_weight=4, num_bits_activation=4)\n    (activations, activation_quantizers) = self._convert_list(\n        quantize_config.get_activations_and_quantizers(layer))\n\n    self._assert_activation_quantizers(activation_quantizers)\n    self.assertEqual([layer.activation], activations)\n\n  def testSetsQuantizeWeights(self):\n    layer = self._simple_dense_layer()\n    quantize_kernel = K.variable(np.ones(layer.kernel.shape.as_list()))\n\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        ['kernel'], ['activation'], False,\n        num_bits_weight=4, num_bits_activation=4)\n    quantize_config.set_quantize_weights(layer, [quantize_kernel])\n\n    self._assert_kernel_equality(layer.kernel, quantize_kernel)\n\n  def testSetsQuantizeActivations(self):\n    layer = self._simple_dense_layer()\n    quantize_activation = keras.activations.relu\n\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        ['kernel'], ['activation'], False,\n        num_bits_weight=4, num_bits_activation=4)\n    quantize_config.set_quantize_activations(layer, [quantize_activation])\n\n    self.assertEqual(layer.activation, quantize_activation)\n\n  def testSetsQuantizeWeights_ErrorOnWrongNumberOfWeights(self):\n    layer = self._simple_dense_layer()\n    quantize_kernel = K.variable(np.ones(layer.kernel.shape.as_list()))\n\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        ['kernel'], ['activation'], False,\n        num_bits_weight=4, num_bits_activation=4)\n\n    with self.assertRaises(ValueError):\n      quantize_config.set_quantize_weights(layer, [])\n\n    with self.assertRaises(ValueError):\n      quantize_config.set_quantize_weights(layer,\n                                           [quantize_kernel, quantize_kernel])\n\n  def testSetsQuantizeWeights_ErrorOnWrongShapeOfWeight(self):\n    layer = self._simple_dense_layer()\n    quantize_kernel = K.variable(np.ones([1, 2]))\n\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        ['kernel'], ['activation'], False,\n        num_bits_weight=4, num_bits_activation=4)\n\n    with self.assertRaises(ValueError):\n      quantize_config.set_quantize_weights(layer, [quantize_kernel])\n\n  def testSetsQuantizeActivations_ErrorOnWrongNumberOfActivations(self):\n    layer = self._simple_dense_layer()\n    quantize_activation = keras.activations.relu\n\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        ['kernel'], ['activation'], False,\n        num_bits_weight=4, num_bits_activation=4)\n\n    with self.assertRaises(ValueError):\n      quantize_config.set_quantize_activations(layer, [])\n\n    with self.assertRaises(ValueError):\n      quantize_config.set_quantize_activations(\n          layer, [quantize_activation, quantize_activation])\n\n  def testGetsResultQuantizers_ReturnsQuantizer(self):\n    layer = self._simple_dense_layer()\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        [], [], True, num_bits_weight=4, num_bits_activation=4)\n\n    output_quantizers = quantize_config.get_output_quantizers(layer)\n\n    self.assertLen(output_quantizers, 1)\n    self._assert_activation_quantizers(output_quantizers)\n\n  def testGetsResultQuantizers_EmptyWhenFalse(self):\n    layer = self._simple_dense_layer()\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        [], [], False, num_bits_weight=4, num_bits_activation=4)\n\n    output_quantizers = quantize_config.get_output_quantizers(layer)\n\n    self.assertEqual([], output_quantizers)\n\n  def testSerialization(self):\n    quantize_config = n_bit_registry.DefaultNBitQuantizeConfig(\n        ['kernel'], ['activation'], False,\n        num_bits_weight=4, num_bits_activation=4)\n\n    expected_config = {\n        'class_name': 'DefaultNBitQuantizeConfig',\n        'config': {\n            'weight_attrs': ['kernel'],\n            'activation_attrs': ['activation'],\n            'quantize_output': False,\n            'num_bits_weight': 4,\n            'num_bits_activation': 4\n        }\n    }\n    serialized_quantize_config = serialize_keras_object(quantize_config)\n\n    self.assertEqual(expected_config, serialized_quantize_config)\n\n    quantize_config_from_config = deserialize_keras_object(\n        serialized_quantize_config,\n        module_objects=globals(),\n        custom_objects=n_bit_registry._types_dict())\n\n    self.assertEqual(quantize_config, quantize_config_from_config)\n\n\nclass DefaultNBitQuantizeConfigRNNTest(tf.test.TestCase, _TestHelper):\n\n  def setUp(self):\n    super(DefaultNBitQuantizeConfigRNNTest, self).setUp()\n\n    self.cell1 = l.LSTMCell(3)\n    self.cell2 = l.GRUCell(2)\n    self.layer = l.RNN([self.cell1, self.cell2])\n    self.layer.build(input_shape=(3, 2))\n\n    self.quantize_config = n_bit_registry.DefaultNBitQuantizeConfigRNN(\n        [['kernel', 'recurrent_kernel'], ['kernel', 'recurrent_kernel']],\n        [['activation', 'recurrent_activation'],\n         ['activation', 'recurrent_activation']], False,\n        num_bits_weight=4, num_bits_activation=4)\n\n  def _expected_weights(self):\n    return [self.cell1.kernel, self.cell1.recurrent_kernel,\n            self.cell2.kernel, self.cell2.recurrent_kernel]\n\n  def _expected_activations(self):\n    return [self.cell1.activation, self.cell1.recurrent_activation,\n            self.cell2.activation, self.cell2.recurrent_activation]\n\n  def _dummy_weights(self, weight):\n    return K.variable(np.ones(weight.shape.as_list()))\n\n  def testGetsQuantizeWeightsAndQuantizers(self):\n    (weights, weight_quantizers) = self._convert_list(\n        self.quantize_config.get_weights_and_quantizers(self.layer))\n\n    self._assert_weight_quantizers(weight_quantizers)\n    self.assertEqual(self._expected_weights(), weights)\n\n  def testGetsQuantizeActivationsAndQuantizers(self):\n    (activations, activation_quantizers) = self._convert_list(\n        self.quantize_config.get_activations_and_quantizers(self.layer))\n\n    self._assert_activation_quantizers(activation_quantizers)\n    self.assertEqual(self._expected_activations(), activations)\n\n  def testSetsQuantizeWeights(self):\n    quantize_weights = [\n        self._dummy_weights(self.cell1.kernel),\n        self._dummy_weights(self.cell1.recurrent_kernel),\n        self._dummy_weights(self.cell2.kernel),\n        self._dummy_weights(self.cell2.recurrent_kernel)\n    ]\n\n    self.quantize_config.set_quantize_weights(self.layer, quantize_weights)\n\n    self.assertEqual(self._expected_weights(), quantize_weights)\n\n  def testSetsQuantizeActivations(self):\n    quantize_activations = [keras.activations.relu, keras.activations.softmax,\n                            keras.activations.relu, keras.activations.softmax]\n\n    self.quantize_config.set_quantize_activations(self.layer,\n                                                  quantize_activations)\n\n    self.assertEqual(self._expected_activations(), quantize_activations)\n\n  def testSetsQuantizeWeights_ErrorOnWrongNumberOfWeights(self):\n    with self.assertRaises(ValueError):\n      self.quantize_config.set_quantize_weights(self.layer, [])\n\n    quantize_weights = [\n        self._dummy_weights(self.cell1.kernel),\n        self._dummy_weights(self.cell1.recurrent_kernel),\n    ]\n    with self.assertRaises(ValueError):\n      self.quantize_config.set_quantize_weights(self.layer, quantize_weights)\n\n  def testSetsQuantizeWeights_ErrorOnWrongShapeOfWeight(self):\n    quantize_weights = [\n        self._dummy_weights(self.cell1.kernel),\n        self._dummy_weights(self.cell1.recurrent_kernel),\n        K.variable(np.ones([1, 2])),  # Incorrect shape.\n        self._dummy_weights(self.cell2.recurrent_kernel)\n    ]\n\n    with self.assertRaises(ValueError):\n      self.quantize_config.set_quantize_weights(self.layer, quantize_weights)\n\n  def testSetsQuantizeActivations_ErrorOnWrongNumberOfActivations(self):\n    quantize_activation = keras.activations.relu\n\n    with self.assertRaises(ValueError):\n      self.quantize_config.set_quantize_activations(self.layer, [])\n\n    with self.assertRaises(ValueError):\n      self.quantize_config.set_quantize_activations(\n          self.layer, [quantize_activation, quantize_activation])\n\n  def testSerialization(self):\n    expected_config = {\n        'class_name': 'DefaultNBitQuantizeConfigRNN',\n        'config': {\n            'weight_attrs': [['kernel', 'recurrent_kernel'],\n                             ['kernel', 'recurrent_kernel']],\n            'activation_attrs': [['activation', 'recurrent_activation'],\n                                 ['activation', 'recurrent_activation']],\n            'quantize_output': False,\n            'num_bits_weight': 4,\n            'num_bits_activation': 4\n        }\n    }\n    serialized_quantize_config = serialize_keras_object(self.quantize_config)\n\n    self.assertEqual(expected_config, serialized_quantize_config)\n\n    quantize_config_from_config = deserialize_keras_object(\n        serialized_quantize_config,\n        module_objects=globals(),\n        custom_objects=n_bit_registry._types_dict())\n\n    self.assertEqual(self.quantize_config, quantize_config_from_config)\n\n\nclass ActivationQuantizeConfigTest(tf.test.TestCase):\n\n  def testRaisesErrorUnsupportedActivation(self):\n    quantize_config = n_bit_registry.DefaultNBitActivationQuantizeConfig(\n        num_bits_weight=4, num_bits_activation=4)\n\n    with self.assertRaises(ValueError):\n      quantize_config.get_output_quantizers(keras.layers.Activation('selu'))\n\n    with self.assertRaises(ValueError):\n      quantize_config.get_output_quantizers(\n          keras.layers.Activation(lambda x: x))\n\n\nif __name__ == '__main__':\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "tensorflow/model-optimization", "file_path": "tensorflow_model_optimization/python/core/quantization/keras/experimental/default_n_bit/default_n_bit_transforms.py", "content": "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Default 8-bit transforms.\"\"\"\n\nimport collections\nimport inspect\n\nfrom keras import backend\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantize_aware_activation\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantize_layer\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantizers\nfrom tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit import default_n_bit_quantize_configs as configs\nfrom tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit import default_n_bit_quantize_registry\nfrom tensorflow_model_optimization.python.core.quantization.keras.graph_transformations import transforms\n\nLayerNode = transforms.LayerNode\nLayerPattern = transforms.LayerPattern\n\nkeras = tf.keras\n\n\ndef _get_conv_bn_layers(bn_layer_node):\n  bn_layer = bn_layer_node.layer\n  conv_layer = bn_layer_node.input_layers[0].layer\n  return conv_layer, bn_layer\n\n\ndef _get_weights(bn_layer_node):\n  \"\"\"Returns weight values for fused layer, including copying original values in unfused version.\"\"\"\n\n  return collections.OrderedDict(\n      list(bn_layer_node.input_layers[0].weights.items())\n      + list(bn_layer_node.weights.items()))\n\n\ndef _get_params(conv_layer, bn_layer, relu_layer=None):\n  \"\"\"Retrieve conv_bn params within wrapped layers.\"\"\"\n  if 'use_bias' in conv_layer['config']:\n    if conv_layer['config']['use_bias']:\n      raise ValueError(\n          'use_bias should not be set to True in a Conv layer when followed '\n          'by BatchNormalization. The bias in the Conv would be redundant '\n          'with the one in the BatchNormalization.')\n\n    del conv_layer['config']['use_bias']\n\n  if 'name' in bn_layer['config']:\n    del bn_layer['config']['name']\n\n  # TODO(pulkitb): remove key conflicts\n  params = dict(\n      list(conv_layer['config'].items()) + list(bn_layer['config'].items()))\n\n  if relu_layer is not None:\n    params['post_activation'] = keras.layers.deserialize(relu_layer)\n\n  return params\n\n\ndef _get_layer_node(fused_layer, weights):\n  layer_config = keras.layers.serialize(fused_layer)\n  layer_config['name'] = layer_config['config']['name']\n  # This config tracks which layers get quantized, and whether they have a\n  # custom QuantizeConfig.\n  layer_metadata = {'quantize_config': None}\n\n  return LayerNode(layer_config, weights, metadata=layer_metadata)\n\n\ndef _get_quantize_config(layer_node):\n  return layer_node.metadata.get('quantize_config')\n\n\ndef _has_custom_quantize_config(*layer_nodes):\n  for layer_node in layer_nodes:\n    if _get_quantize_config(layer_node) is not None:\n      return True\n  return False\n\n\ndef _normalize_tuple(value):\n  if isinstance(value, int):\n    return (value,)\n  else:\n    return tuple(value)\n\n\nclass Conv2DBatchNormQuantize(transforms.Transform):\n  \"\"\"Ensure FQ does not get placed between Conv and BatchNorm.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'BatchNormalization|SyncBatchNormalization',\n        inputs=[LayerPattern(\n            'Conv2D|DepthwiseConv2D', config={'activation': 'linear'})])\n\n  def _replace(self, bn_layer_node, conv_layer_node):\n    if _has_custom_quantize_config(bn_layer_node, conv_layer_node):\n      return bn_layer_node\n\n    conv_layer_node.layer['config']['activation'] = (\n        keras.activations.serialize(quantize_aware_activation.NoOpActivation()))\n    bn_layer_node.metadata['quantize_config'] = (\n        configs.DefaultNBitOutputQuantizeConfig(\n            num_bits_weight=self._num_bits_weight,\n            num_bits_activation=self._num_bits_activation))\n\n    return bn_layer_node\n\n  def replacement(self, match_layer):\n    bn_layer_node = match_layer\n    conv_layer_node = match_layer.input_layers[0]\n\n    return self._replace(bn_layer_node, conv_layer_node)\n\n  def custom_objects(self):\n    return {\n        'NoOpQuantizeConfig':\n            configs.NoOpQuantizeConfig,\n        'NoOpActivation':\n            quantize_aware_activation.NoOpActivation\n    }\n\n\nclass Conv2DReshapeBatchNormQuantize(Conv2DBatchNormQuantize):\n  \"\"\"Ensure FQ does not get placed between Conv, Reshape and BatchNorm.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(Conv2DReshapeBatchNormQuantize, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'BatchNormalization|SyncBatchNormalization',\n        inputs=[LayerPattern(\n            'Lambda', config={'name': 'sepconv1d_squeeze.*'},\n            inputs=[LayerPattern(\n                'Conv2D|DepthwiseConv2D',\n                config={'activation': 'linear'})])])\n\n  def replacement(self, match_layer):\n    bn_layer_node = match_layer\n    reshape_layer_node = bn_layer_node.input_layers[0]\n    conv_layer_node = reshape_layer_node.input_layers[0]\n\n    return self._replace(bn_layer_node, conv_layer_node)\n\n\nclass Conv2DBatchNormReLUQuantize(Conv2DBatchNormQuantize):\n  \"\"\"Ensure FQ does not get placed between Conv, BatchNorm and ReLU.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(Conv2DBatchNormReLUQuantize, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        # TODO(pulkitb): Enhance match to only occur for relu, relu1 and relu6\n        'ReLU',\n        inputs=[super(Conv2DBatchNormReLUQuantize, self).pattern()])\n\n  def _replace(self, relu_layer_node, bn_layer_node, conv_layer_node):\n    if _has_custom_quantize_config(\n        relu_layer_node, bn_layer_node, conv_layer_node):\n      return relu_layer_node\n\n    conv_layer_node.layer['config']['activation'] = (\n        keras.activations.serialize(quantize_aware_activation.NoOpActivation()))\n    bn_layer_node.metadata['quantize_config'] = (\n        configs.NoOpQuantizeConfig())\n\n    return relu_layer_node\n\n  def replacement(self, match_layer):\n    relu_layer_node = match_layer\n    bn_layer_node = relu_layer_node.input_layers[0]\n    conv_layer_node = bn_layer_node.input_layers[0]\n\n    return self._replace(relu_layer_node, bn_layer_node, conv_layer_node)\n\n\nclass Conv2DBatchNormActivationQuantize(Conv2DBatchNormReLUQuantize):\n  \"\"\"Ensure FQ does not get placed between Conv, BatchNorm and ReLU.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(Conv2DBatchNormActivationQuantize, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'Activation',\n        config={'activation': 'relu'},\n        inputs=[Conv2DBatchNormQuantize.pattern(self)])\n\n\nclass Conv2DReshapeBatchNormReLUQuantize(Conv2DBatchNormReLUQuantize):\n  \"\"\"Ensure FQ does not get placed between Conv, BatchNorm and ReLU.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(Conv2DReshapeBatchNormReLUQuantize, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'ReLU',\n        inputs=[Conv2DReshapeBatchNormQuantize.pattern(self)])\n\n  def replacement(self, match_layer):\n    relu_layer_node = match_layer\n    bn_layer_node = relu_layer_node.input_layers[0]\n    squeeze_layer_node = bn_layer_node.input_layers[0]\n    conv_layer_node = squeeze_layer_node.input_layers[0]\n\n    return self._replace(relu_layer_node, bn_layer_node, conv_layer_node)\n\n\nclass Conv2DReshapeBatchNormActivationQuantize(\n    Conv2DReshapeBatchNormReLUQuantize):\n  \"\"\"Ensure FQ does not get placed between Conv, BatchNorm and ReLU.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(Conv2DReshapeBatchNormActivationQuantize, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'Activation',\n        config={'activation': 'relu'},\n        inputs=[Conv2DReshapeBatchNormQuantize.pattern(self)])\n\n\nclass DenseBatchNormQuantize(transforms.Transform):\n  \"\"\"Transform to be applied to \"Dense\"+ \"BatchNorm\" Graph.\n\n  This transform disables Quantization between Dense and BatchNorm\n  to ensure FQ does not get placed between them.\n  \"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'BatchNormalization|SyncBatchNormalization',\n        inputs=[LayerPattern('Dense', config={'activation': 'linear'})])\n\n  def _replace(self, bn_layer_node, dense_layer_node):\n    if _has_custom_quantize_config(bn_layer_node, dense_layer_node):\n      return bn_layer_node\n\n    dense_layer_node.layer['config']['activation'] = (\n        keras.activations.serialize(quantize_aware_activation.NoOpActivation()))\n    bn_layer_node.metadata['quantize_config'] = (\n        configs.DefaultNBitOutputQuantizeConfig(\n            num_bits_weight=self._num_bits_weight,\n            num_bits_activation=self._num_bits_activation))\n    return bn_layer_node\n\n  def replacement(self, match_layer):\n    bn_layer_node = match_layer\n    dense_layer_node = match_layer.input_layers[0]\n\n    return self._replace(bn_layer_node, dense_layer_node)\n\n  def custom_objects(self):\n    return {\n        'DefaultNBitOutputQuantizeConfig':\n            configs.DefaultNBitOutputQuantizeConfig,\n        'NoOpQuantizeConfig':\n            configs.NoOpQuantizeConfig,\n        'NoOpActivation': quantize_aware_activation.NoOpActivation\n    }\n\n\nclass DenseBatchNormReLUQuantize(DenseBatchNormQuantize):\n  \"\"\"Transform to be applied to \"Dense\"+ \"BatchNorm\" + \"ReLU\" Graph.\n\n  This transform disables Quantization between Dense, BatchNorm and ReLU\n  to ensure FQ does not get placed between them.\n  \"\"\"\n\n  def pattern(self):\n    return LayerPattern(\n        'ReLU', inputs=[super(DenseBatchNormReLUQuantize, self).pattern()])\n\n  def _replace(self, relu_layer_node, bn_layer_node, dense_layer_node):\n    if _has_custom_quantize_config(relu_layer_node, bn_layer_node,\n                                   dense_layer_node):\n      return relu_layer_node\n\n    dense_layer_node.layer['config']['activation'] = (\n        keras.activations.serialize(quantize_aware_activation.NoOpActivation()))\n    bn_layer_node.metadata['quantize_config'] = (\n        configs.NoOpQuantizeConfig())\n\n    return relu_layer_node\n\n  def replacement(self, match_layer):\n    relu_layer_node = match_layer\n    bn_layer_node = relu_layer_node.input_layers[0]\n    dense_layer_node = bn_layer_node.input_layers[0]\n\n    return self._replace(relu_layer_node, bn_layer_node, dense_layer_node)\n\n\nclass DenseBatchNormActivationQuantize(DenseBatchNormReLUQuantize):\n  \"\"\"Transform to be applied to \"Dense\"+ \"BatchNorm\" + \"ReLU\" Graph.\n\n  This transform disables Quantization between Dense, BatchNorm and ReLU\n  to ensure FQ does not get placed between them.\n  \"\"\"\n\n  def pattern(self):\n    return LayerPattern(\n        'Activation',\n        config={'activation': 'relu'},\n        inputs=[DenseBatchNormQuantize.pattern(self)])\n\n\nclass SeparableConv1DQuantize(transforms.Transform):\n  \"\"\"Add QAT support for Keras SeparableConv1D layer.\n\n  Transforms SeparableConv1D into a SeparableConv2D invocation. The Keras\n  SeparableConv1D layer internally uses the same code as a SeparbaleConv2D\n  layer. It simple expands and squeezes the tensor dimensions before and after\n  the convolutions. Applying this transform ensures the QAT handling for\n  SeparableConv2D kicks in and handles the FQ placement properly.\n\n  Maps:\n  Input -> SeparableConv1D -> Output\n    to\n  Input -> Lambda(ExpandDims) -> SeparableConv2D -> Lambda(Squeeze) -> Output\n\n  Unlike SeparableConv2DQuantize, this does not break the layer into\n  DepthwiseConv and Conv separately, since no DepthwiseConv1D exists.\n  \"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern('SeparableConv1D')\n\n  def _get_name(self, prefix):\n    # TODO(pulkitb): Move away from `backend.unique_object_name` since it isn't\n    # exposed as externally usable.\n    return backend.unique_object_name(prefix)\n\n  def replacement(self, match_layer):\n    if _has_custom_quantize_config(match_layer):\n      return match_layer\n\n    sepconv1d_layer = match_layer.layer\n    sepconv1d_config = sepconv1d_layer['config']\n    sepconv1d_weights = list(match_layer.weights.values())\n\n    padding = sepconv1d_config['padding']\n    # SepConv2D does not accept causal padding, and SepConv1D has some special\n    # handling for it.\n    # TODO(pulkitb): Add support for causal padding.\n    if padding == 'causal':\n      raise ValueError('SeparableConv1D with causal padding is not supported.')\n\n    # TODO(pulkitb): Handle other base_layer args such as dtype, input_dim etc.\n\n    sepconv2d_layer = tf.keras.layers.SeparableConv2D(\n        filters=sepconv1d_config['filters'],\n        kernel_size=(1,) + _normalize_tuple(sepconv1d_config['kernel_size']),\n        strides=_normalize_tuple(sepconv1d_config['strides']) * 2,\n        padding=padding,\n        data_format=sepconv1d_config['data_format'],\n        dilation_rate=(1,) + _normalize_tuple(\n            sepconv1d_config['dilation_rate']),\n        depth_multiplier=sepconv1d_config['depth_multiplier'],\n        activation=sepconv1d_config['activation'],\n        use_bias=sepconv1d_config['use_bias'],\n        depthwise_initializer=sepconv1d_config['depthwise_initializer'],\n        pointwise_initializer=sepconv1d_config['pointwise_initializer'],\n        bias_initializer=sepconv1d_config['bias_initializer'],\n        depthwise_regularizer=sepconv1d_config['depthwise_regularizer'],\n        pointwise_regularizer=sepconv1d_config['pointwise_regularizer'],\n        bias_regularizer=sepconv1d_config['bias_regularizer'],\n        activity_regularizer=sepconv1d_config['activity_regularizer'],\n        depthwise_constraint=sepconv1d_config['depthwise_constraint'],\n        pointwise_constraint=sepconv1d_config['pointwise_constraint'],\n        bias_constraint=sepconv1d_config['bias_constraint'],\n        # TODO(pulkitb): Rethink what to do for name. Using the same name leads\n        # to confusion, since it's typically separable_conv1d\n        name=sepconv1d_config['name'] + '_QAT_SepConv2D',\n        trainable=sepconv1d_config['trainable']\n    )\n\n    sepconv2d_weights = collections.OrderedDict()\n    sepconv2d_weights['depthwise_kernel:0'] = np.expand_dims(\n        sepconv1d_weights[0], 0)\n    sepconv2d_weights['pointwise_kernel:0'] = np.expand_dims(\n        sepconv1d_weights[1], 0)\n    if sepconv1d_config['use_bias']:\n      sepconv2d_weights['bias:0'] = sepconv1d_weights[2]\n\n    if sepconv1d_config['data_format'] == 'channels_last':\n      spatial_dim = 1\n    else:\n      spatial_dim = 2\n\n    sepconv2d_layer_config = keras.layers.serialize(sepconv2d_layer)\n    sepconv2d_layer_config['name'] = sepconv2d_layer.name\n\n    # Needed to ensure these new layers are considered for quantization.\n    sepconv2d_metadata = {'quantize_config': None}\n\n    # TODO(pulkitb): Consider moving from Lambda to custom ExpandDims/Squeeze.\n\n    # Layer before SeparableConv2D which expands input tensors to match 2D.\n    expand_layer = tf.keras.layers.Lambda(\n        lambda x: tf.expand_dims(x, spatial_dim),\n        name=self._get_name('sepconv1d_expand'))\n    expand_layer_config = keras.layers.serialize(expand_layer)\n    expand_layer_config['name'] = expand_layer.name\n    expand_layer_metadata = {\n        'quantize_config':\n            configs.NoOpQuantizeConfig()}\n\n    squeeze_layer = tf.keras.layers.Lambda(\n        lambda x: tf.squeeze(x, [spatial_dim]),\n        name=self._get_name('sepconv1d_squeeze'))\n    squeeze_layer_config = keras.layers.serialize(squeeze_layer)\n    squeeze_layer_config['name'] = squeeze_layer.name\n    squeeze_layer_metadata = {\n        'quantize_config':\n            configs.NoOpQuantizeConfig()}\n\n    return LayerNode(\n        squeeze_layer_config,\n        metadata=squeeze_layer_metadata,\n        input_layers=[LayerNode(\n            sepconv2d_layer_config,\n            weights=sepconv2d_weights,\n            metadata=sepconv2d_metadata,\n            input_layers=[LayerNode(\n                expand_layer_config, metadata=expand_layer_metadata)]\n            )])\n\n\nclass SeparableConvQuantize(transforms.Transform):\n  \"\"\"Break SeparableConv into a DepthwiseConv and Conv layer.\n\n  SeparableConv is a composition of a DepthwiseConv and a Conv layer. For the\n  purpose of quantization, a FQ operation needs to be placed between the output\n  of DepthwiseConv and the following Conv.\n\n  This is needed since there is a dynamic tensor in between the two layers, and\n  it's range information needs to be captured by the FakeQuant op to ensure\n  full int8 quantization of the layers is possible.\n\n  Splitting the layer into 2 ensures that each individual layer is handled\n  correctly with respect to quantization.\n  \"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern('SeparableConv2D')\n\n  def replacement(self, match_layer):\n    if _has_custom_quantize_config(match_layer):\n      return match_layer\n\n    sepconv_layer = match_layer.layer\n    sepconv_weights = list(match_layer.weights.values())\n\n    # TODO(pulkitb): SeparableConv has kwargs other than constructor args which\n    # need to be handled.\n    # Applicable to both layers: trainable, dtype, name\n    # Applicable to dconv: input_dim, input_shape, batch_input_shape, batch_size\n    # Needs special handling: weights\n    # Unknown: dynamic, autocast\n\n    dconv_layer = tf.keras.layers.DepthwiseConv2D(\n        kernel_size=sepconv_layer['config']['kernel_size'],\n        strides=sepconv_layer['config']['strides'],\n        padding=sepconv_layer['config']['padding'],\n        depth_multiplier=sepconv_layer['config']['depth_multiplier'],\n        data_format=sepconv_layer['config']['data_format'],\n        dilation_rate=sepconv_layer['config']['dilation_rate'],\n        activation=None,\n        use_bias=False,\n        depthwise_initializer=sepconv_layer['config']['depthwise_initializer'],\n        depthwise_regularizer=sepconv_layer['config']['depthwise_regularizer'],\n        depthwise_constraint=sepconv_layer['config']['depthwise_constraint'],\n        trainable=sepconv_layer['config']['trainable']\n    )\n    dconv_weights = collections.OrderedDict()\n    dconv_weights['depthwise_kernel:0'] = sepconv_weights[0]\n    dconv_layer_config = keras.layers.serialize(dconv_layer)\n    dconv_layer_config['name'] = dconv_layer.name\n    # Needed to ensure these new layers are considered for quantization.\n    dconv_metadata = {'quantize_config': None}\n\n    conv_layer = tf.keras.layers.Conv2D(\n        filters=sepconv_layer['config']['filters'],\n        kernel_size=(1, 1),  # (1,) * rank\n        strides=(1, 1),\n        padding='valid',\n        data_format=sepconv_layer['config']['data_format'],\n        dilation_rate=sepconv_layer['config']['dilation_rate'],\n        groups=1,\n        activation=sepconv_layer['config']['activation'],\n        use_bias=sepconv_layer['config']['use_bias'],\n        kernel_initializer=sepconv_layer['config']['pointwise_initializer'],\n        bias_initializer=sepconv_layer['config']['bias_initializer'],\n        kernel_regularizer=sepconv_layer['config']['pointwise_regularizer'],\n        bias_regularizer=sepconv_layer['config']['bias_regularizer'],\n        activity_regularizer=sepconv_layer['config']['activity_regularizer'],\n        kernel_constraint=sepconv_layer['config']['pointwise_constraint'],\n        bias_constraint=sepconv_layer['config']['bias_constraint'],\n        trainable=sepconv_layer['config']['trainable']\n    )\n    conv_weights = collections.OrderedDict()\n    conv_weights['kernel:0'] = sepconv_weights[1]\n    if sepconv_layer['config']['use_bias']:\n      conv_weights['bias:0'] = sepconv_weights[2]\n    conv_layer_config = keras.layers.serialize(conv_layer)\n    conv_layer_config['name'] = conv_layer.name\n    # Needed to ensure these new layers are considered for quantization.\n    conv_metadata = {'quantize_config': None}\n\n    dconv_layer_node = LayerNode(\n        dconv_layer_config, weights=dconv_weights, metadata=dconv_metadata)\n    return LayerNode(\n        conv_layer_config,\n        weights=conv_weights,\n        input_layers=[dconv_layer_node],\n        metadata=conv_metadata)\n\n\nclass LayerReLUQuantize(transforms.Transform):\n  \"\"\"Ensure FQ does not get placed between Add and ReLU.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'ReLU', inputs=[LayerPattern('Add|Conv2D|DepthwiseConv2D|Dense')])\n\n  def replacement(self, match_layer):\n    relu_layer_node = match_layer\n    add_layer_node = relu_layer_node.input_layers[0]\n\n    add_layer_node.metadata['quantize_config'] = (\n        configs.NoOpQuantizeConfig())\n\n    return match_layer\n\n  def custom_objects(self):\n    return {\n        'NoOpQuantizeConfig':\n            configs.NoOpQuantizeConfig,\n    }\n\n\nclass LayerReluActivationQuantize(LayerReLUQuantize):\n  \"\"\"Ensure FQ does not get placed between Add and ReLU.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(LayerReluActivationQuantize, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'Activation',\n        config={'activation': 'relu'},\n        inputs=[LayerPattern('Add|Conv2D|DepthwiseConv2D|Dense')])\n\n\nclass InputLayerQuantize(transforms.Transform):\n  \"\"\"Quantizes InputLayer, by adding QuantizeLayer after it.\n\n  InputLayer => InputLayer -> QuantizeLayer\n  \"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern('InputLayer')\n\n  def replacement(self, match_layer):\n    quant_layer = quantize_layer.QuantizeLayer(\n        quantizers.AllValuesQuantizer(\n            num_bits=self._num_bits_activation, per_axis=False,\n            symmetric=False, narrow_range=False))  # activation/output\n    layer_config = keras.layers.serialize(quant_layer)\n    layer_config['name'] = quant_layer.name\n\n    quant_layer_node = LayerNode(\n        layer_config,\n        input_layers=[match_layer])\n\n    return quant_layer_node\n\n  def custom_objects(self):\n    return {\n        'QuantizeLayer': quantize_layer.QuantizeLayer,\n        'MovingAverageQuantizer': quantizers.MovingAverageQuantizer,\n        'AllValuesQuantizer': quantizers.AllValuesQuantizer\n    }\n\n\nclass ConcatTransform(transforms.Transform):\n  \"\"\"Transform for Concatenate. Quantize only after concatenation.\"\"\"\n\n  # pylint:disable=protected-access\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    # TODO(pulkitb): Write a clean way to handle length patterns.\n    return LayerPattern(\n        'Concatenate', inputs=[LayerPattern('.*'), LayerPattern('.*')])\n\n  def _get_layer_type(self, layer_class_name):\n    keras_layers = inspect.getmembers(tf.keras.layers, inspect.isclass)\n    for layer_name, layer_type in keras_layers:\n      if layer_name == layer_class_name:\n        return layer_type\n    return None\n\n  def _disable_output_quantize(self, quantize_config):\n    # TODO(pulkitb): Disabling quantize_config may also require handling\n    # activation quantizers. Handle that properly.\n    quantize_config.get_output_quantizers = lambda layer: []\n\n  def replacement(self, match_layer):\n    concat_layer_node = match_layer\n    feeding_layer_nodes = match_layer.input_layers\n\n    default_registry = (\n        default_n_bit_quantize_registry.DefaultNBitQuantizeRegistry(\n            num_bits_weight=self._num_bits_weight,\n            num_bits_activation=self._num_bits_activation))\n\n    feed_quantize_configs = []\n    for feed_layer_node in feeding_layer_nodes:\n      quantize_config = feed_layer_node.metadata.get('quantize_config')\n      if not quantize_config:\n        layer_class = self._get_layer_type(feed_layer_node.layer['class_name'])\n        if layer_class is None:\n          # Concat has an input layer we don't recognize. Return.\n          return match_layer\n\n        if layer_class == keras.layers.Concatenate:\n          # Input layer to Concat is also Concat. Don't quantize it.\n          feed_layer_node.metadata['quantize_config'] = (\n              configs.NoOpQuantizeConfig())\n          continue\n\n        if not default_registry._is_supported_layer(layer_class):\n          # Feeding layer is not supported by registry\n          return match_layer\n\n        quantize_config = default_registry._get_quantize_config(layer_class)\n        feed_layer_node.metadata['quantize_config'] = quantize_config\n\n      feed_quantize_configs.append(quantize_config)\n\n    # TODO(pulkitb): this currently only disables output quantize config, but\n    # cannot properly handle if the FQ was added to the activation. Hand this\n    # properly.\n    for quantize_config in feed_quantize_configs:\n      self._disable_output_quantize(quantize_config)\n\n    if not concat_layer_node.metadata.get('quantize_config'):\n      concat_layer_node.metadata['quantize_config'] = (\n          configs.DefaultNBitOutputQuantizeConfig(\n              num_bits_weight=self._num_bits_weight,\n              num_bits_activation=self._num_bits_activation))\n\n    return concat_layer_node\n\n  # pylint:enable=protected-access\n\n\nclass ConcatTransform3Inputs(ConcatTransform):\n  \"\"\"Transform for 3 inputs Concatenate.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(ConcatTransform3Inputs, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'Concatenate',\n        inputs=[LayerPattern('.*'), LayerPattern('.*'), LayerPattern('.*')])\n\n\nclass ConcatTransform4Inputs(ConcatTransform):\n  \"\"\"Transform for 4 inputs Concatenate.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(ConcatTransform4Inputs, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'Concatenate',\n        inputs=[LayerPattern('.*'), LayerPattern('.*'), LayerPattern('.*'),\n                LayerPattern('.*')])\n\n\nclass ConcatTransform5Inputs(ConcatTransform):\n  \"\"\"Transform for 5 inputs Concatenate.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(ConcatTransform5Inputs, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'Concatenate',\n        inputs=[LayerPattern('.*'), LayerPattern('.*'), LayerPattern('.*'),\n                LayerPattern('.*'), LayerPattern('.*')])\n\n\nclass ConcatTransform6Inputs(ConcatTransform):\n  \"\"\"Transform for 6 inputs Concatenate.\"\"\"\n\n  def __init__(self, num_bits_weight: int = 8, num_bits_activation: int = 8):\n    super(ConcatTransform6Inputs, self).__init__(\n        num_bits_weight=num_bits_weight,\n        num_bits_activation=num_bits_activation)\n    self._num_bits_weight = num_bits_weight\n    self._num_bits_activation = num_bits_activation\n\n  def pattern(self):\n    return LayerPattern(\n        'Concatenate',\n        inputs=[LayerPattern('.*'), LayerPattern('.*'), LayerPattern('.*'),\n                LayerPattern('.*'), LayerPattern('.*'), LayerPattern('.*')])\n", "framework": "tensorflow"}
{"repo_name": "GitYiheng/reinforcement_learning_test", "file_path": "test03_monte_carlo/t25_rlvps03_033720022018.py", "content": "import tensorflow as tf # neural network for function approximation\nimport gym # environment\nimport numpy as np # matrix operation and math functions\nfrom gym import wrappers\nimport gym_morph # customized environment for cart-pole\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport time\n\nstart_time = time.time()\n\nMAX_TEST = 10\n\nfor test_num in range(1,MAX_TEST+1):\n\n    # Hyperparameters\n    RANDOM_NUMBER_SEED = test_num\n\n    ENVIRONMENT1 = \"morph-v0\"\n\n    MAX_EPISODES = 8000 # number of episodes\n    EPISODE_LENGTH = 500 # single episode length\n\n    HIDDEN_SIZE = 24\n    DISPLAY_WEIGHTS = False # Help debug weight update\n\n    gamma = 0.99  # Discount per step\n\n    RENDER = False  # Render the cart-pole system\n    VIDEO_INTERVAL = 100  # Generate a video at this interval\n\n    CONSECUTIVE_TARGET = 100 # Including previous 100 rewards\n\n    CONST_LR = True # Constant or decaying learing rate\n    # Constant learning rate\n    const_learning_rate_in = 0.001\n    # Decay learning rate\n    start_learning_rate_in = 0.003\n    decay_steps_in = 100\n    decay_rate_in = 0.95\n\n    DIR_PATH_SAVEFIG = \"/root/cartpole_plot/\"\n\n    if CONST_LR:\n        learning_rate = const_learning_rate_in\n\n        file_name_savefig = \"el\" + str(EPISODE_LENGTH) \\\n        + \"_hn\" + str(HIDDEN_SIZE) \\\n        + \"_clr\" + str(learning_rate).replace(\".\", \"p\") \\\n        + \"_test\" + str(test_num) \\\n        + \".png\"\n    else:\n        start_learning_rate = start_learning_rate_in\n        decay_steps = decay_steps_in\n        decay_rate = decay_rate_in\n\n        file_name_savefig = \"el\" + str(EPISODE_LENGTH) \\\n        + \"_hn\" + str(HIDDEN_SIZE) \\\n        + \"_dlr_slr\" + str(start_learning_rate).replace(\".\", \"p\") \\\n        + \"_ds\" + str(decay_steps) \\\n        + \"_dr\" + str(decay_rate).replace(\".\", \"p\") \\\n        + \"_test\" + str(test_num) \\\n        + \".png\"\n\n    env = gym.make(ENVIRONMENT1)\n\n    env.seed(RANDOM_NUMBER_SEED)\n    np.random.seed(RANDOM_NUMBER_SEED)\n    tf.set_random_seed(RANDOM_NUMBER_SEED)\n\n    # Input and output sizes\n    input_size = 4\n    output_size = 2\n    # input_size = env.observation_space.shape[0]\n    # try:\n    #     output_size = env.action_space.shape[0]\n    # except AttributeError:\n    #     output_size = env.action_space.n\n\n    # Tensorflow network setup\n    x = tf.placeholder(tf.float32, shape=(None, input_size))\n    y = tf.placeholder(tf.float32, shape=(None, 1))\n\n    if not CONST_LR:\n        # decay learning rate\n        global_step = tf.Variable(0, trainable=False)\n        learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, decay_steps, decay_rate, staircase=False)\n\n    expected_returns = tf.placeholder(tf.float32, shape=(None, 1))\n\n    # Xavier (2010) weights initializer for uniform distribution:\n    # x = sqrt(6. / (in + out)); [-x, x]\n    w_init = tf.contrib.layers.xavier_initializer()\n\n    hidden_W = tf.get_variable(\"W1\", shape=[input_size, HIDDEN_SIZE],\n    initializer=w_init)\n    hidden_B = tf.Variable(tf.zeros(HIDDEN_SIZE))\n    dist_W = tf.get_variable(\"W2\", shape=[HIDDEN_SIZE, output_size],\n    initializer=w_init)\n    dist_B = tf.Variable(tf.zeros(output_size))\n    hidden = tf.nn.elu(tf.matmul(x, hidden_W) + hidden_B)\n    dist = tf.tanh(tf.matmul(hidden, dist_W) + dist_B)\n\n    dist_soft = tf.nn.log_softmax(dist)\n    dist_in = tf.matmul(dist_soft, tf.Variable([[1.], [0.]]))\n    pi = tf.contrib.distributions.Bernoulli(dist_in)\n    pi_sample = pi.sample()\n    log_pi = pi.log_prob(y)\n\n    if CONST_LR:\n        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n        train = optimizer.minimize(-1.0 * expected_returns * log_pi)\n    else:\n        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n        train = optimizer.minimize(-1.0 * expected_returns * log_pi, global_step=global_step)\n\n    # saver = tf.train.Saver()\n\n    # Create and initialize a session\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    def run_episode(environment, ep, render=False):\n        raw_reward = 0\n        discounted_reward = 0\n        cumulative_reward = []\n        discount = 1.0\n        states = []\n        actions = []\n        obs = environment.reset()\n        done = False\n        while not done:\n            states.append(obs)\n            cumulative_reward.append(discounted_reward)\n            if render and ((ep % VIDEO_INTERVAL) == 0):\n                environment.render()\n            action = sess.run(pi_sample, feed_dict={x: [obs]})[0]\n            actions.append(action)\n            obs, reward, done, info = env.step(action[0])\n            raw_reward += reward\n            if reward > 0:\n                discounted_reward += reward * discount\n            else:\n                discounted_reward += reward\n            discount *= gamma\n        return raw_reward, discounted_reward, cumulative_reward, states, actions\n\n\n    def display_weights(session):\n        w1 = session.run(hidden_W)\n        b1 = session.run(hidden_B)\n        w2 = session.run(dist_W)\n        b2 = session.run(dist_B)\n        print(w1, b1, w2, b2)\n\n    returns = []\n    mean_returns = []\n    for ep in range(MAX_EPISODES):\n        raw_G, discounted_G, cumulative_G, ep_states, ep_actions = \\\n            run_episode(env, ep, RENDER)\n        expected_R = np.transpose([discounted_G - np.array(cumulative_G)])\n        sess.run(train, feed_dict={x: ep_states, y: ep_actions,\n                                   expected_returns: expected_R})\n        if DISPLAY_WEIGHTS:\n            display_weights(sess)\n\n        returns.append(raw_G)\n        running_returns = returns[max(0, ep-CONSECUTIVE_TARGET):(ep+1)]\n        mean_return = np.mean(running_returns)\n        mean_returns.append(mean_return)\n        if CONST_LR:\n            msg = \"Test: {}/{}, Episode: {}/{}, Time: {}, Learning rate: {}, Return: {}, Last {} returns mean: {}\"\n            msg = msg.format(test_num, MAX_TEST, ep+1, MAX_EPISODES, time.strftime('%H:%M:%S', time.gmtime(time.time()-start_time)), learning_rate, raw_G, CONSECUTIVE_TARGET, mean_return)\n            print(msg)\n        else:\n            msg = \"Test: {}/{}, Episode: {}/{}, Time: {}, Learning rate: {}, Return: {}, Last {} returns mean: {}\"\n            msg = msg.format(test_num, MAX_TEST, ep+1, MAX_EPISODES, time.strftime('%H:%M:%S', time.gmtime(time.time()-start_time)), sess.run(learning_rate), raw_G, CONSECUTIVE_TARGET, mean_return)\n            print(msg)\n\n    env.close() # close openai gym environment\n    tf.reset_default_graph() # clear tensorflow graph\n\n    # Plot\n    # plt.style.use('ggplot')\n    plt.style.use('dark_background')\n    episodes_plot = np.arange(MAX_EPISODES)\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    fig.subplots_adjust(top=0.85)\n    if CONST_LR:\n        ax.set_title(\"The Cart-Pole Problem Test %i \\n \\\n        Episode Length: %i   \\\n        Discount Factor: %.2f \\n \\\n        Number of Hidden Neuron: %i   \\\n        Constant Learning Rate: %.5f\" % (test_num, EPISODE_LENGTH, gamma, HIDDEN_SIZE, learning_rate))\n    else:\n        ax.set_title(\"The Cart-Pole Problem Test %i \\n \\\n        EpisodeLength: %i  DiscountFactor: %.2f  NumHiddenNeuron: %i \\n \\\n        Decay Learning Rate: (start: %.5f, steps: %i, rate: %.2f)\" % (test_num, EPISODE_LENGTH, gamma, HIDDEN_SIZE, start_learning_rate, decay_steps, decay_rate))\n    ax.set_xlabel(\"Episode\")\n    ax.set_ylabel(\"Return\")\n    ax.set_ylim((0, EPISODE_LENGTH))\n    ax.grid(linestyle='--')\n    ax.plot(episodes_plot, returns, label='Instant return')\n    ax.plot(episodes_plot, mean_returns, label='Averaged return')\n    legend = ax.legend(loc='best', shadow=True)\n    fig.savefig(DIR_PATH_SAVEFIG + file_name_savefig, dpi=500)\n    # plt.show()\n", "framework": "tensorflow"}
{"repo_name": "icoxfog417/tensorflow_qrnn", "file_path": "test_tf_qrnn_forward.py", "content": "import unittest\nimport numpy as np\nimport tensorflow as tf\nfrom tf_qrnn import QRNN\n\n\nclass TestQRNNForward(unittest.TestCase):\n\n    def test_qrnn_linear_forward(self):\n        batch_size = 100\n        sentence_length = 5\n        word_size = 10\n        size = 5\n        data = self.create_test_data(batch_size, sentence_length, word_size)\n\n        with tf.Graph().as_default() as q_linear:\n            qrnn = QRNN(in_size=word_size, size=size, conv_size=1)\n            X = tf.placeholder(tf.float32, [batch_size, sentence_length, word_size])\n            forward_graph = qrnn.forward(X)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                hidden = sess.run(forward_graph, feed_dict={X: data})\n                self.assertEqual((batch_size, size), hidden.shape)\n\n    def test_qrnn_with_previous(self):\n        batch_size = 100\n        sentence_length = 5\n        word_size = 10\n        size = 5\n        data = self.create_test_data(batch_size, sentence_length, word_size)\n\n        with tf.Graph().as_default() as q_with_previous:\n            qrnn = QRNN(in_size=word_size, size=size, conv_size=2)\n            X = tf.placeholder(tf.float32, [batch_size, sentence_length, word_size])\n            forward_graph = qrnn.forward(X)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                hidden = sess.run(forward_graph, feed_dict={X: data})\n                self.assertEqual((batch_size, size), hidden.shape)\n\n    def test_qrnn_convolution(self):\n        batch_size = 100\n        sentence_length = 5\n        word_size = 10\n        size = 5\n        data = self.create_test_data(batch_size, sentence_length, word_size)\n\n        with tf.Graph().as_default() as q_conv:\n            qrnn = QRNN(in_size=word_size, size=size, conv_size=3)\n            X = tf.placeholder(tf.float32, [batch_size, sentence_length, word_size])\n            forward_graph = qrnn.forward(X)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                hidden = sess.run(forward_graph, feed_dict={X: data})\n                self.assertEqual((batch_size, size), hidden.shape)\n\n    def create_test_data(self, batch_size, sentence_length, word_size):\n        batch = []\n        for b in range(batch_size):\n            sentence = np.random.rand(sentence_length, word_size)\n            batch.append(sentence)\n        return np.array(batch)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "framework": "tensorflow"}
{"repo_name": "NifTK/NiftyNet", "file_path": "tests/sampler_csvpatch_v2_test.py", "content": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom niftynet.contrib.csv_reader.csv_reader import CSVReader\nfrom niftynet.contrib.csv_reader.sampler_csvpatch import CSVPatchSampler\nfrom niftynet.engine.image_window import N_SPATIAL\n# from niftynet.engine.sampler_uniform import UniformSampler\nfrom niftynet.engine.sampler_uniform_v2 import rand_spatial_coordinates\nfrom niftynet.io.image_reader import ImageReader\nfrom niftynet.io.image_sets_partitioner import ImageSetsPartitioner\nfrom niftynet.utilities.util_common import ParserNamespace\nfrom tests.niftynet_testcase import NiftyNetTestCase\n\nDYNAMIC_MOD_DATA = {\n    'T1':\n    ParserNamespace(\n        csv_file='',\n        path_to_search='data/csv_data',\n        filename_contains=(),\n        filename_not_contains=('_', 'csv'),\n        interp_order=3,\n        csv_data_file='',\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(69, 69, 69),\n        loader=None),\n    'sampler':\n    ParserNamespace(\n        csv_file='',\n        path_to_search='',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file='data/csv_data/ICBMTest3.csv')\n}\n\nDYNAMIC_MOD_TASK = ParserNamespace(\n    image=('T1', ), label=('T1', ), sampler=('sampler', ))\n\nLARGE_MOD_DATA = {\n    'T1':\n    ParserNamespace(\n        csv_file='',\n        path_to_search='data/csv_data',\n        filename_contains=(),\n        filename_not_contains=('_', 'csv'),\n        interp_order=3,\n        csv_data_file='',\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(75, 75, 75),\n        loader=None),\n    'sampler':\n    ParserNamespace(\n        csv_file='',\n        path_to_search='',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file='data/csv_data/ICBMTest2.csv')\n}\nLARGE_MOD_DATA_2_ELEMENTS = {\n    'T1':\n    ParserNamespace(\n        csv_file='',\n        path_to_search='data/csv_data',\n        filename_contains=(),\n        filename_not_contains=('_', 'csv'),\n        interp_order=3,\n        csv_data_file='',\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(75, 75, 75),\n        loader=None),\n    'sampler':\n    ParserNamespace(\n        csv_file='',\n        path_to_search='',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file='data/csv_data/ICBMTest4.csv')\n}\nLARGE_MOD_TASK = ParserNamespace(\n    image=('T1', ), label=('T1', ), sampler=('sampler', ))\n\nCSV_DATA = {\n    'sampler':\n    ParserNamespace(\n        csv_file='',\n        path_to_search='',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file='data/csv_data/ICBMTest3.csv')\n}\n\nCSV_DATA_TWO_ELEMENTS = {\n    'sampler':\n    ParserNamespace(\n        csv_file='',\n        path_to_search='',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file='data/csv_data/ICBMTest4.csv')\n}\n\nCSVBAD_DATA = {\n    'sampler':\n    ParserNamespace(\n        csv_file='',\n        path_to_search='',\n        filename_contains=(),\n        filename_not_contains=(),\n        interp_order=0,\n        pixdim=None,\n        axcodes=None,\n        spatial_window_size=(),\n        loader=None,\n        csv_data_file='data/csv_data/ICBMTest.csv')\n}\n\ndata_partitioner = ImageSetsPartitioner()\n# multi_mod_list = data_partitioner.initialise(MULTI_MOD_DATA).get_file_list()\n# mod_2d_list = data_partitioner.initialise(MOD_2D_DATA).get_file_list()\ndynamic_list = data_partitioner.initialise(DYNAMIC_MOD_DATA).get_file_list()\n\n# def get_3d_reader():\n#     reader = ImageReader(['image'])\n#     reader.initialise(MULTI_MOD_DATA, MULTI_MOD_TASK, multi_mod_list)\n#     return reader\n\n# def get_2d_reader():\n#     reader = ImageReader(['image'])\n#     reader.initialise(MOD_2D_DATA, MOD_2D_TASK, mod_2d_list)\n#     return reader\n\n\ndef get_dynamic_window_reader():\n    reader = ImageReader(['image'])\n    reader.initialise(DYNAMIC_MOD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return reader\n\n\ndef get_large_window_reader():\n    reader = ImageReader(['image'])\n    reader.initialise(LARGE_MOD_DATA, LARGE_MOD_TASK, dynamic_list)\n    return reader\n\n\ndef get_large_window_reader_two_elements():\n    reader = ImageReader(['image'])\n    reader.initialise(LARGE_MOD_DATA_2_ELEMENTS, LARGE_MOD_TASK, dynamic_list)\n    return reader\n\n\n# def get_concentric_window_reader():\n#     reader = ImageReader(['image', 'label'])\n#     reader.initialise(MULTI_WINDOW_DATA, MULTI_WINDOW_TASK, multi_mod_list)\n#     return reader\n\n\ndef get_csvpatch_reader_two_elements():\n    csv_reader = CSVReader(['sampler'])\n    csv_reader.initialise(CSV_DATA_TWO_ELEMENTS, DYNAMIC_MOD_TASK,\n                          dynamic_list)\n    return csv_reader\n\n\ndef get_csvpatch_reader():\n    csv_reader = CSVReader(['sampler'])\n    csv_reader.initialise(CSV_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return csv_reader\n\n\ndef get_csvpatchbad_reader():\n    csv_reader = CSVReader(['sampler'])\n    csv_reader.initialise(CSVBAD_DATA, DYNAMIC_MOD_TASK, dynamic_list)\n    return csv_reader\n\n\nclass CSVPatchSamplerTest(NiftyNetTestCase):\n    def test_3d_csvsampler_init(self):\n        sampler = CSVPatchSampler(\n            reader=get_dynamic_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=DYNAMIC_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out['image_location']\n            # print(img_loc)\n            self.assertAllClose(out['image'].shape, (2, 69, 69, 69, 1))\n        sampler.close_all()\n\n    def test_pad_init(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3)\n\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out['image_location']\n            # print(img_loc)\n            self.assertAllClose(out['image'].shape[1:], (75, 75, 75, 1))\n        sampler.close_all()\n\n    def test_padd_volume(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out['image_location']\n            # print(img_loc)\n            self.assertAllClose(out['image'].shape[1:], (75, 75, 75, 1))\n        sampler.close_all()\n\n    def test_change_orientation(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3)\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out['image_location']\n            # print(img_loc)\n            self.assertAllClose(out['image'].shape[1:], (75, 75, 75, 1))\n        sampler.close_all()\n\n    def test_random_init(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3,\n            mode_correction='random')\n        with self.cached_session() as sess:\n            sampler.set_num_threads(2)\n            out = sess.run(sampler.pop_batch_op())\n            img_loc = out['image_location']\n            # print(img_loc)\n            self.assertAllClose(out['image'].shape[1:], (75, 75, 75, 1))\n        sampler.close_all()\n\n    def test_remove_element_two_elements(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader_two_elements(),\n            csv_reader=get_csvpatch_reader_two_elements(),\n            window_sizes=LARGE_MOD_DATA_2_ELEMENTS,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3,\n            mode_correction='remove')\n        with self.cached_session() as sess:\n            sampler.set_num_threads(1)\n            try:\n                out = sess.run(sampler.pop_batch_op())\n                passed = True\n            except Exception:\n                passed = False\n            self.assertTrue(passed)\n\n    def test_remove_element_one_element(self):\n        sampler = CSVPatchSampler(\n            reader=get_large_window_reader(),\n            csv_reader=get_csvpatch_reader(),\n            window_sizes=LARGE_MOD_DATA,\n            batch_size=2,\n            windows_per_image=1,\n            queue_length=3,\n            mode_correction='remove')\n        with self.assertRaisesRegexp(Exception, \"\"):\n            with self.cached_session() as sess:\n                sampler.set_num_threads(1)\n                out = sess.run(sampler.pop_batch_op())\n\n    def test_ill_init(self):\n        with self.assertRaisesRegexp(Exception, \"\"):\n            sampler = \\\n                CSVPatchSampler(reader=get_dynamic_window_reader(),\n                                     csv_reader=get_csvpatchbad_reader(),\n                                     window_sizes=DYNAMIC_MOD_DATA,\n                                     batch_size=2,\n                                     windows_per_image=10,\n                                     queue_length=3)\n\n    #\n\n    # def test_close_early(self):\n    #     sampler = UniformSampler(reader=get_dynamic_window_reader(),\n    #                              window_sizes=DYNAMIC_MOD_DATA,\n    #                              batch_size=2,\n    #                              windows_per_image=10,\n    #                              queue_length=10)\n\n\nclass RandomCoordinatesTest(NiftyNetTestCase):\n    def assertCoordinatesAreValid(self, coords, img_size, win_size):\n        for coord in coords:\n            for i in range(len(coord.shape)):\n                self.assertTrue(coord[i] >= int(win_size[i] / 2))\n\n                self.assertTrue(coord[i] <= img_size[i] - int(win_size[i] / 2))\n\n    def test_3d_coordinates(self):\n        img_size = [8, 9, 10]\n        win_size = [7, 9, 4]\n        coords = rand_spatial_coordinates(32, img_size, win_size, None)\n        self.assertAllEqual(coords.shape, (32, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, img_size, win_size)\n\n    def test_2d_coordinates(self):\n\n        cropped_map = np.zeros((256, 512, 1))\n        img_size = [8, 9, 1]\n        win_size = [8, 8, 1]\n        coords = rand_spatial_coordinates(64, img_size, win_size, None)\n        self.assertAllEqual(coords.shape, (64, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, img_size, win_size)\n\n    def test_1d_coordinates(self):\n        cropped_map = np.zeros((1, 1, 1))\n        img_size = [4, 1, 1]\n        win_size = [2, 1, 1]\n        coords = rand_spatial_coordinates(20, img_size, win_size, None)\n        # print(coords)\n        self.assertAllEqual(coords.shape, (20, N_SPATIAL))\n        self.assertCoordinatesAreValid(coords, img_size, win_size)\n\n\nif __name__ == \"__main__\":\n    tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "takat0m0/test_code", "file_path": "tf2/gan/main.py", "content": "# -*- coding:utf-8 -*-\n\nimport os\nimport sys\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\n\nfrom mnist_download import get_data\nfrom trainer import Trainer\nfrom model import Model\n\nif __name__ == '__main__':\n    # -- parameter --\n    z_dim = 100    \n    batch_size = 256\n    num_epoch = 100\n    \n    # -- data --\n    train_num = 10000\n    imgs, _ = get_data()\n    train_imgs = imgs[:train_num]\n\n    # -- trainer and model --\n    trainer = Trainer()\n    model = Model()\n\n    # -- train loop --\n    num_one_epoch = train_num // batch_size\n    idxs = [_ for _ in range(train_num)]\n    \n    for epoch in range(num_epoch):\n        # -- shuffle data --\n        tmp_idxs = np.random.permutation(idxs)\n        train_imgs = train_imgs[tmp_idxs]\n\n        g_total_loss = 0.0\n        d_total_loss = 0.0\n        \n        for i in range(num_one_epoch):\n            batch_imgs = train_imgs[i * batch_size: (i + 1) * batch_size]\n            z = np.random.uniform(-1, 1, [batch_size, z_dim]).astype(np.float32)\n            g_loss, d_loss = trainer(model, batch_imgs, z)\n\n            g_loss = g_loss.numpy()\n            g_total_loss += g_loss/num_one_epoch\n            \n            d_loss = d_loss.numpy()            \n            d_total_loss += d_loss/num_one_epoch\n\n        print('epoch:{}, g_loss:{}, d_loss:{}'.format(epoch, g_total_loss, d_total_loss))\n        model.gen.save('./model.dump')\n        z = np.random.uniform(-1, 1, [1, z_dim]).astype(np.float32)\n        fig = model.make_fig(z)[0]\n        fig = (fig + 1.0) * 127.5\n        fig = np.reshape(fig, (28, 28))\n        cv2.imwrite('test.png', fig)\n", "framework": "tensorflow"}
{"repo_name": "geffy/tffm", "file_path": "tffm/base.py", "content": "import tensorflow as tf\nfrom .core import TFFMCore\nfrom sklearn.base import BaseEstimator\nfrom abc import ABCMeta, abstractmethod\nimport six\nfrom tqdm import tqdm\nimport numpy as np\nimport os\n\n\ndef batcher(X_, y_=None, w_=None, batch_size=-1):\n    \"\"\"Split data to mini-batches.\n\n    Parameters\n    ----------\n    X_ : {numpy.array, scipy.sparse.csr_matrix}, shape (n_samples, n_features)\n        Training vector, where n_samples in the number of samples and\n        n_features is the number of features.\n\n    y_ : np.array or None, shape (n_samples,)\n        Target vector relative to X.\n\n    w_ : np.array or None, shape (n_samples,)\n        Vector of sample weights.\n\n    batch_size : int\n        Size of batches.\n        Use -1 for full-size batches\n\n    Yields\n    -------\n    ret_x : {numpy.array, scipy.sparse.csr_matrix}, shape (batch_size, n_features)\n        Same type as input\n\n    ret_y : np.array or None, shape (batch_size,)\n\n    ret_w : np.array or None, shape (batch_size,)\n    \"\"\"\n    n_samples = X_.shape[0]\n\n    if batch_size == -1:\n        batch_size = n_samples\n    if batch_size < 1:\n       raise ValueError('Parameter batch_size={} is unsupported'.format(batch_size))\n\n    for i in range(0, n_samples, batch_size):\n        upper_bound = min(i + batch_size, n_samples)\n        ret_x = X_[i:upper_bound]\n        ret_y = None\n        ret_w = None\n        if y_ is not None:\n            ret_y = y_[i:i + batch_size]\n        if w_ is not None:\n            ret_w = w_[i:i + batch_size]\n        yield (ret_x, ret_y, ret_w)\n\n\ndef batch_to_feeddict(X, y, w, core):\n    \"\"\"Prepare feed dict for session.run() from mini-batch.\n    Convert sparse format into tuple (indices, values, shape) for tf.SparseTensor\n    Parameters\n    ----------\n    X : {numpy.array, scipy.sparse.csr_matrix}, shape (batch_size, n_features)\n        Training vector, where batch_size in the number of samples and\n        n_features is the number of features.\n    y : np.array, shape (batch_size,)\n        Target vector relative to X.\n    core : TFFMCore\n        Core used for extract appropriate placeholders\n    Returns\n    -------\n    fd : dict\n        Dict with formatted placeholders\n    \"\"\"\n    fd = {}\n    if core.input_type == 'dense':\n        fd[core.train_x] = X.astype(np.float32)\n    else:\n        # sparse case\n        X_sparse = X.tocoo()\n        fd[core.raw_indices] = np.hstack(\n            (X_sparse.row[:, np.newaxis], X_sparse.col[:, np.newaxis])\n        ).astype(np.int64)\n        fd[core.raw_values] = X_sparse.data.astype(np.float32)\n        fd[core.raw_shape] = np.array(X_sparse.shape).astype(np.int64)\n\n    if y is not None:\n        fd[core.train_y] = y.astype(np.float32)\n    if w is not None:\n        fd[core.train_w] = w.astype(np.float32)\n\n    return fd\n\n\nclass TFFMBaseModel(six.with_metaclass(ABCMeta, BaseEstimator)):\n    \"\"\"Base class for FM.\n    This class implements L2-regularized arbitrary order FM model.\n\n    It supports arbitrary order of interactions and has linear complexity in the\n    number of features (a generalization of the approach described in Lemma 3.1\n    in the referenced paper, details will be added soon).\n\n    It can handle both dense and sparse input. Only numpy.array and CSR matrix are\n    allowed as inputs; any other input format should be explicitly converted.\n\n    Support logging/visualization with TensorBoard.\n\n\n    Parameters (for initialization)\n    ----------\n    batch_size : int, default: -1\n        Number of samples in mini-batches. Shuffled every epoch.\n        Use -1 for full gradient (whole training set in each batch).\n\n    n_epoch : int, default: 100\n        Default number of epoches.\n        It can be overrived by explicitly provided value in fit() method.\n\n    log_dir : str or None, default: None\n        Path for storing model stats during training. Used only if is not None.\n        WARNING: If such directory already exists, it will be removed!\n        You can use TensorBoard to visualize the stats:\n        `tensorboard --logdir={log_dir}`\n\n    session_config : tf.ConfigProto or None, default: None\n        Additional setting passed to tf.Session object.\n        Useful for CPU/GPU switching, setting number of threads and so on,\n        `tf.ConfigProto(device_count = {'GPU': 0})` will disable GPU (if enabled)\n\n    verbose : int, default: 0\n        Level of verbosity.\n        Set 1 for tensorboard info only and 2 for additional stats every epoch.\n\n    kwargs : dict, default: {}\n        Arguments for TFFMCore constructor.\n        See TFFMCore's doc for details.\n\n    Attributes\n    ----------\n    core : TFFMCore or None\n        Computational graph with internal utils.\n        Will be initialized during first call .fit()\n\n    session : tf.Session or None\n        Current execution session or None.\n        Should be explicitly terminated via calling destroy() method.\n\n    steps : int\n        Counter of passed lerning epochs, used as step number for writing stats\n\n    n_features : int\n        Number of features used in this dataset.\n        Inferred during the first call of fit() method.\n\n    intercept : float, shape: [1]\n        Intercept (bias) term.\n\n    weights : array of np.array, shape: [order]\n        Array of underlying representations.\n        First element will have shape [n_features, 1],\n        all the others -- [n_features, rank].\n\n    Notes\n    -----\n    You should explicitly call destroy() method to release resources.\n    See TFFMCore's doc for details.\n    \"\"\"\n\n\n    def init_basemodel(self, n_epochs=100, batch_size=-1,\n                       log_dir=None,session_config=None,\n                       verbose=0, seed=None,sample_weight=None,\n                       pos_class_weight=None,**core_arguments):\n        core_arguments['seed'] = seed\n        self.core = TFFMCore(**core_arguments)\n        self.batch_size = batch_size\n        self.n_epochs = n_epochs\n        self.need_logs = log_dir is not None\n        self.log_dir = log_dir\n        self.session_config = session_config\n        self.verbose = verbose\n        self.steps = 0\n        self.seed = seed\n        self.sample_weight = sample_weight\n        self.pos_class_weight = pos_class_weight\n\n    def initialize_session(self):\n        \"\"\"Start computational session on builded graph.\n        Initialize summary logger (if needed).\n        \"\"\"\n        if self.core.graph is None:\n            raise 'Graph not found. Try call .core.build_graph() before .initialize_session()'\n        if self.need_logs:\n            self.summary_writer = tf.summary.FileWriter(self.log_dir, self.core.graph)\n            if self.verbose > 0:\n                full_log_path = os.path.abspath(self.log_dir)\n                print('Initialize logs, use: \\ntensorboard --logdir={}'.format(full_log_path))\n        self.session = tf.Session(config=self.session_config, graph=self.core.graph)\n        self.session.run(self.core.init_all_vars)\n\n\n    def _fit(self, X_, y_, w_, n_epochs=None, show_progress=False):\n        if self.core.n_features is None:\n            self.core.set_num_features(X_.shape[1])\n\n        assert self.core.n_features==X_.shape[1], 'Different num of features in initialized graph and input'\n\n        if self.core.graph is None:\n            self.core.build_graph()\n            self.initialize_session()\n\n        if n_epochs is None:\n            n_epochs = self.n_epochs\n\n        # For reproducible results\n        if self.seed:\n            np.random.seed(self.seed)\n        \n        # Training cycle\n        for epoch in tqdm(range(n_epochs), unit='epoch', disable=(not show_progress)):\n            # generate permutation\n            perm = np.random.permutation(X_.shape[0])\n            epoch_loss = []\n            # iterate over batches\n            for bX, bY, bW in batcher(X_[perm], y_=y_[perm], w_=w_[perm], batch_size=self.batch_size):\n                fd = batch_to_feeddict(bX, bY, bW, core=self.core)\n                ops_to_run = [self.core.trainer, self.core.target, self.core.summary_op]\n                result = self.session.run(ops_to_run, feed_dict=fd)\n                _, batch_target_value, summary_str = result\n                epoch_loss.append(batch_target_value)\n                # write stats \n                if self.need_logs:\n                    self.summary_writer.add_summary(summary_str, self.steps)\n                    self.summary_writer.flush()\n                self.steps += 1\n            if self.verbose > 1:\n                    print('[epoch {}]: mean target value: {}'.format(epoch, np.mean(epoch_loss)))\n\n    def decision_function(self, X, pred_batch_size=None):\n        if self.core.graph is None:\n            raise sklearn.exceptions.NotFittedError(\"Call fit before prediction\")\n        output = []\n        if pred_batch_size is None:\n            pred_batch_size = self.batch_size\n\n        for bX, bY, bW in batcher(X, y_=None, w_=None, batch_size=pred_batch_size):\n            fd = batch_to_feeddict(bX, bY, bW, core=self.core)\n            output.append(self.session.run(self.core.outputs, feed_dict=fd))\n        distances = np.concatenate(output).reshape(-1)\n        # WARNING: be careful with this reshape in case of multiclass\n        return distances\n\n    @abstractmethod\n    def predict(self, X, pred_batch_size=None):\n        \"\"\"Predict target values for X.\"\"\"\n\n    @property\n    def intercept(self):\n        \"\"\"Export bias term from tf.Variable to float.\"\"\"\n        return self.core.b.eval(session=self.session)\n\n    @property\n    def weights(self):\n        \"\"\"Export underlying weights from tf.Variables to np.arrays.\"\"\"\n        return [x.eval(session=self.session) for x in self.core.w]\n\n    def save_state(self, path):\n        self.core.saver.save(self.session, path)\n\n    def load_state(self, path):\n        if self.core.graph is None:\n            self.core.build_graph()\n            self.initialize_session()\n        self.core.saver.restore(self.session, path)\n\n    def destroy(self):\n        \"\"\"Terminates session and destroyes graph.\"\"\"\n        self.session.close()\n        self.core.graph = None\n", "framework": "tensorflow"}
{"repo_name": "GoogleCloudPlatform/professional-services", "file_path": "tools/agile-machine-learning-api/tests/tests_input_dask.py", "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nUnit tests for input_pipeline_dask\n\"\"\"\nimport sys\nimport os\nsys.path.append(os.path.abspath('codes/'))\nfrom shutil import copyfile\nimport unittest\nfrom trainer import input_pipeline_dask as test\nimport dask\nimport pandas as pd\nimport tensorflow as tf\n\nCSV_PATH = os.path.abspath('data/iris_formatted.csv')\nTASK_TYPE = 'classification'\nTARGET_VAR = 'label'\nCOLUMN_TO_DROP = ['Cluster_indices']\nTASK_NAME = 'classification'\nNUM_EPOCHS = 1\nBATCH_SIZE = 4\nBUFFER_SIZE = 4\nNAME = 'train'\nMODE = 23\nCOLUMN_NAMES = os.path.abspath('data/iris_names.txt')\nNA_VALUES = None\nDATA_TYPE = {'id': 'int',\n             'a': 'float',\n             'b': 'float',\n             'c': 'float',\n             'd': 'float',\n             'label': 'int',\n             'Cluster_indices': 'int'}\n\n\nclass BasicInput(unittest.TestCase):\n    \"\"\"Class which will perform unittests\"\"\"\n\n    def is_not_used(self):\n        \"\"\"Function to remove warning\"\"\"\n        pass\n\n    def init_inputreader(self):\n        \"\"\"\n        Initialise class InputReader\n        \"\"\"\n        self.is_not_used()\n        return test.InputReader(\n            csv_path=CSV_PATH,\n            task_type=TASK_TYPE,\n            target_var=TARGET_VAR,\n            gcs_path=False,\n            na_values=NA_VALUES)\n\n    def init_basicstats(self):\n        \"\"\"\n        Initialise class BasicStats\n        \"\"\"\n        self.is_not_used()\n        return test.BasicStats()\n\n    def init_dataset(self):\n        \"\"\"\n        Initialise class Dataset\n        \"\"\"\n        iread = self.init_inputreader()\n        stats = self.init_basicstats()\n        ddf, _ = iread._parse_csv()\n        copyfile(CSV_PATH, '/tmp/data.csv')\n        ddf, _, _, csv_defaults = stats.clean_data(\n            df=ddf,\n            target_var=TARGET_VAR,\n            task_type=TASK_TYPE,\n            name=TASK_NAME\n        )\n        return test.DatasetInput(\n            num_epochs=NUM_EPOCHS,\n            batch_size=BATCH_SIZE,\n            buffer_size=BUFFER_SIZE,\n            csv_defaults=csv_defaults,\n            csv_cols=ddf.columns,\n            target_var=TARGET_VAR,\n            task_type=TASK_TYPE\n        )\n\n    def test__parse_csv(self):\n        \"\"\"\n        Testing function parse_csv\n        \"\"\"\n        iread = self.init_inputreader()\n        data = iread._parse_csv()\n        self.assertEqual(type(data[0]), dask.dataframe.core.DataFrame)\n        self.assertEqual(len(data[0]), 150)\n        self.assertIsInstance(data[1], list)\n        self.assertListEqual(\n            data[1], ['id', 'a', 'b', 'c', 'd', 'label', 'Cluster_indices'])\n\n    def test_drop_cols(self):\n        \"\"\"\n        Testing function drop_cols\n        \"\"\"\n        iread = self.init_inputreader()\n        ddf, _ = iread._parse_csv()\n        data = iread.drop_cols(\n            df=ddf,\n            col_names=COLUMN_TO_DROP\n        )\n        self.assertFalse(set(data.columns) <= set(COLUMN_TO_DROP))\n\n    def test_dropping_zero_var_cols(self):\n        \"\"\"\n        Testing function drop_cols\n        \"\"\"\n        copyfile(CSV_PATH, '/tmp/data.csv')\n        iread = self.init_inputreader()\n        stats = self.init_basicstats()\n        ddf, _ = iread._parse_csv()\n        ddf, _, std_dev, _ = stats.clean_data(\n            df=ddf,\n            task_type=TASK_TYPE,\n            target_var=TARGET_VAR,\n            name=TASK_NAME\n        )\n        stats.dropping_zero_var_cols(\n            df=ddf,\n            target_var=TARGET_VAR,\n            stddev_list=std_dev\n        )\n        std_list = ddf.std(axis=0, skipna=True)\n        std = dask.compute(std_list)\n        for i in range(len(std[0])):\n            self.assertNotEqual(std[0][i], 0)\n\n    def test_clean_data(self):\n        \"\"\"\n        Testing function clean_csv\n        \"\"\"\n        copyfile(CSV_PATH, '/tmp/data.csv')\n        iread = self.init_inputreader()\n        stats = self.init_basicstats()\n        ddf, _ = iread._parse_csv()\n        data, mean, std_dev, csv_defaults = stats.clean_data(\n            df=ddf,\n            task_type=TASK_TYPE,\n            target_var=TARGET_VAR,\n            name=NAME\n        )\n\n        self_computed_mean = dask.compute(ddf.mean())\n        self.assertListEqual(list(mean), list(self_computed_mean[0]))\n        self_computed_std_dev = dask.compute(ddf.std(axis=0, skipna=True))\n        self.assertListEqual(list(std_dev), list(self_computed_std_dev[0]))\n        self.assertIsInstance(data, dask.dataframe.core.DataFrame)\n        self.assertIsInstance(mean, pd.core.series.Series)\n        self.assertIsInstance(std_dev, pd.core.series.Series)\n        self.assertIsInstance(csv_defaults, list)\n\n    def test_creating_lime_explainer(self):\n        copyfile(CSV_PATH, '/tmp/data.csv')\n        iread = self.init_inputreader()\n        stats = self.init_basicstats()\n        ddf, _ = iread._parse_csv()\n        stats.creating_explainer_lime(\n            df=ddf,\n            target_var=TARGET_VAR\n        )\n        self.assertTrue(os.path.isfile('/tmp/lime_explainer'))\n\n    def test_normalize(self):\n        \"\"\"\n        Testing function normalise\n        \"\"\"\n        iread = self.init_inputreader()\n        stats = self.init_basicstats()\n        ddf, _ = iread._parse_csv()\n        copyfile(CSV_PATH, '/tmp/data.csv')\n        ddf, mean, std_dev, _ = stats.clean_data(\n            df=ddf,\n            task_type=TASK_TYPE,\n            target_var=TARGET_VAR,\n            name=NAME)\n        data = stats.normalize(\n            df=ddf,\n            target_var=TARGET_VAR,\n            mean_list=mean,\n            stddev_list=std_dev\n        )\n        dataframe = dask.compute(data)[0]\n        rows = dataframe.columns\n        for row in rows:\n            if row != 'label':\n                col = dask.compute(data)[0][row]\n                self.assertAlmostEqual(col.std(), 1)\n        self.assertIsInstance(data, dask.dataframe.core.DataFrame)\n\n    def test_calculate_stats(self):\n        \"\"\"\n        Testing function calculate_stats\n        \"\"\"\n        iread = self.init_inputreader()\n        stats = self.init_basicstats()\n        ddf, _ = iread._parse_csv()\n        mean, median, mode_dict, std_dev = stats.calculate_stats(\n            df=ddf,\n            target_var=TARGET_VAR\n        )\n        self_computed_mean = dask.compute(ddf.mean())\n        self.assertListEqual(list(mean), list(self_computed_mean[0]))\n        self_computed_std_dev = dask.compute(ddf.std(axis=0, skipna=True))\n        self.assertListEqual(list(std_dev), list(self_computed_std_dev[0]))\n        self_computed_median = dask.compute(ddf.quantile(0.5))\n        self.assertListEqual(list(median), list(self_computed_median[0]))\n        self.assertIsInstance(mean, pd.core.series.Series)\n        self.assertIsInstance(std_dev, pd.core.series.Series)\n        self.assertIsInstance(median, pd.core.series.Series)\n        self.assertIsInstance(mode_dict, dict)\n\n    def test_impute(self):\n        \"\"\"\n        Testing function impute\n        \"\"\"\n        iread = self.init_inputreader()\n        stats = self.init_basicstats()\n        ddf, _ = iread._parse_csv()\n        _, median, _, _ = stats.calculate_stats(\n            df=ddf,\n            target_var=TARGET_VAR\n        )\n        data = stats.impute(\n            df=ddf,\n            target_var=TARGET_VAR,\n            median=median,\n            mode=MODE\n        )\n        imputed_data = dask.compute(data.isnull().sum())\n        rows = ddf.columns\n        for row in rows:\n            col = imputed_data[0][row]\n            self.assertEqual(col, 0)\n        self.assertIsInstance(data, dask.dataframe.core.DataFrame)\n\n    def test_find_vocab(self):\n        \"\"\"\n        Testing function find_vocab\n        \"\"\"\n        iread = self.init_inputreader()\n        stats = self.init_basicstats()\n        ddf, _ = iread._parse_csv()\n        col_mapping = stats.find_vocab(\n            df=ddf\n        )\n        test_col_mapping = {'a': 0, 'c': 0, 'b': 0, 'd': 0,\n                            'label': 0, 'Cluster_indices': 0, 'id': 0}\n        self.assertIsInstance(col_mapping, dict)\n        self.assertDictEqual(col_mapping, test_col_mapping)\n\n    def test_kmeans_input_fn(self):\n        \"\"\"\n        Testing function kmeans_input_fn\n        \"\"\"\n        copyfile(CSV_PATH, '/tmp/clean_1_train.csv')\n        data = self.init_dataset()\n        inp_fn = data.kmeans_input_fn(\n            name=NAME\n        )\n        self.assertIsInstance(inp_fn, tf.Tensor)\n\n    def test_creating_feature_cols(self):\n        \"\"\"\n        Testing function of _create_feature_columns function\n        \"\"\"\n        iread = self.init_inputreader()\n        stats = self.init_basicstats()\n        data = self.init_dataset()\n        ddf, _ = iread._parse_csv()\n        mean, _, _, std_dev = stats.calculate_stats(\n            df=ddf,\n            target_var=TARGET_VAR\n        )\n        vocab = stats.find_vocab(ddf)\n        feat_cols = data._create_feature_columns(\n            dictionary=vocab,\n            mean=mean,\n            std_dev=std_dev\n        )\n        test_vocab = {'a': 0, 'c': 0, 'b': 0, 'd': 0,\n                      'label': 0, 'Cluster_indices': 0, 'id': 0}\n        self.assertIsInstance(vocab, dict)\n        self.assertIsInstance(feat_cols, list)\n        self.assertDictEqual(vocab, test_vocab)\n\n    def test_input_fn(self):\n        \"\"\"\n        Testing function input_csv\n        \"\"\"\n        data = self.init_dataset()\n        inp_fn = data.input_fn(\n            name=NAME\n        )\n        for _, v in inp_fn[0].items():\n            self.assertIsInstance(v, tf.Tensor)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "framework": "tensorflow"}
{"repo_name": "google/trax", "file_path": "trax/rl/actor_critic.py", "content": "# coding=utf-8\n# Copyright 2022 The Trax Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Classes for RL training in Trax.\"\"\"\n\nimport functools\nimport os\n\nimport gin\nimport gym\nimport numpy as np\nimport tensorflow as tf\n\nfrom trax import data\nfrom trax import fastmath\nfrom trax import layers as tl\nfrom trax import shapes\nfrom trax import supervised\nfrom trax.fastmath import numpy as jnp\nfrom trax.optimizers import adam\nfrom trax.rl import advantages as rl_advantages\nfrom trax.rl import distributions\nfrom trax.rl import policy_tasks\nfrom trax.rl import training as rl_training\nfrom trax.rl import value_tasks\nfrom trax.supervised import lr_schedules as lr\n\n\nclass ActorCriticAgent(rl_training.PolicyAgent):\n  \"\"\"Trains policy and value models using actor-critic methods.\n\n  Attrs:\n    on_policy (bool): Whether the algorithm is on-policy. Used in the data\n      generators. Should be set in derived classes.\n  \"\"\"\n\n  on_policy = None\n\n  def __init__(self, task,\n               value_model=None,\n               value_optimizer=None,\n               value_lr_schedule=lr.multifactor,\n               value_batch_size=64,\n               value_train_steps_per_epoch=500,\n               value_evals_per_epoch=1,\n               value_eval_steps=1,\n               n_shared_layers=0,\n               added_policy_slice_length=0,\n               n_replay_epochs=1,\n               scale_value_targets=False,\n               q_value=False,\n               q_value_aggregate='logsumexp',\n               q_value_temperature=1.0,\n               q_value_n_samples=1,\n               q_value_normalization=False,\n               offline=False,\n               **kwargs):  # Arguments of PolicyAgent come here.\n    \"\"\"Configures the actor-critic trainer.\n\n    Args:\n      task: `RLTask` instance to use.\n      value_model: Model to use for the value function.\n      value_optimizer: Optimizer to train the value model.\n      value_lr_schedule: lr schedule for value model training.\n      value_batch_size: Batch size for value model training.\n      value_train_steps_per_epoch: Number of steps are we using to train the\n          value model in each epoch.\n      value_evals_per_epoch: Number of value trainer evaluations per RL epoch.\n          Every evaluation, we also synchronize the weights of the target\n          network.\n      value_eval_steps: Number of value trainer steps per evaluation; only\n          affects metric reporting.\n      n_shared_layers: Number of layers to share between value and policy\n          models.\n      added_policy_slice_length: How much longer should slices of\n          trajectories be for policy than for value training; this\n          is useful for TD calculations and only affect the length\n          of elements produced for policy batches; value batches\n          have maximum length set by `max_slice_length` in `**kwargs`.\n      n_replay_epochs: Number of last epochs to take into the replay buffer;\n          only makes sense for off-policy algorithms.\n      scale_value_targets: If `True`, scale value function targets by\n          `1 / (1 - gamma)`.\n      q_value: If `True`, use Q-values as baselines.\n      q_value_aggregate: How to aggregate Q-values. Options: 'mean', 'max',\n        'softmax', 'logsumexp'.\n      q_value_temperature: Temperature parameter for the 'softmax' and\n        'logsumexp' aggregation methods.\n      q_value_n_samples: Number of samples to average over when calculating\n          baselines based on Q-values.\n      q_value_normalization: How to normalize Q-values before aggregation.\n          Allowed values: 'std', 'abs', `None`. If `None`, don't normalize.\n      offline: Whether to train in offline mode. This matters for some\n        algorithms, e.g. QWR.\n      **kwargs: Arguments for `PolicyAgent` superclass.\n    \"\"\"\n    self._n_shared_layers = n_shared_layers\n    self._value_batch_size = value_batch_size\n    self._value_train_steps_per_epoch = value_train_steps_per_epoch\n    self._value_evals_per_epoch = value_evals_per_epoch\n    self._value_eval_steps = value_eval_steps\n\n    # The 2 below will be initalized in super.__init__ anyway, but are needed\n    # to construct value batches which are needed before PolicyAgent init\n    # since policy input creation calls the value model -- hence this code.\n    self._task = task\n    self._max_slice_length = kwargs.get('max_slice_length', 1)\n    self._added_policy_slice_length = added_policy_slice_length\n    self._n_replay_epochs = n_replay_epochs\n    task.set_n_replay_epochs(n_replay_epochs)\n\n    if scale_value_targets:\n      self._value_network_scale = 1 / (1 - self._task.gamma)\n    else:\n      self._value_network_scale = 1\n\n    self._q_value = q_value\n    self._q_value_aggregate = q_value_aggregate\n    self._q_value_temperature = q_value_temperature\n    self._q_value_n_samples = q_value_n_samples\n    self._q_value_normalization = q_value_normalization\n\n    is_discrete = isinstance(self._task.action_space, gym.spaces.Discrete)\n    self._is_discrete = is_discrete\n    self._vocab_size = None\n    self._sample_all_discrete_actions = False\n    if q_value and is_discrete:\n      self._vocab_size = self.task.action_space.n\n      # TODO(lukaszkaiser): the code below is specific to AWR, move it.\n      # If n_samples = n_actions, we'll take them all in actor and reweight.\n      if self._q_value_n_samples == self._vocab_size:\n        # TODO(lukaszkaiser): set this explicitly once it's in AWR Trainer.\n        self._sample_all_discrete_actions = True\n    if offline and is_discrete:\n      raise NotImplementedError(\n          'Offline training is only supported for continuous action spaces for '\n          'now.'\n      )\n    self._offline = offline\n\n    if q_value:\n      value_model = functools.partial(value_model,\n                                      inject_actions=True,\n                                      is_discrete=is_discrete,\n                                      vocab_size=self._vocab_size)\n    self._value_eval_model = value_model(mode='eval')\n    self._value_eval_model.init(self._value_model_signature)\n    self._value_eval_jit = tl.jit_forward(\n        self._value_eval_model.pure_fn, fastmath.local_device_count(),\n        do_mean=False)\n\n    # Initialize policy training.\n    super().__init__(task, **kwargs)\n\n    # Initialize training of the value function.\n    value_output_dir = kwargs.get('output_dir', None)\n    if value_output_dir is not None:\n      value_output_dir = os.path.join(value_output_dir, 'value')\n      # If needed, create value_output_dir and missing parent directories.\n      if not tf.io.gfile.isdir(value_output_dir):\n        tf.io.gfile.makedirs(value_output_dir)\n    self._value_inputs = data.inputs.Inputs(\n        train_stream=lambda _: self.value_batches_stream())\n    self._value_trainer = supervised.Trainer(\n        model=value_model,\n        optimizer=value_optimizer,\n        lr_schedule=value_lr_schedule(),\n        loss_fn=tl.L2Loss(),\n        inputs=self._value_inputs,\n        output_dir=value_output_dir,\n        metrics={'value_loss': tl.L2Loss(),\n                 'value_mean': self.value_mean})\n\n  @property\n  def value_mean(self):\n    \"\"\"The mean value of the value function.\"\"\"\n    # TODO(henrykm): A better solution would take into account the masks\n    def f(values):\n      return jnp.mean(values)\n    return tl.Fn('ValueMean', f)\n\n  @property\n  def _value_model_signature(self):\n    obs_sig = shapes.signature(self._task.observation_space)\n    target_sig = mask_sig = shapes.ShapeDtype(\n        shape=(1, 1, 1),\n    )\n    inputs_sig = (obs_sig.replace(shape=(1, 1) + obs_sig.shape),)\n    if self._q_value:\n      act_sig = shapes.signature(self._task.action_space)\n      inputs_sig += (act_sig.replace(shape=(1, 1) + act_sig.shape),)\n    return (*inputs_sig, target_sig, mask_sig)\n\n  @property\n  def _replay_epochs(self):\n    if self.on_policy:\n      assert self._n_replay_epochs == 1, (\n          'Non-unit replay buffer size only makes sense for off-policy '\n          'algorithms.'\n      )\n    return [-(ep + 1) for ep in range(self._n_replay_epochs)]\n\n  def _run_value_model(self, observations, dist_inputs):\n    if dist_inputs is None:\n      dist_inputs = jnp.zeros(\n          observations.shape[:2] + (self._policy_dist.n_inputs,)\n      )\n\n    actions = None\n    if self._q_value:\n      if self._sample_all_discrete_actions:\n        # Since we want to sample all actions, start by creating their list.\n        act = np.arange(self._vocab_size)\n        # Now act is a vector [0, ..., vocab_size-1], but we'll need to tile it.\n        # Add extra dimenstions so it's the same dimensionality as dist_inputs.\n        act = jnp.reshape(act, [-1] + [1] * (len(dist_inputs.shape) - 1))\n        # Now act is [vocab_size, 1, ..., 1], dimensionality of dist_inputs.\n      dist_inputs = jnp.broadcast_to(\n          dist_inputs, (self._q_value_n_samples,) + dist_inputs.shape)\n      if self._sample_all_discrete_actions:\n        actions = act + jnp.zeros(dist_inputs.shape[:-1], dtype=jnp.int32)\n        actions = jnp.swapaxes(actions, 0, 1)\n      # Swapping the n_samples and batch_size axes, so the input is split\n      # between accelerators along the batch_size axis.\n      dist_inputs = jnp.swapaxes(dist_inputs, 0, 1)\n      if not self._sample_all_discrete_actions:\n        actions = self._policy_dist.sample(dist_inputs)\n      log_probs = self._policy_dist.log_prob(dist_inputs, actions)\n      obs = observations\n      obs = jnp.reshape(obs, [obs.shape[0], 1] + list(obs.shape[1:]))\n      inputs = (obs, actions)\n    else:\n      log_probs = None\n      inputs = (observations,)\n\n    n_devices = fastmath.local_device_count()\n    weights = tl.for_n_devices(self._value_eval_model.weights, n_devices)\n    state = tl.for_n_devices(self._value_eval_model.state, n_devices)\n    rng = self._value_eval_model.rng\n    values, _ = self._value_eval_jit(inputs, weights, state, rng)\n    values *= self._value_network_scale\n    values = jnp.squeeze(values, axis=-1)  # Remove the singleton depth dim.\n    return (values, actions, log_probs)\n\n  def _aggregate_values(self, values, aggregate, act_log_probs):\n    # Normalize the Q-values before aggragetion, so it can adapt to the scale\n    # of the returns. This does not affect mean and max aggregation.\n    scale = 1\n    epsilon = 1e-5\n    if self._q_value_normalization == 'std':\n      scale = jnp.std(values) + epsilon\n    elif self._q_value_normalization == 'abs':\n      scale = jnp.mean(jnp.abs(values - jnp.mean(values))) + epsilon\n    values /= scale\n\n    temp = self._q_value_temperature\n    if self._q_value:\n      assert values.shape[:2] == (\n          self._value_batch_size, self._q_value_n_samples\n      )\n      if aggregate == 'max':\n        # max_a Q(s, a)\n        values = jnp.max(values, axis=1)\n      elif aggregate == 'softmax':\n        # sum_a (Q(s, a) * w(s, a))\n        # where w(s, .) = softmax (Q(s, .) / T)\n        weights = tl.Softmax(axis=1)(values / temp)\n        values = jnp.sum(values * weights, axis=1)\n      elif aggregate == 'logsumexp':\n        # log(mean_a exp(Q(s, a) / T)) * T\n        n = values.shape[1]\n        values = (fastmath.logsumexp(values / temp, axis=1) - jnp.log(n)) * temp\n      else:\n        assert aggregate == 'mean'\n        # mean_a Q(s, a)\n        if self._sample_all_discrete_actions:\n          values = jnp.sum(values * jnp.exp(act_log_probs), axis=1)\n        else:\n          values = jnp.mean(values, axis=1)\n\n    # Re-scale the Q-values after aggregation.\n    values *= scale\n    return np.array(values)  # Move the values to CPU.\n\n  def _get_dist_inputs(self, trajectory):\n    if not self._offline:\n      return trajectory.dist_inputs\n    else:\n      return trajectory.action\n\n  def value_batches_stream(self):\n    \"\"\"Use the RLTask self._task to create inputs to the value model.\"\"\"\n    max_slice_length = self._max_slice_length + self._added_policy_slice_length\n    for np_trajectory in self._task.trajectory_batch_stream(\n        self._value_batch_size,\n        max_slice_length=max_slice_length,\n        min_slice_length=(1 + self._added_policy_slice_length),\n        margin=self._added_policy_slice_length,\n        epochs=self._replay_epochs,\n    ):\n      dist_inputs = self._get_dist_inputs(np_trajectory)\n      (values, _, act_log_probs) = self._run_value_model(\n          np_trajectory.observation, dist_inputs\n      )\n      values = self._aggregate_values(\n          values, self._q_value_aggregate, act_log_probs)\n\n      # TODO(pkozakowski): Add some shape assertions and docs.\n      # Calculate targets based on the advantages over the target network - this\n      # allows TD learning for value networks.\n      advantages = self._advantage_estimator(\n          rewards=np_trajectory.reward,\n          returns=np_trajectory.return_,\n          values=values,\n          dones=np_trajectory.done,\n          discount_mask=np_trajectory.env_info.discount_mask,\n      )\n      length = advantages.shape[1]\n      values = values[:, :length]\n      target_returns = values + advantages\n\n      inputs = (np_trajectory.observation[:, :length],)\n      if self._q_value:\n        inputs += (np_trajectory.action[:, :length],)\n\n      # Insert an extra depth dimension, so the target shape is consistent with\n      # the network output shape.\n      yield (\n          # Inputs: observations and maybe actions.\n          *inputs,\n          # Targets: computed returns.\n          target_returns[:, :, None] / self._value_network_scale,\n          # Mask to zero-out padding.\n          np_trajectory.mask[:, :length, None],\n      )\n\n  def policy_inputs(self, trajectory, values):\n    \"\"\"Create inputs to policy model from a TimeStepBatch and values.\n\n    Args:\n      trajectory: a TimeStepBatch, the trajectory to create inputs from\n      values: a numpy array: value function computed on trajectory\n\n    Returns:\n      a tuple of numpy arrays of the form (inputs, x1, x2, ...) that will be\n      passed to the policy model; policy model will compute outputs from\n      inputs and (outputs, x1, x2, ...) will be passed to self.policy_loss\n      which should be overridden accordingly.\n    \"\"\"\n    return NotImplementedError\n\n  def policy_batches_stream(self):\n    \"\"\"Use the RLTask self._task to create inputs to the policy model.\"\"\"\n    # Maximum slice length for policy is max_slice_len + the added policy len.\n    max_slice_length = self._max_slice_length + self._added_policy_slice_length\n    for np_trajectory in self._task.trajectory_batch_stream(\n        self._policy_batch_size,\n        epochs=self._replay_epochs,\n        max_slice_length=max_slice_length,\n        margin=self._added_policy_slice_length,\n    ):\n      dist_inputs = self._get_dist_inputs(np_trajectory)\n      (values, _, act_log_probs) = self._run_value_model(\n          np_trajectory.observation, dist_inputs)\n      values = self._aggregate_values(values, 'mean', act_log_probs)\n      if len(values.shape) != 2:\n        raise ValueError('Values are expected to have shape ' +\n                         '[batch_size, length], got: %s' % str(values.shape))\n      if values.shape[0] != self._policy_batch_size:\n        raise ValueError('Values first dimension should = policy batch size, ' +\n                         '%d != %d' %(values.shape[0], self._policy_batch_size))\n      yield self.policy_inputs(np_trajectory, values)\n\n  def train_epoch(self):\n    \"\"\"Trains RL for one epoch.\"\"\"\n    # Copy policy state accumulated during data collection to the trainer.\n    self._policy_trainer.model_state = self._policy_collect_model.state\n\n    # Copy policy weights and state to value trainer.\n    if self._n_shared_layers > 0:\n      _copy_model_weights_and_state(\n          0, self._n_shared_layers, self._policy_trainer, self._value_trainer\n      )\n\n    # Update the target value network.\n    self._value_eval_model.weights = self._value_trainer.model_weights\n    self._value_eval_model.state = self._value_trainer.model_state\n\n    n_value_evals = rl_training.remaining_evals(\n        self._value_trainer.step,\n        self._epoch,\n        self._value_train_steps_per_epoch,\n        self._value_evals_per_epoch)\n    for _ in range(n_value_evals):\n      self._value_trainer.train_epoch(\n          self._value_train_steps_per_epoch // self._value_evals_per_epoch,\n          self._value_eval_steps,\n      )\n      # Update the target value network.\n      self._value_eval_model.weights = self._value_trainer.model_weights\n      self._value_eval_model.state = self._value_trainer.model_state\n\n    # Copy value weights and state to policy trainer.\n    if self._n_shared_layers > 0:\n      _copy_model_weights_and_state(\n          0, self._n_shared_layers, self._value_trainer, self._policy_trainer\n      )\n    n_policy_evals = rl_training.remaining_evals(\n        self._policy_trainer.step,\n        self._epoch,\n        self._policy_train_steps_per_epoch,\n        self._policy_evals_per_epoch)\n    # Check if there was a restart after value training finishes and policy not.\n    stopped_after_value = (n_value_evals == 0 and\n                           n_policy_evals < self._policy_evals_per_epoch)\n    should_copy_weights = self._n_shared_layers > 0 and not stopped_after_value\n    if should_copy_weights:\n      _copy_model_weights_and_state(\n          0, self._n_shared_layers, self._value_trainer, self._policy_trainer\n      )\n\n    # Update the target value network.\n    self._value_eval_model.weights = self._value_trainer.model_weights\n    self._value_eval_model.state = self._value_trainer.model_state\n\n    for _ in range(n_policy_evals):\n      self._policy_trainer.train_epoch(\n          self._policy_train_steps_per_epoch // self._policy_evals_per_epoch,\n          self._policy_eval_steps,\n      )\n\n  def close(self):\n    self._value_trainer.close()\n    super().close()\n\n\ndef _copy_model_weights_and_state(  # pylint: disable=invalid-name\n    start, end, from_trainer, to_trainer, copy_optimizer_slots=False\n):\n  \"\"\"Copy model weights[start:end] from from_trainer to to_trainer.\"\"\"\n  from_weights = from_trainer.model_weights\n  to_weights = list(to_trainer.model_weights)\n  shared_weights = from_weights[start:end]\n  to_weights[start:end] = shared_weights\n  to_trainer.model_weights = to_weights\n\n  from_state = from_trainer.model_state\n  to_state = list(to_trainer.model_state)\n  shared_state = from_state[start:end]\n  to_state[start:end] = shared_state\n  to_trainer.model_state = to_state\n\n  if copy_optimizer_slots:\n    # TODO(lukaszkaiser): make a nicer API in Trainer to support this.\n    # Currently we use the hack below. Note [0] since that's the model w/o loss.\n    # pylint: disable=protected-access\n    from_slots = from_trainer._opt_state.slots[0][start:end]\n    to_slots = to_trainer._opt_state.slots[0]\n    # The lines below do to_slots[start:end] = from_slots, but on tuples.\n    new_slots = to_slots[:start] + from_slots[start:end] + to_slots[end:]\n    new_slots = tuple([new_slots] + list(to_trainer._opt_state.slots[1:]))\n    to_trainer._opt_state = to_trainer._opt_state._replace(slots=new_slots)\n    # pylint: enable=protected-access\n\n\nclass AdvantageBasedActorCriticAgent(ActorCriticAgent):\n  \"\"\"Base class for advantage-based actor-critic algorithms.\"\"\"\n\n  def __init__(\n      self,\n      task,\n      advantage_estimator=rl_advantages.td_lambda,\n      advantage_normalization=True,\n      advantage_normalization_epsilon=1e-5,\n      advantage_normalization_factor=1.0,\n      added_policy_slice_length=0,\n      **kwargs\n  ):\n    self._advantage_estimator = advantage_estimator(\n        gamma=task.gamma, margin=added_policy_slice_length\n    )\n    self._advantage_normalization = advantage_normalization\n    self._advantage_normalization_epsilon = advantage_normalization_epsilon\n    self._advantage_normalization_factor = advantage_normalization_factor\n    super().__init__(\n        task, added_policy_slice_length=added_policy_slice_length, **kwargs\n    )\n\n  def policy_inputs(self, trajectory, values):\n    \"\"\"Create inputs to policy model from a TimeStepBatch and values.\"\"\"\n    # How much TD to use is determined by the added policy slice length,\n    # as the policy batches need to be this much longer to calculate TD.\n    advantages = self._advantage_estimator(\n        rewards=trajectory.reward,\n        returns=trajectory.return_,\n        values=values,\n        dones=trajectory.done,\n        discount_mask=trajectory.env_info.discount_mask,\n    )\n    # Observations should be the same length as advantages - so if we are\n    # using n_extra_steps, we need to trim the length to match.\n    obs = trajectory.observation[:, :advantages.shape[1]]\n    act = trajectory.action[:, :advantages.shape[1]]\n    mask = trajectory.mask[:, :advantages.shape[1]]  # Mask to zero-out padding.\n    if trajectory.dist_inputs is not None:\n      dist_inputs = self._get_dist_inputs(trajectory)\n      dist_inputs = dist_inputs[:, :advantages.shape[1]]\n    else:\n      dist_inputs = jnp.zeros(advantages.shape + (self._policy_dist.n_inputs,))\n    # Shape checks to help debugging.\n    if len(advantages.shape) != 2:\n      raise ValueError('Advantages are expected to have shape ' +\n                       '[batch_size, length], got: %s' % str(advantages.shape))\n    if act.shape[0:2] != advantages.shape:\n      raise ValueError('First 2 dimensions of actions should be the same as in '\n                       'advantages, %s != %s' % (act.shape[0:2],\n                                                 advantages.shape))\n    if obs.shape[0:2] != advantages.shape:\n      raise ValueError('First 2 dimensions of observations should be the same '\n                       'as in advantages, %s != %s' % (obs.shape[0:2],\n                                                       advantages.shape))\n    if dist_inputs.shape[:2] != advantages.shape:\n      raise ValueError('First 2 dimensions of dist_inputs should be the same '\n                       'as in advantages, %s != %s' % (dist_inputs.shape[:2],\n                                                       advantages.shape))\n    if mask.shape != advantages.shape:\n      raise ValueError('Mask and advantages shapes should be the same'\n                       ', %s != %s' % (mask.shape, advantages.shape))\n    return (obs, act, advantages, dist_inputs, mask)\n\n  @property\n  def policy_loss_given_log_probs(self):\n    \"\"\"Policy loss given action log-probabilities.\"\"\"\n    raise NotImplementedError\n\n  def _preprocess_advantages(self, advantages):\n    if self._advantage_normalization:\n      advantages = self._advantage_normalization_factor * (\n          (advantages - jnp.mean(advantages)) /\n          (jnp.std(advantages) + self._advantage_normalization_epsilon)\n      )\n    return advantages\n\n  @property\n  def policy_loss(self, **unused_kwargs):\n    \"\"\"Policy loss.\"\"\"\n    def LossInput(dist_inputs, actions, advantages, old_dist_inputs):  # pylint: disable=invalid-name\n      \"\"\"Calculates action log probabilities and normalizes advantages.\"\"\"\n      advantages = self._preprocess_advantages(advantages)\n      log_probs = self._policy_dist.log_prob(dist_inputs, actions)\n      old_log_probs = self._policy_dist.log_prob(old_dist_inputs, actions)\n      return (log_probs, advantages, old_log_probs)\n\n    return tl.Serial(\n        tl.Fn('LossInput', LossInput, n_out=3),\n        # Policy loss is expected to consume\n        # (log_probs, advantages, old_log_probs, mask).\n        self.policy_loss_given_log_probs,\n    )\n\n  @property\n  def policy_metrics(self):\n    metrics = super().policy_metrics\n    metrics.update({\n        'advantage_mean': self.advantage_mean,\n        'advantage_std': self.advantage_std,\n    })\n    return metrics\n\n  @property\n  def advantage_mean(self):\n    return tl.Serial([\n        # (dist_inputs, advantages, old_dist_inputs, mask)\n        tl.Select([1]),  # Select just the advantages.\n        tl.Fn('AdvantageMean', lambda x: jnp.mean(x)),  # pylint: disable=unnecessary-lambda\n    ])\n\n  @property\n  def advantage_std(self):\n    return tl.Serial([\n        # (dist_inputs, advantages, old_dist_inputs, mask)\n        tl.Select([1]),  # Select just the advantages.\n        tl.Fn('AdvantageStd', lambda x: jnp.std(x)),  # pylint: disable=unnecessary-lambda\n    ])\n\n\n# TODO(pkozakowski): Move to a better place.\n@gin.configurable(module='trax.rl')\ndef every(n_steps):\n  \"\"\"Returns True every n_steps, for use as *_at functions in various places.\"\"\"\n  return lambda step: step % n_steps == 0\n\n\n# TODO(pkozakowski): Rewrite all interleaved actor-critic algos to subclass\n# this, then rename to ActorCriticAgent and remove the other base classes.\nclass LoopActorCriticAgent(rl_training.Agent):\n  \"\"\"Base class for actor-critic algorithms based on `Loop`.\"\"\"\n\n  on_policy = None\n\n  def __init__(\n      self, task, model_fn,\n      optimizer=adam.Adam,\n      policy_lr_schedule=lr.multifactor,\n      policy_n_steps_per_epoch=1000,\n      policy_weight_fn=(lambda x: x),\n      value_lr_schedule=lr.multifactor,\n      value_n_steps_per_epoch=1000,\n      value_sync_at=(lambda x: x % 100 == 0),\n      advantage_estimator=rl_advantages.monte_carlo,\n      batch_size=64,\n      network_eval_at=None,\n      n_eval_batches=1,\n      max_slice_length=1,\n      margin=0,\n      n_replay_epochs=1,\n      **kwargs\n  ):\n    \"\"\"Initializes LoopActorCriticAgent.\n\n    Args:\n      task: `RLTask` instance to use.\n      model_fn: Function mode -> Trax model, building a joint policy and value\n        network.\n      optimizer: Optimizer for the policy and value networks.\n      policy_lr_schedule: Learning rate schedule for the policy network.\n      policy_n_steps_per_epoch: Number of steps to train the policy network for\n        in each epoch.\n      policy_weight_fn: Function advantages -> weights for calculating the\n        log probability weights in policy training.\n      value_lr_schedule: Learning rate schedule for the value network.\n      value_n_steps_per_epoch: Number of steps to train the value network for\n        in each epoch.\n      value_sync_at: Function step -> bool indicating when to synchronize the\n        target network with the trained network in value training.\n      advantage_estimator: Advantage estimator to use in policy and value\n        training.\n      batch_size: Batch size for training the networks.\n      network_eval_at: Function step -> bool indicating in when to evaluate the\n        networks.\n      n_eval_batches: Number of batches to compute the network evaluation\n        metrics on.\n      max_slice_length: Maximum length of a trajectory slice to train on.\n      margin: Number of timesteps to add at the end of each trajectory slice for\n        better advantage estimation.\n      n_replay_epochs: Number of epochs of trajectories to store in the replay\n        buffer.\n      **kwargs: Keyword arguments forwarded to Agent.\n    \"\"\"\n    super().__init__(task, **kwargs)\n\n    self._policy_dist = distributions.create_distribution(\n        self.task.action_space\n    )\n    model_fn = functools.partial(\n        model_fn,\n        policy_distribution=self._policy_dist,\n    )\n    train_model = model_fn(mode='train')\n    eval_model = model_fn(mode='eval')\n\n    trajectory_batch_stream = self._init_trajectory_batch_stream(\n        batch_size, max_slice_length, margin, n_replay_epochs\n    )\n    advantage_estimator = advantage_estimator(task.gamma, margin=margin)\n    (value_train_task, value_eval_task) = self._init_value_tasks(\n        trajectory_batch_stream,\n        optimizer=optimizer(),\n        lr_schedule=value_lr_schedule(),\n        advantage_estimator=advantage_estimator,\n        train_model=train_model,\n        eval_model=eval_model,\n        sync_at=value_sync_at,\n        n_steps_per_epoch=value_n_steps_per_epoch,\n        n_eval_batches=n_eval_batches,\n    )\n    (policy_train_task, policy_eval_task) = self._init_policy_tasks(\n        trajectory_batch_stream,\n        optimizer=optimizer(),\n        lr_schedule=policy_lr_schedule(),\n        advantage_estimator=advantage_estimator,\n        value_train_task=value_train_task,\n        weight_fn=policy_weight_fn,\n        n_eval_batches=n_eval_batches,\n    )\n    self._init_loop(\n        train_model=train_model,\n        eval_model=eval_model,\n        policy_train_and_eval_task=(policy_train_task, policy_eval_task),\n        value_train_and_eval_task=(value_train_task, value_eval_task),\n        eval_at=network_eval_at,\n        policy_n_steps_per_epoch=policy_n_steps_per_epoch,\n        value_n_steps_per_epoch=value_n_steps_per_epoch,\n    )\n    self._init_collection(model_fn, policy_train_task.sample_batch)\n\n  def _init_trajectory_batch_stream(\n      self, batch_size, max_slice_length, margin, n_replay_epochs\n  ):\n    assert self.on_policy is not None, 'Attribute \"on_policy\" not set.'\n    if self.on_policy:\n      assert n_replay_epochs == 1, (\n          'Non-unit replay buffer size only makes sense for off-policy '\n          'algorithms.'\n      )\n    self._task.set_n_replay_epochs(n_replay_epochs)\n    self._max_slice_length = max_slice_length\n    return self._task.trajectory_batch_stream(\n        batch_size,\n        epochs=[-(ep + 1) for ep in range(n_replay_epochs)],\n        min_slice_length=(1 + margin),\n        max_slice_length=(self._max_slice_length + margin),\n        margin=margin,\n    )\n\n  def _init_value_tasks(\n      self,\n      trajectory_batch_stream,\n      optimizer,\n      lr_schedule,\n      advantage_estimator,\n      train_model,\n      eval_model,\n      sync_at,\n      n_steps_per_epoch,\n      n_eval_batches,\n  ):\n    def sync_also_at_epoch_boundaries(step):\n      return sync_at(step) or (\n          # 0 - end of the epoch, 1 - beginning of the next.\n          step % n_steps_per_epoch in (0, 1)\n      )\n\n    head_selector = tl.Select([1])\n    value_train_task = value_tasks.ValueTrainTask(\n        trajectory_batch_stream,\n        optimizer,\n        lr_schedule,\n        advantage_estimator=advantage_estimator,\n        model=train_model,\n        target_model=eval_model,\n        target_scale=(1 - self.task.gamma),\n        sync_at=sync_also_at_epoch_boundaries,\n        head_selector=head_selector,\n    )\n    value_eval_task = value_tasks.ValueEvalTask(\n        value_train_task, n_eval_batches, head_selector\n    )\n    return (value_train_task, value_eval_task)\n\n  def _init_policy_tasks(\n      self,\n      trajectory_batch_stream,\n      optimizer,\n      lr_schedule,\n      advantage_estimator,\n      value_train_task,\n      weight_fn,\n      n_eval_batches,\n  ):\n    head_selector = tl.Select([0], n_in=2)\n    policy_train_task = policy_tasks.PolicyTrainTask(\n        trajectory_batch_stream,\n        optimizer,\n        lr_schedule,\n        self._policy_dist,\n        advantage_estimator=advantage_estimator,\n        value_fn=value_train_task.value,\n        weight_fn=weight_fn,\n        head_selector=head_selector,\n    )\n    policy_eval_task = policy_tasks.PolicyEvalTask(\n        policy_train_task, n_eval_batches, head_selector\n    )\n    return (policy_train_task, policy_eval_task)\n\n  def _init_loop(\n      self,\n      train_model,\n      eval_model,\n      policy_train_and_eval_task,\n      value_train_and_eval_task,\n      eval_at,\n      policy_n_steps_per_epoch,\n      value_n_steps_per_epoch,\n  ):\n    (policy_train_task, policy_eval_task) = policy_train_and_eval_task\n    (value_train_task, value_eval_task) = value_train_and_eval_task\n\n    if self._output_dir is not None:\n      model_output_dir = os.path.join(self._output_dir, 'model')\n    else:\n      model_output_dir = None\n\n    self._n_train_steps_per_epoch = (\n        policy_n_steps_per_epoch + value_n_steps_per_epoch\n    )\n\n    checkpoint_at = lambda step: step % self._n_train_steps_per_epoch == 0\n\n    def which_task(step):\n      if step % self._n_train_steps_per_epoch < value_n_steps_per_epoch:\n        return 1\n      else:\n        return 0\n\n    self._loop = supervised.training.Loop(\n        model=train_model,\n        tasks=(policy_train_task, value_train_task),\n        eval_model=eval_model,\n        eval_tasks=(policy_eval_task, value_eval_task),\n        output_dir=model_output_dir,\n        eval_at=eval_at,\n        checkpoint_at=checkpoint_at,\n        which_task=which_task,\n    )\n\n    # Validate the restored checkpoints.\n    # TODO(pkozakowski): Move this to the base class once all Agents use Loop.\n    if self._loop.step != self._epoch * self._n_train_steps_per_epoch:\n      raise ValueError(\n          'The number of Loop steps must equal the number of Agent epochs '\n          'times the number of steps per epoch, got {}, {} and {}.'.format(\n              self._loop.step, self._epoch, self._n_train_steps_per_epoch\n          )\n      )\n\n  def _init_collection(self, model_fn, sample_batch):\n    self._collect_model = model_fn(mode='collect')\n    self._collect_model.init(shapes.signature(sample_batch))\n\n  @property\n  def loop(self):\n    \"\"\"Loop exposed for testing.\"\"\"\n    return self._loop\n\n  def policy(self, trajectory, temperature=1.0):\n    \"\"\"Policy function that allows to play using this agent.\"\"\"\n    tr_slice = trajectory.suffix(self._max_slice_length)\n    trajectory_np = tr_slice.to_np(timestep_to_np=self.task.timestep_to_np)\n    return rl_training.network_policy(\n        collect_model=self._collect_model,\n        policy_distribution=self._policy_dist,\n        loop=self.loop,\n        trajectory_np=trajectory_np,\n        head_index=0,\n        temperature=temperature,\n    )\n\n  def train_epoch(self):\n    \"\"\"Trains RL for one epoch.\"\"\"\n    # Copy policy state accumulated during data collection to the trainer.\n    self._loop.update_weights_and_state(state=self._collect_model.state)\n    # Perform one gradient step per training epoch to ensure we stay on policy.\n    self._loop.run(n_steps=self._n_train_steps_per_epoch)\n\n\n### Implementations of common actor-critic algorithms.\n\n\nclass A2C(AdvantageBasedActorCriticAgent):\n  \"\"\"Trains policy and value models using the A2C algorithm.\"\"\"\n\n  on_policy = True\n\n  def __init__(self, task, entropy_coeff=0.01, **kwargs):\n    \"\"\"Configures the A2C Trainer.\"\"\"\n    self._entropy_coeff = entropy_coeff\n    super().__init__(task, **kwargs)\n\n  @property\n  def policy_loss_given_log_probs(self):\n    \"\"\"Definition of the Advantage Actor Critic (A2C) loss.\"\"\"\n    # A2C is one of the most basic actor-critic RL algorithms.\n    # TODO(henrykm) re-factor f into rl_layers and finally share code between\n    # actor_critic.py and actor_critic_joint.py - requires change of inputs\n    # in actor_critic_joint.py from dist_inputs to log_probs.\n    def f(log_probs, advantages, old_log_probs, mask):\n      del old_log_probs  # Not used in A2C.\n      # log_probs of the shape float32[128,1]\n      # advantages of the shape int32[128,1]\n      # mask of the shape int32[128,1]\n      if log_probs.shape != advantages.shape:\n        raise ValueError('New log-probs and advantages shapes '\n                         'should be the same, %s != %s' % (log_probs.shape,\n                                                           advantages.shape))\n      if log_probs.shape != mask.shape:\n        raise ValueError('New log-probs and mask shapes should be the same'\n                         ', %s != %s' % (log_probs.shape, mask.shape))\n\n      a2c_objective = -jnp.sum(log_probs * advantages * mask) / jnp.sum(mask)\n\n      entropy_vec = self._policy_dist.entropy(log_probs) * self._entropy_coeff\n      entropy_loss = jnp.mean(entropy_vec)\n\n      combined_loss = a2c_objective - entropy_loss\n\n      return combined_loss\n\n    return tl.Fn('A2CLoss', f)\n\n\nclass PPO(AdvantageBasedActorCriticAgent):\n  \"\"\"The Proximal Policy Optimization Algorithm aka PPO.\n\n  Trains policy and value models using the PPO algorithm.\n  \"\"\"\n\n  on_policy = True\n\n  def __init__(self, task, epsilon=0.2, entropy_coeff=0.01, **kwargs):\n    \"\"\"Configures the PPO Trainer.\"\"\"\n    self._entropy_coeff = entropy_coeff\n    self._epsilon = epsilon\n    super().__init__(task, **kwargs)\n\n  @property\n  def policy_loss_given_log_probs(self):\n    \"\"\"Definition of the Proximal Policy Optimization loss.\"\"\"\n    def f(new_log_probs, advantages, old_log_probs, mask):\n      # new_log_probs of the shape float32[128,1]\n      # advantages of the shape int32[128,1]\n      # old_log_probs of the shape int32[128,1]\n      # mask of the shape int32[128,1]\n      if new_log_probs.shape != advantages.shape:\n        raise ValueError('New log-probs and advantages shapes '\n                         'should be the same, %s != %s' % (new_log_probs.shape,\n                                                           advantages.shape))\n      if new_log_probs.shape != old_log_probs.shape:\n        raise ValueError('New log-probs and old log-probs shapes '\n                         'should be the same, %s != %s' % (new_log_probs.shape,\n                                                           old_log_probs.shape))\n      if new_log_probs.shape != mask.shape:\n        raise ValueError('New log-probs and mask shapes should be the same'\n                         ', %s != %s' % (new_log_probs.shape, mask.shape))\n\n      # The ratio between new_probs and old_probs expressed\n      # using log_probs and exponentiation\n      probs_ratio = jnp.exp(new_log_probs - old_log_probs)\n      if advantages.shape != probs_ratio.shape:\n        raise ValueError('New log-probs and old log probs shapes '\n                         'should be the same, %s != %s' % (advantages.shape,\n                                                           probs_ratio.shape))\n      unclipped_objective = probs_ratio * advantages\n      clipped_objective = jnp.clip(probs_ratio,\n                                   1 - self._epsilon,\n                                   1 + self._epsilon) * advantages\n\n      if unclipped_objective.shape != probs_ratio.shape:\n        raise ValueError('unclipped_objective and clipped_objective shapes '\n                         'should be the same, %s != %s' % (\n                             unclipped_objective.shape,\n                             clipped_objective.shape))\n\n      ppo_objective = jnp.minimum(unclipped_objective, clipped_objective)\n\n      if ppo_objective.shape != mask.shape:\n        raise ValueError('ppo_objective and mask shapes '\n                         'should be the same, %s != %s' % (\n                             ppo_objective.shape,\n                             mask.shape))\n\n      ppo_loss = -jnp.sum(ppo_objective * mask) / jnp.sum(mask)\n      entropy_vec = self._policy_dist.entropy(\n          new_log_probs) * self._entropy_coeff\n      entropy_loss = jnp.mean(entropy_vec)\n      combined_loss = ppo_loss - entropy_loss\n\n      return combined_loss\n    return tl.Fn('PPOLoss', f)\n\n\ndef _weighted_percentiles(x, thresholds):\n  \"\"\"Calculate weights for x by percentile-and-weights given in thresholds.\n\n  Thresholds contain a list of (p, weight, minumum). For each threshold,\n  all elements of x that are above the p-th percentile *and* above minimum\n  get the weight weight, and all other get the weight 0.\n  The result is the sum over all thresholds.\n\n  Args:\n    x: tensor to calculate the weights for\n    thresholds: list of triples (percentile, weight, minimum) used to\n      calculate the weights (see above how)\n\n  Returns:\n    weights, a tensor of the same shape as x\n  \"\"\"\n  res = []\n  for (percentile, weight, minimum) in thresholds:\n    threshold = jnp.percentile(x, percentile)\n    if minimum is not None:\n      threshold = jnp.maximum(minimum, threshold)\n    zero_ones = jnp.where(x < threshold, jnp.zeros_like(x), jnp.ones_like(x))\n    res.append(weight * zero_ones)\n  return sum(res)\n\n\n# AWR is an off-policy actor-critic RL algorithm.\ndef awr_weights(advantages, beta, thresholds):\n  if thresholds:\n    return _weighted_percentiles(advantages, thresholds)\n  return jnp.exp(advantages / beta)\n\n\n# Helper functions for computing AWR metrics.\ndef awr_metrics(beta, thresholds, preprocess_layer=None):\n  return {  # pylint: disable=g-complex-comprehension\n      'awr_weight_' + name: awr_weight_stat(name, fn, beta, thresholds,\n                                            preprocess_layer)\n      for (name, fn) in [\n          ('mean', jnp.mean),\n          ('std', jnp.std),\n          ('min', jnp.min),\n          ('max', jnp.max),\n      ]\n  }\n\n\ndef awr_weight_stat(stat_name, stat_fn, beta, thresholds, preprocess_layer):\n  # Select just the advantages if preprocess layer is not given.\n  preprocess = tl.Select([1]) if preprocess_layer is None else preprocess_layer\n  return tl.Serial([\n      preprocess,\n      tl.Fn(\n          'AWRWeight' + stat_name.capitalize(),\n          lambda x: stat_fn(awr_weights(x, beta, thresholds)),\n      ),\n  ])\n\n\ndef AWRLoss(beta, w_max, thresholds):  # pylint: disable=invalid-name\n  \"\"\"Definition of the Advantage Weighted Regression (AWR) loss.\"\"\"\n  def f(log_probs, advantages, old_log_probs, mask):\n    del old_log_probs  # Not used in AWR.\n    weights = jnp.minimum(awr_weights(advantages, beta, thresholds), w_max)\n    return -jnp.sum(log_probs * weights * mask) / jnp.sum(mask)\n  return tl.Fn('AWRLoss', f)\n\n\nclass AWR(AdvantageBasedActorCriticAgent):\n  \"\"\"Trains policy and value models using AWR.\"\"\"\n\n  on_policy = False\n\n  def __init__(self, task, beta=1.0, w_max=20.0, thresholds=None, **kwargs):\n    \"\"\"Configures the AWR Trainer.\"\"\"\n    self._beta = beta\n    self._w_max = w_max\n    self._thresholds = thresholds\n    super().__init__(task, **kwargs)\n\n  @property\n  def policy_loss_given_log_probs(self):\n    \"\"\"Policy loss.\"\"\"\n    return AWRLoss(beta=self._beta, w_max=self._w_max,\n                   thresholds=self._thresholds)  # pylint: disable=no-value-for-parameter\n\n\nclass LoopAWR(LoopActorCriticAgent):\n  \"\"\"Advantage Weighted Regression.\"\"\"\n\n  on_policy = False\n\n  def __init__(self, task, model_fn, beta=1.0, w_max=20, **kwargs):\n    def policy_weight_fn(advantages):\n      return jnp.minimum(jnp.exp(advantages / beta), w_max)\n    super().__init__(\n        task, model_fn, policy_weight_fn=policy_weight_fn, **kwargs\n    )\n\n\ndef SamplingAWRLoss(beta, w_max, thresholds,  # pylint: disable=invalid-name\n                    reweight=False, sampled_all_discrete=False):\n  \"\"\"Definition of the Advantage Weighted Regression (AWR) loss.\"\"\"\n  def f(log_probs, advantages, old_log_probs, mask):\n    if reweight:  # Use new policy weights for sampled actions instead.\n      mask *= jnp.exp(fastmath.stop_gradient(log_probs) - old_log_probs)\n    if sampled_all_discrete:  # Actions were sampled uniformly; weight them.\n      mask *= jnp.exp(old_log_probs)\n    weights = jnp.minimum(awr_weights(advantages, beta, thresholds), w_max)\n    return -jnp.sum(log_probs * weights * mask) / jnp.sum(mask)\n  return tl.Fn('SamplingAWRLoss', f)\n\n\nclass SamplingAWR(AdvantageBasedActorCriticAgent):\n  \"\"\"Trains policy and value models using Sampling AWR.\"\"\"\n\n  on_policy = False\n\n  def __init__(self, task, beta=1.0, w_max=20.0, thresholds=None,\n               reweight=False, **kwargs):\n    \"\"\"Configures the AWR Trainer.\"\"\"\n    self._beta = beta\n    self._w_max = w_max\n    self._thresholds = thresholds\n    self._reweight = reweight\n    super().__init__(task, q_value=True, **kwargs)\n\n  def _policy_inputs_to_advantages(self, preprocess):\n    \"\"\"A layer that computes advantages from policy inputs.\"\"\"\n    def fn(dist_inputs, actions, q_values, act_log_probs, mask):\n      del dist_inputs, actions, mask\n      q_values = jnp.swapaxes(q_values, 0, 1)\n      act_log_probs = jnp.swapaxes(act_log_probs, 0, 1)\n      if self._sample_all_discrete_actions:\n        values = jnp.sum(q_values * jnp.exp(act_log_probs), axis=0)\n      else:\n        values = jnp.mean(q_values, axis=0)\n      advantages = q_values - values  # Broadcasting values over n_samples\n      if preprocess:\n        advantages = self._preprocess_advantages(advantages)\n      return advantages\n    return tl.Fn('PolicyInputsToAdvantages', fn)\n\n  @property\n  def policy_metrics(self):\n    metrics = {\n        'policy_loss': self.policy_loss,\n        'advantage_mean': tl.Serial(\n            self._policy_inputs_to_advantages(False),\n            tl.Fn('Mean', lambda x: jnp.mean(x))  # pylint: disable=unnecessary-lambda\n        ),\n        'advantage_std': tl.Serial(\n            self._policy_inputs_to_advantages(False),\n            tl.Fn('Std', lambda x: jnp.std(x))  # pylint: disable=unnecessary-lambda\n        )\n    }\n    metrics.update(awr_metrics(\n        self._beta, self._thresholds,\n        preprocess_layer=self._policy_inputs_to_advantages(True)))\n    return metrics\n\n  @property\n  def policy_loss(self, **unused_kwargs):\n    \"\"\"Policy loss.\"\"\"\n    def LossInput(dist_inputs, actions, q_values, act_log_probs, mask):  # pylint: disable=invalid-name\n      \"\"\"Calculates action log probabilities and normalizes advantages.\"\"\"\n      # (batch_size, n_samples, ...) -> (n_samples, batch_size, ...)\n      q_values = jnp.swapaxes(q_values, 0, 1)\n      mask = jnp.swapaxes(mask, 0, 1)\n      actions = jnp.swapaxes(actions, 0, 1)\n      act_log_probs = jnp.swapaxes(act_log_probs, 0, 1)\n\n      # TODO(pkozakowski,lukaszkaiser): Try max here, or reweighting?\n      if self._sample_all_discrete_actions:\n        values = jnp.sum(q_values * jnp.exp(act_log_probs), axis=0)\n      else:\n        values = jnp.mean(q_values, axis=0)\n      advantages = q_values - values  # Broadcasting values over n_samples\n      advantages = self._preprocess_advantages(advantages)\n\n      # Broadcast inputs and calculate log-probs\n      dist_inputs = jnp.broadcast_to(\n          dist_inputs, (self._q_value_n_samples,) + dist_inputs.shape)\n      log_probs = self._policy_dist.log_prob(dist_inputs, actions)\n      return (log_probs, advantages, act_log_probs, mask)\n\n    return tl.Serial(\n        tl.Fn('LossInput', LossInput, n_out=4),\n        # Policy loss is expected to consume\n        # (log_probs, advantages, old_log_probs, mask).\n        SamplingAWRLoss(\n            beta=self._beta, w_max=self._w_max, thresholds=self._thresholds,\n            reweight=self._reweight,\n            sampled_all_discrete=self._sample_all_discrete_actions)\n    )\n\n  def policy_batches_stream(self):\n    \"\"\"Use the RLTask self._task to create inputs to the policy model.\"\"\"\n    # For now TD-0 estimation of the value. TODO(pkozakowski): Support others?\n    for np_trajectory in self._task.trajectory_batch_stream(\n        self._policy_batch_size,\n        epochs=self._replay_epochs,\n        max_slice_length=self._max_slice_length,\n    ):\n      dist_inputs = self._get_dist_inputs(np_trajectory)\n      (q_values, actions, act_log_probs) = self._run_value_model(\n          np_trajectory.observation, dist_inputs)\n      shapes.assert_same_shape(q_values, act_log_probs)\n\n      # q_values shape: (batch_size, n_samples, length)\n      if len(q_values.shape) != 3:\n        raise ValueError('Q-values are expected to have shape [batch_size, ' +\n                         'n_samples, length], got: %s' % str(q_values.shape))\n      if q_values.shape[1] != self._q_value_n_samples:\n        raise ValueError('Q-values dimension 1 should = n_samples, %d != %d'\n                         % (q_values.shape[1], self._q_value_n_samples))\n      if q_values.shape[0] != self._policy_batch_size:\n        raise ValueError('Q-values dimension 0 should = policy batch size, ' +\n                         '%d!=%d' %(q_values.shape[1], self._policy_batch_size))\n\n      mask = np_trajectory.mask\n      mask = np.reshape(mask, [mask.shape[0], 1] + list(mask.shape[1:]))\n      mask = jnp.broadcast_to(mask, q_values.shape)\n      shapes.assert_same_shape(mask, q_values)\n      yield (np_trajectory.observation, actions, q_values, act_log_probs, mask)\n", "framework": "tensorflow"}
{"repo_name": "yao-matrix/mProto", "file_path": "use_case/image_cnn/apps/ocr/base_trainer.py", "content": "#!/usr/bin/env python\n# coding=utf-8\n\n# Author: YAO Matrix (yaoweifeng0301@126.com)\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\nimport time\nimport datetime\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom data_io import load_minst_data\nfrom metrics import precision, error_rate\n\nVALIDATION_SIZE = 5000  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 26\nEVAL_BATCH_SIZE = 64\nEVAL_FREQUENCY = 100  # Number of steps between evaluations.\n\nmodule_dir = os.path.dirname(os.path.abspath(__file__))\nmodule_name = os.path.basename(__file__).split('.')[0]\n\nlog_path = os.path.join(module_dir, os.path.pardir, os.path.pardir, 'logs', module_name + '_' + datetime.date.today().strftime('%Y%m%d') + '.log')\n\nlogger = logging.getLogger(module_name)\nlogger.setLevel(logging.DEBUG)\nfh = logging.FileHandler(log_path)\nch = logging.StreamHandler()\nfh.setLevel(logging.DEBUG)\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter('[%(asctime)s][%(name)s][%(levelname)s]: %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\nlogger.addHandler(fh)\nlogger.addHandler(ch)\n\n\nFLAGS = tf.app.flags.FLAGS\n\ndef main(argv = None):\n\n  # load mnist into numpy arrays.\n  train_data, train_labels = load_minst_data(t = 'train')\n  test_data, test_labels = load_minst_data(t = 'test')\n\n  height = train_data.shape[1]\n  width = train_data.shape[2]\n  channel = (train_data.shape[3] if train_data.ndim > 3 else 1)\n\n  label_max = np.amax(train_labels)\n  label_min = np.amin(train_labels)\n  num_labels = label_max - label_min + 1\n\n  # Generate a validation set.\n  validation_data = train_data[:VALIDATION_SIZE, ...]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_data = train_data[VALIDATION_SIZE:, ...]\n  train_labels = train_labels[VALIDATION_SIZE:]\n  num_epochs = NUM_EPOCHS\n  train_size = train_labels.shape[0]\n\n  # This is where training samples and labels are fed to the graph.\n  # These placeholder nodes will be fed a batch of training data at each\n  # training step using the {feed_dict} argument to the Run() call below.\n  train_data_node = tf.placeholder(\n      tf.float32,\n      shape = (BATCH_SIZE, height, width, channel))\n  train_labels_node = tf.placeholder(tf.int64, shape = (BATCH_SIZE,))\n\n  eval_data = tf.placeholder(\n      tf.float32,\n      shape = (EVAL_BATCH_SIZE, height, width, channel))\n\n  # The variables below hold all the trainable weights. They are passed an\n  # initial value which will be assigned when we call:\n  # {tf.initialize_all_variables().run()}\n  conv1_weights = tf.Variable(\n      tf.truncated_normal([5, 5, channel, 32],  # 5x5 filter, depth 32.\n                          stddev = 0.1,\n                          seed = SEED),\n      name = \"conv1_weights\")\n  conv1_biases = tf.Variable(tf.zeros([32]), name = \"conv1_biases\")\n  \n  conv2_weights = tf.Variable(\n      tf.truncated_normal([5, 5, 32, 64],\n                          stddev = 0.1,\n                          seed = SEED),\n      name = \"conv2_weights\")\n  conv2_biases = tf.Variable(tf.constant(0.1, shape = [64]), name = \"conv2_biases\")\n  \n  fc1_weights = tf.Variable(  # fully connected, depth 512.\n      tf.truncated_normal(\n          [height // 4 * width // 4 * 64, 512],\n          stddev = 0.1,\n          seed = SEED),\n      name = \"fc1_weights\")\n  fc1_biases = tf.Variable(tf.constant(0.1, shape = [512]), name = \"fc1_biases\")\n  \n  fc2_weights = tf.Variable(\n      tf.truncated_normal([512, num_labels],\n                          stddev = 0.1,\n                          seed = SEED),\n      name = \"fc2_weights\")\n  fc2_biases = tf.Variable(tf.constant(0.1, shape = [num_labels]), name = \"fc2_biases\")\n\n  # We will replicate the model structure for the training subgraph, as well\n  # as the evaluation subgraphs, while sharing the trainable parameters.\n  def lenet(data, train = False):\n    \"\"\"LeNet definition.\"\"\"\n    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n    # the same size as the input). Note that {strides} is a 4D array whose\n    # shape matches the data layout: [n, h, w, c].\n    conv1 = tf.nn.conv2d(data,\n                         conv1_weights,\n                         strides = [1, 1, 1, 1],\n                         padding = 'SAME')\n    # Bias and rectified linear non-linearity.\n    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n    # Max pooling. The kernel size spec {ksize} also follows the layout of\n    # the data. Here we have a pooling window of 2, and a stride of 2.\n    pool1 = tf.nn.max_pool(relu1,\n                           ksize = [1, 2, 2, 1],\n                           strides = [1, 2, 2, 1],\n                           padding = 'SAME')\n    conv2 = tf.nn.conv2d(pool1,\n                         conv2_weights,\n                         strides = [1, 1, 1, 1],\n                         padding = 'SAME')\n    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n    pool2 = tf.nn.max_pool(relu2,\n                           ksize = [1, 2, 2, 1],\n                           strides = [1, 2, 2, 1],\n                           padding = 'SAME')\n    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n    # fully connected layers.\n    pool_shape = pool2.get_shape().as_list()\n    reshape = tf.reshape(pool2,\n                         [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n    # Fully connected layer. Note that the '+' operation automatically\n    # broadcasts the biases.\n    fc1 = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n    # Add a 50% dropout during training only. Dropout also scales\n    # activations such that no rescaling is needed at evaluation time.\n    if train:\n      fc1 = tf.nn.dropout(fc1, 0.5, seed = SEED)\n    return tf.matmul(fc1, fc2_weights) + fc2_biases\n\n  # Training computation: logits + cross-entropy loss.\n  logits = lenet(train_data_node, True)\n  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, train_labels_node))\n\n  # L2 regularization for the fully connected parameters.\n  regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n  # Add the regularization term to the loss.\n  loss += 5e-4 * regularizers\n\n  # Optimizer: set up a variable that's incremented once per batch and\n  # controls the learning rate decay.\n  batch = tf.Variable(0)\n  # Decay once per epoch, using an exponential schedule starting at 0.01.\n  learning_rate = tf.train.exponential_decay(\n      0.01,                # Base learning rate.\n      batch * BATCH_SIZE,  # Current index into the dataset.\n      train_size,          # Decay step.\n      0.95,                # Decay rate.\n      staircase = True)\n  # Use simple momentum for the optimization.\n  optimizer = tf.train.MomentumOptimizer(learning_rate,\n                                         0.9).minimize(loss,\n                                                       global_step = batch)\n\n  # Predictions for the current training minibatch.\n  train_prediction = tf.nn.softmax(logits)\n\n  # Predictions for the test and validation, which we'll compute less often.\n  eval_prediction = tf.nn.softmax(lenet(eval_data))\n\n  # Small utility function to evaluate a dataset by feeding batches of data to\n  # {eval_data} and pulling the results from {eval_predictions}.\n  # Saves memory and enables this to run on smaller GPUs.\n  def eval_in_batches(data, sess):\n    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n    size = data.shape[0]\n    if size < EVAL_BATCH_SIZE:\n      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n    predictions = np.ndarray(shape = (size, num_labels), dtype = np.float32)\n    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n      end = begin + EVAL_BATCH_SIZE\n      if end <= size:\n        predictions[begin:end, :] = sess.run(\n            eval_prediction,\n            feed_dict={eval_data: data[begin:end, ...]})\n      else:\n        batch_predictions = sess.run(\n            eval_prediction,\n            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n        predictions[begin:, :] = batch_predictions[begin - size:, :]\n    return predictions\n\n  # Create a local session to run the training.\n  start_time = time.time()\n  model_dir = os.path.join(module_dir, os.path.pardir, os.path.pardir, 'models') \n  with tf.Session() as sess:\n    # Run all the initializers to prepare the trainable parameters.\n    tf.initialize_all_variables().run()\n    logger.info('Initialized!')\n    # Loop through training steps.\n    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n      # Compute the offset of the current minibatch in the data.\n      # Note that we could use better randomization across epochs.\n      offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n      batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n      batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n      # This dictionary maps the batch data (as a numpy array) to the\n      # node in the graph it should be fed to.\n      feed_dict = {train_data_node: batch_data,\n                   train_labels_node: batch_labels}\n      # Run the graph and fetch some of the nodes.\n      _, l, lr, predictions = sess.run(\n          [optimizer, loss, learning_rate, train_prediction],\n          feed_dict=feed_dict)\n      if step % EVAL_FREQUENCY == 0:\n        elapsed_time = time.time() - start_time\n        start_time = time.time()\n        logger.info('Step %d (epoch %.2f), %.1f ms' %\n              (step, float(step) * BATCH_SIZE / train_size,\n               1000 * elapsed_time / EVAL_FREQUENCY))\n        logger.info('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n        logger.info('Minibatch training error: %.1f%%' % error_rate(predictions, batch_labels))\n        logger.info('Validation error: %.1f%%' % error_rate(eval_in_batches(validation_data, sess), validation_labels))\n        sys.stdout.flush()\n\n    # Finally print the result!\n    test_precision = precision(eval_in_batches(test_data, sess), test_labels)\n    logger.info('Test precision: %.1f%%' % test_precision)\n\n    # Model persistence\n    saver = tf.train.Saver([conv1_weights, conv1_biases, conv2_weights, conv2_biases, fc1_weights, fc1_biases])\n    model_path = os.path.join(model_dir, \"base\", \"lenet_base.ckpt\")\n    save_path = saver.save(sess, model_path)\n    logger.info(\"Model saved in file: %s\" % save_path)\n\nif __name__ == '__main__':\n  tf.app.run()\n", "framework": "tensorflow"}
{"repo_name": "google-research/federated", "file_path": "utils/datasets/stackoverflow_tag_prediction_test.py", "content": "# Copyright 2020, Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport collections\nfrom unittest import mock\n\nimport tensorflow as tf\nimport tensorflow_federated as tff\n\nfrom utils.datasets import stackoverflow_tag_prediction\n\n\nTEST_DATA = collections.OrderedDict(\n    creation_date=(['unused date']),\n    score=([tf.constant(0, dtype=tf.int64)]),\n    tags=(['unused test tag']),\n    title=(['unused title']),\n    tokens=(['one must imagine']),\n    type=(['unused type']),\n)\n\n\nclass PreprocessFnTest(tf.test.TestCase):\n\n  def test_word_tokens_to_ids_without_oov(self):\n    word_vocab = ['A', 'B', 'C']\n    tag_vocab = ['D', 'E', 'F']\n    to_ids_fn = stackoverflow_tag_prediction.build_to_ids_fn(\n        word_vocab, tag_vocab)\n    data = {'tokens': 'A B C', 'title': '', 'tags': ''}\n    processed = to_ids_fn(data)\n    self.assertAllClose(self.evaluate(processed[0]), [1 / 3, 1 / 3, 1 / 3])\n\n  def test_word_tokens_to_ids_with_oov(self):\n    word_vocab = ['A', 'B']\n    tag_vocab = ['D', 'E', 'F']\n    to_ids_fn = stackoverflow_tag_prediction.build_to_ids_fn(\n        word_vocab, tag_vocab)\n    data = {'tokens': 'A B C', 'title': '', 'tags': ''}\n    processed = to_ids_fn(data)\n    self.assertAllClose(self.evaluate(processed[0]), [1 / 3, 1 / 3])\n\n  def test_tag_tokens_to_ids_without_oov(self):\n    word_vocab = ['A', 'B', 'C']\n    tag_vocab = ['D', 'E', 'F']\n    to_ids_fn = stackoverflow_tag_prediction.build_to_ids_fn(\n        word_vocab, tag_vocab)\n    data = {'tokens': '', 'title': '', 'tags': 'D|E|F'}\n    processed = to_ids_fn(data)\n    self.assertAllClose(self.evaluate(processed[1]), [1, 1, 1])\n\n  def test_tag_tokens_to_ids_with_oov(self):\n    word_vocab = ['A', 'B', 'C']\n    tag_vocab = ['D', 'E']\n    to_ids_fn = stackoverflow_tag_prediction.build_to_ids_fn(\n        word_vocab, tag_vocab)\n    data = {'tokens': '', 'title': '', 'tags': 'D|E|F'}\n    processed = to_ids_fn(data)\n    self.assertAllClose(self.evaluate(processed[1]), [1, 1])\n\n  def test_join_word_tokens_with_title(self):\n    word_vocab = ['A', 'B', 'C']\n    tag_vocab = ['D', 'E', 'F']\n    to_ids_fn = stackoverflow_tag_prediction.build_to_ids_fn(\n        word_vocab, tag_vocab)\n    data = {'tokens': 'A B C', 'title': 'A B', 'tags': ''}\n    processed = to_ids_fn(data)\n    self.assertAllClose(self.evaluate(processed[0]), [2 / 5, 2 / 5, 1 / 5])\n\n\nSTACKOVERFLOW_MODULE = 'tensorflow_federated.simulation.datasets.stackoverflow'\n\n\nclass FederatedDatasetTest(tf.test.TestCase):\n\n  @mock.patch(STACKOVERFLOW_MODULE + '.load_tag_counts')\n  @mock.patch(STACKOVERFLOW_MODULE + '.load_word_counts')\n  @mock.patch(STACKOVERFLOW_MODULE + '.load_data')\n  def test_preprocess_applied(self, mock_load_data, mock_load_word_counts,\n                              mock_load_tag_counts):\n    if tf.config.list_logical_devices('GPU'):\n      self.skipTest('skip GPU test')\n    # Mock out the actual data loading from disk. Assert that the preprocessing\n    # function is applied to the client data, and that only the ClientData\n    # objects we desired are used.\n    #\n    # The correctness of the preprocessing function is tested in other tests.\n    mock_train = mock.create_autospec(tff.simulation.datasets.ClientData)\n    mock_validation = mock.create_autospec(tff.simulation.datasets.ClientData)\n    mock_test = mock.create_autospec(tff.simulation.datasets.ClientData)\n    mock_load_data.return_value = (mock_train, mock_validation, mock_test)\n    # Return a factor word dictionary.\n    mock_load_word_counts.return_value = collections.OrderedDict(a=1)\n    mock_load_tag_counts.return_value = collections.OrderedDict(a=1)\n\n    _, _ = stackoverflow_tag_prediction.get_federated_datasets(\n        word_vocab_size=1000,\n        tag_vocab_size=500,\n        train_client_batch_size=10,\n        test_client_batch_size=100,\n        train_client_epochs_per_round=1,\n        test_client_epochs_per_round=1,\n        max_elements_per_train_client=128,\n        max_elements_per_test_client=-1)\n\n    # Assert the validation ClientData isn't used.\n    mock_load_data.assert_called_once()\n    self.assertEmpty(mock_validation.mock_calls)\n\n    # Assert the training and testing data are preprocessed.\n    self.assertEqual(mock_train.mock_calls,\n                     mock.call.preprocess(mock.ANY).call_list())\n    self.assertEqual(mock_test.mock_calls,\n                     mock.call.preprocess(mock.ANY).call_list())\n\n    # Assert the word counts were loaded once to apply to each dataset.\n    mock_load_word_counts.assert_called_once()\n\n    # Assert the tag counts were loaded once to apply to each dataset.\n    mock_load_tag_counts.assert_called_once()\n\n\nclass CentralizedDatasetTest(tf.test.TestCase):\n\n  @mock.patch(STACKOVERFLOW_MODULE + '.load_tag_counts')\n  @mock.patch(STACKOVERFLOW_MODULE + '.load_word_counts')\n  @mock.patch(STACKOVERFLOW_MODULE + '.load_data')\n  def test_preprocess_applied(self, mock_load_data, mock_load_word_counts,\n                              mock_load_tag_counts):\n    if tf.config.list_logical_devices('GPU'):\n      self.skipTest('skip GPU test')\n    # Mock out the actual data loading from disk. Assert that the preprocessing\n    # function is applied to the client data, and that only the ClientData\n    # objects we desired are used.\n    #\n    # The correctness of the preprocessing function is tested in other tests.\n    sample_ds = tf.data.Dataset.from_tensor_slices(TEST_DATA)\n\n    mock_train = mock.create_autospec(tff.simulation.datasets.ClientData)\n    mock_train.create_tf_dataset_from_all_clients = mock.Mock(\n        return_value=sample_ds)\n\n    mock_validation = mock.create_autospec(tff.simulation.datasets.ClientData)\n\n    mock_test = mock.create_autospec(tff.simulation.datasets.ClientData)\n    mock_test.create_tf_dataset_from_all_clients = mock.Mock(\n        return_value=sample_ds)\n\n    mock_load_data.return_value = (mock_train, mock_validation, mock_test)\n    mock_load_word_counts.return_value = collections.OrderedDict(a=1)\n    mock_load_tag_counts.return_value = collections.OrderedDict(a=1)\n\n    _, _, _ = stackoverflow_tag_prediction.get_centralized_datasets(\n        word_vocab_size=1000,\n        tag_vocab_size=500,\n        train_batch_size=10,\n        validation_batch_size=50,\n        test_batch_size=100,\n        num_validation_examples=10000)\n\n    # Assert the validation ClientData isn't used.\n    mock_load_data.assert_called_once()\n    self.assertEmpty(mock_validation.mock_calls)\n\n    # Assert the validation ClientData isn't used, and the train and test\n    # are amalgamated into datasets single datasets over all clients.\n    mock_load_data.assert_called_once()\n    self.assertEmpty(mock_validation.mock_calls)\n    self.assertEqual(mock_train.mock_calls,\n                     mock.call.create_tf_dataset_from_all_clients().call_list())\n    self.assertEqual(mock_test.mock_calls,\n                     mock.call.create_tf_dataset_from_all_clients().call_list())\n\n    # Assert the word counts were loaded once to apply to each dataset.\n    mock_load_word_counts.assert_called_once()\n    mock_load_tag_counts.assert_called_once()\n\n\nif __name__ == '__main__':\n  tf.test.main()\n", "framework": "tensorflow"}
