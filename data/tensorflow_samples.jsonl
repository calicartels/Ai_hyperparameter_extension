{"repo_name": "google/uncertainty-baselines", "file_path": "uncertainty_baselines/models/bert_sngp.py", "content": "# coding=utf-8\n# Copyright 2022 The Uncertainty Baselines Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"SNGP with BERT encoder.\n\nSpectral-normalized neural GP (SNGP) [1] is a simple method to improve\na deterministic neural network's uncertainty by applying spectral\nnormalization to the hidden layers, and then replace the dense output layer\nwith a Gaussian process layer.\n\n## References:\n\n[1]: Jeremiah Liu et al. Simple and Principled Uncertainty Estimation with\n     Deterministic Deep Learning via Distance Awareness.\n     _arXiv preprint arXiv:2006.10108_, 2020.\n     https://arxiv.org/abs/2006.10108\n\n[2]: Ashish Vaswani et al. Attention Is All You Need.\n     _Neural Information Processing System_, 2017.\n     https://papers.nips.cc/paper/7181-attention-is-all-you-need\n\"\"\"\nimport functools\n\nfrom typing import Any, Dict, Mapping, Optional\n\nimport edward2 as ed\nimport tensorflow as tf\n\nfrom official.modeling import tf_utils\nfrom official.nlp.bert import configs as bert_configs\nfrom official.nlp.modeling import layers as bert_layers\nfrom official.nlp.modeling import networks as bert_encoder\n\n_EinsumDense = tf.keras.layers.experimental.EinsumDense\n\n# A dict of regex patterns and their replacements. Use to update weight names\n# in a classic pre-trained checkpoint to those in\n# SpectralNormalizedTransformerEncoder.\nCHECKPOINT_REPL_PATTERNS = {\n    '/intermediate': '/feedforward/intermediate',\n    '/output': '/feedforward/output'\n}\n\n\ndef make_spec_norm_dense_layer(**spec_norm_kwargs: Mapping[str, Any]):\n  \"\"\"Defines a spectral-normalized EinsumDense layer.\n\n  Args:\n    **spec_norm_kwargs: Keyword arguments to the SpectralNormalization layer\n      wrapper.\n\n  Returns:\n    (callable) A function that defines a dense layer and wraps it with\n      SpectralNormalization.\n  \"\"\"\n\n  def spec_norm_dense(*dense_args, **dense_kwargs):\n    base_layer = _EinsumDense(*dense_args, **dense_kwargs)\n    # Inhere base_layer name to match with those in a classic BERT checkpoint.\n    return ed.layers.SpectralNormalization(\n        base_layer, inhere_layer_name=True, **spec_norm_kwargs)\n\n  return spec_norm_dense\n\n\nclass SpectralNormalizedFeedforwardLayer(tf.keras.layers.Layer):\n  \"\"\"Two-layer feed-forward network with spectral-normalized dense layers.\n\n  This class implements a drop-in replacement of the feedforward_block module\n  within tensorflow_models.official.nlp.modeling.layers.TransformerScaffold,\n  with additional options for applying spectral normalization to its hidden\n  weights, and for turning off layer normalization.\n\n  The intended use of this class is as below:\n\n  >>> feedforward_cls = functools.partial(\n        SpectralNormalizedFeedforwardLayer,\n        spec_norm_hparams=spec_norm_hparams)\n  >>> common_kwargs = {\n        'kernel_initializer': 'glorot_uniform'\n      }\n  >>> feedforward_cfg = {\n        'inner_dim': 1024,\n        'inner_activation': 'gelu',\n        'dropout': 0.1,\n        'name': 'feedforward',\n      }\n  >>> feedforward_cfg.update(common_kwargs)\n  >>> feedforward_block = feedforward_cls(**feedforward_cfg)\n  \"\"\"\n\n  def __init__(self,\n               inner_dim: int,\n               inner_activation: str,\n               # TODO(yquan): Remove the following 2 unused fields after they\n               # are removed from TransformerScaffold.py\n               intermediate_size: int,\n               intermediate_activation: str,\n               dropout: float,\n               use_layer_norm: bool = True,\n               use_spec_norm: bool = False,\n               spec_norm_kwargs: Optional[Mapping[str, Any]] = None,\n               name: str = 'feedforward',\n               **common_kwargs: Mapping[str, Any]):\n    \"\"\"Initializer.\n\n    The arguments corresponds to the keyword arguments in feedforward_cls\n    in the TransformerScaffold class.\n\n    Args:\n      inner_dim: Size of the intermediate layer.\n      inner_activation: Activation function to be used for the intermediate\n        layer.\n      intermediate_size (to-be-removed): Same as inner_dim.\n      intermediate_activation (to-be-removed): Same as inner_activation.\n      dropout: Dropout rate.\n      use_layer_norm: Whether to use layer normalization.\n      use_spec_norm: Whether to use spectral normalization.\n      spec_norm_kwargs: Keyword arguments to the spectral normalization layer.\n      name: Layer name.\n      **common_kwargs: Other common keyword arguments for the hidden dense\n        layers.\n    \"\"\"\n    super().__init__(name=name)\n    self._inner_dim = inner_dim\n    self._inner_activation = inner_activation\n    self._dropout = dropout\n    self._use_layer_norm = use_layer_norm\n    self._use_spec_norm = use_spec_norm\n    self._spec_norm_kwargs = spec_norm_kwargs\n    self._common_kwargs = common_kwargs\n\n    # Defines the EinsumDense layer.\n    if self._use_spec_norm:\n      self.einsum_dense_layer = make_spec_norm_dense_layer(**spec_norm_kwargs)\n    else:\n      self.einsum_dense_layer = _EinsumDense\n\n  def build(self, input_shape: tf.TensorShape) -> None:\n    hidden_size = input_shape.as_list()[-1]\n\n    self._intermediate_dense = self.einsum_dense_layer(\n        'abc,cd->abd',\n        output_shape=(None, self._inner_dim),\n        bias_axes='d',\n        name='intermediate',\n        **self._common_kwargs)\n    policy = tf.keras.mixed_precision.global_policy()\n    if policy.name == 'mixed_bfloat16':\n      # bfloat16 causes BERT with the LAMB optimizer to not converge\n      # as well, so we use float32.\n      policy = tf.float32\n    self._intermediate_activation_layer = tf.keras.layers.Activation(\n        self._inner_activation, dtype=policy)\n    self._output_dense = self.einsum_dense_layer(\n        'abc,cd->abd',\n        output_shape=(None, hidden_size),\n        bias_axes='d',\n        name='output',\n        **self._common_kwargs)\n    self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout)\n    # Use float32 in layernorm for numeric stability.\n    self._output_layer_norm = tf.keras.layers.LayerNormalization(\n        name='output_layer_norm', axis=-1, epsilon=1e-12, dtype=tf.float32)\n\n    super().build(input_shape)\n\n  def call(self,\n           inputs: tf.Tensor,\n           training: Optional[bool] = None) -> tf.Tensor:\n    intermediate_output = self._intermediate_dense(inputs)\n    intermediate_output = self._intermediate_activation_layer(\n        intermediate_output)\n    layer_output = self._output_dense(intermediate_output)\n    layer_output = self._output_dropout(layer_output, training=training)\n    # During mixed precision training, attention_output is from layer norm\n    # and is always fp32 for now. Cast layer_output to fp32 for the subsequent\n    # add.\n    layer_output = tf.cast(layer_output, tf.float32)\n    residual_output = layer_output + inputs\n\n    if self._use_layer_norm:\n      return self._output_layer_norm(residual_output)\n    return residual_output\n\n  def get_config(self) -> Dict[str, Any]:\n    config = super().get_config()\n    config.update({\n        'inner_dim': self._inner_dim,\n        'inner_activation': self._inner_activation,\n        'intermediate_size': self._inner_dim,\n        'intermediate_activation': self._inner_activation,\n        'dropout': self._dropout,\n        'use_layer_norm': self._use_layer_norm,\n        'use_spec_norm': self._use_spec_norm,\n        'spec_norm_kwargs': self._spec_norm_kwargs\n    })\n    config.update(self._common_kwargs)\n    return config\n\n\nclass SpectralNormalizedMultiHeadAttention(tf.keras.layers.MultiHeadAttention):\n  \"\"\"Multi-head attention with spectral-normalized dense layers.\n\n  This is an implementation of multi-headed attention layer [2] with the option\n  to replace the original EinsumDense layer with its spectral-normalized\n  counterparts.\n  \"\"\"\n\n  def __init__(self,\n               use_spec_norm: bool = False,\n               spec_norm_kwargs: Optional[Dict[str, Any]] = None,\n               **kwargs: Dict[str, Any]):\n    super().__init__(**kwargs)\n    self._use_spec_norm = use_spec_norm\n    self._spec_norm_kwargs = spec_norm_kwargs\n    self._spec_norm_dense_layer = make_spec_norm_dense_layer(**spec_norm_kwargs)\n\n  def _update_einsum_dense(\n      self, einsum_dense_layer: tf.keras.layers.Layer) -> tf.keras.layers.Layer:\n    \"\"\"Updates the EinsumDense layer to its spectral-normalized counterparts.\"\"\"\n    if not self._use_spec_norm:\n      return einsum_dense_layer\n\n    # Overwrites EinsumDense using the same arguments.\n    einsum_dense_kwargs = einsum_dense_layer.get_config()\n    return self._spec_norm_dense_layer(**einsum_dense_kwargs)\n\n  def _build_from_signature(self,\n                            query: tf.Tensor,\n                            value: tf.Tensor,\n                            key: Optional[tf.Tensor] = None):\n    \"\"\"Builds layers and variables.\n\n    This function overwrites the default _build_from_signature to build dense\n    layers from self.einsum_dense_layer. Once the method is called,\n    self._built_from_signature will be set to True.\n\n    Args:\n      query: query tensor or TensorShape.\n      value: value tensor or TensorShape.\n      key: key tensor or TensorShape.\n    \"\"\"\n    super()._build_from_signature(query, value, key)  # pytype: disable=attribute-error  # typed-keras\n    # Overwrites EinsumDense layers.\n    # TODO(b/168256394): Enable spectral normalization also for key, query and\n    # value layers in the self-attention module.\n    self._output_dense = self._update_einsum_dense(self._output_dense)\n\n  def get_config(self):\n    config = super().get_config()\n    config['use_spec_norm'] = self._use_spec_norm\n    config['spec_norm_kwargs'] = self._spec_norm_kwargs\n    return config\n\n\nclass SpectralNormalizedTransformer(bert_layers.TransformerScaffold):\n  \"\"\"Transformer layer with spectral-normalized dense layers.\"\"\"\n\n  def __init__(self,\n               use_layer_norm_att: bool = True,\n               use_layer_norm_ffn: bool = True,\n               use_spec_norm_att: bool = False,\n               use_spec_norm_ffn: bool = False,\n               spec_norm_kwargs: Optional[Mapping[str, Any]] = None,\n               **kwargs):\n    \"\"\"Initializer.\n\n    Args:\n      use_layer_norm_att: Whether to use layer normalization in the attention\n        layer.\n      use_layer_norm_ffn: Whether to use layer normalization in the feedforward\n        layer.\n      use_spec_norm_att: Whether to use spectral normalization in the attention\n        layer.\n      use_spec_norm_ffn: Whether to use spectral normalization in the\n        feedforward layer.\n      spec_norm_kwargs: Keyword arguments to the spectral normalization layer.\n      **kwargs: Additional keyword arguments to TransformerScaffold.\n    \"\"\"\n    self._use_layer_norm_att = use_layer_norm_att\n    self._use_layer_norm_ffn = use_layer_norm_ffn\n    self._use_spec_norm_att = use_spec_norm_att\n    self._use_spec_norm_ffn = use_spec_norm_ffn\n    self._spec_norm_kwargs = spec_norm_kwargs\n\n    feedforward_cls = functools.partial(\n        SpectralNormalizedFeedforwardLayer,\n        use_layer_norm=self._use_layer_norm_ffn,\n        use_spec_norm=self._use_spec_norm_ffn,\n        spec_norm_kwargs=self._spec_norm_kwargs)\n\n    attention_cls = functools.partial(\n        SpectralNormalizedMultiHeadAttention,\n        use_spec_norm=self._use_spec_norm_att,\n        spec_norm_kwargs=self._spec_norm_kwargs)\n\n    super().__init__(\n        feedforward_cls=feedforward_cls, attention_cls=attention_cls, **kwargs)\n\n  def call(self, inputs):\n    \"\"\"Overwrites default call function to allow diabling layernorm.\"\"\"\n    if isinstance(inputs, (list, tuple)) and len(inputs) == 2:\n      input_tensor, attention_mask = inputs\n    else:\n      input_tensor, attention_mask = (inputs, None)\n\n    attention_output = self._attention_layer(\n        query=input_tensor, value=input_tensor, attention_mask=attention_mask)\n    attention_output = self._attention_dropout(attention_output)\n    attention_output = input_tensor + attention_output\n    if self._use_layer_norm_att:\n      attention_output = self._attention_layer_norm(attention_output)\n\n    if self._feedforward_block is None:\n      intermediate_output = self._intermediate_dense(attention_output)\n      intermediate_output = self._intermediate_activation_layer(\n          intermediate_output)\n      layer_output = self._output_dense(intermediate_output)\n      layer_output = self._output_dropout(layer_output)\n      # During mixed precision training, attention_output is from layer norm\n      # and is always fp32 for now. Cast layer_output to fp32 for the subsequent\n      # add.\n      layer_output = tf.cast(layer_output, tf.float32)\n      layer_output = self._output_layer_norm(layer_output + attention_output)\n    else:\n      layer_output = self._feedforward_block(attention_output)\n\n    return layer_output\n\n\nclass SpectralNormalizedTransformerEncoder(bert_encoder.EncoderScaffold):\n  \"\"\"Spectral-normalized Transformer Encoder with default embedding layer.\"\"\"\n\n  def __init__(\n      self,\n      use_spec_norm_att: bool = False,\n      use_spec_norm_ffn: bool = False,\n      use_spec_norm_plr: bool = False,\n      use_layer_norm_att: bool = True,\n      use_layer_norm_ffn: bool = True,\n      # A dict of kwargs to pass to the Transformer class.\n      hidden_cfg: Optional[Dict[str, Any]] = None,\n      **kwargs: Mapping[str, Any]):\n    \"\"\"Initializer.\"\"\"\n    hidden_cls = SpectralNormalizedTransformer\n\n    # Add layer normalization arguments to default transformer config.\n    normalization_cfg = {\n        'use_layer_norm_att': use_layer_norm_att,\n        'use_layer_norm_ffn': use_layer_norm_ffn,\n        'use_spec_norm_att': use_spec_norm_att,\n        'use_spec_norm_ffn': use_spec_norm_ffn,\n    }\n\n    if hidden_cfg:\n      hidden_cfg.update(normalization_cfg)\n    else:\n      hidden_cfg = normalization_cfg\n\n    # Intialize default layers.\n    super().__init__(hidden_cls=hidden_cls, hidden_cfg=hidden_cfg, **kwargs)\n\n    # Rebuild BERT model graph using default layers.\n    seq_length = self._embedding_cfg.get('seq_length', None)\n\n    # Create inputs layers.\n    word_ids = tf.keras.layers.Input(\n        shape=(seq_length,), dtype=tf.int32, name='input_word_ids')\n    mask = tf.keras.layers.Input(\n        shape=(seq_length,), dtype=tf.int32, name='input_mask')\n    type_ids = tf.keras.layers.Input(\n        shape=(seq_length,), dtype=tf.int32, name='input_type_ids')\n    inputs = [word_ids, mask, type_ids]\n\n    # Define Input Embeddings Layers.\n    word_embeddings = self._embedding_layer(word_ids)\n    position_embeddings = self._position_embedding_layer(word_embeddings)\n    type_embeddings = self._type_embedding_layer(type_ids)\n\n    embeddings = tf.keras.layers.Add()(\n        [word_embeddings, position_embeddings, type_embeddings])\n    # TODO(jereliu): Add option to disable embedding layer normalization.\n    embeddings = self._embedding_norm_layer(embeddings)\n    embeddings = (\n        tf.keras.layers.Dropout(\n            rate=self._embedding_cfg['dropout_rate'])(embeddings))\n\n    # Define self-attention layers. Rename to match with BERT checkpoint.\n    attention_mask = bert_layers.SelfAttentionMask()([embeddings, mask])\n    data = embeddings\n\n    layer_output_data = []\n    self._hidden_layers = []\n    for i in range(self._num_hidden_instances):\n      layer = hidden_cls(\n          **self._hidden_cfg,\n          name='transformer/layer_%d' % i)  # Rename to match BERT checkpoint.\n      data = layer([data, attention_mask])\n      layer_output_data.append(data)\n      self._hidden_layers.append(layer)\n\n    # Extract BERT encoder output (i.e., the CLS token).\n    first_token_tensor = (\n        tf.keras.layers.Lambda(lambda x: tf.squeeze(x[:, 0:1, :], axis=1))(\n            layer_output_data[-1]))\n\n    # Define the pooler layer (i.e., the output layer), and optionally apply\n    # spectral normalization.\n    self._pooler_layer = tf.keras.layers.Dense(\n        units=self._pooled_output_dim,\n        activation='tanh',\n        kernel_initializer=self._pooler_layer_initializer,\n        name='pooler_transform')\n    if use_spec_norm_plr:\n      self._pooler_layer = ed.layers.SpectralNormalization(\n          self._pooler_layer,\n          inhere_layer_name=True,\n          **hidden_cfg['spec_norm_kwargs'])\n\n    cls_output = self._pooler_layer(first_token_tensor)\n\n    if self._return_all_layer_outputs:\n      outputs = [layer_output_data, cls_output]\n    else:\n      outputs = [layer_output_data[-1], cls_output]\n\n    # Compile model with updated graph.\n    super(bert_encoder.EncoderScaffold, self).__init__(\n        inputs=inputs, outputs=outputs, **self._kwargs)\n\n\ndef get_spectral_normalized_transformer_encoder(\n    bert_config: bert_configs.BertConfig,\n    spec_norm_kwargs: Mapping[str, Any],\n    use_layer_norm_att: bool = True,\n    use_layer_norm_ffn: bool = True,\n    use_spec_norm_att: bool = False,\n    use_spec_norm_ffn: bool = False,\n    use_spec_norm_plr: bool = False) -> SpectralNormalizedTransformerEncoder:\n  \"\"\"Creates a SpectralNormalizedTransformerEncoder from a bert_config.\n\n  Args:\n    bert_config: A 'BertConfig' object.\n    spec_norm_kwargs: Keyword arguments to the spectral normalization layer.\n    use_layer_norm_att: (bool) Whether to apply layer normalization to the\n      attention layer.\n    use_layer_norm_ffn: (bool) Whether to apply layer normalization to the\n      feedforward layer.\n    use_spec_norm_att: (bool) Whether to apply spectral normalization to the\n      attention layer.\n    use_spec_norm_ffn: (bool) Whether to apply spectral normalization to the\n      feedforward layer.\n    use_spec_norm_plr: (bool) Whether to apply spectral normalization to the\n      final pooler layer for CLS token.\n\n  Returns:\n    A SpectralNormalizedTransformerEncoder object.\n  \"\"\"\n  embedding_cfg = dict(\n      vocab_size=bert_config.vocab_size,\n      type_vocab_size=bert_config.type_vocab_size,\n      hidden_size=bert_config.hidden_size,\n      max_seq_length=bert_config.max_position_embeddings,\n      initializer=tf.keras.initializers.TruncatedNormal(\n          stddev=bert_config.initializer_range),\n      dropout_rate=bert_config.hidden_dropout_prob,\n  )\n  hidden_cfg = dict(\n      num_attention_heads=bert_config.num_attention_heads,\n      inner_dim=bert_config.intermediate_size,\n      inner_activation=tf_utils.get_activation(bert_config.hidden_act),\n      dropout_rate=bert_config.hidden_dropout_prob,\n      attention_dropout_rate=bert_config.attention_probs_dropout_prob,\n      kernel_initializer=tf.keras.initializers.TruncatedNormal(\n          stddev=bert_config.initializer_range),\n      spec_norm_kwargs=spec_norm_kwargs,\n  )\n  kwargs = dict(\n      embedding_cfg=embedding_cfg,\n      num_hidden_instances=bert_config.num_hidden_layers,\n      pooled_output_dim=bert_config.hidden_size,\n      pooler_layer_initializer=tf.keras.initializers.TruncatedNormal(\n          stddev=bert_config.initializer_range))\n\n  return SpectralNormalizedTransformerEncoder(\n      use_layer_norm_att=use_layer_norm_att,\n      use_layer_norm_ffn=use_layer_norm_ffn,\n      use_spec_norm_att=use_spec_norm_att,\n      use_spec_norm_ffn=use_spec_norm_ffn,\n      use_spec_norm_plr=use_spec_norm_plr,\n      hidden_cfg=hidden_cfg,\n      **kwargs)\n\n\nclass BertGaussianProcessClassifier(tf.keras.Model):\n  \"\"\"Classifier model based on a Gaussian process with BERT encoder.\"\"\"\n\n  def __init__(self,\n               network: tf.keras.Model,\n               num_classes: int,\n               num_heads: int,\n               gp_layer_kwargs: Dict[str, Any],\n               initializer: Optional[tf.keras.initializers.Initializer] = None,\n               dropout_rate: float = 0.1,\n               use_gp_layer: bool = True,\n               **kwargs: Mapping[str, Any]):\n    \"\"\"Initializer.\n\n    Args:\n      network: A transformer network. This network should output a sequence\n        output and a classification output. Furthermore, it should expose its\n        embedding table via a \"get_embedding_table\" method.\n      num_classes: Number of classes to predict from the classification network.\n      num_heads: Number of additional output heads.\n      gp_layer_kwargs: Keyword arguments to Gaussian process layer.\n      initializer: The initializer (if any) to use in the classification\n        networks. Defaults to a Glorot uniform initializer.\n      dropout_rate: The dropout probability of the cls head.\n      use_gp_layer: Whether to use Gaussian process output layer.\n      **kwargs: Additional keyword arguments.\n    \"\"\"\n    self._self_setattr_tracking = False\n    self._network = network\n    self._config = {\n        'network': network,\n        'num_classes': num_classes,\n        'initializer': initializer,\n        'dropout_rate': dropout_rate,\n        'use_gp_layer': use_gp_layer,\n        'gp_layer_kwargs': gp_layer_kwargs\n    }\n\n    # We want to use the inputs of the passed network as the inputs to this\n    # Model. To do this, we need to keep a handle to the network inputs for use\n    # when we construct the Model object at the end of init.\n    inputs = network.inputs\n\n    # Construct classifier using CLS token of the BERT encoder output.\n    _, cls_output = network(inputs)\n    cls_output = tf.keras.layers.Dropout(rate=dropout_rate)(cls_output)\n\n    # Produce final logits.\n    if use_gp_layer:\n      # We use the stddev=0.05 (i.e., the tf keras default)\n      # for the distribution of the random features instead of stddev=1.\n      # (which is often suggested by the theoretical literature).\n      # The reason is deep BERT model is sensitive to the scaling of the\n      # initializers.\n      self.classifier = ed.layers.RandomFeatureGaussianProcess(\n          units=num_classes,\n          scale_random_features=False,\n          use_custom_random_features=True,\n          kernel_initializer=initializer,\n          custom_random_features_initializer=(\n              tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)),\n          **gp_layer_kwargs)\n    else:\n      self.classifier = bert_encoder.Classification(\n          input_width=cls_output.shape[-1],\n          num_classes=num_classes,\n          initializer=initializer,\n          output='logits',\n          name='sentence_prediction')\n    outputs = self.classifier(cls_output)\n\n    # Build additional heads if num_heads > 1.\n    if num_heads > 1:\n      outputs = [outputs]\n      for head_id in range(1, num_heads):\n        additional_outputs = tf.keras.layers.Dense(\n            num_classes,\n            activation=None,\n            kernel_initializer=initializer,\n            name=f'predictions/transform/logits_{head_id}')(\n                cls_output)\n\n        outputs.append(additional_outputs)\n\n    super().__init__(inputs=inputs, outputs=outputs, **kwargs)\n\n\ndef bert_sngp_model(num_classes,\n                    bert_config,\n                    gp_layer_kwargs,\n                    spec_norm_kwargs,\n                    num_heads=1,\n                    use_gp_layer=True,\n                    use_spec_norm_att=True,\n                    use_spec_norm_ffn=True,\n                    use_layer_norm_att=False,\n                    use_layer_norm_ffn=False,\n                    use_spec_norm_plr=False):\n  \"\"\"Creates a BERT SNGP classifier model.\"\"\"\n  last_layer_initializer = tf.keras.initializers.TruncatedNormal(\n      stddev=bert_config.initializer_range)\n\n  # Build encoder model.\n  sngp_bert_encoder = get_spectral_normalized_transformer_encoder(\n      bert_config,\n      spec_norm_kwargs,\n      use_layer_norm_att=use_layer_norm_att,\n      use_layer_norm_ffn=use_layer_norm_ffn,\n      use_spec_norm_att=use_spec_norm_att,\n      use_spec_norm_ffn=use_spec_norm_ffn,\n      use_spec_norm_plr=use_spec_norm_plr)\n\n  # Build classification model.\n  sngp_bert_model = BertGaussianProcessClassifier(\n      sngp_bert_encoder,\n      num_classes=num_classes,\n      num_heads=num_heads,\n      initializer=last_layer_initializer,\n      dropout_rate=bert_config.hidden_dropout_prob,\n      use_gp_layer=use_gp_layer,\n      gp_layer_kwargs=gp_layer_kwargs)\n\n  return sngp_bert_model, sngp_bert_encoder\n", "framework": "tensorflow"}
{"repo_name": "google/uncertainty-baselines", "file_path": "baselines/jft/bit_deterministic.py", "content": "# coding=utf-8\n# Copyright 2022 The Uncertainty Baselines Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Deterministic BiT ResNet on JFT-300M.\"\"\"\n\nfrom functools import partial  # pylint: disable=g-importing-member so standard\nimport multiprocessing\nimport numbers\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom clu import metric_writers\nfrom clu import parameter_overview\nfrom clu import periodic_actions\nfrom clu import preprocess_spec\nimport flax\nimport flax.jax_utils as flax_utils\nimport jax\nimport jax.numpy as jnp\nfrom ml_collections.config_flags import config_flags\nimport numpy as np\nimport robustness_metrics as rm\n\nimport tensorflow as tf\nfrom tensorflow.io import gfile\nimport uncertainty_baselines as ub\nimport checkpoint_utils  # local file import from baselines.jft\nimport data_uncertainty_utils  # local file import from baselines.jft\nimport input_utils  # local file import from baselines.jft\nimport ood_utils  # local file import from baselines.jft\nimport preprocess_utils  # local file import from baselines.jft\nimport train_utils  # local file import from baselines.jft\n\n# TODO(dusenberrymw): Open-source remaining imports.\nfewshot = None\n\n\nconfig_flags.DEFINE_config_file(\n    'config', None, 'Training configuration.', lock_config=True)\nflags.DEFINE_string('output_dir', default=None, help='Work unit directory.')\nflags.DEFINE_integer(\n    'num_cores', default=None, help='Unused. How many devices being used.')\nflags.DEFINE_boolean(\n    'use_gpu', default=None, help='Unused. Whether or not running on GPU.')\nflags.DEFINE_string('tpu', None,\n                    'Unused. Name of the TPU. Only used if use_gpu is False.')\n\nFLAGS = flags.FLAGS\n\n\ndef fetch_temperature(config, replicated_params):\n  if config.model.temperature > 0:\n    return config.model.temperature\n  lower = config.model.temperature_lower_bound\n  upper = config.model.temperature_upper_bound\n  t = jax.nn.sigmoid(\n      replicated_params['temp_layer']['temperature_pre_sigmoid'][0])\n  return float((upper - lower) * t + lower)\n\n\ndef main(config, output_dir):\n\n  seed = config.get('seed', 0)\n  rng = jax.random.PRNGKey(seed)\n  tf.random.set_seed(seed)\n\n  if config.get('data_dir'):\n    logging.info('data_dir=%s', config.data_dir)\n  logging.info('Output dir: %s', output_dir)\n\n  save_checkpoint_path = None\n  if config.get('checkpoint_steps'):\n    gfile.makedirs(output_dir)\n    save_checkpoint_path = os.path.join(output_dir, 'checkpoint.npz')\n\n  # Create an asynchronous multi-metric writer.\n  writer = metric_writers.create_default_writer(\n      output_dir, just_logging=jax.process_index() > 0)\n\n  # The pool is used to perform misc operations such as logging in async way.\n  pool = multiprocessing.pool.ThreadPool()\n\n  def write_note(note):\n    if jax.host_id() == 0:\n      logging.info('NOTE: %s', note)\n  write_note('Initializing...')\n\n  # Verify settings to make sure no checkpoints are accidentally missed.\n  if config.get('keep_checkpoint_steps'):\n    assert config.get('checkpoint_steps'), 'Specify `checkpoint_steps`.'\n    assert config.keep_checkpoint_steps % config.checkpoint_steps == 0, (\n        f'`keep_checkpoint_steps` ({config.checkpoint_steps}) should be'\n        f'divisible by `checkpoint_steps ({config.checkpoint_steps}).`')\n\n  batch_size = config.batch_size\n  batch_size_eval = config.get('batch_size_eval', batch_size)\n  if (batch_size % jax.device_count() != 0 or\n      batch_size_eval % jax.device_count() != 0):\n    raise ValueError(f'Batch sizes ({batch_size} and {batch_size_eval}) must '\n                     f'be divisible by device number ({jax.device_count()})')\n\n  local_batch_size = batch_size // jax.host_count()\n  local_batch_size_eval = batch_size_eval // jax.host_count()\n  logging.info(\n      'Global batch size %d on %d hosts results in %d local batch size. '\n      'With %d devices per host (%d devices total), that\\'s a %d per-device '\n      'batch size.',\n      batch_size, jax.host_count(), local_batch_size,\n      jax.local_device_count(), jax.device_count(),\n      local_batch_size // jax.local_device_count())\n\n  write_note('Initializing train dataset...')\n  rng, train_ds_rng = jax.random.split(rng)\n  train_ds_rng = jax.random.fold_in(train_ds_rng, jax.process_index())\n  train_ds = input_utils.get_data(\n      dataset=config.dataset,\n      split=config.train_split,\n      rng=train_ds_rng,\n      process_batch_size=local_batch_size,\n      preprocess_fn=preprocess_spec.parse(\n          spec=config.pp_train, available_ops=preprocess_utils.all_ops()),\n      shuffle_buffer_size=config.shuffle_buffer_size,\n      prefetch_size=config.get('prefetch_to_host', 2),\n      data_dir=config.get('data_dir'))\n\n  write_note('Initializing val dataset(s)...')\n\n  def _get_val_split(dataset, split, pp_eval, data_dir=None):\n    # We do ceil rounding such that we include the last incomplete batch.\n    nval_img = input_utils.get_num_examples(\n        dataset,\n        split=split,\n        process_batch_size=local_batch_size_eval,\n        drop_remainder=False,\n        data_dir=data_dir)\n    val_steps = int(np.ceil(nval_img / batch_size_eval))\n    logging.info('Running validation for %d steps for %s, %s', val_steps,\n                 dataset, split)\n\n    if isinstance(pp_eval, str):\n      pp_eval = preprocess_spec.parse(\n          spec=pp_eval, available_ops=preprocess_utils.all_ops())\n\n    val_ds = input_utils.get_data(\n        dataset=dataset,\n        split=split,\n        rng=None,\n        process_batch_size=local_batch_size_eval,\n        preprocess_fn=pp_eval,\n        cache=config.get('val_cache', 'batched'),\n        num_epochs=1,\n        repeat_after_batching=True,\n        shuffle=False,\n        prefetch_size=config.get('prefetch_to_host', 2),\n        drop_remainder=False,\n        data_dir=data_dir)\n\n    return val_ds\n\n  val_ds_splits = {\n      'val':\n          _get_val_split(\n              config.dataset,\n              split=config.val_split,\n              pp_eval=config.pp_eval,\n              data_dir=config.get('data_dir'))\n  }\n\n  if config.get('test_split'):\n    val_ds_splits.update({\n        'test':\n            _get_val_split(\n                config.dataset,\n                split=config.test_split,\n                pp_eval=config.pp_eval,\n                data_dir=config.get('data_dir'))\n    })\n\n  if config.get('eval_on_cifar_10h'):\n    cifar10_to_cifar10h_fn = data_uncertainty_utils.create_cifar10_to_cifar10h_fn(\n        config.get('data_dir', None))\n    preprocess_fn = preprocess_spec.parse(\n        spec=config.pp_eval_cifar_10h, available_ops=preprocess_utils.all_ops())\n    pp_eval = lambda ex: preprocess_fn(cifar10_to_cifar10h_fn(ex))\n    val_ds_splits['cifar_10h'] = _get_val_split(\n        'cifar10',\n        split=config.get('cifar_10h_split') or 'test',\n        pp_eval=pp_eval,\n        data_dir=config.get('data_dir'))\n  elif config.get('eval_on_imagenet_real'):\n    imagenet_to_real_fn = data_uncertainty_utils.create_imagenet_to_real_fn()\n    preprocess_fn = preprocess_spec.parse(\n        spec=config.pp_eval_imagenet_real,\n        available_ops=preprocess_utils.all_ops())\n    pp_eval = lambda ex: preprocess_fn(imagenet_to_real_fn(ex))  # pytype: disable=wrong-arg-types\n    val_ds_splits['imagenet_real'] = _get_val_split(\n        'imagenet2012_real',\n        split=config.get('imagenet_real_split') or 'validation',\n        pp_eval=pp_eval,\n        data_dir=config.get('data_dir'))\n\n  ood_ds = {}\n  if config.get('ood_datasets') and config.get('ood_methods'):\n    if config.get('ood_methods'):  #  config.ood_methods is not a empty list\n      logging.info('loading OOD dataset = %s', config.get('ood_datasets'))\n      ood_ds, ood_ds_names = ood_utils.load_ood_datasets(\n          config.dataset,\n          config.ood_datasets,\n          config.ood_split,\n          config.pp_eval,\n          config.pp_eval_ood,\n          config.ood_methods,\n          config.train_split,\n          config.get('data_dir'),\n          _get_val_split,\n      )\n\n  ntrain_img = input_utils.get_num_examples(\n      config.dataset,\n      split=config.train_split,\n      process_batch_size=local_batch_size,\n      data_dir=config.get('data_dir'))\n  steps_per_epoch = int(ntrain_img / batch_size)\n\n  if config.get('num_epochs'):\n    total_steps = int(config.num_epochs * steps_per_epoch)\n    assert not config.get('total_steps'), 'Set either num_epochs or total_steps'\n  else:\n    total_steps = config.total_steps\n\n  logging.info('Total train data points: %d', ntrain_img)\n  logging.info(\n      'Running for %d steps, that means %f epochs and %d steps per epoch',\n      total_steps, total_steps * batch_size / ntrain_img, steps_per_epoch)\n\n  write_note('Initializing model...')\n  logging.info('config.model = %s', config.get('model'))\n  model = ub.models.bit_resnet(\n      num_classes=config.num_classes, **config.get('model', {}))\n\n  # We want all parameters to be created in host RAM, not on any device, they'll\n  # be sent there later as needed, otherwise we already encountered two\n  # situations where we allocate them twice.\n  @partial(jax.jit, backend='cpu')\n  def init(rng):\n    image_size = tuple(train_ds.element_spec['image'].shape[2:])\n    logging.info('image_size = %s', image_size)\n    dummy_input = jnp.zeros((local_batch_size,) + image_size, jnp.float32)\n    params = flax.core.unfreeze(model.init(rng, dummy_input,\n                                           train=False))['params']\n\n    # Set bias in the head to a low value, such that loss is small initially.\n    params['head']['bias'] = jnp.full_like(\n        params['head']['bias'], config.get('init_head_bias', 0))\n\n    # init head kernel to all zeros for fine-tuning\n    if config.get('model_init'):\n      params['head']['kernel'] = jnp.full_like(params['head']['kernel'], 0)\n\n    return params\n\n  rng, rng_init = jax.random.split(rng)\n  params_cpu = init(rng_init)\n\n  if jax.host_id() == 0:\n    num_params = sum(p.size for p in jax.tree_flatten(params_cpu)[0])\n    parameter_overview.log_parameter_overview(params_cpu)\n    writer.write_scalars(step=0, scalars={'num_params': num_params})\n\n  @partial(jax.pmap, axis_name='batch')\n  def evaluation_fn(params, images, labels, mask):\n    # Ignore the entries with all zero labels for evaluation.\n    mask *= labels.max(axis=1)\n    logits, out = model.apply({'params': flax.core.freeze(params)},\n                              images,\n                              train=False)\n\n    losses = getattr(train_utils, config.get('loss', 'sigmoid_xent'))(\n        logits=logits, labels=labels, reduction=False)\n    loss = jax.lax.psum(losses * mask, axis_name='batch')\n\n    top1_idx = jnp.argmax(logits, axis=1)\n    # Extracts the label at the highest logit index for each image.\n    top1_correct = jnp.take_along_axis(labels, top1_idx[:, None], axis=1)[:, 0]\n    ncorrect = jax.lax.psum(top1_correct * mask, axis_name='batch')\n    n = jax.lax.psum(mask, axis_name='batch')\n\n    metric_args = jax.lax.all_gather([logits, labels, out['pre_logits'], mask],\n                                     axis_name='batch')\n    return ncorrect, loss, n, metric_args\n\n  @partial(jax.pmap, axis_name='batch')\n  def cifar_10h_evaluation_fn(params, images, labels, mask):\n    logits, out = model.apply({'params': flax.core.freeze(params)},\n                              images,\n                              train=False)\n\n    losses = getattr(train_utils, config.get('loss', 'softmax_xent'))(\n        logits=logits, labels=labels, reduction=False)\n    loss = jax.lax.psum(losses, axis_name='batch')\n\n    top1_idx = jnp.argmax(logits, axis=1)\n    # Extracts the label at the highest logit index for each image.\n    one_hot_labels = jnp.eye(10)[jnp.argmax(labels, axis=1)]\n\n    top1_correct = jnp.take_along_axis(\n        one_hot_labels, top1_idx[:, None], axis=1)[:, 0]\n    ncorrect = jax.lax.psum(top1_correct, axis_name='batch')\n    n = jax.lax.psum(one_hot_labels, axis_name='batch')\n\n    metric_args = jax.lax.all_gather([logits, labels, out['pre_logits'], mask],\n                                     axis_name='batch')\n    return ncorrect, loss, n, metric_args\n\n  # Setup function for computing representation.\n  @partial(jax.pmap, axis_name='batch')\n  def representation_fn(params, images, labels, mask):\n    _, outputs = model.apply({'params': flax.core.freeze(params)},\n                             images,\n                             train=False)\n    representation = outputs[config.fewshot.representation_layer]\n    representation = jax.lax.all_gather(representation, 'batch')\n    labels = jax.lax.all_gather(labels, 'batch')\n    mask = jax.lax.all_gather(mask, 'batch')\n    return representation, labels, mask\n\n  # Load the optimizer from flax.\n  opt_name = config.get('optim_name')\n  write_note(f'Initializing {opt_name} optimizer...')\n  opt_def = getattr(flax.optim, opt_name)(**config.get('optim', {}))\n\n  # We jit this, such that the arrays that are created are created on the same\n  # device as the input is, in this case the CPU. Else they'd be on device[0].\n  opt_cpu = jax.jit(opt_def.create)(params_cpu)\n\n  @partial(jax.pmap, axis_name='batch', donate_argnums=(0,))\n  def update_fn(opt, lr, images, labels, rng):\n    \"\"\"Update step.\"\"\"\n\n    measurements = {}\n\n    # Get device-specific loss rng.\n    rng, rng_model = jax.random.split(rng, 2)\n    rng_model_local = jax.random.fold_in(rng_model, jax.lax.axis_index('batch'))\n\n    def loss_fn(params, images, labels):\n      logits, _ = model.apply(\n          {'params': flax.core.freeze(params)}, images,\n          train=True, rngs={'dropout': rng_model_local})\n      accuracy = jnp.mean(jnp.equal(\n          jnp.argmax(logits, axis=-1),\n          jnp.argmax(labels, axis=-1)))\n      return getattr(train_utils, config.get('loss', 'sigmoid_xent'))(\n          logits=logits, labels=labels), accuracy\n\n    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n    (l, train_accuracy), g = grad_fn(opt.target, images, labels)\n    l, g = jax.lax.pmean((l, g), axis_name='batch')\n    measurements['accuracy'] = train_accuracy\n\n    # Log the gradient norm only if we need to compute it anyways (clipping)\n    # or if we don't use grad_accum_steps, as they interact badly.\n    if config.get('grad_accum_steps', 1) == 1 or config.get('grad_clip_norm'):\n      grads, _ = jax.tree_flatten(g)\n      l2_g = jnp.sqrt(sum([jnp.vdot(p, p) for p in grads]))\n      measurements['l2_grads'] = l2_g\n\n    # Optionally resize the global gradient to a maximum norm. We found this\n    # useful in some cases across optimizers, hence it's in the main loop.\n    if config.get('grad_clip_norm'):\n      g_factor = jnp.minimum(1.0, config.grad_clip_norm / l2_g)\n      g = jax.tree_util.tree_map(lambda p: g_factor * p, g)\n    opt = opt.apply_gradient(g, learning_rate=lr)\n\n    decay_rules = config.get('weight_decay', []) or []\n    if isinstance(decay_rules, numbers.Number):\n      decay_rules = [('.*kernel.*', decay_rules)]\n    sched_m = lr\n    def decay_fn(v, wd):\n      return (1.0 - sched_m * wd) * v\n\n    opt = opt.replace(\n        target=train_utils.tree_map_with_regex(decay_fn, opt.target,\n                                               decay_rules))\n\n    params, _ = jax.tree_flatten(opt.target)\n    measurements['l2_params'] = jnp.sqrt(sum([jnp.vdot(p, p) for p in params]))\n\n    return opt, l, rng, measurements\n\n  # Other things besides optimizer state to be stored.\n  rng, rng_loop = jax.random.split(rng, 2)\n  rngs_loop = flax_utils.replicate(rng_loop)\n  checkpoint_extra = dict(accum_train_time=0.0, rngs_loop=rngs_loop)\n\n  # Decide how to initialize training. The order is important.\n  # 1. Always resumes from the existing checkpoint, e.g. resumes a finetune job.\n  # 2. Resume from a previous checkpoint, e.g. start a cooldown training job.\n  # 3. Initialize model from something, e,g, start a fine-tuning job.\n  # 4. Train from scratch.\n  resume_checkpoint_path = None\n  if save_checkpoint_path and gfile.exists(save_checkpoint_path):\n    resume_checkpoint_path = save_checkpoint_path\n  elif config.get('resume'):\n    resume_checkpoint_path = config.resume\n  if resume_checkpoint_path:\n    write_note('Resume training from checkpoint...')\n    checkpoint_tree = {'opt': opt_cpu, 'extra': checkpoint_extra}\n    checkpoint = checkpoint_utils.load_checkpoint(checkpoint_tree,\n                                                  resume_checkpoint_path)\n    opt_cpu, checkpoint_extra = checkpoint['opt'], checkpoint['extra']\n    rngs_loop = checkpoint_extra['rngs_loop']\n  elif config.get('model_init'):\n    write_note(f'Initialize model from {config.model_init}...')\n    reinit_params = config.get('model_reinit_params',\n                               ('head/kernel', 'head/bias'))\n    logging.info('Reinitializing these parameters: %s', reinit_params)\n    # We only support \"no head\" fine-tuning for now.\n    loaded_params = checkpoint_utils.load_checkpoint(tree=None,\n                                                     path=config.model_init)\n    loaded = checkpoint_utils.restore_from_pretrained_params(\n        params_cpu,\n        loaded_params,\n        model_representation_size=None,\n        model_classifier=None,\n        reinit_params=reinit_params)\n    opt_cpu = opt_cpu.replace(target=loaded)\n    if jax.host_id() == 0:\n      logging.info('Restored parameter overview:')\n      parameter_overview.log_parameter_overview(loaded)\n\n  write_note('Kicking off misc stuff...')\n  first_step = int(opt_cpu.state.step)  # Might be a DeviceArray type.\n  logging.info('first_step = %s', first_step)\n  if first_step == 0 and jax.host_id() == 0:\n    writer.write_hparams(dict(config))\n\n  chrono = train_utils.Chrono(first_step, total_steps, batch_size,\n                              checkpoint_extra['accum_train_time'])\n\n  # Note: switch to ProfileAllHosts() if you need to profile all hosts.\n  # (Xprof data become much larger and take longer to load for analysis)\n  profiler = periodic_actions.Profile(\n      # Create profile after every restart to analyze pre-emption related\n      # problems and assure we get similar performance in every run.\n      logdir=output_dir, first_profile=first_step + 10)\n\n  # TODO(dusenberrymw): Remove manual replication by updating pmap axes.\n  write_note(f'Replicating...\\n{chrono.note}')\n  opt_repl = flax_utils.replicate(opt_cpu)\n\n  write_note(f'Initializing few-shotters...\\n{chrono.note}')\n  fewshotter = None\n  if 'fewshot' in config and fewshot is not None:\n    fewshotter = fewshot.FewShotEvaluator(\n        representation_fn, config.fewshot,\n        config.fewshot.get('batch_size') or batch_size_eval)\n\n  checkpoint_writer = None\n\n  lr_fn = train_utils.create_learning_rate_schedule(total_steps,\n                                                    **config.get('lr', {}))\n\n  # Prefetch all iterators, starting at the current first step.\n  if first_step > 0:\n    write_note('Advancing the dataset after resuming from a checkpoint...')\n    # TODO(dusenberrymw): Look into checkpointing dataset state instead.\n    train_ds = train_ds.skip(first_step)\n\n  # TODO(dusenberrymw): According to flax docs, prefetching shouldn't be\n  # necessary for TPUs.\n  lr_iter = train_utils.prefetch_scalar(\n      map(lr_fn, range(first_step, total_steps)),\n      config.get('prefetch_to_device', 1))\n  train_iter = input_utils.start_input_pipeline(\n      train_ds, config.get('prefetch_to_device', 1))\n\n  # Note: we return the train loss, val loss, and fewshot best l2s for use in\n  # reproducibility unit tests.\n  train_loss = -jnp.inf\n  val_loss = {val_name: -jnp.inf for val_name, _ in val_ds_splits.items()}\n  fewshot_results = {'dummy': {(0, 1): -jnp.inf}}\n\n  write_note(f'First step compilations...\\n{chrono.note}')\n  for step in range(first_step + 1, total_steps + 1):\n    with jax.profiler.StepTraceAnnotation('train_step', step_num=step):\n      train_batch = next(train_iter)\n      lr_repl = next(lr_iter)\n      opt_repl, loss_value, rngs_loop, extra_measurements = update_fn(\n          opt_repl,\n          lr_repl,\n          train_batch['image'],\n          train_batch['labels'],\n          rng=rngs_loop)\n\n    if jax.host_id() == 0:\n      profiler(step)\n\n    # Checkpoint saving\n    if train_utils.itstime(\n        step, config.get('checkpoint_steps'), total_steps, process=0):\n      write_note('Checkpointing...')\n      chrono.pause()\n      train_utils.checkpointing_timeout(checkpoint_writer,\n                                        config.get('checkpoint_timeout', 1))\n      checkpoint_extra['accum_train_time'] = chrono.accum_train_time\n      checkpoint_extra['rngs_loop'] = rngs_loop\n      # We need to transfer the weights over now or else we risk keeping them\n      # alive while they'll be updated in a future step, creating hard to debug\n      # memory errors (see b/160593526). Also, takes device 0's params only.\n      opt_cpu = jax.tree_util.tree_map(lambda x: np.array(x[0]), opt_repl)\n\n      # Check whether we want to keep a copy of the current checkpoint.\n      copy_step = None\n      if train_utils.itstime(step, config.get('keep_checkpoint_steps'),\n                             total_steps):\n        write_note('Keeping a checkpoint copy...')\n        copy_step = step\n\n      # Checkpoint should be a nested dictionary or FLAX datataclasses from\n      # `flax.struct`. Both can be present in a checkpoint.\n      checkpoint = {'opt': opt_cpu, 'extra': checkpoint_extra}\n      checkpoint_writer = pool.apply_async(\n          checkpoint_utils.save_checkpoint,\n          (checkpoint, save_checkpoint_path, copy_step))\n      chrono.resume()\n\n    # Report training progress\n    if train_utils.itstime(\n        step, config.log_training_steps, total_steps, process=0):\n      write_note('Reporting training progress...')\n      train_accuracy = extra_measurements['accuracy']\n      train_accuracy = jnp.mean(train_accuracy)\n      train_loss = loss_value[0]  # Keep to return for reproducibility tests.\n      timing_measurements, note = chrono.tick(step)\n      write_note(note)\n      train_measurements = {}\n      train_measurements.update({\n          'learning_rate': lr_repl[0],\n          'training_loss': train_loss,\n          'training_accuracy': train_accuracy,\n          'temperature': fetch_temperature(config, opt_repl.target)\n      })\n      train_measurements.update(flax.jax_utils.unreplicate(extra_measurements))\n      train_measurements.update(timing_measurements)\n      writer.write_scalars(step, train_measurements)\n\n    # Report validation performance\n    if train_utils.itstime(step, config.log_eval_steps, total_steps):\n      write_note('Evaluating on the validation set...')\n      chrono.pause()\n      for val_name, val_ds in val_ds_splits.items():\n        # Sets up evaluation metrics.\n        ece_num_bins = config.get('ece_num_bins', 15)\n        auc_num_bins = config.get('auc_num_bins', 1000)\n        ece = rm.metrics.ExpectedCalibrationError(num_bins=ece_num_bins)\n        calib_auc = rm.metrics.CalibrationAUC(correct_pred_as_pos_label=False)\n        oc_auc_0_5 = rm.metrics.OracleCollaborativeAUC(oracle_fraction=0.005,\n                                                       num_bins=auc_num_bins)\n        oc_auc_1 = rm.metrics.OracleCollaborativeAUC(oracle_fraction=0.01,\n                                                     num_bins=auc_num_bins)\n        oc_auc_2 = rm.metrics.OracleCollaborativeAUC(oracle_fraction=0.02,\n                                                     num_bins=auc_num_bins)\n        oc_auc_5 = rm.metrics.OracleCollaborativeAUC(oracle_fraction=0.05,\n                                                     num_bins=auc_num_bins)\n        label_diversity = tf.keras.metrics.Mean()\n        sample_diversity = tf.keras.metrics.Mean()\n        ged = tf.keras.metrics.Mean()\n\n        # Runs evaluation loop.\n        val_iter = input_utils.start_input_pipeline(\n            val_ds, config.get('prefetch_to_device', 1))\n        ncorrect, loss, nseen = 0, 0, 0\n        for batch in val_iter:\n          if val_name == 'cifar_10h':\n            batch_ncorrect, batch_losses, batch_n, batch_metric_args = (\n                cifar_10h_evaluation_fn(opt_repl.target, batch['image'],\n                                        batch['labels'], batch['mask']))\n          else:\n            batch_ncorrect, batch_losses, batch_n, batch_metric_args = (\n                evaluation_fn(opt_repl.target, batch['image'],\n                              batch['labels'], batch['mask']))\n          # All results are a replicated array shaped as follows:\n          # (local_devices, per_device_batch_size, elem_shape...)\n          # with each local device's entry being identical as they got psum'd.\n          # So let's just take the first one to the host as numpy.\n          ncorrect += np.sum(np.array(batch_ncorrect[0]))\n          loss += np.sum(np.array(batch_losses[0]))\n          nseen += np.sum(np.array(batch_n[0]))\n          if config.get('loss', 'sigmoid_xent') != 'sigmoid_xent':\n            # Here we parse batch_metric_args to compute uncertainty metrics.\n            # (e.g., ECE or Calibration AUC).\n            logits, labels, _, masks = batch_metric_args\n            masks = np.array(masks[0], dtype=np.bool)\n            logits = np.array(logits[0])\n            probs = jax.nn.softmax(logits)\n            # From one-hot to integer labels, as required by ECE.\n            int_labels = np.argmax(np.array(labels[0]), axis=-1)\n            int_preds = np.argmax(logits, axis=-1)\n            confidence = np.max(probs, axis=-1)\n            for p, c, l, d, m, label in zip(probs, confidence, int_labels,\n                                            int_preds, masks, labels[0]):\n              ece.add_batch(p[m, :], label=l[m])\n              calib_auc.add_batch(d[m], label=l[m], confidence=c[m])\n              # TODO(jereliu): Extend to support soft multi-class probabilities.\n              oc_auc_0_5.add_batch(d[m], label=l[m], custom_binning_score=c[m])\n              oc_auc_1.add_batch(d[m], label=l[m], custom_binning_score=c[m])\n              oc_auc_2.add_batch(d[m], label=l[m], custom_binning_score=c[m])\n              oc_auc_5.add_batch(d[m], label=l[m], custom_binning_score=c[m])\n\n              if val_name == 'cifar_10h' or val_name == 'imagenet_real':\n                batch_label_diversity, batch_sample_diversity, batch_ged = data_uncertainty_utils.generalized_energy_distance(\n                    label[m], p[m, :], config.num_classes)\n                label_diversity.update_state(batch_label_diversity)\n                sample_diversity.update_state(batch_sample_diversity)\n                ged.update_state(batch_ged)\n\n        val_loss[val_name] = loss / nseen  # Keep for reproducibility tests.\n        val_measurements = {\n            f'{val_name}_prec@1': ncorrect / nseen,\n            f'{val_name}_loss': val_loss[val_name],\n        }\n        if config.get('loss', 'sigmoid_xent') != 'sigmoid_xent':\n          val_measurements[f'{val_name}_ece'] = ece.result()['ece']\n          val_measurements[f'{val_name}_calib_auc'] = calib_auc.result()[\n              'calibration_auc']\n          val_measurements[f'{val_name}_oc_auc_0.5%'] = oc_auc_0_5.result()[\n              'collaborative_auc']\n          val_measurements[f'{val_name}_oc_auc_1%'] = oc_auc_1.result()[\n              'collaborative_auc']\n          val_measurements[f'{val_name}_oc_auc_2%'] = oc_auc_2.result()[\n              'collaborative_auc']\n          val_measurements[f'{val_name}_oc_auc_5%'] = oc_auc_5.result()[\n              'collaborative_auc']\n        writer.write_scalars(step, val_measurements)\n\n        if val_name == 'cifar_10h' or val_name == 'imagenet_real':\n          cifar_10h_measurements = {\n              f'{val_name}_label_diversity': label_diversity.result(),\n              f'{val_name}_sample_diversity': sample_diversity.result(),\n              f'{val_name}_ged': ged.result(),\n          }\n          writer.write_scalars(step, cifar_10h_measurements)\n\n      # OOD eval\n      # Entries in the ood_ds dict include:\n      # (ind_dataset, ood_dataset1, ood_dataset2, ...).\n      # OOD metrics are computed using ind_dataset paired with each of the\n      # ood_dataset. When Mahalanobis distance method is applied, train_ind_ds\n      # is also included in the ood_ds.\n      if ood_ds and config.ood_methods:\n        ood_measurements = ood_utils.eval_ood_metrics(\n            ood_ds,\n            ood_ds_names,\n            config.ood_methods,\n            evaluation_fn,\n            opt_repl.target,\n            n_prefetch=config.get('prefetch_to_device', 1))\n        writer.write_scalars(step, ood_measurements)\n      chrono.resume()\n\n    if 'fewshot' in config and fewshotter is not None:\n      # Compute few-shot on-the-fly evaluation.\n      if train_utils.itstime(step, config.fewshot.log_steps, total_steps):\n        chrono.pause()\n        write_note(f'Few-shot evaluation...\\n{chrono.note}')\n        # Keep `results` to return for reproducibility tests.\n        fewshot_results, best_l2 = fewshotter.run_all(opt_repl.target,\n                                                      config.fewshot.datasets)\n\n        # TODO(dusenberrymw): Remove this once fewshot.py is updated.\n        def make_writer_measure_fn(step):\n\n          def writer_measure(name, value):\n            writer.write_scalars(step, {name: value})\n\n          return writer_measure\n\n        fewshotter.walk_results(\n            make_writer_measure_fn(step), fewshot_results, best_l2)\n        chrono.resume()\n\n    # End of step.\n    if config.get('testing_failure_step'):\n      # Break early to simulate infra failures in test cases.\n      if config.testing_failure_step == step:\n        break\n\n  write_note(f'Done!\\n{chrono.note}')\n  pool.close()\n  pool.join()\n  writer.close()\n\n  # Return final training loss, validation loss, and fewshot results for\n  # reproducibility test cases.\n  return train_loss, val_loss, fewshot_results\n\n\nif __name__ == '__main__':\n  # Adds jax flags to the program.\n  jax.config.config_with_absl()\n\n  # TODO(dusenberrymw): Refactor `main` such that there is a `train_eval`\n  # function that returns values for tests and does not directly access flags,\n  # and then have `main` return None.\n\n  def _main(argv):\n    del argv\n    config = FLAGS.config\n    output_dir = FLAGS.output_dir\n    main(config, output_dir)\n\n  app.run(_main)  # Ignore the returned values from `main`.\n", "framework": "tensorflow"}
{"repo_name": "google/uncertainty-baselines", "file_path": "baselines/imagenet/mimo.py", "content": "# coding=utf-8\n# Copyright 2022 The Uncertainty Baselines Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Multiheaded ResNet-50.\"\"\"\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport robustness_metrics as rm\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport uncertainty_baselines as ub\nimport utils  # local file import from baselines.imagenet\nfrom tensorboard.plugins.hparams import api as hp\n\nflags.DEFINE_integer('ensemble_size', 2, 'Size of ensemble.')\nflags.DEFINE_float('input_repetition_probability', 0.6,\n                   'The probability that the inputs are identical for the'\n                   'ensemble members.')\nflags.DEFINE_integer('batch_repetitions', 2, 'Number of times an example is'\n                     'repeated in a training batch. More repetitions lead to'\n                     'lower variance gradients and increased training time.')\nflags.DEFINE_integer('width_multiplier', 1, 'Integer to multiply the number of'\n                     'typical filters by. \"k\" in ResNet-n-k.')\nflags.DEFINE_integer('per_core_batch_size', 128, 'Batch size per TPU core/GPU.')\nflags.DEFINE_integer('seed', 0, 'Random seed.')\nflags.DEFINE_float('base_learning_rate', 0.1,\n                   'Base learning rate when train batch size is 256.')\nflags.DEFINE_float('one_minus_momentum', 0.1, 'Optimizer momentum.')\nflags.DEFINE_integer(\n    'lr_warmup_epochs', 5,\n    'Number of epochs for a linear warmup to the initial learning rate.')\nflags.DEFINE_list('lr_decay_epochs', ['30', '60', '80'],\n                  'Epochs to decay learning rate by.')\nflags.DEFINE_float('l2', 1e-4, 'L2 coefficient.')\nflags.DEFINE_string('data_dir', None, 'Path to training and testing data.')\nflags.DEFINE_string('output_dir', '/tmp/imagenet',\n                    'The directory where the model weights and '\n                    'training/evaluation summaries are stored.')\nflags.DEFINE_integer('train_epochs', 150, 'Number of training epochs.')\nflags.DEFINE_integer('checkpoint_interval', -1,\n                     'Number of epochs between saving checkpoints. Use -1 to '\n                     'never save checkpoints.')\nflags.DEFINE_integer('num_bins', 15, 'Number of bins for ECE computation.')\n\n# Accelerator flags.\nflags.DEFINE_bool('use_gpu', False, 'Whether to run on GPU or otherwise TPU.')\nflags.DEFINE_bool('use_bfloat16', True, 'Whether to use mixed precision.')\nflags.DEFINE_integer('num_cores', 32, 'Number of TPU cores or number of GPUs.')\nflags.DEFINE_string('tpu', None,\n                    'Name of the TPU. Only used if use_gpu is False.')\nFLAGS = flags.FLAGS\n\n# Number of images in ImageNet-1k train dataset.\nAPPROX_IMAGENET_TRAIN_IMAGES = 1281167\n# Number of images in eval dataset.\nIMAGENET_VALIDATION_IMAGES = 50000\nNUM_CLASSES = 1000\n\n\ndef main(argv):\n  del argv  # unused arg\n  tf.io.gfile.makedirs(FLAGS.output_dir)\n  logging.info('Saving checkpoints at %s', FLAGS.output_dir)\n  tf.random.set_seed(FLAGS.seed)\n\n  train_batch_size = (FLAGS.per_core_batch_size * FLAGS.num_cores\n                      // FLAGS.batch_repetitions)\n  test_batch_size = FLAGS.per_core_batch_size * FLAGS.num_cores\n  steps_per_epoch = APPROX_IMAGENET_TRAIN_IMAGES // train_batch_size\n  steps_per_eval = IMAGENET_VALIDATION_IMAGES // test_batch_size\n\n  data_dir = FLAGS.data_dir\n  if FLAGS.use_gpu:\n    logging.info('Use GPU')\n    strategy = tf.distribute.MirroredStrategy()\n  else:\n    logging.info('Use TPU at %s',\n                 FLAGS.tpu if FLAGS.tpu is not None else 'local')\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.TPUStrategy(resolver)\n\n  # TODO(dusenberrymw,zmariet): Add a validation dataset.\n  train_builder = ub.datasets.ImageNetDataset(\n      split=tfds.Split.TRAIN,\n      use_bfloat16=FLAGS.use_bfloat16,\n      data_dir=data_dir)\n  train_dataset = train_builder.load(\n      batch_size=train_batch_size, strategy=strategy)\n  test_builder = ub.datasets.ImageNetDataset(\n      split=tfds.Split.TEST, use_bfloat16=FLAGS.use_bfloat16, data_dir=data_dir)\n  test_dataset = test_builder.load(\n      batch_size=test_batch_size, strategy=strategy)\n\n  if FLAGS.use_bfloat16:\n    tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n\n  with strategy.scope():\n    logging.info('Building Keras ResNet-50 model')\n    model = ub.models.resnet50_mimo(\n        input_shape=(FLAGS.ensemble_size, 224, 224, 3),\n        num_classes=NUM_CLASSES,\n        ensemble_size=FLAGS.ensemble_size,\n        width_multiplier=FLAGS.width_multiplier)\n    logging.info('Model input shape: %s', model.input_shape)\n    logging.info('Model output shape: %s', model.output_shape)\n    logging.info('Model number of weights: %s', model.count_params())\n    # Scale learning rate and decay epochs by vanilla settings.\n    base_lr = FLAGS.base_learning_rate * train_batch_size / 256\n    decay_epochs = [\n        (FLAGS.train_epochs * 30) // 90,\n        (FLAGS.train_epochs * 60) // 90,\n        (FLAGS.train_epochs * 80) // 90,\n    ]\n    learning_rate = ub.schedules.WarmUpPiecewiseConstantSchedule(\n        steps_per_epoch=steps_per_epoch,\n        base_learning_rate=base_lr,\n        decay_ratio=0.1,\n        decay_epochs=decay_epochs,\n        warmup_epochs=5)\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n                                        momentum=1.0 - FLAGS.one_minus_momentum,\n                                        nesterov=True)\n    metrics = {\n        'train/negative_log_likelihood': tf.keras.metrics.Mean(),\n        'train/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n        'train/loss': tf.keras.metrics.Mean(),\n        'train/ece': rm.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        'test/negative_log_likelihood': tf.keras.metrics.Mean(),\n        'test/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n        'test/ece': rm.metrics.ExpectedCalibrationError(\n            num_bins=FLAGS.num_bins),\n        'test/diversity': rm.metrics.AveragePairwiseDiversity(),\n    }\n\n    for i in range(FLAGS.ensemble_size):\n      metrics['test/nll_member_{}'.format(i)] = tf.keras.metrics.Mean()\n      metrics['test/accuracy_member_{}'.format(i)] = (\n          tf.keras.metrics.SparseCategoricalAccuracy())\n    logging.info('Finished building Keras ResNet-50 model')\n\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n    initial_epoch = 0\n    if latest_checkpoint:\n      # checkpoint.restore must be within a strategy.scope() so that optimizer\n      # slot variables are mirrored.\n      checkpoint.restore(latest_checkpoint)\n      logging.info('Loaded checkpoint %s', latest_checkpoint)\n      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n\n  summary_writer = tf.summary.create_file_writer(\n      os.path.join(FLAGS.output_dir, 'summaries'))\n\n  @tf.function\n  def train_step(iterator):\n    \"\"\"Training StepFn.\"\"\"\n    def step_fn(inputs):\n      \"\"\"Per-Replica StepFn.\"\"\"\n      images = inputs['features']\n      labels = inputs['labels']\n      batch_size = tf.shape(images)[0]\n      main_shuffle = tf.random.shuffle(tf.tile(\n          tf.range(batch_size), [FLAGS.batch_repetitions]))\n      to_shuffle = tf.cast(tf.cast(tf.shape(main_shuffle)[0], tf.float32)\n                           * (1. - FLAGS.input_repetition_probability),\n                           tf.int32)\n      shuffle_indices = [\n          tf.concat([tf.random.shuffle(main_shuffle[:to_shuffle]),\n                     main_shuffle[to_shuffle:]], axis=0)\n          for _ in range(FLAGS.ensemble_size)]\n      images = tf.stack([tf.gather(images, indices, axis=0)\n                         for indices in shuffle_indices], axis=1)\n      labels = tf.stack([tf.gather(labels, indices, axis=0)\n                         for indices in shuffle_indices], axis=1)\n\n      with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        if FLAGS.use_bfloat16:\n          logits = tf.cast(logits, tf.float32)\n\n        negative_log_likelihood = tf.reduce_mean(tf.reduce_sum(\n            tf.keras.losses.sparse_categorical_crossentropy(labels,\n                                                            logits,\n                                                            from_logits=True),\n            axis=1))\n        filtered_variables = []\n        for var in model.trainable_variables:\n          # Apply l2 on the weights. This excludes BN parameters and biases, but\n          # pay caution to their naming scheme.\n          if 'kernel' in var.name or 'bias' in var.name:\n            filtered_variables.append(tf.reshape(var, (-1,)))\n\n        l2_loss = FLAGS.l2 * 2 * tf.nn.l2_loss(\n            tf.concat(filtered_variables, axis=0))\n        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n        loss = negative_log_likelihood + l2_loss\n        scaled_loss = loss / strategy.num_replicas_in_sync\n\n      grads = tape.gradient(scaled_loss, model.trainable_variables)\n      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n      probs = tf.nn.softmax(tf.reshape(logits, [-1, NUM_CLASSES]))\n      flat_labels = tf.reshape(labels, [-1])\n      metrics['train/ece'].add_batch(probs, label=flat_labels)\n      metrics['train/loss'].update_state(loss)\n      metrics['train/negative_log_likelihood'].update_state(\n          negative_log_likelihood)\n      metrics['train/accuracy'].update_state(flat_labels, probs)\n\n    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n      strategy.run(step_fn, args=(next(iterator),))\n\n  @tf.function\n  def test_step(iterator):\n    \"\"\"Evaluation StepFn.\"\"\"\n    def step_fn(inputs):\n      \"\"\"Per-Replica StepFn.\"\"\"\n      images = inputs['features']\n      labels = inputs['labels']\n      images = tf.tile(\n          tf.expand_dims(images, 1), [1, FLAGS.ensemble_size, 1, 1, 1])\n      logits = model(images, training=False)\n      if FLAGS.use_bfloat16:\n        logits = tf.cast(logits, tf.float32)\n      probs = tf.nn.softmax(logits)\n\n      per_probs = tf.transpose(probs, perm=[1, 0, 2])\n      metrics['test/diversity'].add_batch(per_probs)\n      for i in range(FLAGS.ensemble_size):\n        member_probs = probs[:, i]\n        member_loss = tf.keras.losses.sparse_categorical_crossentropy(\n            labels, member_probs)\n        metrics['test/nll_member_{}'.format(i)].update_state(member_loss)\n        metrics['test/accuracy_member_{}'.format(i)].update_state(\n            labels, member_probs)\n\n      # Negative log marginal likelihood computed in a numerically-stable way.\n      labels_tiled = tf.tile(\n          tf.expand_dims(labels, 1), [1, FLAGS.ensemble_size])\n      log_likelihoods = -tf.keras.losses.sparse_categorical_crossentropy(\n          labels_tiled, logits, from_logits=True)\n      negative_log_likelihood = tf.reduce_mean(\n          -tf.reduce_logsumexp(log_likelihoods, axis=[1]) +\n          tf.math.log(float(FLAGS.ensemble_size)))\n      probs = tf.math.reduce_mean(probs, axis=1)  # marginalize\n\n      metrics['test/negative_log_likelihood'].update_state(\n          negative_log_likelihood)\n      metrics['test/accuracy'].update_state(labels, probs)\n      metrics['test/ece'].add_batch(probs, label=labels)\n\n    for _ in tf.range(tf.cast(steps_per_eval, tf.int32)):\n      strategy.run(step_fn, args=(next(iterator),))\n\n  metrics.update({'test/ms_per_example': tf.keras.metrics.Mean()})\n\n  train_iterator = iter(train_dataset)\n  start_time = time.time()\n  for epoch in range(initial_epoch, FLAGS.train_epochs):\n    logging.info('Starting to run epoch: %s', epoch)\n    train_step(train_iterator)\n\n    current_step = (epoch + 1) * steps_per_epoch\n    max_steps = steps_per_epoch * FLAGS.train_epochs\n    time_elapsed = time.time() - start_time\n    steps_per_sec = float(current_step) / time_elapsed\n    eta_seconds = (max_steps - current_step) / steps_per_sec\n    message = ('{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. '\n               'ETA: {:.0f} min. Time elapsed: {:.0f} min'.format(\n                   current_step / max_steps,\n                   epoch + 1,\n                   FLAGS.train_epochs,\n                   steps_per_sec,\n                   eta_seconds / 60,\n                   time_elapsed / 60))\n    logging.info(message)\n\n    test_iterator = iter(test_dataset)\n    logging.info('Starting to run eval of epoch: %s', epoch)\n    test_start_time = time.time()\n    test_step(test_iterator)\n    ms_per_example = (time.time() - test_start_time) * 1e6 / test_batch_size\n    metrics['test/ms_per_example'].update_state(ms_per_example)\n\n    logging.info('Train Loss: %.4f, Accuracy: %.2f%%',\n                 metrics['train/loss'].result(),\n                 metrics['train/accuracy'].result() * 100)\n    logging.info('Test NLL: %.4f, Accuracy: %.2f%%',\n                 metrics['test/negative_log_likelihood'].result(),\n                 metrics['test/accuracy'].result() * 100)\n    for i in range(FLAGS.ensemble_size):\n      logging.info('Member %d Test Loss: %.4f, Accuracy: %.2f%%',\n                   i, metrics['test/nll_member_{}'.format(i)].result(),\n                   metrics['test/accuracy_member_{}'.format(i)].result() * 100)\n\n    total_results = {name: metric.result() for name, metric in metrics.items()}\n    # Results from Robustness Metrics themselves return a dict, so flatten them.\n    total_results = utils.flatten_dictionary(total_results)\n    with summary_writer.as_default():\n      for name, result in total_results.items():\n        tf.summary.scalar(name, result, step=epoch + 1)\n\n    for _, metric in metrics.items():\n      metric.reset_states()\n\n    if (FLAGS.checkpoint_interval > 0 and\n        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n      checkpoint_name = checkpoint.save(os.path.join(\n          FLAGS.output_dir, 'checkpoint'))\n      logging.info('Saved checkpoint to %s', checkpoint_name)\n\n  final_save_name = os.path.join(FLAGS.output_dir, 'model')\n  model.save(final_save_name)\n  logging.info('Saved model to %s', final_save_name)\n  with summary_writer.as_default():\n    hp.hparams({\n        'base_learning_rate': FLAGS.base_learning_rate,\n        'one_minus_momentum': FLAGS.one_minus_momentum,\n        'l2': FLAGS.l2,\n        'batch_repetitions': FLAGS.batch_repetitions,\n    })\n\nif __name__ == '__main__':\n  app.run(main)\n", "framework": "tensorflow"}
{"repo_name": "google/uncertainty-baselines", "file_path": "uncertainty_baselines/datasets/cityscapes_test.py", "content": "# coding=utf-8\n# Copyright 2022 The Uncertainty Baselines Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for Cityscapes.\"\"\"\n\nfrom absl.testing import parameterized\nimport tensorflow as tf\nimport uncertainty_baselines as ub\n\n\n# TODO(dusenberrymw): Use TFDS mocking.\nclass CityscapesDatasetTest(ub.datasets.DatasetTest, parameterized.TestCase):\n\n  # TODO(dusenberrymw): Rename to `test_dataset_size`.\n  def testCityscapesDatasetShape(self):\n    super()._testDatasetSize(\n        ub.datasets.CityscapesDataset,\n        image_size=(1024, 2048, 3),\n        label_size=(1024, 2048, 1),\n        validation_percent=0.1)\n\n  def test_expected_features(self):\n    builder = ub.datasets.ImageNetDataset('train')\n    dataset = builder.load(batch_size=1)\n    self.assertEqual(list(dataset.element_spec.keys()), ['features', 'labels'])\n\n    builder_with_file_name = ub.datasets.ImageNetDataset(\n        'train', include_file_name=True)\n    dataset_with_file_name = builder_with_file_name.load(batch_size=1)\n    self.assertEqual(\n        list(dataset_with_file_name.element_spec.keys()),\n        ['features', 'labels', 'file_name'])\n\nif __name__ == '__main__':\n  tf.test.main()\n", "framework": "tensorflow"}
{"repo_name": "mixturemodel-flow/tensorflow", "file_path": "tensorflow/examples/tutorials/layers/cnn_mnist.py", "content": "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\"\"\"Convolutional Neural Network Estimator for MNIST, built with tf.layers.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef cnn_model_fn(features, labels, mode):\n  \"\"\"Model function for CNN.\"\"\"\n  # Input Layer\n  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n  # MNIST images are 28x28 pixels, and have one color channel\n  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n\n  # Convolutional Layer #1\n  # Computes 32 features using a 5x5 filter with ReLU activation.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 28, 28, 1]\n  # Output Tensor Shape: [batch_size, 28, 28, 32]\n  conv1 = tf.layers.conv2d(\n      inputs=input_layer,\n      filters=32,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n\n  # Pooling Layer #1\n  # First max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 28, 28, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 32]\n  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n  # Convolutional Layer #2\n  # Computes 64 features using a 5x5 filter.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 14, 14, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 64]\n  conv2 = tf.layers.conv2d(\n      inputs=pool1,\n      filters=64,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n\n  # Pooling Layer #2\n  # Second max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 14, 14, 64]\n  # Output Tensor Shape: [batch_size, 7, 7, 64]\n  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n  # Flatten tensor into a batch of vectors\n  # Input Tensor Shape: [batch_size, 7, 7, 64]\n  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n  # Dense Layer\n  # Densely connected layer with 1024 neurons\n  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n  # Output Tensor Shape: [batch_size, 1024]\n  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n\n  # Add dropout operation; 0.6 probability that element will be kept\n  dropout = tf.layers.dropout(\n      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n  # Logits layer\n  # Input Tensor Shape: [batch_size, 1024]\n  # Output Tensor Shape: [batch_size, 10]\n  logits = tf.layers.dense(inputs=dropout, units=10)\n\n  predictions = {\n      # Generate predictions (for PREDICT and EVAL mode)\n      \"classes\": tf.argmax(input=logits, axis=1),\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n      # `logging_hook`.\n      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n  }\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  # Calculate Loss (for both TRAIN and EVAL modes)\n  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n  loss = tf.losses.softmax_cross_entropy(\n      onehot_labels=onehot_labels, logits=logits)\n\n  # Configure the Training Op (for TRAIN mode)\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step())\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n  # Add evaluation metrics (for EVAL mode)\n  eval_metric_ops = {\n      \"accuracy\": tf.metrics.accuracy(\n          labels=labels, predictions=predictions[\"classes\"])}\n  return tf.estimator.EstimatorSpec(\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\ndef main(unused_argv):\n  # Load training and eval data\n  mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n  train_data = mnist.train.images  # Returns np.array\n  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n  eval_data = mnist.test.images  # Returns np.array\n  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n\n  # Create the Estimator\n  mnist_classifier = tf.estimator.Estimator(\n      model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n\n  # Set up logging for predictions\n  # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n  tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n  logging_hook = tf.train.LoggingTensorHook(\n      tensors=tensors_to_log, every_n_iter=50)\n\n  # Train the model\n  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n      x={\"x\": train_data},\n      y=train_labels,\n      batch_size=100,\n      num_epochs=None,\n      shuffle=True)\n  mnist_classifier.train(\n      input_fn=train_input_fn,\n      steps=20000,\n      hooks=[logging_hook])\n\n  # Evaluate the model and print results\n  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n      x={\"x\": eval_data},\n      y=eval_labels,\n      num_epochs=1,\n      shuffle=False)\n  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n  print(eval_results)\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n", "framework": "tensorflow"}
{"repo_name": "chenmich/learn_rnn_tensorflow", "file_path": "model_in_rnn.py", "content": "# Copyright 2017 The Chenmich Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n''' This modular is for model\n'''\nimport datetime\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as pyl\nimport data_reader as dr\n\n\n#model parameters\nBATCH_SIZE = 5\nNUM_BATCH = 2000\nSEQUENCE_LENGTH = 200\nFEATURE_SIZE = 5\nMU = 1.401157\nINIT_VALUE = 0.618\n\nNUM_UNITS = 12\nNUM_CELL_STACK = 3\nECHO = 400\ndef printFn(content):\n    for _ in range(10):\n        print(content)\nsess = tf.Session()\n#model\ncell = tf.contrib.rnn.BasicLSTMCell(num_units=NUM_UNITS, state_is_tuple=True)\ncells = tf.contrib.rnn.MultiRNNCell([cell for _ in range(NUM_CELL_STACK)])\ninit_state = state = cell.zero_state(BATCH_SIZE, dtype=tf.float32)\n\nW = tf.get_variable(name=\"weight\", shape=[NUM_UNITS, FEATURE_SIZE], dtype=tf.float64)\nB = tf.get_variable(name=\"bias\", shape=[SEQUENCE_LENGTH, FEATURE_SIZE],dtype=tf.float64)\n_X = tf.placeholder(dtype=tf.float64, shape=[BATCH_SIZE, SEQUENCE_LENGTH, FEATURE_SIZE])\n_Y = tf.placeholder(dtype=tf.float64, shape=[BATCH_SIZE, SEQUENCE_LENGTH, FEATURE_SIZE])\noutputs, state = tf.nn.dynamic_rnn(cell=cells, inputs=_X,\n                                   sequence_length=[SEQUENCE_LENGTH]*BATCH_SIZE,\n                                   dtype=tf.float64)\n#computer predicted value\ndef predicted(outputs):\n    predict_values = []\n    for i in range(BATCH_SIZE):\n        predicted_tensor = tf.matmul(outputs[i], W) + B\n        predict_values.append(predicted_tensor)\n    prediction = tf.stack([x for x in predict_values])\n    return prediction\n\ndef lossFn(predicted, real):\n    substract_tensor = predicted - real\n    substract_square_tensor = tf.square(substract_tensor)\n    loss = tf.reduce_sum(substract_square_tensor) / BATCH_SIZE\n    return loss\n#\n\nloss = lossFn(predicted(outputs), _Y)\n#prepare for train\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain = optimizer.minimize(loss)\ninit = tf.global_variables_initializer()\nsess.run(init)\nwriter = tf.summary.FileWriter(\"c://tmp/data/graph\", sess.graph)\n#train\nprint()\nprint()\nprint()\nprint(\"Training the model......\")\npyl_x = []\npyl_y = []\nstart = datetime.datetime.now()\nprint(\"start time is \", start)\nfor echo in range(0, ECHO+1):\n    losses = []\n\n    for x, y in dr.non_linear_parabolic_curve_map_data_reader(num_batch=NUM_BATCH,\n                                                              batch_size=BATCH_SIZE,\n                                                              sequence_length=SEQUENCE_LENGTH,\n                                                              feature_size=FEATURE_SIZE):\n        sess.run(train, feed_dict={_X:x, _Y:y})\n        _loss = sess.run(loss, feed_dict={_X:x, _Y:y})\n        losses.append(_loss)\n    _loss_ = np.sum(losses, 0) / NUM_BATCH\n    if (echo) % 10 == 0:\n        print(echo, _loss_)\n        pyl_x.append(echo)\n        pyl_y.append(_loss_)\n\nend = datetime.datetime.now()\nprint()\nprint(\"The training is over\")\nprint(\"The end time is \", end)\nprint(\"total time for training is \", end-start)\n\npyl.plot(pyl_x, pyl_y)\npyl.show()\n", "framework": "tensorflow"}
{"repo_name": "GoogleCloudPlatform/training-data-analyst", "file_path": "blogs/textclassification/txtcls1/trainer/model.py", "content": "#!/usr/bin/env python\n\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport shutil\nimport tensorflow as tf\nimport tensorflow.contrib.learn as tflearn\nimport tensorflow.contrib.layers as tflayers\nfrom tensorflow.contrib.learn.python.learn import learn_runner\nimport tensorflow.contrib.metrics as metrics\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.contrib import lookup\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# variables set by init()\nBUCKET = None\nTRAIN_STEPS = 1000\nWORD_VOCAB_FILE = None \nN_WORDS = -1\n\n# hardcoded into graph\nBATCH_SIZE = 32\n\n# describe your data\nTARGETS = ['nytimes', 'github', 'techcrunch']\nMAX_DOCUMENT_LENGTH = 20\nCSV_COLUMNS = ['source', 'title']\nLABEL_COLUMN = 'source'\nDEFAULTS = [['null'], ['null']]\nPADWORD = 'ZYXW'\n\ndef init(bucket, num_steps):\n  global BUCKET, TRAIN_STEPS, WORD_VOCAB_FILE, N_WORDS\n  BUCKET = bucket\n  TRAIN_STEPS = num_steps\n  WORD_VOCAB_FILE = 'gs://{}/txtcls1/vocab_words'.format(BUCKET)\n  N_WORDS = save_vocab('gs://{}/txtcls1/train.csv'.format(BUCKET), 'title', WORD_VOCAB_FILE);\n\ndef save_vocab(trainfile, txtcolname, outfilename):\n  if trainfile.startswith('gs://'):\n    import subprocess\n    tmpfile = \"vocab.csv\"\n    subprocess.check_call(\"gsutil cp {} {}\".format(trainfile, tmpfile).split(\" \"))\n    filename = tmpfile\n  else:\n    filename = trainfile\n  import pandas as pd\n  df = pd.read_csv(filename, header=None, sep='\\t', names=['source', 'title'])\n  # the text to be classified\n  vocab_processor = tflearn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, min_frequency=10)\n  vocab_processor.fit(df[txtcolname])\n\n  with gfile.Open(outfilename, 'wb') as f:\n    f.write(\"{}\\n\".format(PADWORD))\n    for word, index in vocab_processor.vocabulary_._mapping.iteritems():\n      f.write(\"{}\\n\".format(word))\n  nwords = len(vocab_processor.vocabulary_)\n  print('{} words into {}'.format(nwords, outfilename))\n  return nwords + 2  # PADWORD and <UNK>\n\ndef read_dataset(prefix):\n  # use prefix to create filename\n  filename = 'gs://{}/txtcls1/{}*csv*'.format(BUCKET, prefix)\n  if prefix == 'train':\n    mode = tf.contrib.learn.ModeKeys.TRAIN\n  else:\n    mode = tf.contrib.learn.ModeKeys.EVAL\n   \n  # the actual input function passed to TensorFlow\n  def _input_fn():\n    # could be a path to one file or a file pattern.\n    input_file_names = tf.train.match_filenames_once(filename)\n    filename_queue = tf.train.string_input_producer(input_file_names, shuffle=True)\n \n    # read CSV\n    reader = tf.TextLineReader()\n    _, value = reader.read_up_to(filename_queue, num_records=BATCH_SIZE)\n    #value = tf.train.shuffle_batch([value], BATCH_SIZE, capacity=10*BATCH_SIZE, min_after_dequeue=BATCH_SIZE, enqueue_many=True, allow_smaller_final_batch=False)\n    value_column = tf.expand_dims(value, -1)\n    columns = tf.decode_csv(value_column, record_defaults=DEFAULTS, field_delim='\\t')\n    features = dict(zip(CSV_COLUMNS, columns))\n    label = features.pop(LABEL_COLUMN)\n\n    # make targets numeric\n    table = tf.contrib.lookup.index_table_from_tensor(\n                   mapping=tf.constant(TARGETS), num_oov_buckets=0, default_value=-1)\n    target = table.lookup(label)\n\n    return features, target\n  \n  return _input_fn\n\n# CNN model parameters\nEMBEDDING_SIZE = 10\nWINDOW_SIZE = EMBEDDING_SIZE\nSTRIDE = int(WINDOW_SIZE/2)\ndef cnn_model(features, target, mode):\n    table = lookup.index_table_from_file(vocabulary_file=WORD_VOCAB_FILE, num_oov_buckets=1, default_value=-1)\n    \n    # string operations\n    titles = tf.squeeze(features['title'], [1])\n    words = tf.string_split(titles)\n    densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n    numbers = table.lookup(densewords)\n    padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n    padded = tf.pad(numbers, padding)\n    sliced = tf.slice(padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n    print('words_sliced={}'.format(words))  # (?, 20)\n\n    # layer to take the words and convert them into vectors (embeddings)\n    embeds = tf.contrib.layers.embed_sequence(sliced, vocab_size=N_WORDS, embed_dim=EMBEDDING_SIZE)\n    print('words_embed={}'.format(embeds)) # (?, 20, 10)\n    \n    # now do convolution\n    conv = tf.contrib.layers.conv2d(embeds, 1, WINDOW_SIZE, stride=STRIDE, padding='SAME') # (?, 4, 1)\n    conv = tf.nn.relu(conv) # (?, 4, 1)\n    words = tf.squeeze(conv, [2]) # (?, 4)\n    print('words_conv={}'.format(words)) # (?, 4)\n\n    n_classes = len(TARGETS)\n\n    logits = tf.contrib.layers.fully_connected(words, n_classes, activation_fn=None)\n    #print('logits={}'.format(logits)) # (?, 3)\n    predictions_dict = {\n      'source': tf.gather(TARGETS, tf.argmax(logits, 1)),\n      'class': tf.argmax(logits, 1),\n      'prob': tf.nn.softmax(logits)\n    }\n\n    if mode == tf.contrib.learn.ModeKeys.TRAIN or mode == tf.contrib.learn.ModeKeys.EVAL:\n       loss = tf.losses.sparse_softmax_cross_entropy(target, logits)\n       train_op = tf.contrib.layers.optimize_loss(\n         loss,\n         tf.contrib.framework.get_global_step(),\n         optimizer='Adam',\n         learning_rate=0.01)\n    else:\n       loss = None\n       train_op = None\n\n    return tflearn.ModelFnOps(\n      mode=mode,\n      predictions=predictions_dict,\n      loss=loss,\n      train_op=train_op)\n\n\ndef serving_input_fn():\n    feature_placeholders = {\n      'title': tf.placeholder(tf.string, [None]),\n    }\n    features = {\n      key: tf.expand_dims(tensor, -1)\n      for key, tensor in feature_placeholders.items()\n    }\n    return tflearn.utils.input_fn_utils.InputFnOps(\n      features,\n      None,\n      feature_placeholders)\n\ndef get_train():\n  return read_dataset('train')\n\ndef get_valid():\n  return read_dataset('eval')\n\nfrom tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\ndef experiment_fn(output_dir):\n    # run experiment\n    return tflearn.Experiment(\n        tflearn.Estimator(model_fn=cnn_model, model_dir=output_dir),\n        train_input_fn=get_train(),\n        eval_input_fn=get_valid(),\n        eval_metrics={\n            'acc': tflearn.MetricSpec(\n                metric_fn=metrics.streaming_accuracy, prediction_key='class'\n            )\n        },\n        export_strategies=[saved_model_export_utils.make_export_strategy(\n            serving_input_fn,\n            default_output_alternative_key=None,\n            exports_to_keep=1\n        )],\n        train_steps = TRAIN_STEPS\n    )\n\n", "framework": "tensorflow"}
{"repo_name": "GoogleCloudPlatform/training-data-analyst", "file_path": "courses/machine_learning/deepdive/09_sequence/labs/sinemodel/model.py", "content": "#!/usr/bin/env python\n\n# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nTIMESERIES_COL = 'height'\nN_OUTPUTS = 1  # in each sequence, 1-49 are features, and 50 is label\nSEQ_LEN = None\nDEFAULTS = None\nN_INPUTS = None\n\n\ndef init(hparams):\n    global SEQ_LEN, DEFAULTS, N_INPUTS\n    SEQ_LEN = hparams['sequence_length']\n    DEFAULTS = [[0.0] for x in range(0, SEQ_LEN)]\n    N_INPUTS = SEQ_LEN - N_OUTPUTS\n\n\ndef linear_model(features, mode, params):\n    X = features[TIMESERIES_COL]\n    #TODO: finish linear model\n    pass\n\ndef dnn_model(features, mode, params):\n  X = features[TIMESERIES_COL]\n  #TODO: finish DNN model\n  pass\n\ndef cnn_model(features, mode, params):\n  X = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1]) # as a 1D \"sequence\" with only one time-series observation (height)\n  #TODO: finish CNN model\n  pass\n\ndef rnn_model(features, mode, params):\n  # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n  x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n  #TODO: finish rnn model\n  pass\n\n# 2-layer RNN\ndef rnn2_model(features, mode, params):\n  x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n  #TODO: finish 2-layer rnn model\n  pass\n\n# create N-1 predictions\ndef rnnN_model(features, mode, params):\n    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n\n    # 2. configure the RNN\n    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n    # 'outputs' contains the state of the final layer for every time step\n    # not just the last time step (?,N_INPUTS, final cell size)\n    \n    # 3. pass state for each time step through a DNN, to get a prediction\n    # for each time step \n    h1 = tf.layers.dense(outputs, cells.output_size, activation=tf.nn.relu)\n    h2 = tf.layers.dense(h1, cells.output_size // 2, activation=tf.nn.relu)\n    predictions = tf.layers.dense(h2, 1, activation=None)  # (?, N_INPUTS, 1)\n    predictions = tf.reshape(predictions, [-1, N_INPUTS])\n    return predictions # return prediction for each time step\n\n\n# read data and convert to needed format\ndef read_dataset(filename, mode, batch_size=512):\n    def _input_fn():\n        def decode_csv(row):\n            # row is a string tensor containing the contents of one row\n            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n            label = features.pop()  # remove last feature and use as label\n            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n            return {TIMESERIES_COL: features}, label\n\n        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n        dataset = tf.data.Dataset.list_files(filename)\n        # Read in data from files\n        dataset = dataset.flat_map(tf.data.TextLineDataset)\n        # Parse text lines as comma-separated values (CSV)\n        dataset = dataset.map(decode_csv)\n\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            num_epochs = None  # loop indefinitely\n            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n        else:\n            num_epochs = 1  # end-of-input after this\n\n        dataset = dataset.repeat(num_epochs).batch(batch_size)\n        return dataset.make_one_shot_iterator().get_next()\n\n    return _input_fn\n\n\ndef serving_input_fn():\n    feature_placeholders = {\n        TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n    }\n\n    features = {\n        key: tf.expand_dims(tensor, -1)\n        for key, tensor in feature_placeholders.items()\n    }\n    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n\n    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n\n\ndef compute_errors(features, labels, predictions):\n    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n\n    if predictions.shape[1] == 1:\n        loss = tf.losses.mean_squared_error(labels, predictions)\n        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n        return loss, rmse\n    else:\n        # one prediction for every input in sequence\n        # get 1-N of (x + label)\n        labelsN = tf.concat([features[TIMESERIES_COL], labels], axis=1)\n        labelsN = labelsN[:, 1:]\n        # loss is computed from the last 1/3 of the series\n        N = (2 * N_INPUTS) // 3\n        loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n        # rmse is computed from last prediction and last label\n        lastPred = predictions[:, -1]\n        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n        return loss, rmse\n\n# RMSE when predicting same as last value\ndef same_as_last_benchmark(features, labels):\n    predictions = features[TIMESERIES_COL][:,-1] # last value in input sequence\n    return tf.metrics.root_mean_squared_error(labels, predictions)\n\n\n# create the inference model\ndef sequence_regressor(features, labels, mode, params):\n    # 1. run the appropriate model\n    model_functions = {\n        'linear': linear_model,\n        'dnn': dnn_model,\n        'cnn': cnn_model,\n        'rnn': rnn_model,\n        'rnn2': rnn2_model,\n        'rnnN': rnnN_model}\n    model_function = model_functions[params['model']]\n    predictions = model_function(features, mode, params)\n\n    # 2. loss function, training/eval ops\n    loss = None\n    train_op = None\n    eval_metric_ops = None\n    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n        loss, rmse = compute_errors(features, labels, predictions)\n\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            # this is needed for batch normalization, but has no effect otherwise\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                # 2b. set up training operation\n                train_op = tf.contrib.layers.optimize_loss(\n                    loss,\n                    tf.train.get_global_step(),\n                    learning_rate=params['learning_rate'],\n                    optimizer=\"Adam\")\n\n        # 2c. eval metric\n        eval_metric_ops = {\n            \"RMSE\": rmse,\n            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n        }\n\n    # 3. Create predictions\n    if predictions.shape[1] != 1:\n        predictions = predictions[:, -1]  # last predicted value\n    predictions_dict = {\"predicted\": predictions}\n\n    # 4. return EstimatorSpec\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions_dict,\n        loss=loss,\n        train_op=train_op,\n        eval_metric_ops=eval_metric_ops,\n        export_outputs={\n            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n    )\n\n\ndef train_and_evaluate(output_dir, hparams):\n    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n    get_train = read_dataset(hparams['train_data_path'],\n                             tf.estimator.ModeKeys.TRAIN,\n                             hparams['train_batch_size'])\n    get_valid = read_dataset(hparams['eval_data_path'],\n                             tf.estimator.ModeKeys.EVAL,\n                             1000)\n    estimator = tf.estimator.Estimator(model_fn=sequence_regressor,\n                                       params=hparams,\n                                       config=tf.estimator.RunConfig(\n                                           save_checkpoints_secs=\n                                           hparams['min_eval_frequency']),\n                                       model_dir=output_dir)\n    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n                                        max_steps=hparams['train_steps'])\n    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n                                      steps=None,\n                                      exporters=exporter,\n                                      start_delay_secs=hparams['eval_delay_secs'],\n                                      throttle_secs=hparams['min_eval_frequency'])\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n", "framework": "tensorflow"}
{"repo_name": "miyosuda/async_deep_reinforce", "file_path": "a3c_training_thread.py", "content": "# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport time\nimport sys\n\nfrom game_state import GameState\nfrom game_state import ACTION_SIZE\nfrom game_ac_network import GameACFFNetwork, GameACLSTMNetwork\n\nfrom constants import GAMMA\nfrom constants import LOCAL_T_MAX\nfrom constants import ENTROPY_BETA\nfrom constants import USE_LSTM\n\nLOG_INTERVAL = 100\nPERFORMANCE_LOG_INTERVAL = 1000\n\nclass A3CTrainingThread(object):\n  def __init__(self,\n               thread_index,\n               global_network,\n               initial_learning_rate,\n               learning_rate_input,\n               grad_applier,\n               max_global_time_step,\n               device):\n\n    self.thread_index = thread_index\n    self.learning_rate_input = learning_rate_input\n    self.max_global_time_step = max_global_time_step\n\n    if USE_LSTM:\n      self.local_network = GameACLSTMNetwork(ACTION_SIZE, thread_index, device)\n    else:\n      self.local_network = GameACFFNetwork(ACTION_SIZE, thread_index, device)\n\n    self.local_network.prepare_loss(ENTROPY_BETA)\n\n    with tf.device(device):\n      var_refs = [v._ref() for v in self.local_network.get_vars()]\n      self.gradients = tf.gradients(\n        self.local_network.total_loss, var_refs,\n        gate_gradients=False,\n        aggregation_method=None,\n        colocate_gradients_with_ops=False)\n\n    self.apply_gradients = grad_applier.apply_gradients(\n      global_network.get_vars(),\n      self.gradients )\n      \n    self.sync = self.local_network.sync_from(global_network)\n    \n    self.game_state = GameState(113 * thread_index)\n    \n    self.local_t = 0\n\n    self.initial_learning_rate = initial_learning_rate\n\n    self.episode_reward = 0\n\n    # variable controling log output\n    self.prev_local_t = 0\n\n  def _anneal_learning_rate(self, global_time_step):\n    learning_rate = self.initial_learning_rate * (self.max_global_time_step - global_time_step) / self.max_global_time_step\n    if learning_rate < 0.0:\n      learning_rate = 0.0\n    return learning_rate\n\n  def choose_action(self, pi_values):\n    return np.random.choice(range(len(pi_values)), p=pi_values)\n\n  def _record_score(self, sess, summary_writer, summary_op, score_input, score, global_t):\n    summary_str = sess.run(summary_op, feed_dict={\n      score_input: score\n    })\n    summary_writer.add_summary(summary_str, global_t)\n    summary_writer.flush()\n    \n  def set_start_time(self, start_time):\n    self.start_time = start_time\n\n  def process(self, sess, global_t, summary_writer, summary_op, score_input):\n    states = []\n    actions = []\n    rewards = []\n    values = []\n\n    terminal_end = False\n\n    # copy weights from shared to local\n    sess.run( self.sync )\n\n    start_local_t = self.local_t\n\n    if USE_LSTM:\n      start_lstm_state = self.local_network.lstm_state_out\n    \n    # t_max times loop\n    for i in range(LOCAL_T_MAX):\n      pi_, value_ = self.local_network.run_policy_and_value(sess, self.game_state.s_t)\n      action = self.choose_action(pi_)\n\n      states.append(self.game_state.s_t)\n      actions.append(action)\n      values.append(value_)\n\n      if (self.thread_index == 0) and (self.local_t % LOG_INTERVAL == 0):\n        print(\"pi={}\".format(pi_))\n        print(\" V={}\".format(value_))\n\n      # process game\n      self.game_state.process(action)\n\n      # receive game result\n      reward = self.game_state.reward\n      terminal = self.game_state.terminal\n\n      self.episode_reward += reward\n\n      # clip reward\n      rewards.append( np.clip(reward, -1, 1) )\n\n      self.local_t += 1\n\n      # s_t1 -> s_t\n      self.game_state.update()\n      \n      if terminal:\n        terminal_end = True\n        print(\"score={}\".format(self.episode_reward))\n\n        self._record_score(sess, summary_writer, summary_op, score_input,\n                           self.episode_reward, global_t)\n          \n        self.episode_reward = 0\n        self.game_state.reset()\n        if USE_LSTM:\n          self.local_network.reset_state()\n        break\n\n    R = 0.0\n    if not terminal_end:\n      R = self.local_network.run_value(sess, self.game_state.s_t)\n\n    actions.reverse()\n    states.reverse()\n    rewards.reverse()\n    values.reverse()\n\n    batch_si = []\n    batch_a = []\n    batch_td = []\n    batch_R = []\n\n    # compute and accmulate gradients\n    for(ai, ri, si, Vi) in zip(actions, rewards, states, values):\n      R = ri + GAMMA * R\n      td = R - Vi\n      a = np.zeros([ACTION_SIZE])\n      a[ai] = 1\n\n      batch_si.append(si)\n      batch_a.append(a)\n      batch_td.append(td)\n      batch_R.append(R)\n\n    cur_learning_rate = self._anneal_learning_rate(global_t)\n\n    if USE_LSTM:\n      batch_si.reverse()\n      batch_a.reverse()\n      batch_td.reverse()\n      batch_R.reverse()\n\n      sess.run( self.apply_gradients,\n                feed_dict = {\n                  self.local_network.s: batch_si,\n                  self.local_network.a: batch_a,\n                  self.local_network.td: batch_td,\n                  self.local_network.r: batch_R,\n                  self.local_network.initial_lstm_state: start_lstm_state,\n                  self.local_network.step_size : [len(batch_a)],\n                  self.learning_rate_input: cur_learning_rate } )\n    else:\n      sess.run( self.apply_gradients,\n                feed_dict = {\n                  self.local_network.s: batch_si,\n                  self.local_network.a: batch_a,\n                  self.local_network.td: batch_td,\n                  self.local_network.r: batch_R,\n                  self.learning_rate_input: cur_learning_rate} )\n      \n    if (self.thread_index == 0) and (self.local_t - self.prev_local_t >= PERFORMANCE_LOG_INTERVAL):\n      self.prev_local_t += PERFORMANCE_LOG_INTERVAL\n      elapsed_time = time.time() - self.start_time\n      steps_per_sec = global_t / elapsed_time\n      print(\"### Performance : {} STEPS in {:.0f} sec. {:.0f} STEPS/sec. {:.2f}M STEPS/hour\".format(\n        global_t,  elapsed_time, steps_per_sec, steps_per_sec * 3600 / 1000000.))\n\n    # return advanced local step size\n    diff_local_t = self.local_t - start_local_t\n    return diff_local_t\n    \n", "framework": "tensorflow"}
{"repo_name": "xzturn/tensorflow", "file_path": "tensorflow/python/tpu/tpu_embedding_gradient.py", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===================================================================\n\"\"\"Optional helper for gradient handling.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.tpu.ops import tpu_ops\n\n\ndef get_gradients_through_compute_gradients(optimizer, loss, activations):\n  \"\"\"Compute gradients to send to TPU embedding.\n\n  Args:\n    optimizer: a subclass of optimizer.Optimizer, usually CrossShardOptimizer.\n      Used to call compute_gradients().\n    loss: a Tensor to call optimizer.compute_gradients() on.\n    activations: an OrderedDict mapping feature_name to Tensors of activations.\n\n  Returns:\n    An OrderedDict mapping from feature name Strings to Tensors of gradients of\n      the loss wrt the activations of the features.\n  \"\"\"\n  activation_list = activations.values()\n  grads_and_vars = optimizer.compute_gradients(loss, activation_list)\n  grads = [grad for grad, _ in grads_and_vars]\n  feature_to_gradient_dict = collections.OrderedDict(\n      zip(activations.keys(), grads))\n  return feature_to_gradient_dict\n\n\ndef create_dummy_table_variables(tpu_embedding):\n  \"\"\"Create dummy embedding table variables.\n\n  The sole purpose of these dummy variables are to trigger gradient\n  calculation wrt them so that the gradients wrt activation can be captured\n  and later sent to TPU embedding.\n\n  Args:\n    tpu_embedding: TPUEmbedding, dummy table variables will be created for use\n      with tpu_embedding.\n\n  Returns:\n    A tuple of dummy variables and their initializer.\n\n  Raises:\n    RuntimeError: if collection to store gradients already exists and is not\n    empty.\n  \"\"\"\n  dummy_table_variables = collections.OrderedDict()\n  for table_id, table in enumerate(tpu_embedding.table_to_features_dict):\n    dummy_table_variables[table] = (\n        # Explicitly specifying collections prevents this variable from\n        # being added to the GLOBAL_VARIABLES collection, so that Saver()\n        # ignores it.\n        # But Tensorflow optimizer creates slot variable for these dummy\n        # variable, e.g. tpu_embedding_dummy_table_variable_mlp_user/Adam{_1},\n        # which will be in GLOBAL_VARIABLES collection,\n        variable_scope.get_variable(\n            'tpu_embedding_dummy_table_variable_{}'.format(table),\n            dtype=dtypes.float32,\n            shape=[1],\n            use_resource=True,\n            trainable=True,\n            collections=['tpu_embedding_dummy_table_variables']))\n\n    g = ops.get_default_graph()\n    table_gradients = g.get_collection_ref(\n        'tpu_embedding_gradients_table_{}'.format(table_id))\n    if table_gradients:\n      raise RuntimeError(\n          'tpu_embedding_gradients_table_{} is not empty.'.format(table_id))\n    table_gradients.extend(\n        [None] * len(tpu_embedding.table_to_features_dict[table]))\n\n  return (dummy_table_variables,\n          variables.variables_initializer(\n              dummy_table_variables.values(),\n              name='tpu_embedding_dummy_table_variables_init'))\n\n\ndef hook_dummy_table_variables_to_activations(tpu_embedding, activations,\n                                              dummy_table_variables):\n  \"\"\"Have activations depend on dummy table variables for gradient intercept.\n\n  Args:\n    tpu_embedding: TPUEmbedding, activations and dummy_table_variables are from\n      tpu_embedding.\n    activations: An OrderedDict of feature name String to activation tensors.\n    dummy_table_variables: An OrderedDict of table name String to dummy table\n      variables.\n\n  Returns:\n    An OrderedDict of feature name String to activation tensors, which can be\n      used just as the activations input.\n  \"\"\"\n  new_activations = collections.OrderedDict()\n  for feature in activations:\n    table = tpu_embedding.feature_to_config_dict[feature].table_id\n    new_activations[feature] = tpu_ops.tpu_embedding_activations(\n        dummy_table_variables[table],\n        activations[feature],\n        table_id=list(tpu_embedding.table_to_config_dict).index(table),\n        lookup_id=tpu_embedding.table_to_features_dict[table].index(feature))\n  return new_activations\n\n\ndef get_gradients_through_dummy_table_variables(tpu_embedding):\n  \"\"\"Get gradients wrt the activations of each feature.\n\n  Args:\n    tpu_embedding: TPUEmbedding, create dummy table variable to be used with\n      tpu_embedding.\n\n  Returns:\n    An OrderedDict mapping feature name to gradient.\n\n  Raises:\n    ValueError: if some gradients are not defined.\n  \"\"\"\n  g = ops.get_default_graph()\n  feature_to_gradient_dict = collections.OrderedDict()\n  for table_id, table in enumerate(tpu_embedding.table_to_config_dict):\n    table_gradients = g.get_collection(\n        'tpu_embedding_gradients_table_{}'.format(table_id))\n    if all(gradient is None for gradient in table_gradients):\n      raise ValueError(\n          'Table {} with id {} has undefined gradients: this is probably '\n          'because the model asked TPUEmbedding to compute activations that '\n          'were not used.'.format(table, table_id))\n    if any(gradient is None for gradient in table_gradients):\n      # TODO(bfontain): create a white-list for optimizers which are compatible\n      # with `tf.stop_gradient`.\n      logging.warn(\n          'Table {} with id {} has undefined gradients: this is probably '\n          'because the model asked TPUEmbedding to compute activations that '\n          'were not used, or tf.stop_gradient() is applied. Gradients of zeros '\n          'are sent back to TPUEmbedding instead. Gradients of zeros and no '\n          'gradients are equivalent for SGD, AdaGrad, FTRL, momentum, etc, but '\n          'might differ for other optimizers due to implementation of tpu '\n          'embedding optimizers.'.format(table, table_id))\n    for feature, gradient in zip(tpu_embedding.table_to_features_dict[table],\n                                 table_gradients):\n      if gradient is not None:\n        feature_to_gradient_dict[feature] = gradient\n      else:\n        dimension = tpu_embedding.table_to_config_dict[table].dimension\n        batch_size = tpu_embedding.batch_size_per_core\n        max_sequence_length = (\n            tpu_embedding.feature_to_config_dict[feature].max_sequence_length)\n        if max_sequence_length:\n          feature_to_gradient_dict[feature] = array_ops.zeros(\n              [batch_size, max_sequence_length, dimension])\n        else:\n          feature_to_gradient_dict[feature] = array_ops.zeros(\n              [batch_size, dimension])\n\n  return feature_to_gradient_dict\n", "framework": "tensorflow"}
