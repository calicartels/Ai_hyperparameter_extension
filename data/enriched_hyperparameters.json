[
  {
    "id": "ml_46148db3",
    "framework": "unknown",
    "source_url": "https://github.com/Honkl/general-ai/blob/master/Controller/reinforcement/ddpg/actor_network_bn.py",
    "hyperparameters": [
      {
        "name": "activation",
        "value": "tf.identity",
        "line_number": 90,
        "char_start": 53,
        "char_end": 64,
        "code_context": "        W3 = tf.Variable(tf.random_uniform([layer2_size, action_dim], -3e-3, 3e-3))\n        b3 = tf.Variable(tf.random_uniform([action_dim], -3e-3, 3e-3))\n\n        layer0_bn = self.batch_norm_layer(state_input, training_phase=is_training, scope_bn='batch_norm_0',\n                                          activation=tf.identity)\n        layer1 = tf.matmul(layer0_bn, W1) + b1\n        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, W2) + b2\n        layer2_bn = self.batch_norm_layer(layer2, training_phase=is_training, scope_bn='batch_norm_2',",
        "param_type": "other",
        "explanation": "The activation function, `tf.identity`, defines the output of a given layer in a neural network. In this case, it simply passes the input unchanged. This can be useful when no transformation is desired in the output, allowing for direct computation or comparison to earlier inputs.",
        "typical_range": "The most common values for activation functions include ReLU, Leaky ReLU, Tanh, and Sigmoid, depending on the desired learning and output scaling behaviors. However, using the specific value of `tf.identity`, or no activation function at all, is also a potential choice based on the specific architecture and learning requirements.",
        "alternatives": [
          {
            "value": "tf.nn.relu",
            "scenario": "This activates values over zero for a non-linear activation response, typically accelerating convergence."
          },
          {
            "value": "tf.nn.tanh",
            "scenario": "This compresses values between +1 and -1 to facilitate smooth learning dynamics with gradients that are not sensitive to output value magnitude."
          },
          {
            "value": "tf.nn.leaky_relu",
            "scenario": "This provides non-linear activation while avoiding dying ReLU neurons through a small non-zero slope for negative values to potentially aid learning from sparse features."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      },
      {
        "name": "activation",
        "value": "tf.nn.relu",
        "line_number": 93,
        "char_start": 53,
        "char_end": 63,
        "code_context": "        layer0_bn = self.batch_norm_layer(state_input, training_phase=is_training, scope_bn='batch_norm_0',\n                                          activation=tf.identity)\n        layer1 = tf.matmul(layer0_bn, W1) + b1\n        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, W2) + b2\n        layer2_bn = self.batch_norm_layer(layer2, training_phase=is_training, scope_bn='batch_norm_2',\n                                          activation=tf.nn.relu)\n\n        action_output = tf.tanh(tf.matmul(layer2_bn, W3) + b3)",
        "param_type": "other",
        "explanation": "Activation function applied directly after batch normalization layers. This introduces non-linearity into the network, allowing it to learn complex relationships between input and output.",
        "typical_range": "While tf.nn.relu is commonly used, other popular choices include: \n  * tf.nn.tanh: Useful when output values need to be between -1 and 1. \n  * tf.nn.leaky_relu: Offers improved gradient flow compared to standard ReLU. \n  * tf.nn.sigmoid: Suitable for output values in the range 0 to 1.",
        "alternatives": [
          {
            "value": "tf.nn.tanh",
            "scenario": "When output values need to be bounded between -1 and 1, such as in pixel intensity prediction or data normalization."
          },
          {
            "value": "tf.nn.leaky_relu",
            "scenario": "To mitigate the 'dying ReLU' problem where neurons become inactive due to negative inputs, impacting gradient flow. Useful for tasks with sparse activations."
          },
          {
            "value": "tf.nn.sigmoid",
            "scenario": "For binary classification tasks where output values represent probabilities (0 to 1), or for problems with a naturally bounded output range."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      },
      {
        "name": "activation",
        "value": "tf.nn.relu",
        "line_number": 96,
        "char_start": 53,
        "char_end": 63,
        "code_context": "        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, W2) + b2\n        layer2_bn = self.batch_norm_layer(layer2, training_phase=is_training, scope_bn='batch_norm_2',\n                                          activation=tf.nn.relu)\n\n        action_output = tf.tanh(tf.matmul(layer2_bn, W3) + b3)\n\n        return state_input, action_output, [W1, b1, W2, b2, W3, b3], is_training\n",
        "param_type": "other",
        "explanation": "The activation function determines what happens to the output of a specific layer in a neural network. It introduces non-linearity into the model, allowing it to learn complex patterns and relationships between data points. In this case, \"tf.nn.relu\" specifies the rectified linear unit (ReLU) function, which applies a threshold to the input and sets it to zero if it's negative.",
        "typical_range": "There is no typical range for activation functions as the choice depends on the specific problem and network architecture. However, certain activation functions are more commonly used for specific tasks or network types. For example, ReLU is popular for its computational efficiency and ease of optimization, making it suitable for large-scale image recognition models.",
        "alternatives": [
          {
            "value": "tf.nn.sigmoid",
            "scenario": "When dealing with data between 0 and 1, such as probabilities or outputs from recurrent neural networks."
          },
          {
            "value": "tf.nn.tanh",
            "scenario": "When dealing with data between -1 and 1, particularly in recurrent neural networks with gated units."
          },
          {
            "value": "tf.nn.leaky_relu",
            "scenario": "To alleviate the vanishing gradient problem associated with ReLU by introducing a small non-zero slope for negative inputs."
          },
          {
            "value": "Custom activation functions",
            "scenario": "When specific requirements or experimental purposes necessitate a unique activation behavior."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      },
      {
        "name": "activation",
        "value": "tf.identity",
        "line_number": 110,
        "char_start": 53,
        "char_end": 64,
        "code_context": "        target_update = ema.apply(net)\n        target_net = [ema.average(x) for x in net]\n\n        layer0_bn = self.batch_norm_layer(state_input, training_phase=is_training, scope_bn='target_batch_norm_0',\n                                          activation=tf.identity)\n\n        layer1 = tf.matmul(layer0_bn, target_net[0]) + target_net[1]\n        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='target_batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, target_net[2]) + target_net[3]",
        "param_type": "other",
        "explanation": "The `activation` hyperparameter in TensorFlow determines the activation function applied to the output of a layer. In this case, it is set to `tf.identity`, which means no activation function is applied. This allows the raw output of the layer to be passed through.",
        "typical_range": "The choice of activation function depends on the specific problem and layer type. Common activation functions include `tf.nn.relu`, `tf.nn.tanh`, `tf.nn.sigmoid`, and `tf.nn.softmax`.",
        "alternatives": [
          {
            "value": "tf.nn.relu",
            "scenario": "Use ReLU when you want to introduce non-linearity to the network and promote sparsity."
          },
          {
            "value": "tf.nn.tanh",
            "scenario": "Use tanh when you want to constrain the output values between -1 and 1."
          },
          {
            "value": "tf.nn.sigmoid",
            "scenario": "Use sigmoid when you want to model the probability of an event (output values between 0 and 1)."
          },
          {
            "value": "tf.nn.softmax",
            "scenario": "Use softmax for multi-class classification problems where the outputs represent probabilities."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      },
      {
        "name": "activation",
        "value": "tf.nn.relu",
        "line_number": 114,
        "char_start": 53,
        "char_end": 63,
        "code_context": "                                          activation=tf.identity)\n\n        layer1 = tf.matmul(layer0_bn, target_net[0]) + target_net[1]\n        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='target_batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, target_net[2]) + target_net[3]\n        layer2_bn = self.batch_norm_layer(layer2, training_phase=is_training, scope_bn='target_batch_norm_2',\n                                          activation=tf.nn.relu)\n\n        action_output = tf.tanh(tf.matmul(layer2_bn, target_net[4]) + target_net[5])",
        "param_type": "other",
        "explanation": "The activation function determines the output of a neuron based on its weighted sum of inputs. In this case, tf.nn.relu activates only positive values, introducing non-linearity.",
        "typical_range": "The typical range for activation functions is diverse, but for ReLU, it's typically between -1 and 1.",
        "alternatives": [
          {
            "value": "tf.nn.leaky_relu",
            "scenario": "When a smoother activation is needed near zero to avoid dead neurons."
          },
          {
            "value": "tf.nn.sigmoid",
            "scenario": "When output needs to be between 0 and 1, for example, for probability values."
          },
          {
            "value": "tf.nn.tanh",
            "scenario": "When a wider range of output is needed, between -1 and 1."
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "high"
        }
      },
      {
        "name": "activation",
        "value": "tf.nn.relu",
        "line_number": 117,
        "char_start": 53,
        "char_end": 63,
        "code_context": "        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='target_batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, target_net[2]) + target_net[3]\n        layer2_bn = self.batch_norm_layer(layer2, training_phase=is_training, scope_bn='target_batch_norm_2',\n                                          activation=tf.nn.relu)\n\n        action_output = tf.tanh(tf.matmul(layer2_bn, target_net[4]) + target_net[5])\n\n        return state_input, action_output, target_update, is_training\n",
        "param_type": "other",
        "explanation": "The activation function used after the first and second batch normalization layers. This function determines how the outputs of these layers are transformed before being passed to subsequent layers.",
        "typical_range": "There is no typical range for this parameter, as the best choice of activation function will vary depending on the dataset and task.",
        "alternatives": [
          {
            "value": "tf.nn.tanh",
            "scenario": "This function is commonly used for regression tasks, where the output values should be bounded between -1 and 1."
          },
          {
            "value": "tf.nn.sigmoid",
            "scenario": "Similar to `tf.nn.tanh`, `tf.nn.sigmoid` is also commonly used for regression tasks. It limits outputs to between 0 and 1."
          },
          {
            "value": "tf.nn.leaky_relu",
            "scenario": "This function addresses the 'dying ReLU' problem by allowing a small non-zero gradient for negative inputs. It may be useful for complex tasks or when training is unstable."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "unknown",
    "code_snippet": "\"\"\"\nThe MIT License (MIT)\n\nCopyright (c) 2016 Flood Sung\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nThis library is used and modified in General Artificial Intelligence for Game Playing project by Jan Kluj.\nSee the original license above.\n\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\nimport numpy as np\nimport math\n\n# Hyper Parameters\nLAYER1_SIZE = 400\nLAYER2_SIZE = 300\nLEARNING_RATE = 1e-4\nTAU = 0.001\n\n\nclass ActorNetwork:\n    \"\"\"docstring for ActorNetwork\"\"\"\n\n    def __init__(self, sess, state_dim, action_dim):\n        self.sess = sess\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        # create actor network\n        self.state_input, self.action_output, self.net, self.is_training = self.create_network(state_dim, action_dim)\n\n        # create target actor network\n        self.target_state_input, self.target_action_output, self.target_update, self.target_is_training = self.create_target_network(\n            state_dim, action_dim, self.net)\n\n        # define training rules\n        self.create_training_method()\n\n        self.sess.run(tf.global_variables_initializer())\n\n        self.update_target()\n\n        # self.load_network()\n\n    def get_parameters(self):\n        data = {}\n        data[\"layers\"] = [LAYER1_SIZE, LAYER2_SIZE]\n        data[\"learning_rate\"] = LEARNING_RATE\n        data[\"tau\"] = TAU\n        return data\n\n    def create_training_method(self):\n        self.q_gradient_input = tf.placeholder(\"float\", [None, self.action_dim])\n        self.parameters_gradients = tf.gradients(self.action_output, self.net, -self.q_gradient_input)\n        self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE).apply_gradients(zip(self.parameters_gradients, self.net))\n\n    def create_network(self, state_dim, action_dim):\n        layer1_size = LAYER1_SIZE\n        layer2_size = LAYER2_SIZE\n\n        state_input = tf.placeholder(\"float\", [None, state_dim])\n        is_training = tf.placeholder(tf.bool)\n\n        W1 = self.variable([state_dim, layer1_size], state_dim)\n        b1 = self.variable([layer1_size], state_dim)\n        W2 = self.variable([layer1_size, layer2_size], layer1_size)\n        b2 = self.variable([layer2_size], layer1_size)\n        W3 = tf.Variable(tf.random_uniform([layer2_size, action_dim], -3e-3, 3e-3))\n        b3 = tf.Variable(tf.random_uniform([action_dim], -3e-3, 3e-3))\n\n        layer0_bn = self.batch_norm_layer(state_input, training_phase=is_training, scope_bn='batch_norm_0',\n                                          activation=tf.identity)\n        layer1 = tf.matmul(layer0_bn, W1) + b1\n        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, W2) + b2\n        layer2_bn = self.batch_norm_layer(layer2, training_phase=is_training, scope_bn='batch_norm_2',\n                                          activation=tf.nn.relu)\n\n        action_output = tf.tanh(tf.matmul(layer2_bn, W3) + b3)\n\n        return state_input, action_output, [W1, b1, W2, b2, W3, b3], is_training\n\n    def create_target_network(self, state_dim, action_dim, net):\n        state_input = tf.placeholder(\"float\", [None, state_dim])\n        is_training = tf.placeholder(tf.bool)\n        ema = tf.train.ExponentialMovingAverage(decay=1 - TAU)\n        target_update = ema.apply(net)\n        target_net = [ema.average(x) for x in net]\n\n        layer0_bn = self.batch_norm_layer(state_input, training_phase=is_training, scope_bn='target_batch_norm_0',\n                                          activation=tf.identity)\n\n        layer1 = tf.matmul(layer0_bn, target_net[0]) + target_net[1]\n        layer1_bn = self.batch_norm_layer(layer1, training_phase=is_training, scope_bn='target_batch_norm_1',\n                                          activation=tf.nn.relu)\n        layer2 = tf.matmul(layer1_bn, target_net[2]) + target_net[3]\n        layer2_bn = self.batch_norm_layer(layer2, training_phase=is_training, scope_bn='target_batch_norm_2',\n                                          activation=tf.nn.relu)\n\n        action_output = tf.tanh(tf.matmul(layer2_bn, target_net[4]) + target_net[5])\n\n        return state_input, action_output, target_update, is_training\n\n    def update_target(self):\n        self.sess.run(self.target_update)\n\n    def train(self, q_gradient_batch, state_batch):\n        self.sess.run(self.optimizer, feed_dict={\n            self.q_gradient_input: q_gradient_batch,\n            self.state_input: state_batch,\n            self.is_training: True\n        })\n\n    def actions(self, state_batch):\n        return self.sess.run(self.action_output, feed_dict={\n            self.state_input: state_batch,\n            self.is_training: True\n        })\n\n    def action(self, state):\n        return self.sess.run(self.action_output, feed_dict={\n            self.state_input: [state],\n            self.is_training: False\n        })[0]\n\n    def target_actions(self, state_batch):\n        return self.sess.run(self.target_action_output, feed_dict={\n            self.target_state_input: state_batch,\n            self.target_is_training: True\n        })\n\n    # f fan-in size\n    def variable(self, shape, f):\n        return tf.Variable(tf.random_uniform(shape, -1 / math.sqrt(f), 1 / math.sqrt(f)))\n\n    def batch_norm_layer(self, x, training_phase, scope_bn, activation=None):\n        return tf.cond(training_phase,\n                       lambda: tf.contrib.layers.batch_norm(x, activation_fn=activation, center=True, scale=True,\n                                                            updates_collections=None, is_training=True, reuse=None,\n                                                            scope=scope_bn, decay=0.9, epsilon=1e-5),\n                       lambda: tf.contrib.layers.batch_norm(x, activation_fn=activation, center=True, scale=True,\n                                                            updates_collections=None, is_training=False, reuse=True,\n                                                            scope=scope_bn, decay=0.9, epsilon=1e-5))\n\n\n'''\n\tdef load_network(self):\n\t\tself.saver = tf.train.Saver()\n\t\tcheckpoint = tf.train.get_checkpoint_state(\"saved_actor_networks\")\n\t\tif checkpoint and checkpoint.model_checkpoint_path:\n\t\t\tself.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n\t\t\tprint \"Successfully loaded:\", checkpoint.model_checkpoint_path\n\t\telse:\n\t\t\tprint \"Could not find old network weights\"\n\tdef save_network(self,time_step):\n\t\tprint 'save actor-network...',time_step\n\t\tself.saver.save(self.sess, 'saved_actor_networks/' + 'actor-network', global_step = time_step)\n\n'''\n"
  },
  {
    "id": "ml_f52865b5",
    "framework": "unknown",
    "source_url": "https://github.com/nlholdem/icodoom/blob/master/ICO1/agent/agent1.py",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "self.learning_rate",
        "line_number": 119,
        "char_start": 69,
        "char_end": 87,
        "code_context": "        h2 = tf.tanh(tf.matmul(h1, W_fc2) + b_fc2)\n        self.y_fc = tf.tanh(tf.matmul(h2, W_fc3) + b_fc3)\n\n        self.fcloss = tf.squared_difference(self.y_fc, self.fcnet_target)\n        self.fcnet_train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.fcloss)\n        self.fcaccuracy = tf.reduce_mean(self.fcloss)\n\n    def make_ffnet(self):\n\n",
        "param_type": "optimizer",
        "explanation": "The learning rate controls the step size that the optimizer takes in the direction of the gradient during training. A higher learning rate leads to faster convergence but may also lead to instability and overshooting the minimum. A lower learning rate leads to slower convergence but may be more stable.",
        "typical_range": "0.001 to 0.1",
        "alternatives": [
          {
            "value": "0.01",
            "scenario": "A good starting point for many problems."
          },
          {
            "value": "0.001",
            "scenario": "If the problem is sensitive to learning rate or the loss function is noisy."
          },
          {
            "value": "0.1",
            "scenario": "If the problem is simple and the loss function is smooth."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "unknown",
    "code_snippet": "from __future__ import print_function\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport ops as my_ops\nimport os\nimport re\nimport itertools as it\n\nclass Agent:\n\n    def __init__(self, sess, args):\n        '''Agent - powered by neural nets, can infer, act, train, test.\n        '''\n        self.sess = sess\n        \n        # input data properties\n        self.state_imgs_shape = args['state_imgs_shape']\n        self.state_meas_shape = args['state_meas_shape']\n        self.meas_for_net = args['meas_for_net']\n        self.meas_for_manual = args['meas_for_manual']\n        self.resolution = args['resolution']\n\n        # preprocessing\n        self.preprocess_input_images = args['preprocess_input_images']\n\n        # net parameters\n        self.conv_params = args['conv_params']\n        self.fc_img_params = args['fc_img_params']\n        self.fc_meas_params = args['fc_meas_params']\n        self.fc_joint_params = args['fc_joint_params']      \n        self.target_dim = args['target_dim']\n\n        self.n_ffnet_input = args['n_ffnet_input']\n        self.n_ffnet_hidden = args['n_ffnet_hidden']\n        self.n_ffnet_output = args['n_ffnet_output']\n        self.learning_rate = args['learning_rate']\n        self.momentum = args['momentum']\n        self.ext_ffnet_output = np.zeros(self.n_ffnet_output)\n        self.ext_covnet_output = np.zeros(self.n_ffnet_output)\n        self.ext_fcnet_output = np.zeros(self.n_ffnet_output)\n        print (\"ffnet_inputs: \", args['n_ffnet_input'])\n        print (\"ffnet_hidden: \", args['n_ffnet_hidden'])\n        print (\"ext_ffnet_output: \", self.ext_ffnet_output.shape)\n\n        self.ffnet_input = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_input])\n        self.ffnet_target = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_output])\n        self.covnet_input = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_input])\n        self.covnet_target = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_output])\n        self.fcnet_input = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_input])\n        self.fcnet_target = tf.placeholder(tf.float32, shape=[None, self.n_ffnet_output])\n\n        self.build_model()\n        self.epoch = 20\n        self.iter = 1\n        \n    def make_convnet(self):\n        n_ffnet_inputs = self.n_ffnet_input\n        n_ffnet_outputs = self.n_ffnet_output\n\n        print(\"COVNET: Inputs: \", n_ffnet_inputs, \" outputs: \", n_ffnet_outputs)\n\n        with tf.name_scope('reshape'):\n            x_image = tf.reshape(self.covnet_input, [-1, self.resolution[0], self.resolution[1], 1])\n\n        with tf.name_scope('conv1'):\n            W_conv1 = my_ops.weight_variable([5, 5, 1, 32])\n            b_conv1 = my_ops.bias_variable([32])\n            h_conv1 = tf.nn.relu(my_ops.conv2d(x_image, W_conv1) + b_conv1)\n\n        with tf.name_scope('pool1'):\n            h_pool1 = my_ops.max_pool_2x2(h_conv1)\n\n        with tf.name_scope('conv2'):\n            W_conv2 = my_ops.weight_variable([5, 5, 32, 64])\n            b_conv2 = my_ops.bias_variable([64])\n            h_conv2 = tf.nn.relu(my_ops.conv2d(h_pool1, W_conv2) + b_conv2)\n\n        with tf.name_scope('pool2'):\n            h_pool2 = my_ops.max_pool_2x2(h_conv2)\n\n        with tf.name_scope('fc1'):\n            W_fc1 = my_ops.weight_variable([int(self.resolution[0]/4) * int(self.resolution[1]/4) * 64, 64])\n            b_fc1 = my_ops.bias_variable([64])\n\n            h_pool2_flat = tf.reshape(h_pool2, [-1, int(self.resolution[0]/4) * int(self.resolution[1]/4) * 64])\n            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n        # single output:\n        with tf.name_scope('fc2'):\n            W_fc2 = my_ops.weight_variable([64, 1])\n            b_fc2 = my_ops.bias_variable([1])\n\n        self.y_conv = tf.tanh(tf.matmul(h_fc1, W_fc2) + b_fc2)\n        self.covloss = tf.squared_difference(self.y_conv, self.covnet_target)\n        self.covnet_train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.covloss)\n        self.covaccuracy = tf.reduce_mean(self.covloss)\n\n    def make_fcnet(self):\n        n_ffnet_inputs = self.n_ffnet_input\n        n_ffnet_outputs = self.n_ffnet_output\n\n        print(\"FCNET: Inputs: \", n_ffnet_inputs, \" outputs: \", n_ffnet_outputs)\n\n        W_fc1 = my_ops.weight_variable([n_ffnet_inputs, 8], 0.003)\n        b_fc1 = my_ops.bias_variable([8])\n\n        W_fc2 = my_ops.weight_variable([8, 2], 0.003)\n        b_fc2 = my_ops.bias_variable([2])\n\n        W_fc3 = my_ops.weight_variable([2, 1], 0.003)\n        b_fc3 = my_ops.bias_variable([1])\n\n        h1 = tf.tanh(tf.matmul(self.fcnet_input, W_fc1) + b_fc1)\n        h2 = tf.tanh(tf.matmul(h1, W_fc2) + b_fc2)\n        self.y_fc = tf.tanh(tf.matmul(h2, W_fc3) + b_fc3)\n\n        self.fcloss = tf.squared_difference(self.y_fc, self.fcnet_target)\n        self.fcnet_train_step = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.fcloss)\n        self.fcaccuracy = tf.reduce_mean(self.fcloss)\n\n    def make_ffnet(self):\n\n\n        n_ffnet_inputs = self.n_ffnet_input\n        n_ffnet_outputs = self.n_ffnet_output\n        print (\"FFNET: in: \", n_ffnet_inputs, \" hid: \", self.n_ffnet_hidden, \" out: \", n_ffnet_outputs)\n\n\n        W_layer1 = my_ops.weight_variable([n_ffnet_inputs, self.n_ffnet_hidden[0]])\n        b_layer1 = my_ops.bias_variable([self.n_ffnet_hidden[0]])\n\n        W_layer2 = my_ops.weight_variable([self.n_ffnet_hidden[0], self.n_ffnet_hidden[1]])\n        b_layer2 = my_ops.bias_variable([self.n_ffnet_hidden[1]])\n\n        W_layer3 = my_ops.weight_variable([self.n_ffnet_hidden[1], n_ffnet_outputs])\n        b_layer3 = my_ops.bias_variable([n_ffnet_outputs])\n\n        h_1 = tf.nn.relu(tf.matmul(self.ffnet_input, W_layer1) + b_layer1)\n        h_2 = tf.nn.relu(tf.matmul(h_1, W_layer2) + b_layer2)\n\n        # dropout\n        #print(\"output shape: \", self.ffnet_output.get_shape(), \"target shape: \", self.ffnet_target.get_shape())\n        #print(\"W3: \", W_layer3.get_shape(), \" bias3: \", b_layer3.get_shape())\n\n        self.ffnet_output = tf.matmul(h_2, W_layer3) + b_layer3\n        #print(\"output shape: \", self.ffnet_output.get_shape(), \"target shape: \", self.ffnet_target.get_shape())\n        #print(\"W3: \", W_layer3.get_shape(), \" bias3: \", b_layer3.get_shape())\n\n        self.loss = tf.squared_difference(self.ffnet_output, self.ffnet_target)\n\n        self.ffnet_train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n\n        self.accuracy = tf.reduce_mean(self.loss)\n#        sess.run(tf.global_variables_initializer())\n\n    def make_net(self, input_images, input_measurements, input_actions, reuse=False):\n        if reuse:\n            tf.get_variable_scope().reuse_variables()\n        \n        self.fc_val_params = np.copy(self.fc_joint_params)\n        self.fc_val_params['out_dims'][-1] = self.target_dim\n        self.fc_adv_params = np.copy(self.fc_joint_params)\n        self.fc_adv_params['out_dims'][-1] = len(self.net_discrete_actions) * self.target_dim\n        print(len(self.net_discrete_actions) * self.target_dim)\n        p_img_conv = my_ops.conv_encoder(input_images, self.conv_params, 'p_img_conv', msra_coeff=0.9)\n        print (\"Conv Params: \", self.conv_params)\n\n        p_img_fc = my_ops.fc_net(my_ops.flatten(p_img_conv), self.fc_img_params, 'p_img_fc', msra_coeff=0.9)\n        print (\"img_params\", self.fc_img_params)\n        p_meas_fc = my_ops.fc_net(input_measurements, self.fc_meas_params, 'p_meas_fc', msra_coeff=0.9)\n        print (\"meas_params\", self.fc_meas_params)\n        p_val_fc = my_ops.fc_net(tf.concat(1, [p_img_fc,p_meas_fc]), self.fc_val_params, 'p_val_fc', last_linear=True, msra_coeff=0.9)\n        print (\"val_params\", self.fc_val_params)\n        p_adv_fc = my_ops.fc_net(tf.concat(1, [p_img_fc,p_meas_fc]), self.fc_adv_params, 'p_adv_fc', last_linear=True, msra_coeff=0.9)\n        print (\"adv_params\", self.fc_adv_params)\n\n        p_adv_fc_nomean = p_adv_fc - tf.reduce_mean(p_adv_fc, reduction_indices=1, keep_dims=True)  \n        \n        self.pred_all_nomean = tf.reshape(p_adv_fc_nomean, [-1, len(self.net_discrete_actions), self.target_dim])\n        self.pred_all = self.pred_all_nomean + tf.reshape(p_val_fc, [-1, 1, self.target_dim])\n        self.pred_relevant = tf.boolean_mask(self.pred_all, tf.cast(input_actions, tf.bool))\n        print (\"make_net: input_actions: \", input_actions)\n        print (\"make_net: pred_all: \", self.pred_all)\n        print (\"make_net: pred_relevant: \", self.pred_relevant)\n\n    def build_model(self):\n\n        #self.make_ffnet()\n        #self.make_convnet()\n        self.make_fcnet()\n        self.saver = tf.train.Saver()\n        tf.initialize_all_variables().run(session=self.sess)\n    \n    def act(self, state_imgs, state_meas, objective):\n        return self.postprocess_actions(self.act_net(state_imgs, state_meas, objective), self.act_manual(state_meas)), None # last output should be predictions, but we omit these for now\n\n    def act_ffnet(self, in_image, in_meas, target):\n\n        net_inputs = in_image\n        net_targets = target\n\n        with self.sess.as_default():\n            self.ext_ffnet_output, hack = self.sess.run([self.ffnet_output, self.ffnet_train_step], feed_dict={\n                self.ffnet_input: net_inputs,\n                self.ffnet_target: net_targets\n            })\n\n            if self.iter % self.epoch == 0:\n                print (\"LOSS: \", self.accuracy.eval(feed_dict={\n                    self.ffnet_input: net_inputs,\n                    self.ffnet_target: net_targets\n                }))\n\n        self.iter = self.iter+1\n\n    def act_covnet(self, in_image, in_meas, target):\n\n        net_inputs = in_image\n        net_targets = target\n\n        with self.sess.as_default():\n            self.sess.run([self.covnet_train_step], feed_dict={\n                self.covnet_input: net_inputs,\n                self.covnet_target: net_targets\n            })\n            self.ext_covnet_output = self.sess.run([self.y_conv], feed_dict={\n                self.covnet_input: net_inputs,\n                self.covnet_target: net_targets\n            })[0]\n\n            if self.iter % self.epoch == 0:\n                print (\"LOSS: \", self.covaccuracy.eval(feed_dict={\n                    self.covnet_input: net_inputs,\n                    self.covnet_target: net_targets\n                }))\n\n        self.iter = self.iter+1\n\n    def act_fcnet(self, in_image, in_meas, target):\n\n        net_inputs = in_image\n        net_targets = target\n\n        if (in_meas[1] > 0):\n\n            with self.sess.as_default():\n                self.sess.run([self.fcnet_train_step], feed_dict={\n                    self.fcnet_input: net_inputs,\n                    self.fcnet_target: net_targets\n                })\n                self.ext_fcnet_output = self.sess.run([self.y_fc], feed_dict={\n                    self.fcnet_input: net_inputs,\n                    self.fcnet_target: net_targets\n                })[0]\n\n                if self.iter % self.epoch == 0:\n                    print (\"LOSS: \", self.fcaccuracy.eval(feed_dict={\n                        self.fcnet_input: net_inputs,\n                        self.fcnet_target: net_targets\n                    }))\n\n            self.iter = self.iter+1\n\n\n    def act_net(self, state_imgs, state_meas, objective):\n        #Act given a state and objective\n        predictions = self.sess.run(self.pred_all, feed_dict={self.input_images: state_imgs, \n                                                            self.input_measurements: state_meas[:,self.meas_for_net]})\n        #print (predictions)\n\n        objectives = np.sum(predictions[:,:,objective[0]]*objective[1][None,None,:], axis=2)    \n        curr_action = np.argmax(objectives, axis=1)\n#        print (\" ** ACTION \", curr_action)\n        return curr_action\n\n    # act_manual is a purely hard-coded method to handle weapons-switching\n    def act_manual(self, state_meas):\n        if len(self.meas_for_manual) == 0:\n            return []\n        else:\n            assert(len(self.meas_for_manual) == 13) # expected to be [AMMO2 AMMO3 AMMO4 AMMO5 AMMO6 AMMO7 WEAPON2 WEAPON3 WEAPON4 WEAPON5 WEAPON6 WEAPON7 SELECTED_WEAPON]\n            assert(self.num_manual_controls == 6) # expected to be [SELECT_WEAPON2 SELECT_WEAPON3 SELECT_WEAPON4 SELECT_WEAPON5 SELECT_WEAPON6 SELECT_WEAPON7]\n\n            curr_act = np.zeros((state_meas.shape[0],self.num_manual_controls), dtype=np.int)\n            for ns in range(state_meas.shape[0]):\n                # always pistol\n                #if not state_meas[ns,self.meas_for_manual[12]] == 2:\n                    #curr_act[ns, 0] = 1\n                # best weapon\n                curr_ammo = state_meas[ns,self.meas_for_manual[:6]]\n                curr_weapons = state_meas[ns,self.meas_for_manual[6:12]]\n                #print(curr_ammo,curr_weapons)\n                available_weapons = np.logical_and(curr_ammo >= np.array([1,2,1,1,1,40]), curr_weapons)\n                if any(available_weapons):\n                    best_weapon = np.nonzero(available_weapons)[0][-1]\n                    if not state_meas[ns,self.meas_for_manual[12]] == best_weapon+2:\n                        curr_act[ns, best_weapon] = 1\n            return curr_act\n\n    def save(self, checkpoint_dir, iter):\n        self.save_path = self.saver.save(self.sess, checkpoint_dir, global_step=iter)\n        print (\"saving model file: \", self.save_path)\n\n\n\n    def load(self, checkpoint_dir):\n        self.saver.restore(self.sess, tf.train.latest_checkpoint(checkpoint_dir))\n        return True\n"
  },
  {
    "id": "ml_7ed95e13",
    "framework": "unknown",
    "source_url": "https://github.com/chaitjo/personalized-dialog/blob/master/MemN2N-split-memory/single_dialog.py",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "self.learning_rate",
        "line_number": 123,
        "char_start": 26,
        "char_end": 44,
        "code_context": "        self.candidates_vec = vectorize_candidates(\n            candidates,self.word_idx, self.candidate_sentence_size)\n        \n        optimizer = tf.train.AdamOptimizer(\n            learning_rate=self.learning_rate, epsilon=self.epsilon)\n        \n        self.sess = tf.Session()\n        \n        self.model = MemN2NDialog(self.batch_size, self.vocab_size, self.n_cand, \n                                  self.sentence_size, self.embedding_size, ",
        "param_type": "optimizer",
        "explanation": "The `learning_rate` parameter controls the step size that the optimizer takes in the direction of the gradient during each update. It determines how quickly the model learns and adapts to the training data.",
        "typical_range": "0.001 to 0.1",
        "alternatives": [
          {
            "value": "0.001",
            "scenario": "If the loss function is decreasing slowly, consider using a lower learning rate to allow for more gradual adjustments and potentially avoid overshooting the optimal solution."
          },
          {
            "value": "0.1",
            "scenario": "If the loss function is decreasing rapidly but you're not seeing significant improvements in model performance, consider using a higher learning rate to speed up the learning process."
          },
          {
            "value": "Adaptive learning rate optimizers (e.g., Adam, RMSprop)",
            "scenario": "These optimizers can automatically adjust the learning rate during training, potentially leading to faster convergence and better generalization."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      },
      {
        "name": "optimizer",
        "value": "optimizer",
        "line_number": 131,
        "char_start": 44,
        "char_end": 53,
        "code_context": "        self.model = MemN2NDialog(self.batch_size, self.vocab_size, self.n_cand, \n                                  self.sentence_size, self.embedding_size, \n                                  self.candidates_vec, session=self.sess, \n                                  hops=self.hops, max_grad_norm=self.max_grad_norm, \n                                  optimizer=optimizer, task_id=task_id)\n        \n        self.saver = tf.train.Saver(max_to_keep=50)\n        \n        self.summary_writer = tf.summary.FileWriter(\n            self.model.root_dir, self.model.graph_output.graph)",
        "param_type": "other",
        "explanation": "The optimizer parameter defines the optimization algorithm used during training. It controls how the model updates its parameters in response to the loss function.",
        "typical_range": "The typical range for this parameter depends on the specific optimization algorithm being used. Some common algorithms include:\n  * Adam: learning_rate=0.001, beta_1=0.9, beta_2=0.999\n  * SGD: learning_rate=0.01, momentum=0.9\n  * RMSprop: learning_rate=0.001, rho=0.9",
        "alternatives": [
          {
            "value": "Adam",
            "scenario": "When dealing with complex, non-convex loss surfaces and wanting faster convergence."
          },
          {
            "value": "SGD",
            "scenario": "When dealing with simpler problems and wanting more control over the learning process."
          },
          {
            "value": "RMSprop",
            "scenario": "When encountering vanishing gradients issues and wanting faster convergence compared to SGD."
          }
        ],
        "impact": {
          "convergence_speed": "The impact on convergence speed depends on the specific optimizer and its parameters. Generally, Adam tends to converge faster than SGD or RMSprop.",
          "generalization": "The impact on generalization depends on the optimizer and its parameters. Adam is often considered to improve generalization due to its adaptive learning rate.",
          "stability": "The impact on stability also depends on the optimizer and its parameters. RMSprop is known for its stability in cases where gradients might fluctuate significantly."
        }
      },
      {
        "name": "batch_size",
        "value": "FLAGS.batch_size",
        "line_number": 298,
        "char_start": 33,
        "char_end": 49,
        "code_context": "    model_dir = \"task\" + str(FLAGS.task_id) + \"_\" + FLAGS.model_dir\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    chatbot = chatBot(FLAGS.data_dir, model_dir, FLAGS.task_id, OOV=FLAGS.OOV,\n                      batch_size=FLAGS.batch_size, memory_size=FLAGS.memory_size,\n                      epochs=FLAGS.epochs, hops=FLAGS.hops, save_vocab=FLAGS.save_vocab,\n                      load_vocab=FLAGS.load_vocab, learning_rate=FLAGS.learning_rate,\n                      embedding_size=FLAGS.embedding_size)\n    \n    if FLAGS.train:",
        "param_type": "training",
        "explanation": "The number of samples used in one training iteration. It affects how frequently the model updates its internal parameters.",
        "typical_range": "32-256 for large datasets, 4-32 for small datasets",
        "alternatives": [
          {
            "value": "32",
            "scenario": "For moderate datasets and memory constraints"
          },
          {
            "value": "64",
            "scenario": "For larger datasets with sufficient memory"
          },
          {
            "value": "128",
            "scenario": "For even larger datasets and/or faster convergence with ample memory"
          }
        ],
        "impact": {
          "convergence_speed": "fast (larger batch size)",
          "generalization": "good (smaller batch size)",
          "stability": "medium (depends on various factors including dataset size)"
        }
      },
      {
        "name": "epochs",
        "value": "FLAGS.epochs",
        "line_number": 299,
        "char_start": 29,
        "char_end": 41,
        "code_context": "    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    chatbot = chatBot(FLAGS.data_dir, model_dir, FLAGS.task_id, OOV=FLAGS.OOV,\n                      batch_size=FLAGS.batch_size, memory_size=FLAGS.memory_size,\n                      epochs=FLAGS.epochs, hops=FLAGS.hops, save_vocab=FLAGS.save_vocab,\n                      load_vocab=FLAGS.load_vocab, learning_rate=FLAGS.learning_rate,\n                      embedding_size=FLAGS.embedding_size)\n    \n    if FLAGS.train:\n        chatbot.train()",
        "param_type": "training",
        "explanation": "The number of times the training dataset is passed through the neural network.",
        "typical_range": "10-1000 epochs (depending on the dataset size and complexity)",
        "alternatives": [
          {
            "value": "200",
            "scenario": "Large dataset, complex model, good performance"
          },
          {
            "value": "50",
            "scenario": "Small dataset, simple model, fast training"
          },
          {
            "value": "1000",
            "scenario": "Very large dataset, complex model, need for high accuracy"
          }
        ],
        "impact": {
          "convergence_speed": "fast->slow (with increasing epochs)",
          "generalization": "improves->might overfit (with increasing epochs)",
          "stability": "high"
        }
      },
      {
        "name": "learning_rate",
        "value": "FLAGS.learning_rate",
        "line_number": 300,
        "char_start": 65,
        "char_end": 84,
        "code_context": "        os.makedirs(model_dir)\n    chatbot = chatBot(FLAGS.data_dir, model_dir, FLAGS.task_id, OOV=FLAGS.OOV,\n                      batch_size=FLAGS.batch_size, memory_size=FLAGS.memory_size,\n                      epochs=FLAGS.epochs, hops=FLAGS.hops, save_vocab=FLAGS.save_vocab,\n                      load_vocab=FLAGS.load_vocab, learning_rate=FLAGS.learning_rate,\n                      embedding_size=FLAGS.embedding_size)\n    \n    if FLAGS.train:\n        chatbot.train()\n    else:",
        "param_type": "optimizer",
        "explanation": "The learning rate defines the step size the optimizer takes during each update step. Smaller values result in slower but more precise learning, while larger values can lead to faster learning but potentially inaccurate results or instability.",
        "typical_range": "A common range for the learning rate is between 0.001 and 0.1, although the optimal value can vary depending on the specific model and task.",
        "alternatives": [
          {
            "value": "Smaller values (e.g., 0.001)",
            "scenario": "When fine-tuning a pre-trained model or dealing with sensitive tasks where stability is crucial."
          },
          {
            "value": "Larger values (e.g., 0.1)",
            "scenario": "When training from scratch or seeking faster convergence during initial training stages."
          }
        ],
        "impact": {
          "convergence_speed": "fast|medium|slow",
          "generalization": "poor|good|excellent",
          "stability": "low|medium|high"
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "unknown",
    "code_snippet": "from __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom data_utils import load_dialog_task, vectorize_data, load_candidates, vectorize_candidates, vectorize_candidates_sparse, tokenize\nfrom sklearn import metrics\nfrom memn2n import MemN2NDialog\nfrom itertools import chain\nfrom six.moves import range, reduce\nimport sys\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport pickle\n\ntf.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate for Adam Optimizer.\")\ntf.flags.DEFINE_float(\"epsilon\", 1e-8, \"Epsilon value for Adam Optimizer.\")\ntf.flags.DEFINE_float(\"max_grad_norm\", 40.0, \"Clip gradients to this norm.\")\ntf.flags.DEFINE_integer(\"evaluation_interval\", 10, \"Evaluate and print results every x epochs\")\ntf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\ntf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\ntf.flags.DEFINE_integer(\"epochs\", 250, \"Number of epochs to train for.\")\ntf.flags.DEFINE_integer(\"embedding_size\", 20, \"Embedding size for embedding matrices.\")\ntf.flags.DEFINE_integer(\"memory_size\", 250, \"Maximum size of memory.\")\ntf.flags.DEFINE_integer(\"task_id\", 1, \"task id, 1 <= id <= 5\")\ntf.flags.DEFINE_integer(\"random_state\", None, \"Random state.\")\ntf.flags.DEFINE_string(\"data_dir\", \"../data/personalized-dialog-dataset/full\", \"Directory containing personalized dialog tasks\")\ntf.flags.DEFINE_string(\"model_dir\", \"model/\", \"Directory containing memn2n model checkpoints\")\ntf.flags.DEFINE_boolean('train', True, 'if True, begin to train')\ntf.flags.DEFINE_boolean('OOV', False, 'if True, use OOV test set')\ntf.flags.DEFINE_boolean('save_vocab', False, 'if True, saves vocabulary')\ntf.flags.DEFINE_boolean('load_vocab', False, 'if True, loads vocabulary instead of building it')\nFLAGS = tf.flags.FLAGS\nprint(\"Started Task:\", FLAGS.task_id)\n\n\nclass chatBot(object):\n    def __init__(self, data_dir, model_dir, task_id, \n                 OOV=False, \n                 memory_size=250,\n                 random_state=None, \n                 batch_size=32, \n                 learning_rate=0.001, \n                 epsilon=1e-8, \n                 max_grad_norm=40.0, \n                 evaluation_interval=10, \n                 hops=3, \n                 epochs=200, \n                 embedding_size=20, \n                 save_vocab=False, \n                 load_vocab=False):\n        \"\"\"Creates wrapper for training and testing a chatbot model.\n\n        Args:\n            data_dir: Directory containing personalized dialog tasks.\n            \n            model_dir: Directory containing memn2n model checkpoints.\n\n            task_id: Personalized dialog task id, 1 <= id <= 5. Defaults to `1`.\n\n            OOV: If `True`, use OOV test set. Defaults to `False`\n\n            memory_size: The max size of the memory. Defaults to `250`.\n\n            random_state: Random state to set graph-level random seed. Defaults to `None`.\n\n            batch_size: Size of the batch for training. Defaults to `32`.\n\n            learning_rate: Learning rate for Adam Optimizer. Defaults to `0.001`.\n\n            epsilon: Epsilon value for Adam Optimizer. Defaults to `1e-8`.\n\n            max_gradient_norm: Maximum L2 norm clipping value. Defaults to `40.0`.\n\n            evaluation_interval: Evaluate and print results every x epochs. \n            Defaults to `10`.\n\n            hops: The number of hops over memory for responding. A hop consists \n            of reading and addressing a memory slot. Defaults to `3`.\n\n            epochs: Number of training epochs. Defualts to `200`.\n\n            embedding_size: The size of the word embedding. Defaults to `20`.\n\n            save_vocab: If `True`, save vocabulary file. Defaults to `False`.\n\n            load_vocab: If `True`, load vocabulary from file. Defaults to `False`.\n        \"\"\"\n        \n        self.data_dir = data_dir\n        self.task_id = task_id\n        self.model_dir = model_dir\n        self.OOV = OOV\n        self.memory_size = memory_size\n        self.random_state = random_state\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon\n        self.max_grad_norm = max_grad_norm\n        self.evaluation_interval = evaluation_interval\n        self.hops = hops\n        self.epochs = epochs\n        self.embedding_size = embedding_size\n        self.save_vocab = save_vocab\n        self.load_vocab = load_vocab\n\n        candidates,self.candid2indx = load_candidates(self.data_dir, self.task_id)\n        self.n_cand = len(candidates)\n        print(\"Candidate Size\", self.n_cand)\n        self.indx2candid = dict((self.candid2indx[key],key) \n                                for key in self.candid2indx)\n        \n        # Task data\n        self.trainData, self.testData, self.valData = load_dialog_task(\n            self.data_dir, self.task_id, self.candid2indx, self.OOV)\n        data = self.trainData + self.testData + self.valData\n        \n        self.build_vocab(data,candidates, self.save_vocab, self.load_vocab)\n        \n        self.candidates_vec = vectorize_candidates(\n            candidates,self.word_idx, self.candidate_sentence_size)\n        \n        optimizer = tf.train.AdamOptimizer(\n            learning_rate=self.learning_rate, epsilon=self.epsilon)\n        \n        self.sess = tf.Session()\n        \n        self.model = MemN2NDialog(self.batch_size, self.vocab_size, self.n_cand, \n                                  self.sentence_size, self.embedding_size, \n                                  self.candidates_vec, session=self.sess, \n                                  hops=self.hops, max_grad_norm=self.max_grad_norm, \n                                  optimizer=optimizer, task_id=task_id)\n        \n        self.saver = tf.train.Saver(max_to_keep=50)\n        \n        self.summary_writer = tf.summary.FileWriter(\n            self.model.root_dir, self.model.graph_output.graph)\n        \n\n    def build_vocab(self,data,candidates,save=False,load=False):\n        \"\"\"Build vocabulary of words from all dialog data and candidates.\"\"\"\n        if load:  \n            # Load from vocabulary file\n            vocab_file = open('vocab'+str(self.task_id)+'.obj', 'rb')\n            vocab = pickle.load(vocab_file)\n        else:\n            vocab = reduce(lambda x, y: x | y,\n                           (set(list(chain.from_iterable(s)) + q) \n                             for p, s, q, a in data))\n            vocab |= reduce(lambda x, y: x | y, \n                            (set(list(chain.from_iterable(p)) + q) \n                             for p, s, q, a in data))\n            vocab |= reduce(lambda x, y: x | y, \n                            (set(candidate) for candidate in candidates) )\n            vocab = sorted(vocab)\n        \n        self.word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n        max_story_size = max(map(len, (s for _, s, _, _ in data)))\n        mean_story_size = int(np.mean([ len(s) for _, s, _, _ in data ]))\n        self.sentence_size = max(map(len, chain.from_iterable(s for _, s, _, _ in data)))\n        self.candidate_sentence_size=max(map(len,candidates))\n        query_size = max(map(len, (q for _, _, q, _ in data)))\n        self.memory_size = min(self.memory_size, max_story_size)\n        self.vocab_size = len(self.word_idx) + 1  # +1 for nil word\n        self.sentence_size = max(query_size, self.sentence_size)  # for the position\n        \n        # Print parameters\n        print(\"vocab size:\", self.vocab_size)\n        print(\"Longest sentence length\", self.sentence_size)\n        print(\"Longest candidate sentence length\", self.candidate_sentence_size)\n        print(\"Longest story length\", max_story_size)\n        print(\"Average story length\", mean_story_size)\n\n        # Save to vocabulary file\n        if save:\n            vocab_file = open('vocab'+str(self.task_id)+'.obj', 'wb')\n            pickle.dump(vocab, vocab_file)\n\n    def train(self):\n        \"\"\"Runs the training algorithm over training set data.\n\n        Performs validation at given evaluation intervals.\n        \"\"\"\n        trainP, trainS, trainQ, trainA = vectorize_data(\n            self.trainData, self.word_idx, self.sentence_size, \n            self.batch_size, self.n_cand, self.memory_size)\n        valP, valS, valQ, valA = vectorize_data(\n            self.valData, self.word_idx, self.sentence_size, \n            self.batch_size, self.n_cand, self.memory_size)\n        n_train = len(trainS)\n        n_val = len(valS)\n        print(\"Training Size\", n_train)\n        print(\"Validation Size\", n_val)\n        tf.set_random_seed(self.random_state)\n        batches = zip(range(0, n_train-self.batch_size, self.batch_size), \n                      range(self.batch_size, n_train, self.batch_size))\n        batches = [(start, end) for start, end in batches]\n        best_validation_accuracy=0\n        \n        # Training loop\n        for t in range(1, self.epochs+1):\n            print('Epoch', t)\n            np.random.shuffle(batches)\n            total_cost = 0.0\n            for start, end in batches:\n                p = trainP[start:end]\n                s = trainS[start:end]\n                q = trainQ[start:end]\n                a = trainA[start:end]\n                cost_t = self.model.batch_fit(p, s, q, a)\n                total_cost += cost_t\n            if t % self.evaluation_interval == 0:\n                # Perform validation\n                train_preds = self.batch_predict(trainP,trainS,trainQ,n_train)\n                val_preds = self.batch_predict(valP,valS,valQ,n_val)\n                train_acc = metrics.accuracy_score(np.array(train_preds), trainA)\n                val_acc = metrics.accuracy_score(val_preds, valA)\n                print('-----------------------')\n                print('Epoch', t)\n                print('Total Cost:', total_cost)\n                print('Training Accuracy:', train_acc)\n                print('Validation Accuracy:', val_acc)\n                print('-----------------------')\n\n                # Write summary\n                train_acc_summary = tf.summary.scalar(\n                    'task_' + str(self.task_id) + '/' + 'train_acc', \n                    tf.constant((train_acc), dtype=tf.float32))\n                val_acc_summary = tf.summary.scalar(\n                    'task_' + str(self.task_id) + '/' + 'val_acc', \n                    tf.constant((val_acc), dtype=tf.float32))\n                merged_summary = tf.summary.merge([train_acc_summary, val_acc_summary])\n                summary_str = self.sess.run(merged_summary)\n                self.summary_writer.add_summary(summary_str, t)\n                self.summary_writer.flush()\n                \n                if val_acc > best_validation_accuracy:\n                    best_validation_accuracy = val_acc\n                    self.saver.save(self.sess, self.model_dir+'model.ckpt',\n                                    global_step=t)\n                    \n    def test(self):\n        \"\"\"Runs testing on testing set data.\n\n        Loads best performing model weights based on validation accuracy.\n        \"\"\"\n        ckpt = tf.train.get_checkpoint_state(self.model_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n        else:\n            print(\"...no checkpoint found...\")\n        \n        testP, testS, testQ, testA = vectorize_data(\n            self.testData, self.word_idx, self.sentence_size, \n            self.batch_size, self.n_cand, self.memory_size)\n        n_test = len(testS)\n        print(\"Testing Size\", n_test)\n        \n        test_preds = self.batch_predict(testP, testS, testQ, n_test)\n        test_acc = metrics.accuracy_score(test_preds, testA)\n        print(\"Testing Accuracy:\", test_acc)\n        \n        # # Un-comment below to view correct responses and predictions \n        # print(testA)\n        # for pred in test_preds:\n        #    print(pred, self.indx2candid[pred])\n\n    def batch_predict(self,P,S,Q,n):\n        \"\"\"Predict answers over the passed data in batches.\n\n        Args:\n            P: Tensor (None, sentence_size)\n            S: Tensor (None, memory_size, sentence_size)\n            Q: Tensor (None, sentence_size)\n            n: int\n\n        Returns:\n            preds: Tensor (None, vocab_size)\n        \"\"\"\n        preds = []\n        for start in range(0, n, self.batch_size):\n            end = start + self.batch_size\n            p = P[start:end]\n            s = S[start:end]\n            q = Q[start:end]\n            pred = self.model.predict(p, s, q)\n            preds += list(pred)\n        return preds\n\n    def close_session(self):\n        self.sess.close()\n\n\nif __name__ == '__main__':\n    model_dir = \"task\" + str(FLAGS.task_id) + \"_\" + FLAGS.model_dir\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    chatbot = chatBot(FLAGS.data_dir, model_dir, FLAGS.task_id, OOV=FLAGS.OOV,\n                      batch_size=FLAGS.batch_size, memory_size=FLAGS.memory_size,\n                      epochs=FLAGS.epochs, hops=FLAGS.hops, save_vocab=FLAGS.save_vocab,\n                      load_vocab=FLAGS.load_vocab, learning_rate=FLAGS.learning_rate,\n                      embedding_size=FLAGS.embedding_size)\n    \n    if FLAGS.train:\n        chatbot.train()\n    else:\n        chatbot.test()\n    \n    chatbot.close_session()\n"
  },
  {
    "id": "ml_b4706286",
    "framework": "unknown",
    "source_url": "https://github.com/chaitjo/personalized-dialog/blob/master/MemN2N/memn2n/memn2n_dialog.py",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "0.01",
        "line_number": 18,
        "char_start": 64,
        "char_end": 68,
        "code_context": "                 hops=3,\n                 max_grad_norm=40.0,\n                 nonlin=None,\n                 initializer=tf.random_normal_initializer(stddev=0.1),\n                 optimizer=tf.train.AdamOptimizer(learning_rate=1e-2),\n                 session=tf.Session(),\n                 name='MemN2N',\n                 task_id=1):\n        \"\"\"Creates an End-To-End Memory Network\n",
        "param_type": "optimizer",
        "explanation": "The `learning_rate` parameter controls the step size used to update the model's weights during training. A higher learning rate means larger updates, which can lead to faster convergence but also potential instability. A lower learning rate means smaller updates, which can lead to slower convergence but also greater stability.",
        "typical_range": "The typical range for the learning rate is between 1e-4 and 1e-1, but the optimal value can vary significantly depending on the specific problem and dataset.",
        "alternatives": [
          {
            "value": "1e-3",
            "scenario": "If the model is converging too slowly, consider increasing the learning rate."
          },
          {
            "value": "1e-4",
            "scenario": "If the model is not converging or is becoming unstable, consider decreasing the learning rate."
          },
          {
            "value": "1e-2",
            "scenario": "This is a good starting point for many problems."
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "unknown",
    "code_snippet": "from __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\nfrom six.moves import range\nfrom datetime import datetime\n\n\nclass MemN2NDialog(object):\n    \"\"\"End-To-End Memory Network.\"\"\"\n    def __init__(self, batch_size, vocab_size, candidates_size, \n                 sentence_size, embedding_size, candidates_vec,\n                 hops=3,\n                 max_grad_norm=40.0,\n                 nonlin=None,\n                 initializer=tf.random_normal_initializer(stddev=0.1),\n                 optimizer=tf.train.AdamOptimizer(learning_rate=1e-2),\n                 session=tf.Session(),\n                 name='MemN2N',\n                 task_id=1):\n        \"\"\"Creates an End-To-End Memory Network\n\n        Args:\n            batch_size: The size of the batch.\n\n            vocab_size: The size of the vocabulary (should include the nil word). \n            The nil word one-hot encoding should be 0.\n\n            sentence_size: The max size of a sentence in the data. All sentences \n            should be padded to this length. If padding is required it should be \n            done with nil one-hot encoding (0).\n\n            candidates_size: The size of candidates\n\n            memory_size: The max size of the memory. Since Tensorflow currently \n            does not support jagged arrays all memories must be padded to this \n            length. If padding is required, the extra memories should be empty \n            memories; memories filled with the nil word ([0, 0, 0, ......, 0]).\n\n            embedding_size: The size of the word embedding.\n\n            candidates_vec: The numpy array of candidates encoding.\n\n            hops: The number of hops. A hop consists of reading and addressing \n            a memory slot. Defaults to `3`.\n\n            max_grad_norm: Maximum L2 norm clipping value. Defaults to `40.0`.\n\n            nonlin: Non-linearity. Defaults to `None`.\n\n            initializer: Weight initializer. Defaults to `tf.random_normal_initializer(stddev=0.1)`.\n\n            optimizer: Optimizer algorithm used for SGD. Defaults to `tf.train.AdamOptimizer(learning_rate=1e-2)`.\n\n            encoding: A function returning a 2D Tensor (sentence_size, embedding_size). \n            Defaults to `position_encoding`.\n\n            session: Tensorflow Session the model is run with. Defaults to `tf.Session()`.\n\n            name: Name of the End-To-End Memory Network. Defaults to `MemN2N`.\n        \"\"\"\n\n        self._batch_size = batch_size\n        self._vocab_size = vocab_size\n        self._candidates_size = candidates_size\n        self._sentence_size = sentence_size\n        self._embedding_size = embedding_size\n        self._hops = hops\n        self._max_grad_norm = max_grad_norm\n        self._nonlin = nonlin\n        self._init = initializer\n        self._opt = optimizer\n        self._name = name\n        self._candidates=candidates_vec\n\n        self._build_inputs()\n        self._build_vars()\n        \n        # Define summary directory\n        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n        self.root_dir = \"%s_%s_%s_%s/\" % ('task', str(task_id),'summary_output', timestamp)\n        \n        # Calculate cross entropy\n        # dimensions: (batch_size, candidates_size)\n        logits = self._inference(self._stories, self._queries)\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits, self._answers, name=\"cross_entropy\")\n        cross_entropy_sum = tf.reduce_sum(cross_entropy, name=\"cross_entropy_sum\")\n\n        # Define loss op\n        loss_op = cross_entropy_sum\n\n        # Gradient pipeline\n        grads_and_vars = self._opt.compute_gradients(loss_op)\n        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v) \n                            for g,v in grads_and_vars]\n        # grads_and_vars = [(add_gradient_noise(g), v) for g,v in grads_and_vars]\n        nil_grads_and_vars = []\n        for g, v in grads_and_vars:\n            if v.name in self._nil_vars:\n                nil_grads_and_vars.append((zero_nil_slot(g), v))\n            else:\n                nil_grads_and_vars.append((g, v))\n        train_op = self._opt.apply_gradients(nil_grads_and_vars, name=\"train_op\")\n\n        # Define predict ops\n        predict_op = tf.argmax(logits, 1, name=\"predict_op\")\n        predict_proba_op = tf.nn.softmax(logits, name=\"predict_proba_op\")\n        predict_log_proba_op = tf.log(predict_proba_op, name=\"predict_log_proba_op\")\n\n        # Assign ops\n        self.loss_op = loss_op\n        self.predict_op = predict_op\n        self.predict_proba_op = predict_proba_op\n        self.predict_log_proba_op = predict_log_proba_op\n        self.train_op = train_op\n        self.graph_output = self.loss_op\n\n        init_op = tf.global_variables_initializer()\n        self._sess = session\n        self._sess.run(init_op)\n\n    def _build_inputs(self):\n        \"\"\"Define input placeholders\"\"\"\n        self._stories = tf.placeholder(\n            tf.int32, [None, None, self._sentence_size], name=\"stories\")\n        self._queries = tf.placeholder(\n            tf.int32, [None, self._sentence_size], name=\"queries\")\n        self._answers = tf.placeholder(tf.int32, [None], name=\"answers\")\n\n    def _build_vars(self):\n        \"\"\"Define trainable variables\"\"\"\n        with tf.variable_scope(self._name):\n            nil_word_slot = tf.zeros([1, self._embedding_size])\n            A = tf.concat(0, [ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n            self.A = tf.Variable(A, name=\"A\")\n            self.H = tf.Variable(self._init([self._embedding_size, self._embedding_size]), name=\"H\")\n            W = tf.concat(0, [ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n            self.W = tf.Variable(W, name=\"W\")\n        self._nil_vars = set([self.A.name,self.W.name])\n\n    def _inference(self, stories, queries):\n        \"\"\"Forward pass through the model\"\"\"\n        with tf.variable_scope(self._name):\n            q_emb = tf.nn.embedding_lookup(self.A, queries)  # Queries vector\n            \n            # Initial state of memory controller for conversation history\n            u_0 = tf.reduce_sum(q_emb, 1)\n            u = [u_0]\n\n            # Iterate over memory for number of hops\n            for count in range(self._hops):\n                m_emb = tf.nn.embedding_lookup(self.A, stories)  # Stories vector \n                m = tf.reduce_sum(m_emb, 2)  # Conversation history memory \n                \n                # Hack to get around no reduce_dot\n                u_temp = tf.transpose(tf.expand_dims(u[-1], -1), [0, 2, 1])\n                dotted = tf.reduce_sum(m * u_temp, 2)\n\n                # Calculate probabilities\n                probs = tf.nn.softmax(dotted)\n                \n                # # Uncomment below to view attention values over memories during inference:\n                # probs = tf.Print(\n                #     probs, ['memory', count, tf.shape(probs), probs], summarize=200)\n\n                probs_temp = tf.transpose(tf.expand_dims(probs, -1), [0, 2, 1])\n                c_temp = tf.transpose(m, [0, 2, 1])\n                \n                # Compute returned vector\n                o_k = tf.reduce_sum(c_temp * probs_temp, 2)\n\n                # Update controller state\n                u_k = tf.matmul(u[-1], self.H) + o_k\n                \n                # Apply nonlinearity\n                if self._nonlin:\n                    u_k = self._nonlin(u_k)\n\n                u.append(u_k)\n\n            candidates_emb=tf.nn.embedding_lookup(self.W, self._candidates)\n            candidates_emb_sum=tf.reduce_sum(candidates_emb,1)\n            \n            return tf.matmul(u_k,tf.transpose(candidates_emb_sum))\n\n    def batch_fit(self, stories, queries, answers):\n        \"\"\"Runs the training algorithm over the passed batch\n\n        Args:\n            stories: Tensor (None, memory_size, sentence_size)\n            queries: Tensor (None, sentence_size)\n            answers: Tensor (None, vocab_size)\n\n        Returns:\n            loss: floating-point number, the loss computed for the batch\n        \"\"\"\n        feed_dict = {self._stories: stories, self._queries: queries, \n                     self._answers: answers}\n        loss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)\n        return loss\n\n    def predict(self, stories, queries):\n        \"\"\"Predicts answers as one-hot encoding.\n\n        Args:\n            stories: Tensor (None, memory_size, sentence_size)\n            queries: Tensor (None, sentence_size)\n\n        Returns:\n            answers: Tensor (None, vocab_size)\n        \"\"\"\n        feed_dict = {self._stories: stories, self._queries: queries}\n        return self._sess.run(self.predict_op, feed_dict=feed_dict)\n\n\ndef zero_nil_slot(t, name=None):\n    \"\"\"Overwrites the nil_slot (first row) of the input Tensor with zeros.\n\n    The nil_slot is a dummy slot and should not be trained and influence\n    the training algorithm.\n    \"\"\"\n    \n    with tf.name_scope(name, \"zero_nil_slot\", [t]) as name:\n        t = tf.convert_to_tensor(t, name=\"t\")\n        s = tf.shape(t)[1]\n        z = tf.zeros(tf.pack([1, s]))\n        return tf.concat(0, [z, tf.slice(t, [1, 0], [-1, -1])], name=name)\n\n\ndef add_gradient_noise(t, stddev=1e-3, name=None):\n    \"\"\"Adds gradient noise as described in http://arxiv.org/abs/1511.06807 [2].\n\n    The input Tensor `t` should be a gradient.\n\n    The output will be `t` + gaussian noise.\n\n    0.001 was said to be a good fixed value for memory networks [2].\n    \"\"\"\n    with tf.name_scope(name, \"add_gradient_noise\", [t, stddev]) as name:\n        t = tf.convert_to_tensor(t, name=\"t\")\n        gn = tf.random_normal(tf.shape(t), stddev=stddev)\n        return tf.add(t, gn, name=name)\n\n"
  },
  {
    "id": "ml_85fecda2",
    "framework": "unknown",
    "source_url": "https://github.com/ram1993/neuralnetwork/blob/master/Tensorflow/CNN.py",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "learning_rate",
        "line_number": 95,
        "char_start": 49,
        "char_end": 62,
        "code_context": "'''\n\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())",
        "param_type": "optimizer",
        "explanation": "The learning rate controls the step size that the optimizer takes during each update of the model's parameters. A high learning rate may cause the model to converge faster, but it can also lead to instability and divergence. A low learning rate may lead to slower convergence, but it can improve stability.",
        "typical_range": "0.001 to 0.1, but can vary depending on the specific problem and dataset.",
        "alternatives": [
          {
            "value": "0.1",
            "scenario": "For large datasets or quickly changing problems"
          },
          {
            "value": "0.01",
            "scenario": "For most problems and datasets"
          },
          {
            "value": "0.001",
            "scenario": "For fine-tuning a model or for complex problems"
          }
        ],
        "impact": {
          "convergence_speed": "higher learning rate: faster | lower learning rate: slower",
          "generalization": "not directly related to generalization",
          "stability": "higher learning rate: lower | lower learning rate: higher"
        }
      }
    ],
    "model_type": "CNN",
    "task": "unknown",
    "code_snippet": "import tensorflow as tf\nimport random\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntf.set_random_seed(105)\n\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\n\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\n\n# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\nkeep_prob = tf.placeholder(tf.float32)\n\n\nX = tf.placeholder(tf.float32, [None, 784])\nY = tf.placeholder(tf.float32, [None, 10])\n\nX_img = tf.reshape(X, [-1, 28, 28, 1])\n\n\n\n# L1 ImgIn shape=(?, 28, 28, 1)\nW1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n#    Conv     -> (?, 28, 28, 32)\n#    Pool     -> (?, 14, 14, 32)\nL1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\nL1 = tf.nn.relu(L1)\nL1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\nL1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n'''\nTensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\nTensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\nTensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\nTensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n'''\n\n# L2 ImgIn shape=(?, 14, 14, 32)\nW2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n#    Conv      ->(?, 14, 14, 64)\n#    Pool      ->(?, 7, 7, 64)\nL2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\nL2 = tf.nn.relu(L2)\nL2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\nL2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n'''\nTensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\nTensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\nTensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\nTensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n'''\n\n# L3 ImgIn shape=(?, 7, 7, 64)\nW3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n#    Conv      ->(?, 7, 7, 128)\n#    Pool      ->(?, 4, 4, 128)\n#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\nL3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\nL3 = tf.nn.relu(L3)\nL3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\nL3 = tf.nn.dropout(L3, keep_prob=keep_prob)\nL3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n'''\nTensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\nTensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\nTensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\nTensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\nTensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n'''\n\n# L4 FC 4x4x128 inputs -> 625 outputs\nW4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625], initializer=tf.contrib.layers.xavier_initializer())\nb4 = tf.Variable(tf.random_normal([625]))\nL4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\nL4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n'''\nTensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\nTensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n'''\n\n# L5 Final FC 625 inputs -> 10 outputs\nW5 = tf.get_variable(\"W5\", shape=[625, 10], initializer=tf.contrib.layers.xavier_initializer())\nb5 = tf.Variable(tf.random_normal([10]))\nhypothesis = tf.matmul(L4, W5) + b5\n'''\nTensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n'''\n\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n\n\nfor epoch in range(training_epochs):\n    avg_cost = 0\n    total_batch = int(mnist.train.num_examples / batch_size)\n\n    for i in range(total_batch):\n        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n        c, _, = sess.run([cost, optimizer], feed_dict=feed_dict)\n        avg_cost += c / total_batch\n\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n\nprint('Learning Finished!')\n"
  },
  {
    "id": "ml_c6010fb3",
    "framework": "unknown",
    "source_url": "https://github.com/SwordYork/sequencing/blob/master/ava_nmt/nmt_infer.py",
    "hyperparameters": [
      {
        "name": "batch_size",
        "value": "batch_size",
        "line_number": 36,
        "char_start": 43,
        "char_end": 53,
        "code_context": "\n    # load parallel data\n    parallel_data_generator = \\\n        build_source_char_inputs(src_vocab, src_data_file,\n                                batch_size=batch_size, buffer_size=96,\n                                mode=MODE.INFER)\n\n    # ------------------------------------\n    # build model\n    # ------------------------------------",
        "param_type": "training",
        "explanation": "The `batch_size` parameter determines the number of data samples processed by the model in a single training or inference step.",
        "typical_range": "16-256, although it can vary significantly depending on the hardware resources and memory constraints.",
        "alternatives": [
          {
            "value": "small (8-32)",
            "scenario": "When memory is limited or for debugging purposes"
          },
          {
            "value": "medium (64-128)",
            "scenario": "For a balance between speed and memory usage"
          },
          {
            "value": "large (256 or more)",
            "scenario": "When abundant memory is available and speed is prioritized"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "low to medium"
        }
      },
      {
        "name": "batch_size",
        "value": "training_configs.batch_size",
        "line_number": 166,
        "char_start": 21,
        "char_end": 48,
        "code_context": "    infer(training_configs.src_vocab, test_src_file,\n          training_configs.trg_vocab,\n          training_configs.params,\n          beam_size=training_configs.beam_size,\n          batch_size=training_configs.batch_size,\n          max_step=training_configs.max_step,\n          model_dir=training_configs.model_dir,\n          output_file=output_file)",
        "param_type": "training",
        "explanation": "The batch size determines the number of samples the model processes before updating its parameters. Smaller batch sizes require less memory but may lead to slower convergence and more noise in the gradient updates. Larger batch sizes can improve convergence speed but may require more memory and can overfit the training data.",
        "typical_range": "16-256, but can vary depending on the dataset and hardware resources.",
        "alternatives": [
          {
            "value": "smaller (e.g., 8 or 16)",
            "scenario": "When memory is limited or the model is prone to overfitting. Smaller batch sizes can also reduce the variance in gradient updates, which can be beneficial for noisy datasets."
          },
          {
            "value": "larger (e.g., 512 or 1024)",
            "scenario": "When memory is not a constraint and faster convergence is desired. Larger batch sizes can also improve the efficiency of utilizing parallel processing resources."
          },
          {
            "value": "dynamic batching",
            "scenario": "When the dataset has variable-length sequences, dynamic batching can improve efficiency by automatically adjusting the batch size based on the sequence length."
          }
        ],
        "impact": {
          "convergence_speed": "medium-fast",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "unknown",
    "code_snippet": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n#   Author: Sword York\n#   GitHub: https://github.com/SwordYork/sequencing\n#   No rights reserved.\n#\nimport argparse\nimport os\nfrom shutil import copyfile\n\nimport numpy\nimport tensorflow as tf\n\nimport config\nfrom build_inputs import build_source_char_inputs\nfrom build_model import build_attention_model\nfrom sequencing import TIME_MAJOR, MODE, optimistic_restore\n\ndef infer(src_vocab, src_data_file, trg_vocab,\n          params, beam_size=1, batch_size=1, max_step=100,\n          output_file='test.out', model_dir='models/'):\n\n    save_output_dir = 'dev_outputs/'\n    if not os.path.exists(save_output_dir):\n        os.makedirs(save_output_dir)\n\n    # ------------------------------------\n    # prepare data\n    # trg_data_file may be empty.\n    # ------------------------------------\n\n    # load parallel data\n    parallel_data_generator = \\\n        build_source_char_inputs(src_vocab, src_data_file,\n                                batch_size=batch_size, buffer_size=96,\n                                mode=MODE.INFER)\n\n    # ------------------------------------\n    # build model\n    # ------------------------------------\n\n    # placeholder\n    source_ids = tf.placeholder(tf.int32, shape=(None, None),\n                                name='source_ids')\n    source_seq_length = tf.placeholder(tf.int32, shape=(None,),\n                                       name='source_seq_length')\n    source_sample_matrix = tf.placeholder(tf.float32, shape=(None, None, None),\n                                       name='source_sample_matrix')\n    source_word_seq_length = tf.placeholder(tf.int32, shape=(None,),\n                                       name='source_word_seq_length')\n\n    target_ids = None\n    target_seq_length = None\n\n    source_placeholders = {'src': source_ids,\n                          'src_len': source_seq_length,\n                          'src_sample_matrix':source_sample_matrix,\n                          'src_word_len': source_word_seq_length}\n    target_placeholders = {'trg': target_ids,\n                          'trg_len': target_seq_length}\n\n\n    decoder_output_eval, decoder_final_state = \\\n        build_attention_model(params, src_vocab, trg_vocab,\n                              source_placeholders,\n                              target_placeholders,\n                              beam_size=beam_size, mode=MODE.INFER,\n                              max_step=max_step)\n\n    # GPU config\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        last_ckpt = tf.train.latest_checkpoint(model_dir)\n        if last_ckpt:\n            optimistic_restore(sess, last_ckpt)\n        else:\n            raise Exception('No checkpoint found ...')\n\n        output_file_name = os.path.join(save_output_dir,\n                                output_file + last_ckpt.split('-')[-1])\n        output_ = open(output_file_name, 'w')\n\n        for step, current_input in enumerate(parallel_data_generator):\n            current_input_dict = current_input._asdict()\n            feed_dict = {}\n            for key in source_placeholders.keys():\n                feed_dict[source_placeholders[key]] = current_input_dict[key]\n\n            # beam_ids_np: [seq_len, beam_size]\n            # predicted_ids_np: [seq_len, beam_size]\n            predicted_ids_np, beam_ids_np, log_probs_np = sess.run(\n                [decoder_output_eval.predicted_ids,\n                 decoder_output_eval.beam_ids,\n                 decoder_final_state.log_probs],\n                feed_dict=feed_dict)\n            \n            src_len_np = current_input_dict['src_len']\n            data_batch_size = len(src_len_np)\n\n            gathered_pred_ids = numpy.zeros_like(beam_ids_np)\n            for idx in range(beam_ids_np.shape[0]):\n                gathered_pred_ids = gathered_pred_ids[:, beam_ids_np[idx] %\n                                                         beam_ids_np.shape[1]]\n                gathered_pred_ids[idx, :] = predicted_ids_np[idx]\n\n            seq_lens = []\n            for idx in range(beam_ids_np.shape[1]):\n                pred_ids_list = gathered_pred_ids[:, idx].tolist()\n                seq_lens.append(pred_ids_list.index(trg_vocab.eos_id) + 1 \\\n                                    if trg_vocab.eos_id in pred_ids_list \\\n                                    else len(pred_ids_list))\n\n            log_probs_np = log_probs_np / numpy.array(seq_lens)\n            log_probs_np_list = numpy.split(log_probs_np, data_batch_size,\n                                            axis=0)\n            each_max_idx = [numpy.argmax(log_prob) + b * beam_size for\n                            b, log_prob in enumerate(log_probs_np_list)]\n\n            pids = gathered_pred_ids[:, each_max_idx]\n\n            for b in range(data_batch_size):\n                p = trg_vocab.id_to_token(pids[:, b].tolist())\n                if TIME_MAJOR:\n                    s = src_vocab.id_to_token(current_input_dict['src'][:, b].tolist())\n                else:\n                    s = src_vocab.id_to_token(current_input_dict['src'][b, :].tolist())\n                print('src:', s)\n                print('prd:', p)\n                print('---------------------------')\n                print('\\n')\n                output_.write(p + '\\n')\n            output_.flush()\n        output_.close()\n        copyfile(output_file_name, output_file)\n\n\nif __name__ == '__main__':\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    all_configs = [i for i in dir(config) if i.startswith('config_')]\n\n    parser = argparse.ArgumentParser(description='Sequencing Training ...')\n    parser.add_argument('--config', choices=all_configs,\n                        help='specific config name, like {}, '\n                             'see config.py'.format(all_configs),\n                        required=True)\n    parser.add_argument('--test-src', type=str,\n                        help='test src file')\n    parser.add_argument('--output-file', type=str,\n                        help='test output file',\n                        default='test.out')\n\n    args = parser.parse_args()\n    training_configs = getattr(config, args.config)()\n\n    test_src_file = args.test_src if args.test_src else training_configs.test_src_file\n\n    output_file = args.output_file\n\n    infer(training_configs.src_vocab, test_src_file,\n          training_configs.trg_vocab,\n          training_configs.params,\n          beam_size=training_configs.beam_size,\n          batch_size=training_configs.batch_size,\n          max_step=training_configs.max_step,\n          model_dir=training_configs.model_dir,\n          output_file=output_file)\n"
  },
  {
    "id": "ml_bce9174e",
    "framework": "unknown",
    "source_url": "https://github.com/shygiants/ChangeGAN/blob/master/change-gan/change-gan/models/change_gan.py",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "learning_rate",
        "line_number": 112,
        "char_start": 22,
        "char_end": 35,
        "code_context": "    l_d_a_fake = tf.reduce_mean(tf.square(logits_a_fake))\n    l_d_a_real = tf.reduce_mean(tf.squared_difference(logits_a_real, 1.))\n    l_d_a = 0.5 * (l_d_a_fake + l_d_a_real)\n    train_op_d_a = tf.train.AdamOptimizer(\n        learning_rate=learning_rate,\n        beta1=0.5,\n        beta2=0.999\n    ).minimize(l_d_a, global_step=global_step, var_list=d_a_vars)\n\n    l_d_b_fake = tf.reduce_mean(tf.square(logits_b_fake))",
        "param_type": "optimizer",
        "explanation": "The `learning_rate` parameter determines the magnitude of updates applied to the model's parameters during training. A higher learning rate can lead to faster convergence but may cause instability and oscillation. Conversely, a lower learning rate provides more stable training but may require longer to converge.",
        "typical_range": "The typical range for the learning rate in Adam optimizer lies between 1e-3 and 1e-5. However, the optimal value depends on the specific problem and dataset.",
        "alternatives": [
          {
            "value": "Higher learning rate (e.g., 1e-2)",
            "scenario": "May be useful when dealing with small datasets or when faster convergence is desired."
          },
          {
            "value": "Lower learning rate (e.g., 1e-6)",
            "scenario": "May be beneficial for complex problems with large datasets or when avoiding instability is crucial."
          },
          {
            "value": "Adaptive learning rate methods (e.g., AdamW)",
            "scenario": "Dynamically adjust the learning rate during training to improve performance and stability."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      },
      {
        "name": "learning_rate",
        "value": "learning_rate",
        "line_number": 121,
        "char_start": 22,
        "char_end": 35,
        "code_context": "    l_d_b_fake = tf.reduce_mean(tf.square(logits_b_fake))\n    l_d_b_real = tf.reduce_mean(tf.squared_difference(logits_b_real, 1.))\n    l_d_b = 0.5 * (l_d_b_fake + l_d_b_real)\n    train_op_d_b = tf.train.AdamOptimizer(\n        learning_rate=learning_rate,\n        beta1=0.5,\n        beta2=0.999\n    ).minimize(l_d_b, global_step=global_step, var_list=d_b_vars)\n\n    l_d = l_d_a + l_d_b",
        "param_type": "optimizer",
        "explanation": "The learning rate determines the step size taken when adjusting the model's parameters in response to the training data. A higher learning rate leads to faster updates, but may cause the model to miss the optimal solution, while a lower learning rate ensures stability but may prolong training time.",
        "typical_range": "0.0001 to 0.1, although it can vary depending on the specific problem and dataset.",
        "alternatives": [
          {
            "value": "Adaptive learning rate optimizers (e.g., Adam, RMSprop)",
            "scenario": "When the learning rate needs to be adjusted dynamically based on the gradient or other factors."
          },
          {
            "value": "Learning rate scheduling",
            "scenario": "When different learning rates are required at different stages of training."
          },
          {
            "value": "Grid search or random search",
            "scenario": "When the optimal learning rate is unknown and needs to be determined through experimentation."
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "medium"
        }
      },
      {
        "name": "learning_rate",
        "value": "learning_rate",
        "line_number": 136,
        "char_start": 22,
        "char_end": 35,
        "code_context": "    l_const_b = tf.reduce_mean(tf.losses.absolute_difference(_remove_bbox(inputs_b), outputs_bab))\n\n    l_g = l_g_a + l_g_b + 10. * (l_const_a + l_const_b)\n    train_op_g = tf.train.AdamOptimizer(\n        learning_rate=learning_rate,\n        beta1=0.5,\n        beta2=0.999\n    ).minimize(l_g, global_step=global_step, var_list=g_vars)\n\n    with tf.name_scope('losses'):",
        "param_type": "optimizer",
        "explanation": "The learning rate controls the step size that the optimizer takes in updating the model's weights during training. In this case, it is used to update the weights of the generator network (g_vars) during the minimization of the generator loss (l_g).",
        "typical_range": "The typical range for the learning rate is between 0.001 and 0.1, but it can vary depending on the specific problem and model architecture.",
        "alternatives": [
          {
            "value": "0.001",
            "scenario": "When using a complex model or a large dataset, a smaller learning rate might be necessary to ensure stability and prevent divergence."
          },
          {
            "value": "0.01",
            "scenario": "This is a common default value for many optimizers and can be a good starting point for most problems."
          },
          {
            "value": "0.1",
            "scenario": "If the training process is slow or stuck in a local minimum, a larger learning rate can be used to speed up convergence. However, this can also lead to instability and divergence if the learning rate is too high."
          }
        ],
        "impact": {
          "convergence_speed": "Variable, depends on the specific problem and learning rate chosen.",
          "generalization": "Good generalization can be achieved with a proper learning rate and careful tuning.",
          "stability": "Low to medium stability depending on the chosen learning rate. Lower learning rates generally lead to higher stability."
        }
      },
      {
        "name": "batch_size",
        "value": "batch_size",
        "line_number": 279,
        "char_start": 19,
        "char_end": 29,
        "code_context": "    image_b = _preprocess_image(image_b, train_image_size, train_image_size, is_training=is_training)\n\n    images_a, images_b, bboxes_a, bboxes_b = tf.train.batch(\n        [image_a, image_b, bbox_a, bbox_b],\n        batch_size=batch_size,\n        num_threads=multiprocessing.cpu_count(),\n        capacity=5 * batch_size)\n\n    batch_queue = slim.prefetch_queue.prefetch_queue(\n        [images_a, images_b, bboxes_a, bboxes_b], capacity=2)",
        "param_type": "training",
        "explanation": "The \\`batch_size\\` parameter controls the number of samples processed in each training iteration. It influences the efficiency and effectiveness of the training process.",
        "typical_range": "The typical range for \\`batch_size\\` can vary significantly depending on the dataset size, model architecture, and available hardware. Generally, larger batch sizes tend to offer faster convergence but may lead to higher memory requirements and potentially lower generalization performance.",
        "alternatives": [
          {
            "value": "32",
            "scenario": "For smaller datasets or limited hardware resources, a batch size of 32 is a common starting point."
          },
          {
            "value": "64",
            "scenario": "For medium-sized datasets and moderate hardware, a batch size of 64 is often a good choice."
          },
          {
            "value": "128",
            "scenario": "For large datasets and powerful hardware, increasing the batch size to 128 or even higher can speed up training."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "unknown",
    "code_snippet": "\"\"\" Contains the definition of the ChangeGAN architecture. \"\"\"\nimport multiprocessing\n\nimport tensorflow as tf\n\nfrom gan_utils import encoder, decoder, transformer, discriminator, preprocess_image\n\nslim = tf.contrib.slim\n\ndefault_image_size = 256\n\n\ndef model_fn(inputs_a, inputs_b, learning_rate, num_blocks=9, is_training=True, scope=None, weight_decay=0.0001):\n    encoder_dims = [32, 64, 128]\n    deep_encoder_dims = [64, 128, 256]\n    decoder_dims = [64, 32, 3]\n    deep_decoder_dims = [128, 64, 3]\n    with tf.variable_scope(scope, 'ChangeGAN', [inputs_a, inputs_b]):\n        with slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\n                            weights_regularizer=slim.l2_regularizer(weight_decay)):\n            with slim.arg_scope([slim.batch_norm],\n                                is_training=True):\n                def converter_ab(inputs_a, reuse=None):\n                    ################\n                    # Encoder part #\n                    ################\n                    z_a = encoder(inputs_a, deep_encoder_dims, scope='Encoder_A', reuse=reuse)\n\n                    # z_a is split into c_b, z_a-b\n                    c_b, z_a_b = tf.split(z_a, num_or_size_splits=2, axis=3)\n\n                    ####################\n                    # Transformer part #\n                    ####################\n                    c_b = transformer(c_b, encoder_dims[-1], num_blocks=num_blocks,\n                                      scope='Transformer_B', reuse=reuse)\n\n                    ################\n                    # Decoder part #\n                    ################\n                    outputs_b = decoder(c_b, decoder_dims, scope='Decoder_B', reuse=reuse)\n\n                    return outputs_b, z_a_b\n\n                def converter_ba(inputs_b, z_a_b, reuse=None):\n                    ################\n                    # Encoder part #\n                    ################\n                    z_b = encoder(inputs_b, encoder_dims, scope='Encoder_B', reuse=reuse)\n\n                    # Concat z_b and z_a-b\n                    c_a = tf.concat([z_b, z_a_b], 3)\n\n                    ####################\n                    # Transformer part #\n                    ####################\n                    c_a = transformer(c_a, deep_encoder_dims[-1], num_blocks=num_blocks,\n                                      scope='Transformer_A', reuse=reuse)\n\n                    ################\n                    # Decoder part #\n                    ################\n                    outputs_a = decoder(c_a, deep_decoder_dims, scope='Decoder_A', reuse=reuse)\n\n                    return outputs_a\n\n                bbox_channel_a = _get_bbox(inputs_a)\n\n                outputs_ab, z_a_b = converter_ab(inputs_a)\n                outputs_bbox_ab = tf.concat([outputs_ab, bbox_channel_a], 3)\n                outputs_ba = converter_ba(inputs_b, z_a_b)\n                outputs_bbox_ba = tf.concat([outputs_ba, bbox_channel_a], 3)\n\n                outputs_bab, _ = converter_ab(outputs_bbox_ba, reuse=True)\n                outputs_aba = converter_ba(outputs_bbox_ab, z_a_b, reuse=True)\n\n                logits_a_real, probs_a_real = discriminator(inputs_a, deep_encoder_dims, scope='Discriminator_A')\n                logits_a_fake, probs_a_fake = discriminator(outputs_bbox_ba, deep_encoder_dims, scope='Discriminator_A', reuse=True)\n                logits_b_real, probs_b_real = discriminator(inputs_b, deep_encoder_dims, scope='Discriminator_B')\n                logits_b_fake, probs_b_fake = discriminator(outputs_bbox_ab, deep_encoder_dims, scope='Discriminator_B', reuse=True)\n\n                outputs = [inputs_a, inputs_b, outputs_ba, outputs_ab, outputs_aba, outputs_bab]\n                outputs = map(lambda image: tf.image.convert_image_dtype(image, dtype=tf.uint8), outputs)\n\n    with tf.name_scope('images'):\n        tf.summary.image('X_A', _remove_bbox(inputs_a))\n        tf.summary.image('X_B', _remove_bbox(inputs_b))\n        tf.summary.image('X_BA', outputs_ba)\n        tf.summary.image('X_AB', outputs_ab)\n        tf.summary.image('X_ABA', outputs_aba)\n        tf.summary.image('X_BAB', outputs_bab)\n\n    global_step = tf.train.get_or_create_global_step()\n\n    if not is_training:\n        return outputs\n\n    t_vars = tf.trainable_variables()\n\n    d_a_vars = [var for var in t_vars if 'Discriminator_A' in var.name]\n    d_b_vars = [var for var in t_vars if 'Discriminator_B' in var.name]\n    g_vars = [var for var in t_vars if 'coder' in var.name or 'Transformer' in var.name]\n\n    ##########\n    # Losses #\n    ##########\n    # Losses for discriminator\n    l_d_a_fake = tf.reduce_mean(tf.square(logits_a_fake))\n    l_d_a_real = tf.reduce_mean(tf.squared_difference(logits_a_real, 1.))\n    l_d_a = 0.5 * (l_d_a_fake + l_d_a_real)\n    train_op_d_a = tf.train.AdamOptimizer(\n        learning_rate=learning_rate,\n        beta1=0.5,\n        beta2=0.999\n    ).minimize(l_d_a, global_step=global_step, var_list=d_a_vars)\n\n    l_d_b_fake = tf.reduce_mean(tf.square(logits_b_fake))\n    l_d_b_real = tf.reduce_mean(tf.squared_difference(logits_b_real, 1.))\n    l_d_b = 0.5 * (l_d_b_fake + l_d_b_real)\n    train_op_d_b = tf.train.AdamOptimizer(\n        learning_rate=learning_rate,\n        beta1=0.5,\n        beta2=0.999\n    ).minimize(l_d_b, global_step=global_step, var_list=d_b_vars)\n\n    l_d = l_d_a + l_d_b\n\n    # Losses for generators\n    l_g_a = tf.reduce_mean(tf.squared_difference(logits_a_fake, 1.))\n    l_g_b = tf.reduce_mean(tf.squared_difference(logits_b_fake, 1.))\n    l_const_a = tf.reduce_mean(tf.losses.absolute_difference(_remove_bbox(inputs_a), outputs_aba))\n    l_const_b = tf.reduce_mean(tf.losses.absolute_difference(_remove_bbox(inputs_b), outputs_bab))\n\n    l_g = l_g_a + l_g_b + 10. * (l_const_a + l_const_b)\n    train_op_g = tf.train.AdamOptimizer(\n        learning_rate=learning_rate,\n        beta1=0.5,\n        beta2=0.999\n    ).minimize(l_g, global_step=global_step, var_list=g_vars)\n\n    with tf.name_scope('losses'):\n        tf.summary.scalar('L_D_A_Fake', l_d_a_fake)\n        tf.summary.scalar('L_D_A_Real', l_d_a_real)\n        tf.summary.scalar('L_D_A', l_d_a)\n        tf.summary.scalar('L_D_B_Fake', l_d_b_fake)\n        tf.summary.scalar('L_D_B_Real', l_d_b_real)\n        tf.summary.scalar('L_D_B', l_d_b)\n        tf.summary.scalar('L_D', l_d)\n\n        tf.summary.scalar('L_G_A', l_g_a)\n        tf.summary.scalar('L_G_B', l_g_b)\n        tf.summary.scalar('L_Const_A', l_const_a)\n        tf.summary.scalar('L_Const_B', l_const_b)\n        tf.summary.scalar('L_G', l_g)\n\n    train_op = tf.group(*[train_op_d_a, train_op_d_b, train_op_g])\n\n    return train_op, global_step, outputs\n\n\ndef input_fn(dataset_a, dataset_b, batch_size=1, num_readers=4, is_training=True):\n    provider_a = slim.dataset_data_provider.DatasetDataProvider(\n        dataset_a,\n        num_readers=num_readers,\n        common_queue_capacity=20 * batch_size,\n        common_queue_min=10 * batch_size)\n    provider_b = slim.dataset_data_provider.DatasetDataProvider(\n        dataset_b,\n        num_readers=num_readers,\n        common_queue_capacity=20 * batch_size,\n        common_queue_min=10 * batch_size)\n    [image_a, bbox_a] = provider_a.get(['image', 'object/bbox'])\n    [image_b, bbox_b] = provider_b.get(['image', 'object/bbox'])\n\n    train_image_size = default_image_size\n\n    def add_channel(image, bbox, padding='ZERO'):\n        ymin = bbox[0]\n        xmin = bbox[1]\n        ymax = bbox[2]\n        xmax = bbox[3]\n\n        image_shape = tf.to_float(tf.shape(image))\n        height = image_shape[0]\n        width = image_shape[1]\n\n        bbox_height = (ymax - ymin) * height\n        bbox_width = (xmax - xmin) * width\n        channel = tf.ones(tf.to_int32(tf.stack([bbox_height, bbox_width])))\n        channel = tf.expand_dims(channel, axis=2)\n\n        pad_top = tf.to_int32(ymin * height)\n        pad_left = tf.to_int32(xmin * width)\n        height = tf.to_int32(height)\n        width = tf.to_int32(width)\n        channel = tf.image.pad_to_bounding_box(channel, pad_top, pad_left, height, width)\n        # TODO: Decide pad one or zero\n        if padding == 'ONE':\n            channel = tf.ones_like(channel) - channel\n\n        image = tf.concat([image, channel], axis=2)\n\n        return image\n\n    image_a = tf.image.convert_image_dtype(image_a, dtype=tf.float32)\n    image_b = tf.image.convert_image_dtype(image_b, dtype=tf.float32)\n    # [Num of boxes, 4] => [4]\n    bbox_a = tf.squeeze(bbox_a, axis=0)\n    bbox_b = tf.squeeze(bbox_b, axis=0)\n    # Add bound box as 4th channel\n    image_a = add_channel(image_a, bbox_a)\n    image_b = add_channel(image_b, bbox_b)\n\n    image_space_a = Image(image_a, bbox_a)\n    image_space_b = Image(image_b, bbox_b)\n\n    # Resize image B\n    ratio = image_space_a.bbox_height / image_space_b.bbox_height\n    image_space_b.resize(ratio)\n\n    # Shift image B to fit bboxes of two images\n    pixel_shift = image_space_a.translate2pxl(image_space_a.bbox_center) - \\\n                  image_space_b.translate2pxl(image_space_b.bbox_center)\n\n    # Calculate ymin and xmin\n    crop_top = tf.less(pixel_shift[0], 0)\n    pad_y = tf.cond(crop_top, true_fn=lambda: 0, false_fn=lambda: pixel_shift[0])\n    crop_ymin = tf.cond(crop_top,\n                        true_fn=lambda: image_space_b.translate2coor(pixel_y=tf.negative(pixel_shift[0])),\n                        false_fn=lambda: 0.)\n    crop_left = tf.less(pixel_shift[1], 0)\n    pad_x = tf.cond(crop_left, true_fn=lambda: 0, false_fn=lambda: pixel_shift[1])\n    crop_xmin = tf.cond(crop_left,\n                        true_fn=lambda: image_space_b.translate2coor(pixel_x=tf.negative(pixel_shift[1])),\n                        false_fn=lambda: 0.)\n\n    # Calculate ymax and xmax\n    over_y = pixel_shift[0] + image_space_b.height - image_space_a.height\n    crop_bottom = tf.greater(over_y, 0)\n    crop_ymax = tf.cond(crop_bottom,\n                        true_fn=lambda: 1. - image_space_b.translate2coor(pixel_y=over_y),\n                        false_fn=lambda: 1.)\n    over_x = pixel_shift[1] + image_space_b.width - image_space_a.width\n    crop_right = tf.greater(over_x, 0)\n    crop_xmax = tf.cond(crop_right,\n                        true_fn=lambda: 1. - image_space_b.translate2coor(pixel_x=over_x),\n                        false_fn=lambda: 1.)\n\n    # Resize, Crop, Pad\n    image_b_cropped = image_space_b.crop(crop_ymin, crop_xmin, crop_ymax, crop_xmax)\n\n    def pad_to_bounding_box(image):\n        return tf.image.pad_to_bounding_box(image, pad_y, pad_x, image_space_a.height, image_space_a.width)\n\n    # Pad differently depending on type of channel\n    image_b_cropped, bbox_channel = _split_image_bbox(image_b_cropped)\n\n    # One padding for RGB\n    rgb_padding = pad_to_bounding_box(tf.ones_like(image_b_cropped))\n    rgb_padding = tf.ones_like(rgb_padding) - rgb_padding\n    # Sample background color and pad\n    rgb_padding *= image_b_cropped[0, 0]\n\n    # Pad for RGB\n    image_b = pad_to_bounding_box(image_b_cropped) + rgb_padding\n\n    # Zero padding for bbox channel\n    bbox_channel = pad_to_bounding_box(bbox_channel)\n\n    # Concat RGB and bbox channel\n    image_b = tf.concat([image_b, bbox_channel], axis=2)\n\n    # Preprocess images\n    image_a = _preprocess_image(image_a, train_image_size, train_image_size, is_training=is_training)\n    image_b = _preprocess_image(image_b, train_image_size, train_image_size, is_training=is_training)\n\n    images_a, images_b, bboxes_a, bboxes_b = tf.train.batch(\n        [image_a, image_b, bbox_a, bbox_b],\n        batch_size=batch_size,\n        num_threads=multiprocessing.cpu_count(),\n        capacity=5 * batch_size)\n\n    batch_queue = slim.prefetch_queue.prefetch_queue(\n        [images_a, images_b, bboxes_a, bboxes_b], capacity=2)\n    images_a, images_b, bboxes_a, bboxes_b = batch_queue.dequeue()\n\n    with tf.name_scope('inputs'):\n        tf.summary.image('X_A', _remove_bbox(images_a))\n        tf.summary.image('X_A_BBox', images_a)\n        tf.summary.image('X_B', _remove_bbox(images_b))\n        tf.summary.image('X_B_BBox', images_b)\n\n    return images_a, images_b, bboxes_a, bboxes_b\n\n\ndef _preprocess_image(image, height, width, is_training=True):\n    if image.dtype != tf.float32:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Central square crop and resize\n    shape = tf.to_float(tf.shape(image))\n    original_height = shape[0]\n    original_width = shape[1]\n    rest = (1. - original_width / original_height) / 2.\n    image = tf.expand_dims(image, 0)\n    images = tf.image.crop_and_resize(image,\n                                      [[rest, 0., 1. - rest, 1.]], [0],\n                                      [height, width])\n    image = tf.squeeze(images, [0])\n    # image = tf.image.resize_images(image, [height, width])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n\n    return image\n\n\ndef _split_image_bbox(image_bbox):\n    image, bbox = tf.split(image_bbox, [3, 1], axis=image_bbox.shape.ndims - 1)\n    return image, bbox\n\n\ndef _remove_bbox(image_bbox):\n    image, bbox = _split_image_bbox(image_bbox)\n    return image\n\n\ndef _get_bbox(image_bbox):\n    image, bbox = _split_image_bbox(image_bbox)\n    return bbox\n\n\nclass Image:\n    def __init__(self, image, bbox):\n        self._image = image\n        self._image_shape = tf.to_float(tf.shape(image))\n        self._height = self._image_shape[0]\n        self._width = self._image_shape[1]\n\n        self._ratio = None\n\n        self._bbox = bbox\n        self._ymin = bbox[0]\n        self._xmin = bbox[1]\n        self._ymax = bbox[2]\n        self._xmax = bbox[3]\n        self._bbox_height = (self._ymax - self._ymin) * self._height\n        self._bbox_width = (self._xmax - self._xmin) * self._width\n\n        self._center_y = (self._ymin + self._ymax) / 2.\n        self._center_x = (self._xmin + self._xmax) / 2.\n\n\n    @property\n    def image(self):\n        return self._image\n\n    @property\n    def height(self):\n        height = self._height\n        if self._ratio is not None:\n            height *= self._ratio\n        return tf.to_int32(height)\n\n    @property\n    def width(self):\n        width = self._width\n        if self._ratio is not None:\n            width *= self._ratio\n        return tf.to_int32(width)\n\n    @property\n    def bbox_height(self):\n        return self._bbox_height\n\n    @property\n    def bbox_center(self):\n        return tf.stack([self._center_y, self._center_x])\n\n    def resize(self, ratio):\n        self._ratio = ratio\n\n    def translate2pxl(self, coor):\n        if coor.dtype != tf.float32:\n            coor = tf.to_float(coor)\n        pixel = coor * self._image_shape[:2]\n        if self._ratio is not None:\n            pixel *= self._ratio\n        return tf.to_int32(pixel)\n\n    def translate2coor(self, pixel_y=None, pixel_x=None):\n        if pixel_y is None and pixel_x is None:\n            raise ValueError\n        if pixel_y is not None and pixel_x is not None:\n            raise ValueError\n\n        divisor = self._image_shape[0 if pixel_y is not None else 1]\n        pixel = pixel_y if pixel_y is not None else pixel_x\n\n        if pixel.dtype != tf.float32:\n            pixel = tf.to_float(pixel)\n\n        if self._ratio is not None:\n            divisor *= self._ratio\n        coor = pixel / divisor\n        return coor\n\n    def crop(self, ymin, xmin, ymax, xmax):\n        image = self._image\n        if self._ratio is not None:\n            target_shape = tf.to_int32(self._image_shape[:2] * self._ratio)\n            image = tf.image.resize_images(image, target_shape)\n\n        shape = tf.to_float(tf.shape(image))\n        height = shape[0]\n        width = shape[1]\n\n        offset_height = tf.to_int32(ymin * height)\n        offset_width = tf.to_int32(xmin * width)\n        target_height = tf.to_int32((ymax - ymin) * height)\n        target_width = tf.to_int32((xmax - xmin) * width)\n        image = tf.image.crop_to_bounding_box(image,\n                                              offset_height,\n                                              offset_width,\n                                              target_height,\n                                              target_width)\n\n        return image\n"
  },
  {
    "id": "te_87cc1e6a",
    "framework": "tensorflow",
    "source_url": "https://github.com/googleforgames/clean-chat/blob/master/components/model/bert/vertex_pipeline/model.py",
    "hyperparameters": [
      {
        "name": "batch_size",
        "value": "batch_size",
        "line_number": 114,
        "char_start": 19,
        "char_end": 29,
        "code_context": "        tf_transform_output.transformed_feature_spec().copy())\n\n    dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern=file_pattern,\n        batch_size=batch_size,\n        features=transformed_feature_spec,\n        reader=_gzip_reader_fn,\n        label_key=LABEL)\n\n    return dataset",
        "param_type": "training",
        "explanation": "Batch size determines the number of samples processed in each training iteration. It influences the efficiency and effectiveness of the training process.",
        "typical_range": "The common range for batch size is between 16 and 256, but it can vary based on factors like dataset size, hardware capabilities, and computational resources.",
        "alternatives": [
          {
            "value": "32",
            "scenario": "A moderate value for balancing between efficient training and utilization of computational resources."
          },
          {
            "value": "128",
            "scenario": "A larger batch size, suitable for datasets with many instances and sufficient hardware resources."
          },
          {
            "value": "8",
            "scenario": "A smaller batch size, suitable for complex datasets where gradient updates might be noisy or datasets are limited."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "medium",
          "stability": "medium"
        }
      },
      {
        "name": "activation",
        "value": "None",
        "line_number": 138,
        "char_start": 46,
        "char_end": 50,
        "code_context": "    encoder = hub.KerasLayer(bert_path, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n\n    model = tf.keras.Model(text_input, net)\n\n    optimizer = optimization.create_optimizer(init_lr=3e-5,\n                                              num_train_steps=5,",
        "param_type": "other",
        "explanation": "The activation parameter controls the non-linearity applied to the output of a network layer, transforming the raw output values to provide a desired effect or optimize for a specific task. In this case, a custom activation function is implemented, but it\u2019s not explicitly provided in the code snippet.",
        "typical_range": "Varies depending on the desired activation function and specific task. Commonly used activation functions include ReLU, sigmoid, softmax, and others, each with different activation value ranges.",
        "alternatives": [
          {
            "value": "'relu'",
            "scenario": "For non-linearity and faster convergence in deep networks."
          },
          {
            "value": "'tanh'",
            "scenario": "For improved gradient propagation compared to sigmoid in early training stages."
          },
          {
            "value": "'softmax'",
            "scenario": "As a final activation for multi-class classification tasks to output a probability distribution."
          }
        ],
        "impact": {
          "convergence_speed": "Depends on the chosen activation function.",
          "generalization": "Depends on the chosen activation function.",
          "stability": "Depends on the chosen activation function."
        }
      },
      {
        "name": "optimizer",
        "value": "optimizer",
        "line_number": 148,
        "char_start": 28,
        "char_end": 37,
        "code_context": "                                              num_warmup_steps=int(0.1 * 5),\n                                              optimizer_type='adamw')\n\n    # add optimizer, loss, etc in appropriate place\n    model.compile(optimizer=optimizer,\n                  loss=tf.keras.losses.MeanAbsoluteError(),\n                  metrics=tf.metrics.MeanSquaredError())\n\n    return model\n",
        "param_type": "other",
        "explanation": "The optimizer parameter determines the algorithm used to update the model's weights based on the loss function. In this case, it's set to 'adamw', which is a popular optimizer that combines the Adam and momentum algorithms.",
        "typical_range": "There is no single 'typical' value for the optimizer parameter, as it depends on the specific task and dataset. However, AdamW is a common choice for deep learning tasks.",
        "alternatives": [
          {
            "value": "adam",
            "scenario": "For faster convergence and good generalization."
          },
          {
            "value": "sgd",
            "scenario": "When dealing with noisy or sparse data, or when memory is limited."
          },
          {
            "value": "rmsprop",
            "scenario": "For dealing with highly non-convex loss surfaces."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      },
      {
        "name": "batch_size",
        "value": "32",
        "line_number": 164,
        "char_start": 41,
        "char_end": 43,
        "code_context": "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n    train_dataset = _input_fn(file_pattern=fn_args.train_files,\n                              tf_transform_output=tf_transform_output,\n                              batch_size=32)\n\n    eval_dataset = _input_fn(file_pattern=fn_args.eval_files,\n                             tf_transform_output=tf_transform_output,\n                             batch_size=32)\n",
        "param_type": "training",
        "explanation": "The batch size determines the number of samples used to update the model parameters in each iteration. A larger batch size can lead to faster convergence but may require more memory and can be less stable.",
        "typical_range": "8-256, but can be higher depending on the hardware and memory constraints.",
        "alternatives": [
          {
            "value": "16",
            "scenario": "When memory is limited or training data is small."
          },
          {
            "value": "64",
            "scenario": "This is a common value that provides a balance between speed and stability."
          },
          {
            "value": "128",
            "scenario": "When training data is large and the hardware can handle it."
          },
          {
            "value": "256",
            "scenario": "For very large datasets and powerful hardware."
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "medium"
        }
      },
      {
        "name": "batch_size",
        "value": "32",
        "line_number": 168,
        "char_start": 40,
        "char_end": 42,
        "code_context": "                              batch_size=32)\n\n    eval_dataset = _input_fn(file_pattern=fn_args.eval_files,\n                             tf_transform_output=tf_transform_output,\n                             batch_size=32)\n\n    model = toxicity_model()\n\n    # Write logs to path\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(",
        "param_type": "training",
        "explanation": "Batch size is the number of training examples used in one iteration of the training process. It controls the amount of data the model sees at once, influencing the model's learning rate and optimization.",
        "typical_range": "16 - 256, with 32 being a common starting point",
        "alternatives": [
          {
            "value": "64",
            "scenario": "When you have more memory and want to accelerate training, especially for large datasets."
          },
          {
            "value": "16",
            "scenario": "When you have limited memory or want to reduce the risk of overfitting on small datasets."
          },
          {
            "value": "128",
            "scenario": "A common intermediate value that often offers a good balance between memory usage and training speed."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "unknown",
    "code_snippet": "##########################################################################\n#\n# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n##########################################################################\n\"\"\"Train the Full BERT Model with Keras\"\"\"\n\nfrom typing import Any, Callable\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_transform as tft\nfrom keras.models import Model\nfrom official.nlp import optimization\nfrom tfx import v1 as tfx\n\nLABEL = 'label'\n\n\n# https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_utils_native_keras.py\n# Apache License, Version 2.0. | Copyright 2019 Google LLC.\ndef _get_tf_examples_serving_signature(\n        model: Model, tf_transform_output: tft.TFTransformOutput) -> Callable:\n    \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n\n    # We need to track the layers in the model in order to save it.\n    # TODO(b/162357359): Revise once the bug is resolved.\n    model.tft_layer_inference = tf_transform_output.transform_features_layer()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n    ])\n    def serve_tf_examples_fn(serialized_tf_example):\n        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n        raw_feature_spec = tf_transform_output.raw_feature_spec()\n        # Remove label feature since these will not be present at serving time.\n        raw_feature_spec.pop('target')\n        raw_features = tf.io.parse_example(serialized_tf_example,\n                                           raw_feature_spec)\n        transformed_features = model.tft_layer_inference(raw_features)\n\n        outputs = model(transformed_features)\n        # TODO(b/154085620): Convert the predicted labels from the model using a\n        # reverse-lookup (opposite of transform.py).\n        return {'outputs': outputs}\n\n    return serve_tf_examples_fn\n\n\n# https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_utils_native_keras.py\n# Apache License, Version 2.0. | Copyright 2019 Google LLC.\ndef _get_transform_features_signature(\n        model: Model, tf_transform_output: tft.TFTransformOutput) -> Callable:\n    \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n\n    # We need to track the layers in the model in order to save it.\n    # TODO(b/162357359): Revise once the bug is resolved.\n    model.tft_layer_eval = tf_transform_output.transform_features_layer()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n    ])\n    def transform_features_fn(serialized_tf_example):\n        \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n        raw_feature_spec = tf_transform_output.raw_feature_spec()\n        raw_features = tf.io.parse_example(serialized_tf_example,\n                                           raw_feature_spec)\n        transformed_features = model.tft_layer_eval(raw_features)\n        return transformed_features\n\n    return transform_features_fn\n\n\n# https://github.com/kubeflow/pipelines/blob/master/samples/core/tfx-oss/utils/taxi_utils.py\n# Apache License, Version 2.0. | Copyright 2019 The Kubeflow Authors.\ndef _gzip_reader_fn(filenames: tf.string) -> tf.data.TFRecordDataset:\n    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n\n\ndef _input_fn(\n    file_pattern: Any,\n    tf_transform_output: tft.TFTransformOutput,\n    batch_size: int = 32,\n) -> tf.data.Dataset:\n    \"\"\"Generates features and label for tuning/training.\n\n    Args:\n        file_pattern (Any): file patterns for training/evalluating fn_args\n        tf_transform_output (tft.TFTransformOutput): the output of the tf.Transform\n        batch_size (int, optional): Batch size for train/eval. Defaults to 32.\n\n    Returns:\n        tf.data.Dataset: Represents data.\n    \"\"\"\n\n    transformed_feature_spec = (\n        tf_transform_output.transformed_feature_spec().copy())\n\n    dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern=file_pattern,\n        batch_size=batch_size,\n        features=transformed_feature_spec,\n        reader=_gzip_reader_fn,\n        label_key=LABEL)\n\n    return dataset\n\n\ndef toxicity_model() -> Model:\n    \"\"\"Toxicity BERT Model\n\n    Returns:\n        keras.models.Model: BERT Model\n    \"\"\"\n    bert_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n    preprocess_path = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(preprocess_path, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(bert_path, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n\n    model = tf.keras.Model(text_input, net)\n\n    optimizer = optimization.create_optimizer(init_lr=3e-5,\n                                              num_train_steps=5,\n                                              num_warmup_steps=int(0.1 * 5),\n                                              optimizer_type='adamw')\n\n    # add optimizer, loss, etc in appropriate place\n    model.compile(optimizer=optimizer,\n                  loss=tf.keras.losses.MeanAbsoluteError(),\n                  metrics=tf.metrics.MeanSquaredError())\n\n    return model\n\n\ndef run_fn(fn_args: tfx.components.FnArgs) -> None:\n    \"\"\"Train the model based on given args.\n    Args:\n      fn_args: Holds args used to train the model as name/value pairs.\n    \"\"\"\n    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n    train_dataset = _input_fn(file_pattern=fn_args.train_files,\n                              tf_transform_output=tf_transform_output,\n                              batch_size=32)\n\n    eval_dataset = _input_fn(file_pattern=fn_args.eval_files,\n                             tf_transform_output=tf_transform_output,\n                             batch_size=32)\n\n    model = toxicity_model()\n\n    # Write logs to path\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=fn_args.model_run_dir, update_freq='batch')\n\n    model.fit(train_dataset,\n              steps_per_epoch=fn_args.train_steps,\n              validation_data=eval_dataset,\n              validation_steps=fn_args.eval_steps,\n              callbacks=[tensorboard_callback])\n\n    signatures = {\n        'serving_default':\n            _get_tf_examples_serving_signature(model, tf_transform_output),\n        'transform_features':\n            _get_transform_features_signature(model, tf_transform_output),\n    }\n\n    model.save(fn_args.serving_model_dir,\n               save_format='tf',\n               signatures=signatures)\n"
  },
  {
    "id": "ml_695fe0be",
    "framework": "unknown",
    "source_url": "https://github.com/google-research/deadunits/blob/master/deadunits/generic_convnet.py",
    "hyperparameters": [
      {
        "name": "activation",
        "value": "None",
        "line_number": 129,
        "char_start": 69,
        "char_end": 73,
        "code_context": "        setattr(self, l_name, tf.keras.layers.GlobalAvgPool2D())\n      elif l_type == 'MP':\n        setattr(self, l_name, tf.keras.layers.MaxPool2D(t[1], t[2]))\n      elif l_type == 'O':\n        setattr(self, l_name, tf.keras.layers.Dense(t[1], activation=None))\n      elif l_type == 'DO':\n        setattr(self, l_name, tf.keras.layers.Dropout(t[1]))\n      else:\n        if l_type == 'C':\n          setattr(",
        "param_type": "other",
        "explanation": "The activation function applied to the output of the layer. A None value indicates that no activation function is applied, which is the default behavior for the Dense layer.",
        "typical_range": "None or any activation function available in TensorFlow/PyTorch libraries (e.g., 'relu', 'sigmoid', 'tanh')",
        "alternatives": [
          {
            "value": "relu",
            "scenario": "For improved convergence and performance on a wide range of tasks."
          },
          {
            "value": "sigmoid",
            "scenario": "For binary classification tasks where the output needs to be between 0 and 1."
          },
          {
            "value": "tanh",
            "scenario": "For regression tasks where the output needs to be between -1 and 1."
          }
        ],
        "impact": {
          "convergence_speed": "Depends on the specific activation function chosen.",
          "generalization": "Varies depending on the activation function and the specific task.",
          "stability": "Generally high, but can be affected by the choice of activation function."
        }
      },
      {
        "name": "activation",
        "value": "None",
        "line_number": 137,
        "char_start": 64,
        "char_end": 68,
        "code_context": "        if l_type == 'C':\n          setattr(\n              self, l_name,\n              wrapper(\n                  tf.keras.layers.Conv2D(t[1], t[2], activation=None, **t[3])))\n        elif l_type == 'D':\n          setattr(self, l_name,\n                  wrapper(tf.keras.layers.Dense(t[1], activation=None)))\n\n        if use_batchnorm:",
        "param_type": "other",
        "explanation": "The activation value controls how data is processed by convolutional and/or dense layers within the model. While currently set to None, meaning no predefined activation function is applied, typical value options could include \"relu\", tanh\", etc.",
        "typical_range": [
          "linear",
          "relu",
          "tanh",
          "sigmoid",
          "softmax"
        ],
        "alternatives": [
          {
            "value": null,
            "scenario": "To retain an unprocessed signal with its original value and distribution. Useful to experiment with custom activation implementations or if the data already comes with inherent non-linearities."
          },
          {
            "value": "relu",
            "scenario": "For models dealing mainly with positive inputs, as in images. Introduces non-linearity while avoiding vanishing gradients and often speeds training."
          },
          {
            "value": "tanh",
            "scenario": "To map values within a -1 and +1 range for more balanced output activations. Suitable for data with both large and small values."
          }
        ],
        "impact": {
          "convergence_speed": "fast,slow",
          "generalization": "variable (depends largely on other parameters and task specifics)",
          "stability": "medium (depends on data characteristics)"
        }
      },
      {
        "name": "activation",
        "value": "None",
        "line_number": 140,
        "char_start": 65,
        "char_end": 69,
        "code_context": "              wrapper(\n                  tf.keras.layers.Conv2D(t[1], t[2], activation=None, **t[3])))\n        elif l_type == 'D':\n          setattr(self, l_name,\n                  wrapper(tf.keras.layers.Dense(t[1], activation=None)))\n\n        if use_batchnorm:\n          c_name = l_name + '_bn'\n          setattr(\n              self, c_name,",
        "param_type": "other",
        "explanation": "The `activation` parameter in TensorFlow/PyTorch determines the **non-linear transformation applied to the output** of a neural network layer. It introduces non-linearity, which is crucial for learning complex patterns in data. Setting the activation to `None` implies using a **linear activation function**, which simply passes the input through without any modification.",
        "typical_range": "The typical range for activation functions depends on the specific function and the task at hand. Common choices include: \n- `relu` (Rectified Linear Unit): 0 for negative inputs, input value for positive inputs. Useful for tasks like image recognition and natural language processing.\n- `sigmoid`: Values between 0 and 1, often used for binary classification problems.\n- `tanh` (Hyperbolic Tangent): Values between -1 and 1, also suitable for binary classification.\n- `softmax`: Values represent probabilities across multiple categories, typically used for multi-class classification.",
        "alternatives": [
          {
            "value": "relu",
            "scenario": "Improves convergence speed and handles sparse data well. Consider using it as a default activation for most hidden layers."
          },
          {
            "value": "sigmoid",
            "scenario": "Appropriate for binary classification tasks where outputs need to be within 0 and 1."
          },
          {
            "value": "tanh",
            "scenario": "Similar to sigmoid but with output values between -1 and 1. Can be used for binary classification or regression tasks."
          },
          {
            "value": "softmax",
            "scenario": "Used for multi-class classification tasks where outputs represent probabilities across multiple categories."
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "unknown",
    "code_snippet": "# coding=utf-8\n# Copyright 2021 The Deadunits Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n\"\"\"Implements common architectures in a generic way using tf.keras.Model.\n\nEach generic model inherits from `tf.keras.Model`.\nYou can use following generic_models for now:\n\n- GenericConvnet: sequential models include Conv2D's + Dense's.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nfrom deadunits.layers import MaskedLayer\nfrom deadunits.layers import MeanReplacer\nfrom deadunits.layers import TaylorScorer\nimport gin\nfrom six.moves import range\nimport tensorflow.compat.v2 as tf\n\n_default_generic_convnet_arch = [['C', 16, 5, {}], ['MP', 2, 2],\n                                 ['C', 32, 5, {}], ['MP', 2, 2], ['F'],\n                                 ['D', 256], ['O', 10]]\n\n\n@gin.configurable\nclass GenericConvnet(tf.keras.Model):\n  \"\"\"Creates a tf.keras.Model from according to the flags and arch provided.\n\n  \"\"\"\n  _allowed_layers = {\n      'C': 'conv',\n      'D': 'dense',\n      'MP': 'maxpool',\n      'DO': 'dropout',\n      'O': 'output',\n      'F': 'flatten',\n      'GA': 'gap',\n  }\n\n  # Each layer should have the following form in the arch definition.\n  # 'C': Conv2D layer in the form ['C', n_units, filter_shape, **kwargs]\n  # 'MP': MaxPool2D layer in the form ['MP', pool_size, strides, **kwargs]\n  # 'D': Dense layer in the form ['D', n_units]\n  # 'DO': Dropout layer in the form ['DO', rate]\n  # 'F': Flatten layer in the form ['F']\n  # 'GA': Global average pooling 2D in the form ['GA']\n  # 'O': Dense layer with no activation in the form ['O', n_units]\n  def __init__(self,\n               model_arch=None,\n               name='GenericCifarConvnet',\n               f_activation=tf.keras.activations.relu,\n               use_batchnorm=False,\n               bn_is_affine=False,\n               use_dropout=False,\n               dropout_rate=0.5,\n               use_mean_replacer=False,\n               use_taylor_scorer=False,\n               use_masked_layers=False):\n    \"\"\"Initializes GenericConvnet instance with correct layers.\n\n    Args:\n      model_arch: list, consists of lists defining the cascaded network. refer\n        to `GenericConvnet._allowed_layers`.\n      name: str, name of the model.\n      f_activation: function, from tf.keras.activations\n      use_batchnorm: bool, if True BatchNormalization layer is used.\n      bn_is_affine: bool, if True BatchNormalization performs affine\n        transformation after the normalization.\n      use_dropout: bool, if True Dropout layer is used.\n      dropout_rate: float, dropout fraction for the Dropout layer.\n      use_mean_replacer: bool, if True MeanReplacer layer is used after each\n        layer.\n      use_taylor_scorer: bool, if True TaylorScorer layer is used after each\n        layer.\n      use_masked_layers: bool, if True each layer is wrapped with MaskedLayer.\n\n    Raises:\n      AssertionError: when the provided `model_arch` is not valid.\n    \"\"\"\n    if model_arch is None:\n      model_arch = _default_generic_convnet_arch\n    self._check_arch(model_arch)\n    super(GenericConvnet, self).__init__(name=name)\n    # Initial configration is saved to be able to clone the model.\n    self.init_config = dict([('model_arch', model_arch), ('name', name),\n                             ('f_activation', f_activation),\n                             ('use_batchnorm', use_batchnorm),\n                             ('bn_is_affine', bn_is_affine),\n                             ('use_dropout', use_dropout),\n                             ('dropout_rate', dropout_rate),\n                             ('use_mean_replacer', use_mean_replacer),\n                             ('use_taylor_scorer', use_taylor_scorer),\n                             ('use_masked_layers', use_masked_layers)])\n    # Wrap the layers if asked.\n    wrapper = lambda l: MaskedLayer(l) if use_masked_layers else l\n    # Forward chain has the attribute names in order and used to orchestrate\n    # the forward pass.\n    forward_chain = []\n    for t in model_arch:\n      # The order is:\n      # Layer + bn + Activation + taylorScorer + meanReplacer + Dropout\n      l_type = t[0]\n      l_name = self._get_layer_name(l_type)\n      forward_chain.append(l_name)\n      # If F(flatten) or O(output), we don't have extra layers(dropout,bn,etc..)\n      if l_type == 'F':\n        setattr(self, l_name, tf.keras.layers.Flatten())\n      elif l_type == 'GA':\n        setattr(self, l_name, tf.keras.layers.GlobalAvgPool2D())\n      elif l_type == 'MP':\n        setattr(self, l_name, tf.keras.layers.MaxPool2D(t[1], t[2]))\n      elif l_type == 'O':\n        setattr(self, l_name, tf.keras.layers.Dense(t[1], activation=None))\n      elif l_type == 'DO':\n        setattr(self, l_name, tf.keras.layers.Dropout(t[1]))\n      else:\n        if l_type == 'C':\n          setattr(\n              self, l_name,\n              wrapper(\n                  tf.keras.layers.Conv2D(t[1], t[2], activation=None, **t[3])))\n        elif l_type == 'D':\n          setattr(self, l_name,\n                  wrapper(tf.keras.layers.Dense(t[1], activation=None)))\n\n        if use_batchnorm:\n          c_name = l_name + '_bn'\n          setattr(\n              self, c_name,\n              tf.keras.layers.BatchNormalization(\n                  center=bn_is_affine, scale=bn_is_affine))\n          forward_chain.append(c_name)\n        # Add activation\n        c_name = l_name + '_a'\n        setattr(self, c_name, f_activation)\n        forward_chain.append(c_name)\n        if use_taylor_scorer:\n          c_name = l_name + '_ts'\n          setattr(self, c_name, TaylorScorer())\n          forward_chain.append(c_name)\n        if use_mean_replacer:\n          c_name = l_name + '_mr'\n          setattr(self, c_name, MeanReplacer())\n          forward_chain.append(c_name)\n        if use_dropout:\n          c_name = l_name + '_dr'\n          setattr(self, c_name, tf.keras.layers.Dropout(dropout_rate))\n          forward_chain.append(c_name)\n    self.forward_chain = forward_chain\n\n  def call(self,\n           inputs,\n           training=False,\n           compute_mean_replacement_saliency=False,\n           compute_removal_saliency=False,\n           is_abs=True,\n           aggregate_values=False,\n           is_replacing=False,\n           return_nodes=None):\n    # We need to save the first_input for initiliazing our clone (see .clone()).\n    if not hasattr(self, 'first_input'):\n      self.first_input = inputs\n    x = inputs\n    return_dict = {}\n    for l_name in self.forward_chain:\n      node = getattr(self, l_name)\n      if isinstance(node, MeanReplacer):\n        x = node(x, is_replacing=is_replacing)\n      elif isinstance(node, TaylorScorer):\n        x = node(\n            x,\n            compute_mean_replacement_saliency=compute_mean_replacement_saliency,\n            compute_removal_saliency=compute_removal_saliency,\n            is_abs=is_abs,\n            aggregate_values=aggregate_values)\n      elif isinstance(\n          node, (tf.keras.layers.BatchNormalization, tf.keras.layers.Dropout)):\n        x = node(x, training=training)\n      else:\n        x = node(x)\n      if return_nodes and l_name in return_nodes:\n        return_dict[l_name] = x\n    if return_nodes:\n      return x, return_dict\n    else:\n      return x\n\n  def propagate_bias(self, l_name, input_tensor):\n    \"\"\"Propagates the given input to the bias of the next unit.\n\n    We expect `input_tensor` having constant values at `input_tensor[...,i]` for\n      every unit `i`. However this is not checked and if it is not constant,\n      mean of the all values are used to update the bias.\n\n    If input_tensor casted into same type as the parameters of the `l_name`.\n\n    Args:\n      l_name: str, name of a MaskedLayer such that `hasattr(self, l_name)` is\n        True.\n      input_tensor: Tensor, same shape as the output shape of the l_name. It\n        should also be a float type. i.e. tf.float16/32/64.\n\n    Raises:\n      ValueError: when the l_name is not in the `self.forward_chain` or if\n        there is no parameterized layer exists after `l_name`.\n      AssertionError: when the input_tensor is not float type.\n    \"\"\"\n    assert (input_tensor.dtype in [tf.float16, tf.float32, tf.float64])\n    current_i = self.forward_chain.index(l_name) + 1\n    if current_i == len(self.forward_chain):\n      raise ValueError('Output layer cannot propagate bias')\n    next_layer = getattr(self, self.forward_chain[current_i])\n    forward_tensor = input_tensor\n    # Including `tf.keras.layers.Dense`, too; since the output layer(Dense)\n    # is not wrapped with `MaskedLayer`.\n    parametered_layers = (MaskedLayer, tf.keras.layers.Dense,\n                          tf.keras.layers.Conv2D)\n    while not isinstance(next_layer, parametered_layers):\n      forward_tensor = next_layer(forward_tensor)\n      current_i += 1\n      if current_i == len(self.forward_chain):\n        raise ValueError('No appropriate layer exists after'\n                         '%s to propagate bias.' % l_name)\n      next_layer = getattr(self, self.forward_chain[current_i])\n    # So now we have propageted bias + currrent_bias. This should be our new\n    # bias.\n    forward_tensor = next_layer(forward_tensor)\n    # During Mean Replacement, forward_tensor[...,i] should be a constant\n    # tensor, but it is not verified.\n    bias2add = tf.reduce_mean(\n        forward_tensor, axis=list(range(forward_tensor.shape.ndims - 1)))\n    if isinstance(next_layer, MaskedLayer):\n      next_layer.layer.weights[1].assign(bias2add)\n    else:\n      next_layer.weights[1].assign(bias2add)\n\n  def get_allowed_layer_keys(self):\n    return list(self._allowed_layers.keys())\n\n  def get_layer_keys(self, layer_type, name_filter=lambda _: True):\n    \"\"\"Returns a list of layer_names matching the type and passing the filter.\n\n    `self.forward_chain` is filtered by type and layer_name.\n    Args:\n      layer_type: layer class to be matched.\n      name_filter: function, returning bool given a layer_name.\n    \"\"\"\n    res = []\n    for l_name in self.forward_chain:\n      if name_filter(l_name) and isinstance(getattr(self, l_name), layer_type):\n        res.append(l_name)\n    return res\n\n  def _get_layer_name(self, l_type):\n    \"\"\"Returns names for different layers by incrementing the counter.\n\n    Args:\n      l_type: str from self._allowed_layers.keys()\n\n    Returns:\n      attr_name: str unique attr name for the layer\n    \"\"\"\n    if not hasattr(self, 'layer_name_counter'):\n      self.layer_name_counter = {k: 1 for k in self._allowed_layers.keys()}\n    i = self.layer_name_counter[l_type]\n    self.layer_name_counter[l_type] += 1\n    return '%s_%d' % (self._allowed_layers[l_type], i)\n\n  def clone(self):\n    new_model = GenericConvnet(**self.init_config)\n    # Initilize the new_model params.\n    new_model(self.first_input)\n    new_model.set_weights(self.get_weights())\n    return new_model\n\n  def _check_arch(self, arch):\n    \"\"\"Checks the arch provided has the right form.\n\n    For some reason tensorflow wraps every list/dict to make it checkpointable.\n    For that reason we are using the super classes from collections module.\n    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/checkpointable/data_structures.py\n    Args:\n      arch: architecture list.\n\n    Raises:\n      AssertionError: If the architecture list is not in the right format.\n    \"\"\"\n    assert arch is not None\n    assert len(arch) >= 1\n    for t in arch:\n      assert isinstance(t, collections.MutableSequence)\n      assert len(t) >= 1\n      assert t[0] in self.get_allowed_layer_keys()\n      if t[0] == 'C':\n        assert len(t) == 4\n        assert isinstance(t[1], int)\n        # t[2] can be an int or list of two integers.\n        assert (isinstance(t[2], int) or\n                (isinstance(t[2], collections.MutableSequence) and\n                 len(t[2]) == 2) and all(isinstance(x, int) for x in t[2]))\n        assert isinstance(t[3], collections.MutableMapping)\n      if t[0] == 'MP':\n        assert len(t) == 3\n        assert (isinstance(t[1], int) or\n                (isinstance(t[1], collections.MutableSequence) and\n                 len(t[1]) == 2) and all(isinstance(x, int) for x in t[1]))\n        assert (isinstance(t[2], int) or\n                (isinstance(t[2], collections.MutableSequence) and\n                 len(t[2]) == 2) and all(isinstance(x, int) for x in t[2]))\n      if t[0] in ('F', 'GA'):\n        assert len(t) == 1\n      if t[0] in ('D', 'O'):\n        assert len(t) == 2\n        assert isinstance(t[1], int)\n      if t[0] == 'DO':\n        assert len(t) == 2\n        assert isinstance(t[1], float) and 0 < t[1] and t[1] < 1\n"
  },
  {
    "id": "ml_4afa13d4",
    "framework": "unknown",
    "source_url": "https://github.com/srom/chessbot/blob/master/estimator/train/model.py",
    "hyperparameters": [
      {
        "name": "activation",
        "value": "tf.nn.elu",
        "line_number": 55,
        "char_start": 63,
        "char_end": 72,
        "code_context": "            self.X: X\n        })\n\n    def _get_evaluation_function(self, X):\n        hidden_1 = tf.layers.dense(X, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_1')\n        hidden_2 = tf.layers.dense(hidden_1, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_2')\n        output = tf.layers.dense(hidden_2, 1, activation=None, name='output')\n        return output\n\n    def _get_loss(self):",
        "param_type": "other",
        "explanation": "The activation function determines how to process values within a layer, affecting outputs. The `tf.nn.elu` function activates neurons according to a scaled Exponential Linear Unit, potentially improving network performance.",
        "typical_range": "There's no strict typical range. The best choice depends on your data, task, and other factors. However, the common values for `alpha`, a significant hyperparameter for ELU, lie between 0.1 and 2.",
        "alternatives": [
          {
            "value": "None (Default)",
            "scenario": "If your problem doesn't require complex activations, no activation might work well. It preserves the linear nature of data."
          },
          {
            "value": "tf.nn.relu",
            "scenario": "tf.nn.relu is a popular choice known for its simplicity and efficiency, especially with negative inputs."
          },
          {
            "value": "tf.nn.relu6",
            "scenario": "tf.nn.relu6 imposes an upper limit of 6 on activations, making it suited for tasks where large outputs might occur. It is also more stable in numerical algorithms due to the bounded output."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      },
      {
        "name": "activation",
        "value": "tf.nn.elu",
        "line_number": 56,
        "char_start": 70,
        "char_end": 79,
        "code_context": "        })\n\n    def _get_evaluation_function(self, X):\n        hidden_1 = tf.layers.dense(X, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_1')\n        hidden_2 = tf.layers.dense(hidden_1, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_2')\n        output = tf.layers.dense(hidden_2, 1, activation=None, name='output')\n        return output\n\n    def _get_loss(self):\n        x_observed_random = self.f_random - self.f_observed",
        "param_type": "other",
        "explanation": "The activation applies the Exponential Linear Unit (ELU) to incoming data, helping the model learn faster by avoiding vanishing gradients. It smoothly transitions between linear and exponential behavior.",
        "typical_range": "None specifically, but ELU generally works well within a range of -1 to 1.",
        "alternatives": [
          {
            "value": "tf.nn.relu",
            "scenario": "When simplicity and computational efficiency are prioritized, but may suffer from dying ReLU problem"
          },
          {
            "value": "tf.nn.leaky_relu",
            "scenario": "Similar to ReLU but addresses the dying ReLU problem with a small non-zero gradient for negative values"
          },
          {
            "value": "tf.nn.tanh",
            "scenario": "Provides smooth gradients throughout, but can have vanishing gradient issues in deeper layers"
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "medium"
        }
      },
      {
        "name": "activation",
        "value": "None",
        "line_number": 57,
        "char_start": 57,
        "char_end": 61,
        "code_context": "\n    def _get_evaluation_function(self, X):\n        hidden_1 = tf.layers.dense(X, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_1')\n        hidden_2 = tf.layers.dense(hidden_1, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_2')\n        output = tf.layers.dense(hidden_2, 1, activation=None, name='output')\n        return output\n\n    def _get_loss(self):\n        x_observed_random = self.f_random - self.f_observed\n        x_parent_observed = self.f_parent + self.f_observed",
        "param_type": "other",
        "explanation": "The activation parameter determines the activation function applied to the output of each layer, except the final layer, in the neural network. It introduces non-linearity into the model, allowing it to learn complex patterns from the data. In this case, the activation function is set to `None` for the output layer, which means no activation function is applied at that stage. This allows the model to directly output real-valued predictions without any transformation.",
        "typical_range": "The typical range for activation functions in neural networks is diverse, varying based on the specific function used and the problem being addressed. Common choices include ReLU, Leaky ReLU, ELU, tanh, sigmoid, and softmax. However, there is no single 'typical' range that applies universally.",
        "alternatives": [
          {
            "value": "None (linear activation)",
            "scenario": "When dealing with regression problems or tasks where the output needs to be interpreted directly in its original scale, a linear activation like None is appropriate."
          },
          {
            "value": "ReLU",
            "scenario": "ReLU is a popular choice for its simplicity and effectiveness in addressing the vanishing gradient problem. It can be used in various types of neural networks, including classification and regression tasks."
          },
          {
            "value": "Leaky ReLU",
            "scenario": "Leaky ReLU is an improvement over ReLU as it aims to mitigate the 'dying ReLU' problem where neurons become inactive. It's suitable for similar tasks as ReLU."
          },
          {
            "value": "ELU",
            "scenario": "ELU offers advantages over ReLU and Leaky ReLU in terms of smoothness and robustness to noise. It can be beneficial for complex tasks requiring accurate learning."
          },
          {
            "value": "tanh",
            "scenario": "Tanh is often used in recurrent neural networks (RNNs) and tasks involving natural language processing (NLP) or time series data analysis."
          },
          {
            "value": "sigmoid",
            "scenario": "Sigmoid is commonly used for binary classification tasks where the output needs to be in the range of 0 to 1, representing the probability of belonging to a particular class."
          },
          {
            "value": "softmax",
            "scenario": "Softmax is essential for multi-class classification, where the output probabilities need to add up to 1, indicating the probability distribution across multiple classes."
          }
        ],
        "impact": {
          "convergence_speed": "The choice of activation function can influence the convergence speed of the neural network training. Some activation functions, like ReLU, help mitigate the vanishing gradient problem, potentially leading to faster convergence. However, the impact can vary based on the specific problem and other factors.",
          "generalization": "The activation function plays a crucial role in determining the model's capacity to generalize to unseen data. Appropriate activation functions can enhance the model's ability to learn complex patterns and make accurate predictions on new samples.",
          "stability": "The stability of the training process can be affected by the activation function. For instance, ReLU is susceptible to the 'dying ReLU' problem, which can lead to instability. Choosing a more stable activation function, like ELU, can improve training stability."
        }
      },
      {
        "name": "learning_rate",
        "value": "learning_rate",
        "line_number": 72,
        "char_start": 57,
        "char_end": 70,
        "code_context": "\n        return tf.reduce_mean(loss_a + loss_b + loss_c, name='loss')\n\n    def _get_training_op(self, learning_rate, epsilon):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)\n        return optimizer.minimize(self.loss)",
        "param_type": "optimizer",
        "explanation": "The learning rate controls the step size that the optimizer takes in the direction of the negative gradient during training. It determines how quickly the model learns and converges to a minimum of the loss function.",
        "typical_range": "0.001 to 0.1",
        "alternatives": [
          {
            "value": "0.0001",
            "scenario": "For very complex models or when fine-tuning pre-trained models."
          },
          {
            "value": "0.01",
            "scenario": "For standard training of deep neural networks."
          },
          {
            "value": "0.1",
            "scenario": "For quick experimentation or exploring larger learning rate schedules."
          }
        ],
        "impact": {
          "convergence_speed": "High learning rates can lead to faster convergence but may also cause instability and overshooting the minimum. Low learning rates lead to slower convergence but may be more stable.",
          "generalization": "Higher learning rates can lead to overfitting, while lower learning rates can lead to underfitting.",
          "stability": "High learning rates can lead to oscillations and instability during training, while lower learning rates are more stable."
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "unknown",
    "code_snippet": "from __future__ import unicode_literals\n\nimport tensorflow as tf\n\n\nINPUT_DIMENSION = 768  # 8 x 8 squares x 12 piece types\nHIDDEN_UNITS = 2048\nKAPPA = 10.0  # Emphasizes f(p) = -f(q)\n\n\nclass ChessDNNEstimator(object):\n\n    def __init__(self, learning_rate, adam_epsilon):\n        with tf.variable_scope(\"input\"):\n            self.X = tf.placeholder(tf.float32, shape=(None, INPUT_DIMENSION), name='X')\n            self.X_parent = tf.placeholder(tf.float32, shape=(None, INPUT_DIMENSION), name='X_parent')\n            self.X_observed = tf.placeholder(tf.float32, shape=(None, INPUT_DIMENSION), name='X_observed')\n            self.X_random = tf.placeholder(tf.float32, shape=(None, INPUT_DIMENSION), name='X_random')\n\n        with tf.variable_scope(\"f_p\"):\n            self.training = tf.placeholder_with_default(False, shape=(), name='training')\n            self.f = self._get_evaluation_function(self.X)\n            tf.get_variable_scope().reuse_variables()\n            self.f_parent = self._get_evaluation_function(self.X_parent)\n            self.f_observed = self._get_evaluation_function(self.X_observed)\n            self.f_random = self._get_evaluation_function(self.X_random)\n\n        with tf.name_scope('loss'):\n            self.loss = self._get_loss()\n\n        with tf.name_scope('train'):\n            self.training_op = self._get_training_op(learning_rate, adam_epsilon)\n\n    def train(self, session, X_parent, X_observed, X_random):\n        session.run(self.training_op, feed_dict={\n            self.X_parent: X_parent,\n            self.X_observed: X_observed,\n            self.X_random: X_random,\n            self.training: True,\n        })\n\n    def compute_loss(self, session, X_parent, X_observed, X_random):\n        return session.run(self.loss, feed_dict={\n            self.X_parent: X_parent,\n            self.X_observed: X_observed,\n            self.X_random: X_random,\n        })\n\n    def evaluate(self, session, X):\n        return session.run(self.f, feed_dict={\n            self.X: X\n        })\n\n    def _get_evaluation_function(self, X):\n        hidden_1 = tf.layers.dense(X, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_1')\n        hidden_2 = tf.layers.dense(hidden_1, HIDDEN_UNITS, activation=tf.nn.elu, name='hidden_2')\n        output = tf.layers.dense(hidden_2, 1, activation=None, name='output')\n        return output\n\n    def _get_loss(self):\n        x_observed_random = self.f_random - self.f_observed\n        x_parent_observed = self.f_parent + self.f_observed\n\n        epsilon_log = 1e-3\n        loss_a = -tf.log(epsilon_log + tf.sigmoid(x_observed_random))\n        loss_b = -tf.log(epsilon_log + tf.sigmoid(KAPPA * x_parent_observed))\n        loss_c = -tf.log(epsilon_log + tf.sigmoid(-KAPPA * x_parent_observed))\n\n        return tf.reduce_mean(loss_a + loss_b + loss_c, name='loss')\n\n    def _get_training_op(self, learning_rate, epsilon):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)\n        return optimizer.minimize(self.loss)\n"
  },
  {
    "id": "ml_184bb53b",
    "framework": "unknown",
    "source_url": "https://github.com/nayutaya/tensorflow-rnn-sin/blob/master/ex1/basic/rnn.py",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "learning_rate",
        "line_number": 53,
        "char_start": 60,
        "char_end": 73,
        "code_context": "random.seed(0)\nnp.random.seed(0)\ntf.set_random_seed(0)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n\nwith tf.Graph().as_default():\n    input_ph      = tf.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n    supervisor_ph = tf.placeholder(tf.float32, [None, num_of_output_nodes], name=\"supervisor\")\n    istate_ph     = tf.placeholder(tf.float32, [None, num_of_hidden_nodes * 2], name=\"istate\") # 1\u30bb\u30eb\u3042\u305f\u308a2\u3064\u306e\u5024\u3092\u5fc5\u8981\u3068\u3059\u308b\u3002",
        "param_type": "optimizer",
        "explanation": "The learning rate controls the step size taken during gradient descent optimization. A larger learning rate will cause the model to learn faster, but may also become unstable and overshoot the optimal solution. A smaller learning rate will cause the model to learn slower, but may be more stable and accurate.",
        "typical_range": "0.001 to 0.1",
        "alternatives": [
          {
            "value": "0.001",
            "scenario": "Use a smaller learning rate for complex models or datasets with high noise."
          },
          {
            "value": "0.01",
            "scenario": "Use a medium learning rate for most models and datasets."
          },
          {
            "value": "0.1",
            "scenario": "Use a larger learning rate for small models or datasets with low noise."
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "poor",
          "stability": "low"
        }
      }
    ],
    "model_type": "RNN",
    "task": "unknown",
    "code_snippet": "\nimport tensorflow as tf\nfrom tensorflow.models.rnn import rnn, rnn_cell\nimport numpy as np\nimport random\n\ndef make_mini_batch(train_data, size_of_mini_batch, length_of_sequences):\n    inputs  = np.empty(0)\n    outputs = np.empty(0)\n    for _ in range(size_of_mini_batch):\n        index   = random.randint(0, len(train_data) - length_of_sequences)\n        part    = train_data[index:index + length_of_sequences]\n        inputs  = np.append(inputs, part[:, 0])\n        outputs = np.append(outputs, part[-1, 1])\n    inputs  = inputs.reshape(-1, length_of_sequences, 1)\n    outputs = outputs.reshape(-1, 1)\n    return (inputs, outputs)\n\ndef make_prediction_initial(train_data, index, length_of_sequences):\n    return train_data[index:index + length_of_sequences, 0]\n\ntrain_data_path             = \"../train_data/normal.npy\"\nnum_of_input_nodes          = 1\nnum_of_hidden_nodes         = 2\nnum_of_output_nodes         = 1\nlength_of_sequences         = 50\nnum_of_training_epochs      = 2000\nlength_of_initial_sequences = 50\nnum_of_prediction_epochs    = 100\nsize_of_mini_batch          = 100\nlearning_rate               = 0.1\nforget_bias                 = 1.0\nprint(\"train_data_path             = %s\" % train_data_path)\nprint(\"num_of_input_nodes          = %d\" % num_of_input_nodes)\nprint(\"num_of_hidden_nodes         = %d\" % num_of_hidden_nodes)\nprint(\"num_of_output_nodes         = %d\" % num_of_output_nodes)\nprint(\"length_of_sequences         = %d\" % length_of_sequences)\nprint(\"num_of_training_epochs      = %d\" % num_of_training_epochs)\nprint(\"length_of_initial_sequences = %d\" % length_of_initial_sequences)\nprint(\"num_of_prediction_epochs    = %d\" % num_of_prediction_epochs)\nprint(\"size_of_mini_batch          = %d\" % size_of_mini_batch)\nprint(\"learning_rate               = %f\" % learning_rate)\nprint(\"forget_bias                 = %f\" % forget_bias)\n\ntrain_data = np.load(train_data_path)\nprint(\"train_data:\", train_data)\n\n# \u4e71\u6570\u30b7\u30fc\u30c9\u3092\u56fa\u5b9a\u3059\u308b\u3002\nrandom.seed(0)\nnp.random.seed(0)\ntf.set_random_seed(0)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n\nwith tf.Graph().as_default():\n    input_ph      = tf.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n    supervisor_ph = tf.placeholder(tf.float32, [None, num_of_output_nodes], name=\"supervisor\")\n    istate_ph     = tf.placeholder(tf.float32, [None, num_of_hidden_nodes * 2], name=\"istate\") # 1\u30bb\u30eb\u3042\u305f\u308a2\u3064\u306e\u5024\u3092\u5fc5\u8981\u3068\u3059\u308b\u3002\n\n    with tf.name_scope(\"inference\") as scope:\n        weight1_var = tf.Variable(tf.truncated_normal([num_of_input_nodes, num_of_hidden_nodes], stddev=0.1), name=\"weight1\")\n        weight2_var = tf.Variable(tf.truncated_normal([num_of_hidden_nodes, num_of_output_nodes], stddev=0.1), name=\"weight2\")\n        bias1_var   = tf.Variable(tf.truncated_normal([num_of_hidden_nodes], stddev=0.1), name=\"bias1\")\n        bias2_var   = tf.Variable(tf.truncated_normal([num_of_output_nodes], stddev=0.1), name=\"bias2\")\n\n        in1 = tf.transpose(input_ph, [1, 0, 2])         # (batch, sequence, data) -> (sequence, batch, data)\n        in2 = tf.reshape(in1, [-1, num_of_input_nodes]) # (sequence, batch, data) -> (sequence * batch, data)\n        in3 = tf.matmul(in2, weight1_var) + bias1_var\n        in4 = tf.split(0, length_of_sequences, in3)     # sequence * (batch, data)\n\n        cell = rnn_cell.BasicLSTMCell(num_of_hidden_nodes, forget_bias=forget_bias)\n        rnn_output, states_op = rnn.rnn(cell, in4, initial_state=istate_ph)\n        output_op = tf.matmul(rnn_output[-1], weight2_var) + bias2_var\n\n    with tf.name_scope(\"loss\") as scope:\n        square_error = tf.reduce_mean(tf.square(output_op - supervisor_ph))\n        loss_op      = square_error\n        tf.scalar_summary(\"loss\", loss_op)\n\n    with tf.name_scope(\"training\") as scope:\n        training_op = optimizer.minimize(loss_op)\n\n    summary_op = tf.merge_all_summaries()\n    init = tf.initialize_all_variables()\n\n    with tf.Session() as sess:\n        saver = tf.train.Saver()\n        summary_writer = tf.train.SummaryWriter(\"data\", graph=sess.graph)\n        sess.run(init)\n\n        for epoch in range(num_of_training_epochs):\n            inputs, supervisors = make_mini_batch(train_data, size_of_mini_batch, length_of_sequences)\n\n            train_dict = {\n                input_ph:      inputs,\n                supervisor_ph: supervisors,\n                istate_ph:     np.zeros((size_of_mini_batch, num_of_hidden_nodes * 2)),\n            }\n            sess.run(training_op, feed_dict=train_dict)\n\n            if (epoch + 1) % 10 == 0:\n                summary_str, train_loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n                summary_writer.add_summary(summary_str, epoch)\n                print(\"train#%d, train loss: %e\" % (epoch + 1, train_loss))\n\n        inputs  = make_prediction_initial(train_data, 0, length_of_initial_sequences)\n        outputs = np.empty(0)\n        states  = np.zeros((num_of_hidden_nodes * 2)),\n\n        print(\"initial:\", inputs)\n        np.save(\"initial.npy\", inputs)\n\n        for epoch in range(num_of_prediction_epochs):\n            pred_dict = {\n                input_ph:  inputs.reshape((1, length_of_sequences, 1)),\n                istate_ph: states,\n            }\n            output, states = sess.run([output_op, states_op], feed_dict=pred_dict)\n            print(\"prediction#%d, output: %f\" % (epoch + 1, output))\n\n            inputs  = np.delete(inputs, 0)\n            inputs  = np.append(inputs, output)\n            outputs = np.append(outputs, output)\n\n        print(\"outputs:\", outputs)\n        np.save(\"output.npy\", outputs)\n\n        saver.save(sess, \"data/model\")\n"
  },
  {
    "id": "ml_5a64b80c",
    "framework": "unknown",
    "source_url": "https://github.com/ucloud/uai-sdk/blob/master/examples/tensorflow/train/imagenet/code/imagenet_main.py",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "learning_rate",
        "line_number": 169,
        "char_start": 24,
        "char_end": 37,
        "code_context": "\n      train_hooks = [logging_hook, examples_sec_hook]\n\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate, momentum=momentum)\n\n      if params.sync:\n        optimizer = tf.train.SyncReplicasOptimizer(\n            optimizer, replicas_to_aggregate=num_workers)\n        sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)",
        "param_type": "optimizer",
        "explanation": "The learning rate determines how much the model updates its parameters based on the error in its predictions. A higher learning rate leads to faster learning, but it may also make the model more likely to get stuck in local optima. A lower learning rate leads to slower learning, but it may also help the model converge to a better solution overall. It is an important hyperparameter to tune for optimal performance.",
        "typical_range": "0.0001-0.1, but can vary significantly depending on the problem and dataset.",
        "alternatives": [
          {
            "value": "Adam",
            "scenario": "Can handle sparse gradients better and can converge faster than SGD"
          },
          {
            "value": "RMSprop",
            "scenario": "More robust to bad initial learning rates, can handle noisy gradients, and adaptive to changing learning rates"
          },
          {
            "value": "Adamax",
            "scenario": "Combines the advantages of RMSprop and Adam, can converge faster, and has a higher stability"
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "medium",
          "stability": "low"
        }
      },
      {
        "name": "momentum",
        "value": "momentum",
        "line_number": 169,
        "char_start": 48,
        "char_end": 56,
        "code_context": "\n      train_hooks = [logging_hook, examples_sec_hook]\n\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate, momentum=momentum)\n\n      if params.sync:\n        optimizer = tf.train.SyncReplicasOptimizer(\n            optimizer, replicas_to_aggregate=num_workers)\n        sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)",
        "param_type": "optimizer",
        "explanation": "The momentum hyperparameter controls the rate at which previous gradients decay during optimization. It helps to accelerate convergence and avoid getting stuck in local minima.",
        "typical_range": "0.5 - 0.9",
        "alternatives": [
          {
            "value": "0.9",
            "scenario": "Most common value, provides good balance between speed and stability."
          },
          {
            "value": "0.5",
            "scenario": "When faster convergence is desired, but at the risk of potential instability."
          },
          {
            "value": "0.99",
            "scenario": "When very small oscillations near the minimum are present, but may lead to slower convergence."
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "medium"
        }
      },
      {
        "name": "batch_size",
        "value": "hparams.train_batch_size",
        "line_number": 308,
        "char_start": 19,
        "char_end": 43,
        "code_context": "        input_fn,\n        data_dir,\n        subset='train',\n        num_shards=num_gpus,\n        batch_size=hparams.train_batch_size,\n        use_distortion_for_training=use_distortion_for_training)\n\n    eval_input_fn = functools.partial(\n        input_fn,\n        data_dir,",
        "param_type": "training",
        "explanation": "The number of samples in a single training iteration. This parameter controls the balance between model capacity and training speed.",
        "typical_range": "32-256 for most tasks, but can be larger for memory-intensive tasks, particularly on GPUs with large memory.",
        "alternatives": [
          {
            "value": "32",
            "scenario": "Standard setting for a balance between memory and performance."
          },
          {
            "value": "64",
            "scenario": "When dealing with tasks that have more parameters or require faster training."
          },
          {
            "value": "128 and above",
            "scenario": "Large batch sizes improve convergence speed on large datasets. However, they require more memory and may face diminishing returns."
          }
        ],
        "impact": {
          "convergence_speed": "high with larger batch sizes, but diminishing returns exist",
          "generalization": "can be affected negatively with larger batch sizes due to overfitting",
          "stability": "increases with larger batch sizes, but requires careful monitoring"
        }
      },
      {
        "name": "batch_size",
        "value": "hparams.eval_batch_size",
        "line_number": 315,
        "char_start": 19,
        "char_end": 42,
        "code_context": "    eval_input_fn = functools.partial(\n        input_fn,\n        data_dir,\n        subset='validation',\n        batch_size=hparams.eval_batch_size,\n        num_shards=num_gpus)\n\n    num_eval_examples = _NUM_IMAGES['validation']\n    if num_eval_examples % hparams.eval_batch_size != 0:\n      raise ValueError(",
        "param_type": "training",
        "explanation": "Batch size determines the number of data samples processed in a single training iteration. It plays a crucial role in model optimization by influencing convergence speed, memory usage, and gradient descent stability.",
        "typical_range": "The optimal batch size depends on factors like dataset size, hardware constraints, and model complexity. Typical values range from 8 to 512, but can vary significantly.",
        "alternatives": [
          {
            "value": "Smaller batch sizes (8-64)",
            "scenario": "Use smaller batches for limited memory, frequent gradient updates, and improved adaptivity to local minima."
          },
          {
            "value": "Medium batch sizes (128-256)",
            "scenario": "This is often a good starting point, balancing convergence speed and memory usage."
          },
          {
            "value": "Large batch sizes (512 or more)",
            "scenario": "Consider larger batches for faster convergence on large datasets with ample GPU memory. However, be cautious of potential instability and vanishing gradients."
          }
        ],
        "impact": {
          "convergence_speed": "Larger batch sizes generally result in faster convergence but can exhibit higher variance. Smaller batches may converge slower but provide more stable updates.",
          "generalization": "The impact on generalization performance is less clear, with some studies suggesting smaller batches might improve generalization on certain datasets.",
          "stability": "Larger batch sizes can be more susceptible to instability and vanishing gradients, particularly for deep neural networks."
        }
      },
      {
        "name": "batch_size",
        "value": "hparams.train_batch_size",
        "line_number": 379,
        "char_start": 23,
        "char_end": 47,
        "code_context": "        input_fn=lambda: input_fn(\n            data_dir,\n            subset='train',\n            num_shards=num_gpus,\n            batch_size=hparams.train_batch_size,\n            use_distortion_for_training=use_distortion_for_training),\n        hooks=[logging_hook])\n\n    print('Starting to evaluate.')\n    eval_results = resnet_classifier.evaluate(",
        "param_type": "training",
        "explanation": "This parameter controls the number of samples per training iteration. Selecting a larger batch size will reduce the frequency of weight updates and improve computation efficiency. However, it can also increase the memory footprint and reduce the ability of the model to capture fine-grained details.",
        "typical_range": "16-64",
        "alternatives": [
          {
            "value": "32",
            "scenario": "Common value that balances efficiency and effectiveness for many models."
          },
          {
            "value": "128",
            "scenario": "Can be used when computational resources are abundant and the task is not particularly memory-intensive."
          },
          {
            "value": "8",
            "scenario": "May be useful for memory-constrained environments or when dealing with very complex tasks that benefit from smaller batches."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      },
      {
        "name": "batch_size",
        "value": "hparams.eval_batch_size",
        "line_number": 388,
        "char_start": 21,
        "char_end": 44,
        "code_context": "    eval_results = resnet_classifier.evaluate(\n        input_fn=lambda: input_fn(\n          data_dir,\n          subset='validation',\n          batch_size=hparams.eval_batch_size,\n          num_shards=num_gpus))\n    print(eval_results)\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()",
        "param_type": "training",
        "explanation": "The batch size controls the number of samples that are processed at once during the validation phase. This determines how efficient the validation is and whether it is representative of the whole validation set.",
        "typical_range": "The typical range for the batch size is between 32 and 256. Smaller batches can be more efficient for memory-constrained situations, while larger batches can be more efficient for computational power and achieving faster convergence.",
        "alternatives": [
          {
            "value": "hparams.batch_size",
            "scenario": "Use the same batch size as used for training for a consistent comparison between the training and validation process."
          },
          {
            "value": "16",
            "scenario": "Use a smaller batch size if the memory footprint during validation is too large or for faster validation."
          },
          {
            "value": "512",
            "scenario": "Use a larger batch size if the validation is computationally intensive and you have sufficient memory available."
          }
        ],
        "impact": {
          "convergence_speed": "N/A - irrelevant for the validation phase",
          "generalization": "Potentially: Using a batch size that is too large may not be representative of the whole validation set and could hurt generalization.",
          "stability": "Low: A smaller batch size can lead to more variance in the validation results."
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "unknown",
    "code_snippet": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"ResNet model for classifying images from Imagenet dataset.\n\nSupport single-host training with one or multiple devices.\n\nResNet as proposed in:\nKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\nDeep Residual Learning for Image Recognition. arXiv:1512.03385\n\n\"\"\"\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport functools\nimport itertools\nimport os\n\nimport imagenet\nimport imagenet_utils\nimport resnet_model\nimport numpy as np\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n_NUM_IMAGES = {\n    'train': 1281167,\n    'validation': 50000,\n}\n\ndef get_model_fn(num_gpus, variable_strategy, num_workers):\n  \"\"\"Returns a function that will build the resnet model.\"\"\"\n\n  def _resnet_model_fn(features, labels, mode, params):\n    \"\"\"Resnet model body.\n\n    Support single host, one or more GPU training. Parameter distribution can\n    be either one of the following scheme.\n    1. CPU is the parameter server and manages gradient updates.\n    2. Parameters are distributed evenly across all GPUs, and the first GPU\n       manages gradient updates.\n\n    Args:\n      features: a list of tensors, one for each tower\n      labels: a list of tensors, one for each tower\n      mode: ModeKeys.TRAIN or EVAL\n      params: Hyperparameters suitable for tuning\n    Returns:\n      A EstimatorSpec object.\n    \"\"\"\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n    weight_decay = params.weight_decay\n    momentum = params.momentum\n\n    tower_features = features\n    tower_labels = labels\n    tower_losses = []\n    tower_gradvars = []\n    tower_preds = []\n\n    # channels first (NCHW) is normally optimal on GPU and channels last (NHWC)\n    # on CPU. The exception is Intel MKL on CPU which is optimal with\n    # channels_last.\n    data_format = params.data_format\n    if not data_format:\n      if num_gpus == 0:\n        data_format = 'channels_last'\n      else:\n        data_format = 'channels_first'\n\n    if num_gpus == 0:\n      num_devices = 1\n      device_type = 'cpu'\n    else:\n      num_devices = num_gpus\n      device_type = 'gpu'\n\n    for i in range(num_devices):\n      worker_device = '/{}:{}'.format(device_type, i)\n      if variable_strategy == 'CPU':\n        device_setter = imagenet_utils.local_device_setter(\n            worker_device=worker_device)\n      elif variable_strategy == 'GPU':\n        device_setter = imagenet_utils.local_device_setter(\n            ps_device_type='gpu',\n            worker_device=worker_device,\n            ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(\n                num_gpus, tf.contrib.training.byte_size_load_fn))\n      with tf.variable_scope('resnet', reuse=bool(i != 0)):\n        with tf.name_scope('tower_%d' % i) as name_scope:\n          with tf.device(device_setter):\n            loss, gradvars, preds = _tower_fn(\n                is_training, weight_decay, tower_features[i], tower_labels[i],\n                data_format, params.resnet_size, params.batch_norm_decay,\n                params.batch_norm_epsilon)\n            tower_losses.append(loss)\n            tower_gradvars.append(gradvars)\n            tower_preds.append(preds)\n            if i == 0:\n              # Only trigger batch_norm moving mean and variance update from\n              # the 1st tower. Ideally, we should grab the updates from all\n              # towers but these stats accumulate extremely fast so we can\n              # ignore the other stats from the other towers without\n              # significant detriment.\n              update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,\n                                             name_scope)\n\n    # Now compute global loss and gradients.\n    gradvars = []\n    with tf.name_scope('gradient_averaging'):\n      all_grads = {}\n      for grad, var in itertools.chain(*tower_gradvars):\n        if grad is not None:\n          all_grads.setdefault(var, []).append(grad)\n      for var, grads in six.iteritems(all_grads):\n        # Average gradients on the same device as the variables\n        # to which they apply.\n        with tf.device(var.device):\n          if len(grads) == 1:\n            avg_grad = grads[0]\n          else:\n            avg_grad = tf.multiply(tf.add_n(grads), 1. / len(grads))\n        gradvars.append((avg_grad, var))\n\n    # Device that runs the ops to apply global gradient updates.\n    consolidation_device = '/gpu:0' if variable_strategy == 'GPU' else '/cpu:0'\n    with tf.device(consolidation_device):\n      # Suggested learning rate scheduling from\n      batches_per_epoch = _NUM_IMAGES['train'] / (params.train_batch_size * num_workers)\n\n      boundaries = [\n          int(batches_per_epoch * epoch) for epoch in [30, 60, 80, 90]]\n      staged_lr = [params.learning_rate * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\n\n      learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),\n                                                  boundaries, staged_lr)\n      tf.identity(learning_rate, name='learning_rate')\n      tf.summary.scalar('learning_rate', learning_rate)\n\n      loss = tf.reduce_mean(tower_losses, name='loss')\n\n      examples_sec_hook = imagenet_utils.ExamplesPerSecondHook(\n          params.train_batch_size, every_n_steps=10)\n\n      tensors_to_log = {'learning_rate': learning_rate, 'loss': loss}\n\n      logging_hook = tf.train.LoggingTensorHook(\n          tensors=tensors_to_log, every_n_iter=100)\n\n      train_hooks = [logging_hook, examples_sec_hook]\n\n      optimizer = tf.train.MomentumOptimizer(\n          learning_rate=learning_rate, momentum=momentum)\n\n      if params.sync:\n        optimizer = tf.train.SyncReplicasOptimizer(\n            optimizer, replicas_to_aggregate=num_workers)\n        sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)\n        train_hooks.append(sync_replicas_hook)\n\n      # Create single grouped train op\n      train_op = [\n          optimizer.apply_gradients(\n              gradvars, global_step=tf.train.get_global_step())\n      ]\n      train_op.extend(update_ops)\n      train_op = tf.group(*train_op)\n\n      predictions = {\n          'classes':\n              tf.concat([p['classes'] for p in tower_preds], axis=0),\n          'probabilities':\n              tf.concat([p['probabilities'] for p in tower_preds], axis=0)\n      }\n      stacked_labels = tf.concat(labels, axis=0)\n      accuracy = tf.metrics.accuracy(stacked_labels, predictions['classes'])\n      metrics = {\n          'accuracy': accuracy\n      }\n      tf.identity(accuracy[1], name='train_accuracy')\n      tf.summary.scalar('train_accuracy', accuracy[1])\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=loss,\n        train_op=train_op,\n        training_hooks=train_hooks,\n        eval_metric_ops=metrics)\n\n  return _resnet_model_fn\n\n\ndef _tower_fn(is_training, weight_decay, feature, label, data_format,\n              resnet_size, batch_norm_decay, batch_norm_epsilon):\n  \"\"\"Build computation tower (Resnet).\n\n  Args:\n    is_training: true if is training graph.\n    weight_decay: weight regularization strength, a float.\n    feature: a Tensor.\n    label: a Tensor.\n    data_format: channels_last (NHWC) or channels_first (NCHW).\n    num_layers: number of layers, an int.\n    batch_norm_decay: decay for batch normalization, a float.\n    batch_norm_epsilon: epsilon for batch normalization, a float.\n\n  Returns:\n    A tuple with the loss for the tower, the gradients and parameters, and\n    predictions.\n\n  \"\"\"\n  network = resnet_model.imagenet_resnet_v2(resnet_size, 1000 + 1, data_format)\n  logits = network(feature, is_training=is_training)\n  tower_pred = {\n      'classes': tf.argmax(input=logits, axis=1),\n      'probabilities': tf.nn.softmax(logits)\n  }\n\n  tower_loss = tf.losses.sparse_softmax_cross_entropy(\n      logits=logits, labels=label)\n  tower_loss = tf.reduce_mean(tower_loss)\n\n  model_params = tf.trainable_variables()\n  tower_loss += weight_decay * tf.add_n(\n      [tf.nn.l2_loss(v) for v in model_params])\n\n  tower_grad = tf.gradients(tower_loss, model_params)\n\n  return tower_loss, zip(tower_grad, model_params), tower_pred\n\n\ndef input_fn(data_dir,\n             subset,\n             num_shards,\n             batch_size,\n             use_distortion_for_training=True):\n  \"\"\"Create input graph for model.\n\n  Args:\n    data_dir: Directory where TFRecords representing the dataset are located.\n    subset: one of 'train', 'validate' and 'eval'.\n    num_shards: num of towers participating in data-parallel training.\n    batch_size: total batch size for training to be divided by the number of\n    shards.\n    use_distortion_for_training: True to use distortions.\n  Returns:\n    two lists of tensors for features and labels, each of num_shards length.\n  \"\"\"\n  with tf.device('/cpu:0'):\n    use_distortion = subset == 'train' and use_distortion_for_training\n    dataset = imagenet.ImagenetDataSet(data_dir, subset, use_distortion)\n\n    feature_shards, label_shards = dataset.make_batch(batch_size, is_training=(subset == 'train'), num_shards=num_shards)\n    return feature_shards, label_shards\n\ndef get_experiment_fn(data_dir,\n                      num_gpus,\n                      variable_strategy,\n                      use_distortion_for_training=True):\n  \"\"\"Returns an Experiment function.\n\n  Experiments perform training on several workers in parallel,\n  in other words experiments know how to invoke train and eval in a sensible\n  fashion for distributed training. Arguments passed directly to this\n  function are not tunable, all other arguments should be passed within\n  tf.HParams, passed to the enclosed function.\n\n  Args:\n      data_dir: str. Location of the data for input_fns.\n      num_gpus: int. Number of GPUs on each worker.\n      variable_strategy: String. CPU to use CPU as the parameter server\n      and GPU to use the GPUs as the parameter server.\n      use_distortion_for_training: bool. See imagenet.ImagenetDataSet.\n  Returns:\n      A function (tf.estimator.RunConfig, tf.contrib.training.HParams) ->\n      tf.contrib.learn.Experiment.\n\n      Suitable for use by tf.contrib.learn.learn_runner, which will run various\n      methods on Experiment (train, evaluate) based on information\n      about the current runner in `run_config`.\n  \"\"\"\n\n  def _experiment_fn(run_config, hparams):\n    \"\"\"Returns an Experiment.\"\"\"\n    # Create estimator.\n    train_input_fn = functools.partial(\n        input_fn,\n        data_dir,\n        subset='train',\n        num_shards=num_gpus,\n        batch_size=hparams.train_batch_size,\n        use_distortion_for_training=use_distortion_for_training)\n\n    eval_input_fn = functools.partial(\n        input_fn,\n        data_dir,\n        subset='validation',\n        batch_size=hparams.eval_batch_size,\n        num_shards=num_gpus)\n\n    num_eval_examples = _NUM_IMAGES['validation']\n    if num_eval_examples % hparams.eval_batch_size != 0:\n      raise ValueError(\n          'validation set size must be multiple of eval_batch_size')\n\n    train_steps = hparams.train_steps\n    eval_steps = num_eval_examples // hparams.eval_batch_size\n \n    classifier = tf.estimator.Estimator(\n        model_fn=get_model_fn(num_gpus, variable_strategy,\n                              run_config.num_worker_replicas or 1),\n        config=run_config,\n        params=hparams)\n\n    return classifier\n  return _experiment_fn\n\n\ndef main(output_dir, data_dir, num_gpus, train_epochs, epochs_per_eval, variable_strategy,\n         use_distortion_for_training, log_device_placement, num_intra_threads,\n         **hparams):\n  # The env variable is on deprecation path, default is set to off.\n  os.environ['TF_SYNC_ON_FINISH'] = '0'\n  os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n\n  # Session configuration.\n  sess_config = tf.ConfigProto(\n      allow_soft_placement=True,\n      log_device_placement=log_device_placement,\n      intra_op_parallelism_threads=num_intra_threads,\n      gpu_options=tf.GPUOptions(force_gpu_compatible=True))\n\n  # In UAI Platform We use output_dir to store model_dir\n  #                 We use data_dir to store input data\n  config = imagenet_utils.RunConfig(\n      session_config=sess_config, model_dir=output_dir).replace(save_checkpoints_secs=1e4)\n\n  hparams=tf.contrib.training.HParams(\n           is_chief=config.is_chief,\n           **hparams)\n\n  resnet_classifier = tf.estimator.Estimator(\n      model_fn=get_model_fn(num_gpus, variable_strategy, config.num_worker_replicas or 1),\n      config=config,\n      params=hparams)\n\n  for _ in range(train_epochs // epochs_per_eval):\n    tensors_to_log = {\n        'learning_rate': 'learning_rate',\n        'train_accuracy': 'train_accuracy'\n    }\n\n    logging_hook = tf.train.LoggingTensorHook(\n        tensors=tensors_to_log, every_n_iter=100)\n\n    print('Starting a training cycle.')\n    resnet_classifier.train(\n        input_fn=lambda: input_fn(\n            data_dir,\n            subset='train',\n            num_shards=num_gpus,\n            batch_size=hparams.train_batch_size,\n            use_distortion_for_training=use_distortion_for_training),\n        hooks=[logging_hook])\n\n    print('Starting to evaluate.')\n    eval_results = resnet_classifier.evaluate(\n        input_fn=lambda: input_fn(\n          data_dir,\n          subset='validation',\n          batch_size=hparams.eval_batch_size,\n          num_shards=num_gpus))\n    print(eval_results)\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  \n  \"\"\"UAI SDK use data_dir output_dir and num_gpus to transfer system specific data\n  \"\"\"\n  parser.add_argument(\n      '--data_dir',\n      type=str,\n      required=True,\n      help='UAI SDK related. The directory where the imagenet input data is stored.')\n  parser.add_argument(\n      '--output_dir',\n      type=str,\n      required=True,\n      help='UAI SDK related. The directory where the model will be stored.')\n  parser.add_argument(\n      '--variable-strategy',\n      choices=['CPU', 'GPU'],\n      type=str,\n      default='CPU',\n      help='Where to locate variable operations')\n  parser.add_argument(\n      '--num_gpus',\n      type=int,\n      default=1,\n      help='UAI SDK related. The number of gpus used.')\n  parser.add_argument(\n    '--resnet_size', type=int, default=50, choices=[18, 34, 50, 101, 152, 200],\n    help='The size of the ResNet model to use.')\n  parser.add_argument(\n      '--num-layers',\n      type=int,\n      default=44,\n      help='The number of layers of the model.')\n  parser.add_argument(\n      '--train-epochs',\n      type=int,\n      default=100,\n      help='The number of epochs to use for training.')\n  parser.add_argument(\n      '--epochs_per_eval', \n      type=int, \n      default=1,\n      help='The number of training epochs to run between evaluations.')\n  parser.add_argument(\n      '--train-batch-size',\n      type=int,\n      default=128,\n      help='Batch size for training.')\n  parser.add_argument(\n      '--eval-batch-size',\n      type=int,\n      default=100,\n      help='Batch size for validation.')\n  parser.add_argument(\n      '--momentum',\n      type=float,\n      default=0.9,\n      help='Momentum for MomentumOptimizer.')\n  parser.add_argument(\n      '--weight-decay',\n      type=float,\n      default=1e-4,\n      help='Weight decay for convolutions.')\n  parser.add_argument(\n      '--learning-rate',\n      type=float,\n      default=0.1,\n      help=\"\"\"\\\n      This is the inital learning rate value. The learning rate will decrease\n      during training. For more details check the model_fn implementation in\n      this file.\\\n      \"\"\")\n  parser.add_argument(\n      '--use-distortion-for-training',\n      type=bool,\n      default=True,\n      help='If doing image distortion for training.')\n  parser.add_argument(\n      '--sync',\n      action='store_true',\n      default=False,\n      help=\"\"\"\\\n      If present when running in a distributed environment will run on sync mode.\\\n      \"\"\")\n  parser.add_argument(\n      '--num-intra-threads',\n      type=int,\n      default=0,\n      help=\"\"\"\\\n      Number of threads to use for intra-op parallelism. When training on CPU\n      set to 0 to have the system pick the appropriate number or alternatively\n      set it to the number of physical CPU cores.\\\n      \"\"\")\n  parser.add_argument(\n      '--num-inter-threads',\n      type=int,\n      default=0,\n      help=\"\"\"\\\n      Number of threads to use for inter-op parallelism. If set to 0, the\n      system will pick an appropriate number.\\\n      \"\"\")\n  parser.add_argument(\n      '--data-format',\n      type=str,\n      default=None,\n      help=\"\"\"\\\n      If not set, the data format best for the training device is used. \n      Allowed values: channels_first (NCHW) channels_last (NHWC).\\\n      \"\"\")\n  parser.add_argument(\n      '--log-device-placement',\n      action='store_true',\n      default=False,\n      help='Whether to log device placement.')\n  parser.add_argument(\n      '--batch-norm-decay',\n      type=float,\n      default=0.997,\n      help='Decay for batch norm.')\n  parser.add_argument(\n      '--batch-norm-epsilon',\n      type=float,\n      default=1e-5,\n      help='Epsilon for batch norm.')\n  parser.add_argument(\n      '--work_dir',\n      type=str,\n      default='/data/',\n      help='UAI SDK related.')\n  parser.add_argument(\n      '--log_dir',\n      type=str,\n      default='/data/data/',\n      help='UAI SDK related.'\n  )\n  args = parser.parse_args()\n\n  if args.num_gpus < 0:\n    raise ValueError(\n        'Invalid GPU count: \\\"--num-gpus\\\" must be 0 or a positive integer.')\n  if args.num_gpus == 0 and args.variable_strategy == 'GPU':\n    raise ValueError('num-gpus=0, CPU must be used as parameter server. Set'\n                     '--variable-strategy=CPU.')\n  if (args.num_layers - 2) % 6 != 0:\n    raise ValueError('Invalid --num-layers parameter.')\n  if args.num_gpus != 0 and args.train_batch_size % args.num_gpus != 0:\n    raise ValueError('--train-batch-size must be multiple of --num-gpus.')\n  if args.num_gpus != 0 and args.eval_batch_size % args.num_gpus != 0:\n    raise ValueError('--eval-batch-size must be multiple of --num-gpus.')\n\n  main(**vars(args))\n"
  }
]