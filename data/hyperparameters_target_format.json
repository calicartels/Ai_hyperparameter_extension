[
  {
    "id": "tf_tfm_001",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/uncertainty_baselines/models/bert_sngp.py",
    "code_snippet": "        name='output_layer_norm', axis=-1, epsilon=1e-12, dtype=tf.float32)",
    "hyperparameters": [
      {
        "name": "epsilon",
        "value": "1e-12",
        "line_number": 176,
        "char_start": 51,
        "char_end": 56,
        "param_type": "optimizer",
        "explanation": "The epsilon parameter controls the numerical stability of the layer normalization layer by adding a small value to the denominator during division. This helps to prevent division by zero errors and improves the overall stability of the model.",
        "typical_range": "(1e-5, 1e-12)",
        "alternatives": [
          {
            "value": "1e-6",
            "scenario": "For moderate stability"
          },
          {
            "value": "1e-8",
            "scenario": "For higher stability"
          },
          {
            "value": "1e-10",
            "scenario": "For very high stability with potential performance impact"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "sequence_prediction",
    "dataset_size": "unspecified"
  },
  {
    "id": "tf_tfm_002",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/uncertainty_baselines/models/bert_sngp.py",
    "code_snippet": "        units=self._pooled_output_dim,",
    "hyperparameters": [
      {
        "name": "units",
        "value": "self._pooled_output_dim",
        "line_number": 423,
        "char_start": 14,
        "char_end": 37,
        "param_type": "architecture",
        "explanation": "The `units` hyperparameter controls the number of neurons in the output layer of a Transformer model. This determines the dimensionality of the model's output representations, which influences the expressiveness and complexity of the learned representations.",
        "typical_range": "[768, 1024]",
        "alternatives": [
          {
            "value": "768",
            "scenario": "For efficient inference and lower computational cost"
          },
          {
            "value": "1024",
            "scenario": "For improved accuracy and expressive power, especially with large datasets"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "sequence_prediction",
    "dataset_size": "unspecified"
  },
  {
    "id": "tf_tfm_003",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/uncertainty_baselines/models/bert_sngp.py",
    "code_snippet": "        units=self._pooled_output_dim,\n        activation='tanh',\n        kernel_initializer=self._pooler_layer_initializer,",
    "hyperparameters": [
      {
        "name": "activation",
        "value": "tanh",
        "line_number": 424,
        "char_start": 19,
        "char_end": 23,
        "param_type": "activation_function",
        "explanation": "The `'tanh'` activation function introduces non-linearity to the output of the 'pooler_layer'. It restricts the output values between -1 and 1, potentially improving model performance in sequence prediction tasks by adding expressiveness while controlling the magnitude of activations.",
        "typical_range": "[-1, 1]",
        "alternatives": [
          {
            "value": "'relu'",
            "scenario": "For faster convergence and potential improvement in training speed"
          },
          {
            "value": "'softmax'",
            "scenario": "For multi-class sequence classification tasks, where the output needs to represent a probability distribution"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "sequence_prediction",
    "dataset_size": "unspecified"
  },
  {
    "id": "tf_tfm_004",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/uncertainty_baselines/models/bert_sngp.py",
    "code_snippet": "      hidden_size=bert_config.hidden_size,",
    "hyperparameters": [
      {
        "name": "hidden_size",
        "value": "bert_config.hidden_size",
        "line_number": 475,
        "char_start": 18,
        "char_end": 41,
        "param_type": "architecture",
        "explanation": "The hidden_size parameter controls the dimensionality of the output vectors from the Transformer's encoders. It determines the complexity of the representations learned by the model and impacts its expressive power. Increasing the hidden size usually improves performance but also increases training time and memory consumption.",
        "typical_range": "[128, 1024]",
        "alternatives": [
          {
            "value": "512",
            "scenario": "Good starting point for most tasks"
          },
          {
            "value": "768",
            "scenario": "Higher complexity for improved performance"
          },
          {
            "value": "1024",
            "scenario": "Even higher complexity for demanding tasks (more data required)"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "sequence_prediction",
    "dataset_size": "unspecified"
  },
  {
    "id": "tf_tfm_005",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/uncertainty_baselines/models/bert_sngp.py",
    "code_snippet": "      self.classifier = ed.layers.RandomFeatureGaussianProcess(\n          units=num_classes,\n          scale_random_features=False,",
    "hyperparameters": [
      {
        "name": "units",
        "value": "num_classes",
        "line_number": 563,
        "char_start": 16,
        "char_end": 27,
        "param_type": "architecture",
        "explanation": "The parameter `units` in a Transformer model defines the number of output nodes in the final layer, which corresponds to the number of classes in the classification task. This parameter directly controls the output dimension of the model and determines the number of possible predictions it can make.",
        "typical_range": "The typical range for `units` is directly related to the number of classes in the specific sequence prediction task. For example, if the task involves classifying text into 10 categories, the `units` parameter would be set to 10.",
        "alternatives": [],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "sequence_prediction",
    "dataset_size": "unspecified"
  },
  {
    "id": "tf_tfm_006",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/uncertainty_baselines/models/bert_sngp.py",
    "code_snippet": "            num_classes,\n            activation=None,\n            kernel_initializer=initializer,",
    "hyperparameters": [
      {
        "name": "activation",
        "value": "None",
        "line_number": 585,
        "char_start": 23,
        "char_end": 27,
        "param_type": "activation_function",
        "explanation": "The activation function applied to the output layer of the transformer network before making predictions. It determines the non-linearity of the model and affects the model's ability to learn complex relationships in the data.",
        "typical_range": "relu, tanh, sigmoid, softmax",
        "alternatives": [
          {
            "value": "relu",
            "scenario": "Used when dealing with positive outputs, often preferred for its computational efficiency."
          },
          {
            "value": "tanh",
            "scenario": "Used when the output values need to be centered around zero, improving gradient flow."
          },
          {
            "value": "sigmoid",
            "scenario": "Used when predicting probabilities, as the output values range between 0 and 1."
          },
          {
            "value": "softmax",
            "scenario": "Required for multi-class classification, normalizes output values to a probability distribution."
          },
          {
            "value": "None",
            "scenario": "No activation function is applied, allowing for a more linear model but potentially affecting performance."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "sequence_prediction",
    "dataset_size": "unspecified"
  },
  {
    "id": "tf_tfm_007",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/uncertainty_baselines/models/bert_sngp.py",
    "code_snippet": "      num_heads=num_heads,",
    "hyperparameters": [
      {
        "name": "num_heads",
        "value": "num_heads",
        "line_number": 624,
        "char_start": 16,
        "char_end": 25,
        "param_type": "architecture",
        "explanation": "The number of heads within the multi-headed attention mechanism, controlling the depth of feature representation and model expressiveness.",
        "typical_range": "1-16",
        "alternatives": [
          {
            "value": "1",
            "scenario": "Compact model size, efficient training"
          },
          {
            "value": "8",
            "scenario": "Balanced performance"
          },
          {
            "value": "16",
            "scenario": "Rich feature representation, potential overfitting"
          }
        ],
        "impact": {
          "convergence_speed": "slow (larger values)",
          "generalization": "good (medium values)",
          "stability": "high (medium values)"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "sequence_prediction",
    "dataset_size": "unspecified"
  },
  {
    "id": "tf_gen_001",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/baselines/jft/bit_deterministic.py",
    "code_snippet": "        cache=config.get('val_cache', 'batched'),\n        num_epochs=1,\n        repeat_after_batching=True,",
    "hyperparameters": [
      {
        "name": "num_epochs",
        "value": "1",
        "line_number": 165,
        "char_start": 19,
        "char_end": 20,
        "param_type": "training",
        "explanation": "Number of times to iterate over the entire training dataset. A higher value generally leads to better training accuracy, but takes more time.",
        "typical_range": "10-1000 epochs",
        "alternatives": [
          {
            "value": "10",
            "scenario": "Quick initial training, good for exploring options"
          },
          {
            "value": "50",
            "scenario": "Balanced speed and accuracy for moderate datasets"
          },
          {
            "value": "100",
            "scenario": "High accuracy for larger datasets or complex tasks"
          },
          {
            "value": "300",
            "scenario": "Maximum precision for large datasets or state-of-the-art performance"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good/excellent (with tuning)",
          "stability": "medium"
        }
      }
    ],
    "model_type": "ResNet",
    "task": "image_classification",
    "dataset_size": "large"
  },
  {
    "id": "tf_gen_002",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/baselines/jft/bit_deterministic.py",
    "code_snippet": "    opt = opt.apply_gradient(g, learning_rate=lr)",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "lr",
        "line_number": 387,
        "char_start": 46,
        "char_end": 48,
        "param_type": "optimizer",
        "explanation": "The learning rate controls the step size used to update the model's weights during training. A higher learning rate can lead to faster convergence but may also result in instability and overshooting the optimal solution. A lower learning rate can lead to slower convergence but may be more stable and achieve better generalization.",
        "typical_range": "0.001-0.1",
        "alternatives": [
          {
            "value": "0.01",
            "scenario": "Fast convergence, but less stable"
          },
          {
            "value": "0.001",
            "scenario": "Slower convergence, but more stable and better generalization"
          },
          {
            "value": "1e-5",
            "scenario": "Fine-tuning pre-trained model"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium-high"
        }
      }
    ],
    "model_type": "ResNet",
    "task": "image_classification",
    "dataset_size": "large"
  },
  {
    "id": "tf_tfm_008",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/baselines/imagenet/mimo.py",
    "code_snippet": "      batch_size=train_batch_size, strategy=strategy)",
    "hyperparameters": [
      {
        "name": "batch_size",
        "value": "train_batch_size",
        "line_number": 106,
        "char_start": 17,
        "char_end": 33,
        "param_type": "training",
        "explanation": "The batch size determines the number of samples processed at once during training. A larger batch size can accelerate training but increase memory usage and potentially affect model stability.",
        "typical_range": "[8, 32, 64, 128, 256]",
        "alternatives": [
          {
            "value": "8",
            "scenario": "When memory is limited or instability is observed"
          },
          {
            "value": "32 or 64",
            "scenario": "For a balance between training speed and memory usage"
          },
          {
            "value": "128 or 256",
            "scenario": "When ample memory is available and faster training is desired"
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "potentially_decreased",
          "stability": "potentially_decreased"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "image_classification",
    "dataset_size": "large"
  },
  {
    "id": "tf_tfm_009",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/baselines/imagenet/mimo.py",
    "code_snippet": "      batch_size=test_batch_size, strategy=strategy)",
    "hyperparameters": [
      {
        "name": "batch_size",
        "value": "test_batch_size",
        "line_number": 110,
        "char_start": 17,
        "char_end": 32,
        "param_type": "training",
        "explanation": "Batch size refers to the number of samples processed before updating the model, impacting convergence speed, memory use, and stability.",
        "typical_range": "16-256 for GPUs, 128-1024 or even higher for TPUs",
        "alternatives": [
          {
            "value": "32",
            "scenario": "Common starting point due to balance between speed and memory usage"
          },
          {
            "value": "128",
            "scenario": "For models with more parameters or GPUs with larger memory"
          },
          {
            "value": "8",
            "scenario": "When memory is limited, but may require more epochs for convergence"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "image_classification",
    "dataset_size": "large"
  },
  {
    "id": "tf_tfm_010",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/baselines/imagenet/mimo.py",
    "code_snippet": "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "learning_rate",
        "line_number": 138,
        "char_start": 54,
        "char_end": 67,
        "param_type": "optimizer",
        "explanation": "The learning rate controls the step size taken during optimization to adjust model weights. It influences convergence speed, accuracy, and stability.",
        "typical_range": "0.001 to 0.1",
        "alternatives": [
          {
            "value": "0.001",
            "scenario": "Start with a low value for fine-tuning or learning from scratch."
          },
          {
            "value": "0.1",
            "scenario": "Use a higher value for early training stages with abundant data."
          }
        ],
        "impact": {
          "convergence_speed": "medium to fast (depending on the initial learning rate)",
          "generalization": "good with proper scheduling, but can be sensitive to the initial value",
          "stability": "medium (can be affected by noise or overfitting if set too high)"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "image_classification",
    "dataset_size": "large"
  },
  {
    "id": "tf_tfm_011",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/baselines/imagenet/mimo.py",
    "code_snippet": "                                        momentum=1.0 - FLAGS.one_minus_momentum,",
    "hyperparameters": [
      {
        "name": "momentum",
        "value": "(1.0 - FLAGS.one_minus_momentum)",
        "line_number": 139,
        "char_start": 49,
        "char_end": 81,
        "param_type": "optimizer",
        "explanation": "Momentum is a method that helps accelerate the training process by adding a fraction of the previous update to the current update. It helps in navigating through saddle points and flat regions in the loss surface, leading to faster convergence.",
        "typical_range": "0.5 to 0.9 typically, but can be 0 or even slightly over 1 in certain cases",
        "alternatives": [
          {
            "value": "0.9",
            "scenario": "Common starting point for momentum value"
          },
          {
            "value": "0.0",
            "scenario": "No momentum"
          },
          {
            "value": "1.0 + epsilon (very small value)",
            "scenario": "Nesterov momentum with a small epsilon to improve oscillations"
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "Transformer",
    "task": "image_classification",
    "dataset_size": "large"
  },
  {
    "id": "tf_tfm_012",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/baselines/imagenet/mimo.py",
    "code_snippet": "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)",
    "hyperparameters": [
      {
        "name": "optimizer",
        "value": "optimizer",
        "line_number": 160,
        "char_start": 60,
        "char_end": 69,
        "param_type": "optimizer",
        "explanation": "The optimizer controls how the model's internal parameters are updated based on the training data to minimize the loss function. Different optimizers have varying learning rates, momentum, and other settings that can impact convergence speed, generalization, and stability.",
        "typical_range": "The typical range for optimizers is dependent on the specific optimizer chosen. For example, Adam typically uses a learning rate of 0.001 while SGD uses 0.1.",
        "alternatives": [
          {
            "value": "Adam",
            "scenario": "When faster convergence is desired"
          },
          {
            "value": "SGD",
            "scenario": "When better generalization is desired"
          },
          {
            "value": "RMSprop",
            "scenario": "When dealing with sparse gradients or noisy data"
          }
        ],
        "impact": {
          "convergence_speed": "The specific impact on convergence speed depends on the chosen optimizer and its settings. Adam is generally faster than SGD, while SGD can sometimes achieve better generalization.",
          "generalization": "Some optimizers like Adam prioritize faster convergence, which can lead to poorer generalization. Others like SGD may converge slower but achieve better generalization.",
          "stability": "The stability also depends on the chosen optimizer. Adam is generally more stable than SGD, especially when dealing with noisy data."
        }
      }
    ],
    "model_type": "Transformer",
    "task": "image_classification",
    "dataset_size": "large"
  },
  {
    "id": "tf_nn_001",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/uncertainty_baselines/datasets/cityscapes_test.py",
    "code_snippet": "    dataset = builder.load(batch_size=1)",
    "hyperparameters": [
      {
        "name": "batch_size",
        "value": "1",
        "line_number": 36,
        "char_start": 38,
        "char_end": 39,
        "param_type": "training",
        "explanation": "The `batch_size` determines the number of samples processed before updating internal model parameters during training. Smaller batch sizes generally lead to noisier gradient updates but may allow for better generalization, while larger batch sizes can be more efficient but may risk getting stuck in local optima.",
        "typical_range": "16-256 depending on the task, dataset size, and hardware resources.",
        "alternatives": [
          {
            "value": "1",
            "scenario": "For debugging or analyzing specific examples."
          },
          {
            "value": "32",
            "scenario": "A good starting point for experimentation."
          },
          {
            "value": "256",
            "scenario": "May be efficient for large datasets and powerful hardware."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "image_classification",
    "dataset_size": "large"
  },
  {
    "id": "tf_nn_002",
    "framework": "tensorflow",
    "source_url": "https://github.com/google/uncertainty-baselines/blob/master/uncertainty_baselines/datasets/cityscapes_test.py",
    "code_snippet": "    dataset_with_file_name = builder_with_file_name.load(batch_size=1)",
    "hyperparameters": [
      {
        "name": "batch_size",
        "value": "1",
        "line_number": 41,
        "char_start": 68,
        "char_end": 69,
        "param_type": "training",
        "explanation": "The batch size parameter defines the number of samples processed by the model during each training step. A larger batch size allows for faster training but can lead to lower generalization and stability.",
        "typical_range": "4-128",
        "alternatives": [
          {
            "value": "1",
            "scenario": "Large datasets with limited memory"
          },
          {
            "value": "16",
            "scenario": "Balanced training speed and resource utilization"
          },
          {
            "value": "128",
            "scenario": "Smaller datasets with sufficient memory"
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "Neural Network",
    "task": "image_classification",
    "dataset_size": "large"
  },
  {
    "id": "tf_cnn_001",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      inputs=input_layer,\n      filters=32,\n      kernel_size=[5, 5],",
    "hyperparameters": [
      {
        "name": "filters",
        "value": "32",
        "line_number": 40,
        "char_start": 14,
        "char_end": 16,
        "param_type": "architecture",
        "explanation": "The 'filters' parameter determines the number of convolutional filters applied in the layer. It directly affects the output channels and complexity of feature extraction.",
        "typical_range": "Number of filters typically ranges from 16 to 256, starting lower in early layers and increasing in deeper layers.",
        "alternatives": [
          {
            "value": "16",
            "scenario": "For initial layers or small datasets"
          },
          {
            "value": "64",
            "scenario": "For intermediate layers and balanced complexity"
          },
          {
            "value": "256",
            "scenario": "For advanced layers or high-resolution images"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_002",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      filters=32,\n      kernel_size=[5, 5],\n      padding=\"same\",",
    "hyperparameters": [
      {
        "name": "kernel_size",
        "value": "[5, 5]",
        "line_number": 41,
        "char_start": 18,
        "char_end": 24,
        "param_type": "architecture",
        "explanation": "Kernel size determines the size of the filter used in the convolutional layer. It controls the receptive field of the filter, influencing the level of detail extracted from the input image.",
        "typical_range": "[1, 7]",
        "alternatives": [
          {
            "value": "[3, 3]",
            "scenario": "Small receptive field for capturing local features"
          },
          {
            "value": "[7, 7]",
            "scenario": "Large receptive field for capturing global features"
          },
          {
            "value": "[1, 1]",
            "scenario": "Identity mapping (no feature extraction)"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_003",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)",
    "hyperparameters": [
      {
        "name": "padding",
        "value": "same",
        "line_number": 42,
        "char_start": 14,
        "char_end": 18,
        "param_type": "preprocessing",
        "explanation": "The `padding` parameter controls how input images are handled at the borders during convolution operations. 'same' padding ensures the output image has the same spatial dimensions as the input image by adding zeros to the borders.",
        "typical_range": [
          "same",
          "valid"
        ],
        "alternatives": [
          {
            "value": "valid",
            "scenario": "When it's crucial to preserve original input dimensions, use 'valid' padding, even if the output size is reduced."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_004",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      activation=tf.nn.relu)",
    "hyperparameters": [
      {
        "name": "activation",
        "value": "tf.nn.relu",
        "line_number": 43,
        "char_start": 17,
        "char_end": 27,
        "param_type": "activation_function",
        "explanation": "This parameter defines the activation function applied to the output of each convolutional layer. The `tf.nn.relu` function specifically allows only positive values to pass through, introducing non-linearity and improving model capacity.",
        "typical_range": [
          "relu",
          "sigmoid",
          "tanh",
          "leaky_relu",
          "elu",
          "selu"
        ],
        "alternatives": [
          {
            "value": "sigmoid",
            "scenario": "When dealing with binary classification problems"
          },
          {
            "value": "tanh",
            "scenario": "For values ranging between -1 and 1"
          },
          {
            "value": "leaky_relu",
            "scenario": "To address the 'dying ReLU' problem"
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_005",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      inputs=pool1,\n      filters=64,\n      kernel_size=[5, 5],",
    "hyperparameters": [
      {
        "name": "filters",
        "value": "64",
        "line_number": 58,
        "char_start": 14,
        "char_end": 16,
        "param_type": "architecture",
        "explanation": "The 'filters' parameter, also known as the number of output channels, determines the depth of the convolutional layer. It controls the number of feature maps extracted from the input, influencing model complexity and performance.",
        "typical_range": "[16, 32, 64, 128, 256, 512]",
        "alternatives": [
          {
            "value": "32",
            "scenario": "Smaller models for limited resources"
          },
          {
            "value": "128",
            "scenario": "Balancing complexity and performance"
          },
          {
            "value": "256",
            "scenario": "High-performance models with ample resources"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_006",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      filters=64,\n      kernel_size=[5, 5],\n      padding=\"same\",",
    "hyperparameters": [
      {
        "name": "kernel_size",
        "value": "[5, 5]",
        "line_number": 59,
        "char_start": 18,
        "char_end": 24,
        "param_type": "architecture",
        "explanation": "The `kernel_size` hyperparameter defines the dimensions of the convolutional filter. It determines the receptive field of the filter and the amount of context captured from the input image. In this case, a kernel size of [5, 5] means the filter will analyze a 5x5 pixel area in the input for feature extraction.",
        "typical_range": "Typical values for `kernel_size` in image classification CNNs range from 3x3 to 7x7, depending on the complexity and scale of the task. Smaller kernel sizes are often used for early layers to extract local features, while larger sizes can be employed in deeper layers for capturing global patterns.",
        "alternatives": [
          {
            "value": "[3, 3]",
            "scenario": "For extracting low-level, localized features in early layers"
          },
          {
            "value": "[7, 7]",
            "scenario": "For capturing more global, contextual information in deeper layers"
          },
          {
            "value": "dynamically adjust based on input image size",
            "scenario": "To adapt filter coverage to different input scales while preserving context"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_007",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)",
    "hyperparameters": [
      {
        "name": "padding",
        "value": "same",
        "line_number": 60,
        "char_start": 14,
        "char_end": 18,
        "param_type": "preprocessing",
        "explanation": "The padding parameter in CNNs controls how the input image is handled at the boundaries. \"same\" padding adds zeros around the image, ensuring the output image has the same dimensions as the input. This is often used to preserve spatial information in the image during convolutions.",
        "typical_range": [
          "same",
          "valid"
        ],
        "alternatives": [
          {
            "value": "valid",
            "scenario": "Used when output size can be smaller than input and you don't care about preserving border information."
          },
          {
            "value": "specific_value",
            "scenario": "Custom padding values for specific needs, like mirroring or reflecting borders."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_008",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      activation=tf.nn.relu)",
    "hyperparameters": [
      {
        "name": "activation",
        "value": "tf.nn.relu",
        "line_number": 61,
        "char_start": 17,
        "char_end": 27,
        "param_type": "activation_function",
        "explanation": "The activation function applied after a convolution layer. It controls the non-linearity of the network and impacts its ability to learn complex patterns.",
        "typical_range": [
          "tf.nn.relu",
          "tf.nn.sigmoid",
          "tf.nn.tanh"
        ],
        "alternatives": [
          {
            "value": "tf.nn.sigmoid",
            "scenario": "Outputting probabilities for binary classification tasks"
          },
          {
            "value": "tf.nn.tanh",
            "scenario": "Improved gradient flow when deep layers are used"
          },
          {
            "value": "tf.keras.layers.LeakyReLU",
            "scenario": "Addressing the 'dying ReLU' problem by allowing small gradients for negative values"
          }
        ],
        "impact": {
          "convergence_speed": "fast",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_009",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)",
    "hyperparameters": [
      {
        "name": "units",
        "value": "1024",
        "line_number": 78,
        "char_start": 51,
        "char_end": 55,
        "param_type": "architecture",
        "explanation": "This parameter controls the number of neurons in the dense layer of the CNN, affecting model complexity and capacity.",
        "typical_range": "[128, 1024, 4096]",
        "alternatives": [
          {
            "value": "128",
            "scenario": "When computational resources are limited"
          },
          {
            "value": "4096",
            "scenario": "When more complex features are needed"
          }
        ],
        "impact": {
          "convergence_speed": "slow",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_010",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)",
    "hyperparameters": [
      {
        "name": "activation",
        "value": "tf.nn.relu",
        "line_number": 78,
        "char_start": 68,
        "char_end": 78,
        "param_type": "activation_function",
        "explanation": "This parameter controls the non-linearity applied to the output of neurons in the network, influencing how the network learns and maps inputs to outputs.",
        "typical_range": "TensorFlow provides a variety of supported activation functions, each with different characteristics. Common choices for image classification tasks include ReLU, LeakyReLU, and Softmax.",
        "alternatives": [
          {
            "value": "tf.nn.leaky_relu(alpha=0.2)",
            "scenario": "To address the dying ReLU problem and improve gradient flow."
          },
          {
            "value": "tf.nn.softplus",
            "scenario": "For smoother and more continuous activation with non-negative outputs."
          },
          {
            "value": "tf.nn.sigmoid",
            "scenario": "For binary classification tasks where outputs are probabilities between 0 and 1."
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "medium"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_011",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "  logits = tf.layers.dense(inputs=dropout, units=10)",
    "hyperparameters": [
      {
        "name": "units",
        "value": "10",
        "line_number": 87,
        "char_start": 49,
        "char_end": 51,
        "param_type": "architecture",
        "explanation": "This parameter controls the number of neurons within the final, fully connected layer of the CNN. It directly influences the number of output categories the model can predict.",
        "typical_range": "16-1024",
        "alternatives": [
          {
            "value": "32",
            "scenario": "When dealing with few classes and small datasets"
          },
          {
            "value": "128",
            "scenario": "Standard value for moderate datasets and a reasonable number of classes"
          },
          {
            "value": "512-1024",
            "scenario": "For handling large, complex datasets with many classes"
          }
        ],
        "impact": {
          "convergence_speed": "slow|medium",
          "generalization": "poor|good|excellent",
          "stability": "medium|high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_012",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)",
    "hyperparameters": [
      {
        "name": "learning_rate",
        "value": "0.001",
        "line_number": 106,
        "char_start": 64,
        "char_end": 69,
        "param_type": "optimizer",
        "explanation": "Learning rate controls the step size the optimizer takes in updating the model parameters towards minimizing the loss function.",
        "typical_range": "(1e-4, 1e-1)",
        "alternatives": [
          {
            "value": "0.1",
            "scenario": "Exploration: use larger values to initially explore a vast parameter space and find a potential minimum"
          },
          {
            "value": "0.0001",
            "scenario": "Fine-tuning: use smaller values to make smaller adjustments and refine the model near a minimum"
          },
          {
            "value": "0.005",
            "scenario": "Default value: suitable for many image classification tasks"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "medium",
          "stability": "high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_013",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      y=train_labels,\n      batch_size=100,\n      num_epochs=None,",
    "hyperparameters": [
      {
        "name": "batch_size",
        "value": "100",
        "line_number": 142,
        "char_start": 17,
        "char_end": 20,
        "param_type": "training",
        "explanation": "The batch size determines the number of samples processed by the model in each training step. It impacts the convergence speed, memory usage, and stability.",
        "typical_range": "32-256",
        "alternatives": [
          {
            "value": "32",
            "scenario": "Better memory efficiency for resource-constrained environments"
          },
          {
            "value": "128",
            "scenario": "Balanced performance for most use cases"
          },
          {
            "value": "256",
            "scenario": "Faster convergence with enough memory available"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_014",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      batch_size=100,\n      num_epochs=None,\n      shuffle=True)",
    "hyperparameters": [
      {
        "name": "num_epochs",
        "value": "None",
        "line_number": 143,
        "char_start": 17,
        "char_end": 21,
        "param_type": "training",
        "explanation": "Num epochs determines the number of times the training dataset is passed through the neural network. It controls the overall exposure of the model to the training data and influences the learning process.",
        "typical_range": "10-200",
        "alternatives": [
          {
            "value": "5-10",
            "scenario": "For quick experimentation or fine-tuning on small datasets"
          },
          {
            "value": "50-100",
            "scenario": "For most image classification tasks with moderate-sized datasets"
          },
          {
            "value": "100-200",
            "scenario": "For large datasets and complex models when better generalization is desired"
          }
        ],
        "impact": {
          "convergence_speed": "medium",
          "generalization": "good",
          "stability": "high"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  },
  {
    "id": "tf_cnn_015",
    "framework": "tensorflow",
    "source_url": "https://github.com/mixturemodel-flow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py",
    "code_snippet": "      y=eval_labels,\n      num_epochs=1,\n      shuffle=False)",
    "hyperparameters": [
      {
        "name": "num_epochs",
        "value": "1",
        "line_number": 154,
        "char_start": 17,
        "char_end": 18,
        "param_type": "training",
        "explanation": "The number of times the entire training dataset is passed through the neural network. Increasing epochs typically improves accuracy but also increases training time.",
        "typical_range": "50-200",
        "alternatives": [
          {
            "value": "10-20",
            "scenario": "Quick initial exploration"
          },
          {
            "value": "500+",
            "scenario": "Squeezing maximum accuracy out of a complex model"
          }
        ],
        "impact": {
          "convergence_speed": "slow",
          "generalization": "could improve if not overfit",
          "stability": "medium"
        }
      }
    ],
    "model_type": "CNN",
    "task": "image_classification",
    "dataset_size": "small"
  }
]