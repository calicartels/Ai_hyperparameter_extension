{
  "id": "te_7280e711",
  "framework": "tensorflow",
  "source_url": "https://github.com/Xeralux/tensorflow/blob/master/tensorflow/python/ops/nn_test.py",
  "hyperparameters": [
    {
      "name": "batch_size",
      "value": "batch_size",
      "line_number": 559,
      "char_start": 24,
      "char_end": 34,
      "code_context": "        (weights, biases, hidden_acts, sampled_vals, exp_logits,\n         exp_labels) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=[1, 0, 2, 3],\n             subtract_log_q=False)\n        logits_tensor, labels_tensor = _compute_sampled_logits(",
      "param_type": "training",
      "class_name": "_GenerateTestData",
      "model_type": "neural network",
      "task": "classification",
      "framework": "pytorch",
      "explanation": "Batch size defines the number of samples processed together in a single training iteration. It controls memory usage, convergence speed, and stability.",
      "typical_range": "32-256",
      "alternatives": [
        {
          "value": "32",
          "scenario": "Limited memory or fast training"
        },
        {
          "value": "128",
          "scenario": "Balance between memory and speed"
        },
        {
          "value": "256",
          "scenario": "Large memory or slower training"
        }
      ],
      "impact": {
        "convergence_speed": "medium",
        "generalization": "good",
        "stability": "medium"
      }
    },
    {
      "name": "batch_size",
      "value": "batch_size",
      "line_number": 595,
      "char_start": 24,
      "char_end": 34,
      "code_context": "        (weights, biases, hidden_acts, sampled_vals, exp_logits,\n         exp_labels) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=[1, 0, 2, 3],\n             subtract_log_q=False)\n        logits_tensor, labels_tensor = _compute_sampled_logits(",
      "param_type": "training",
      "class_name": "_GenerateTestData",
      "model_type": "Unidentified",
      "task": "classification",
      "framework": "tensorflow",
      "explanation": "The batch size determines the number of samples processed before updating the model's internal parameters. It heavily influences training speed, memory usage, and model convergence.",
      "typical_range": "32-256, potentially larger for resource-intensive models",
      "alternatives": [
        {
          "value": "Larger batch size (e.g., 512)",
          "scenario": "Abundant resources, faster training, potentially less stable convergence"
        },
        {
          "value": "Smaller batch size (e.g., 16)",
          "scenario": "Limited resources, slower training, potentially more stable convergence"
        },
        {
          "value": "Adaptive batch sizes",
          "scenario": "Dynamic resource allocation based on hardware capabilities"
        }
      ],
      "impact": {
        "convergence_speed": "Varies depending on resources, model, and task",
        "generalization": "May impact generalization due to variance changes",
        "stability": "Can affect stability during training, especially with very large or small sizes"
      }
    },
    {
      "name": "batch_size",
      "value": "batch_size",
      "line_number": 632,
      "char_start": 24,
      "char_end": 34,
      "code_context": "        (weights, biases, hidden_acts, sampled_vals, _,\n         _) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=sampled,\n             subtract_log_q=False)\n        logits_tensor, _ = _compute_sampled_logits(",
      "param_type": "training",
      "class_name": "_GenerateTestData",
      "model_type": "unknown",
      "task": "classification",
      "framework": "tensorflow",
      "explanation": "The batch size determines the number of samples used in one iteration of training. Larger batch sizes can lead to faster convergence but may require more memory and can be less stable.",
      "typical_range": "32-256",
      "alternatives": [
        {
          "value": "16",
          "scenario": "Limited memory or very small datasets"
        },
        {
          "value": "512",
          "scenario": "Large datasets and sufficient memory"
        }
      ],
      "impact": {
        "convergence_speed": "medium",
        "generalization": "good",
        "stability": "high"
      }
    },
    {
      "name": "batch_size",
      "value": "batch_size",
      "line_number": 676,
      "char_start": 24,
      "char_end": 34,
      "code_context": "        (weights, biases, hidden_acts, sampled_vals, exp_logits,\n         exp_labels) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=[1, 0, 2, 3],\n             subtract_log_q=True)\n        logits_tensor, labels_tensor = _compute_sampled_logits(",
      "param_type": "training",
      "class_name": "_GenerateTestData",
      "model_type": "unknown",
      "task": "classification",
      "framework": "tensorflow",
      "explanation": "The batch size determines the number of samples processed at each training iteration. It influences the speed of training, memory usage, and model convergence.",
      "typical_range": "[16, 128, 256, 512, 1024]",
      "alternatives": [
        {
          "value": "32",
          "scenario": "Start with this typical value"
        },
        {
          "value": "128",
          "scenario": "Increase for faster training, especially with GPUs"
        },
        {
          "value": "8",
          "scenario": "Reduce for limited memory or unstable training"
        }
      ],
      "impact": {
        "convergence_speed": "medium",
        "generalization": "medium",
        "stability": "medium"
      }
    },
    {
      "name": "batch_size",
      "value": "batch_size",
      "line_number": 712,
      "char_start": 24,
      "char_end": 34,
      "code_context": "        (weights, biases, hidden_acts, sampled_vals, exp_logits,\n         exp_labels) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=[1, 0, 2, 3],\n             subtract_log_q=False)\n        weight_shards, bias_shards = self._ShardTestEmbeddings(",
      "param_type": "training",
      "class_name": "_GenerateTestData",
      "model_type": "Unidentified - insufficient information provided",
      "task": "classification",
      "framework": "tensorflow",
      "explanation": "The batch size determines the number of samples processed by the model during each training iteration. It influences the speed of convergence, memory usage, and model stability.",
      "typical_range": "32-256, depending on dataset size, hardware resources, and desired convergence speed",
      "alternatives": [
        {
          "value": "power_of_2",
          "scenario": "Better performance across hardware"
        },
        {
          "value": "8-32",
          "scenario": "Limited memory or small datasets"
        },
        {
          "value": "512-2048",
          "scenario": "Very large datasets with powerful hardware"
        }
      ],
      "impact": {
        "convergence_speed": "fast|slow",
        "generalization": "good|excellent",
        "stability": "low|medium|high"
      }
    },
    {
      "name": "batch_size",
      "value": "batch_size",
      "line_number": 756,
      "char_start": 20,
      "char_end": 30,
      "code_context": "    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_nce_loss = np.sum(",
      "param_type": "training",
      "class_name": "_GenerateTestData",
      "model_type": "Unclear (Neural Network is too broad)",
      "task": "classification",
      "framework": "Tensorflow",
      "explanation": "Batch size determines the number of samples used in each training iteration. It controls the trade-off between convergence speed and resource usage.",
      "typical_range": "16-256, depending on hardware and dataset size",
      "alternatives": [
        {
          "value": "8",
          "scenario": "Limited resources or very large datasets"
        },
        {
          "value": "256",
          "scenario": "Powerful hardware and medium-sized datasets"
        },
        {
          "value": "1024",
          "scenario": "Very powerful hardware and small datasets"
        }
      ],
      "impact": {
        "convergence_speed": {
          "8": "fastest",
          "256": "medium",
          "1024": "slowest"
        },
        "generalization": "Potentially lower with larger batch sizes (higher variance)",
        "stability": "Higher with smaller batch sizes (less prone to getting stuck in local minima)"
      }
    },
    {
      "name": "batch_size",
      "value": "batch_size",
      "line_number": 813,
      "char_start": 20,
      "char_end": 30,
      "code_context": "    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_sampled_softmax_loss = _SoftmaxCrossEntropyWithLogits(",
      "param_type": "training",
      "class_name": "_GenerateTestData",
      "model_type": "Neural Network",
      "task": "classification",
      "framework": "pytorch",
      "explanation": "The batch size is the number of data samples processed by the model during each training step. It affects the convergence speed, generalization, and stability of the training process.",
      "typical_range": "32-512",
      "alternatives": [
        {
          "value": "32",
          "scenario": "Limited resources or small datasets"
        },
        {
          "value": "128",
          "scenario": "Most common starting point"
        },
        {
          "value": "512",
          "scenario": "Large datasets with powerful hardware"
        }
      ],
      "impact": {
        "convergence_speed": "medium",
        "generalization": "good",
        "stability": "medium"
      }
    }
  ],
  "model_type": "Neural Network",
  "task": "classification",
  "dataset_size": "large",
  "code_snippet": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for miscellaneous functionality in tensorflow.ops.nn.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops.nn_impl import _compute_sampled_logits\nfrom tensorflow.python.platform import test as test_lib\n\n\nclass ZeroFractionTest(test_lib.TestCase):\n\n  def _ZeroFraction(self, x):\n    assert x.shape\n    total_elements = np.prod(x.shape)\n    nonzeros = np.count_nonzero(x.flatten())\n    return 1.0 - nonzeros / total_elements\n\n  def testZeroFraction(self):\n    x_shape = [5, 17]\n    x_np = np.random.randint(0, 2, size=x_shape).astype(np.float32)\n    y_np = self._ZeroFraction(x_np)\n    with self.test_session():\n      x_tf = constant_op.constant(x_np)\n      x_tf.set_shape(x_shape)\n      y_tf = nn_impl.zero_fraction(x_tf)\n      y_tf_np = y_tf.eval()\n    eps = 1e-8\n    self.assertAllClose(y_tf_np, y_np, eps)\n\n  def testZeroFractionEmpty(self):\n    with self.test_session():\n      x = np.zeros(0)\n      y = nn_impl.zero_fraction(x).eval()\n      self.assertTrue(np.isnan(y))\n\n\nclass SoftmaxTest(test_lib.TestCase):\n\n  def _softmax(self, x):\n    assert len(x.shape) == 2\n    m = x.max(1)[:, np.newaxis]\n    u = np.exp(x - m)\n    z = u.sum(1)[:, np.newaxis]\n    return u / z\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testSoftmax(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    y_np = self._softmax(x_np)\n    x_tf = constant_op.constant(x_np)\n    y_tf = nn_ops.softmax(x_tf)\n    y_tf_last_dim = nn_ops.softmax(x_tf, 1)\n    y_tf_np = self.evaluate(y_tf)\n    y_tf_last_dim_np = self.evaluate(y_tf_last_dim)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n    self.assertAllClose(y_tf_last_dim_np, y_np, eps)\n\n  def testSoftmaxAxes(self):\n    arr = np.linspace(0., 1, 12).reshape(3, 4)\n    x_neg_axis = nn_ops.softmax(arr, axis=-2)\n    y_pos_axis = nn_ops.softmax(arr, axis=0)\n    z_gt_axis = nn_ops.softmax(arr, axis=4)\n    x_neg_axis_tf = self.evaluate(x_neg_axis)\n    y_pos_axis_tf = self.evaluate(y_pos_axis)\n    z_gt_axis_tf = self.evaluate(z_gt_axis)\n    eps = 1e-3\n    self.assertAllClose(x_neg_axis_tf, y_pos_axis_tf, eps)\n    self.assertAllClose(y_pos_axis_tf, z_gt_axis_tf, eps)\n\n  def testGradient(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    with self.test_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_ops.softmax(x_tf)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n    eps = 1e-8\n    self.assertLess(err, eps)\n\n\nclass LogPoissonLossTest(test_lib.TestCase):\n\n  def _log_poisson_loss(self, x, z, compute_full_loss=False):\n    lpl = np.exp(x) - z * x\n    if compute_full_loss:\n      stirling_approx = z * np.log(z) - z + 0.5 * np.log(2. * np.pi * z)\n      lpl += np.ma.masked_array(stirling_approx, mask=(z <= 1)).filled(0.)\n    return lpl\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testLogPoissonLoss(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    z_np = np.random.randint(0, 5, size=x_shape).astype(np.float32)\n    y_np = self._log_poisson_loss(x_np, z_np, compute_full_loss=False)\n    y_np_stirling = self._log_poisson_loss(x_np, z_np, compute_full_loss=True)\n    y_tf = nn_impl.log_poisson_loss(z_np, x_np, compute_full_loss=False)\n    y_tf_stirling = nn_impl.log_poisson_loss(z_np, x_np, compute_full_loss=True)\n    y_tf_np = self.evaluate(y_tf)\n    y_tf_np_stirling = self.evaluate(y_tf_stirling)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n    self.assertAllClose(y_tf_np_stirling, y_np_stirling, eps)\n\n  def testGradient(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    z_np = np.random.randint(0, 5, size=x_shape).astype(np.float64)\n    with self.test_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_impl.log_poisson_loss(z_np, x_tf, compute_full_loss=False)\n      y_tf_stirling = nn_impl.log_poisson_loss(\n          z_np, x_tf, compute_full_loss=True)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n      err_stirling = gradient_checker.compute_gradient_error(\n          x_tf, x_shape, y_tf_stirling, x_shape)\n    eps = 1e-6\n    self.assertLess(err, eps)\n    self.assertLess(err_stirling, eps)\n\n\nclass LogSoftmaxTest(test_lib.TestCase):\n\n  def _log_softmax(self, x):\n    assert len(x.shape) == 2\n    m = x.max(1)[:, np.newaxis]\n    u = x - m\n    return u - np.log(np.sum(np.exp(u), 1, keepdims=True))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testLogSoftmax(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    y_np = self._log_softmax(x_np)\n    x_tf = constant_op.constant(x_np)\n    y_tf = nn_ops.log_softmax(x_tf)\n    y_tf_np = self.evaluate(y_tf)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n\n  def testLogSoftmaxAxes(self):\n    arr = np.linspace(0., 1, 12).reshape(3, 4)\n    x_neg_axis = nn_ops.log_softmax(arr, axis=-2)\n    y_pos_axis = nn_ops.log_softmax(arr, axis=0)\n    z_gt_axis = nn_ops.log_softmax(arr, axis=4)\n    x_neg_axis_tf = self.evaluate(x_neg_axis)\n    y_pos_axis_tf = self.evaluate(y_pos_axis)\n    z_gt_axis_tf = self.evaluate(z_gt_axis)\n    eps = 1e-3\n    self.assertAllClose(x_neg_axis_tf, y_pos_axis_tf, eps)\n    self.assertAllClose(y_pos_axis_tf, z_gt_axis_tf, eps)\n\n  def testGradient(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    with self.test_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_ops.log_softmax(x_tf)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n    eps = 1e-7\n    self.assertLess(err, eps)\n\n\nclass L2LossTest(test_lib.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testL2Loss(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      x = constant_op.constant(\n          [1.0, 0.0, 3.0, 2.0], shape=[2, 2], name=\"x\", dtype=dtype)\n      l2loss = nn_ops.l2_loss(x)\n      value = self.evaluate(l2loss)\n      self.assertAllClose(7.0, value)\n\n  def testGradient(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)  # Make it reproducible.\n    x_val = np.random.random_sample(x_shape).astype(np.float64)\n    with self.test_session():\n      x = constant_op.constant(x_val, name=\"x\")\n      output = nn_ops.l2_loss(x)\n      err = gradient_checker.compute_gradient_error(x, x_shape, output, [1])\n    print(\"L2Loss gradient err = %g \" % err)\n    err_tolerance = 1e-11\n    self.assertLess(err, err_tolerance)\n\n\nclass L2NormalizeTest(test_lib.TestCase):\n\n  def _l2Normalize(self, x, dim):\n    if isinstance(dim, list):\n      norm = np.linalg.norm(x, axis=tuple(dim))\n      for d in dim:\n        norm = np.expand_dims(norm, d)\n      return x / norm\n    else:\n      norm = np.apply_along_axis(np.linalg.norm, dim, x)\n      return x / np.expand_dims(norm, dim)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testL2Normalize(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float32)\n    for dim in range(len(x_shape)):\n      y_np = self._l2Normalize(x_np, dim)\n      x_tf = constant_op.constant(x_np, name=\"x\")\n      y_tf = nn_impl.l2_normalize(x_tf, dim)\n      self.assertAllClose(y_np, self.evaluate(y_tf))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testL2NormalizeDimArray(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float32)\n    dim = [1, 2]\n    y_np = self._l2Normalize(x_np, dim)\n    x_tf = constant_op.constant(x_np, name=\"x\")\n    y_tf = nn_impl.l2_normalize(x_tf, dim)\n    self.assertAllClose(y_np, self.evaluate(y_tf))\n\n  def testL2NormalizeGradient(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float64)\n    for dim in range(len(x_shape)):\n      with self.test_session():\n        x_tf = constant_op.constant(x_np, name=\"x\")\n        y_tf = nn_impl.l2_normalize(x_tf, dim)\n        err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                      x_shape)\n      print(\"L2Normalize gradient err = %g \" % err)\n      self.assertLess(err, 1e-4)\n\n\nclass DropoutTest(test_lib.TestCase):\n\n  def testDropout(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      with self.test_session():\n        t = constant_op.constant(\n            1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n        dropout = nn_ops.dropout(t, keep_prob)\n        final_count = 0\n        self.assertEqual([x_dim, y_dim], dropout.get_shape())\n        for _ in xrange(0, num_iter):\n          value = dropout.eval()\n          final_count += np.count_nonzero(value)\n          # Verifies that there are only two values: 0 and 1/keep_prob.\n          sorted_value = np.unique(np.sort(value))\n          self.assertEqual(0, sorted_value[0])\n          self.assertAllClose(1 / keep_prob, sorted_value[1])\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testShapedDropout(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability. This time with shaped\n    # noise.\n    x_dim = 40 * 30\n    y_dim = 3\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      with self.test_session():\n        t = constant_op.constant(\n            1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n        dropout = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, 1])\n        self.assertEqual([x_dim, y_dim], dropout.get_shape())\n        final_count = 0\n        for _ in xrange(0, num_iter):\n          value = dropout.eval()\n          final_count += np.count_nonzero(value)\n          # Verifies that there are only two values: 0 and 1/keep_prob.\n          sorted_value = np.unique(np.sort(value))\n          self.assertEqual(0, sorted_value[0])\n          self.assertAllClose(1 / keep_prob, sorted_value[1])\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testShapedDropoutCorrelation(self):\n    # Runs a shaped dropout and tests that the correlations are correct.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      with self.test_session():\n        t = constant_op.constant(\n            1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n        dropout = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, 1])\n        self.assertEqual([x_dim, y_dim], dropout.get_shape())\n        for _ in xrange(0, num_iter):\n          value = dropout.eval()\n          # Verifies that each y column as only one type of activation.\n          for i in xrange(x_dim):\n            sorted_value = np.unique(np.sort(value[i, :]))\n            self.assertEqual(sorted_value.size, 1)\n\n  def testDropoutPlaceholderKeepProb(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      with self.test_session():\n        t = constant_op.constant(\n            1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n        keep_prob_placeholder = array_ops.placeholder(dtypes.float32)\n        dropout = nn_ops.dropout(t, keep_prob_placeholder)\n        final_count = 0\n        self.assertEqual([x_dim, y_dim], dropout.get_shape())\n        for _ in xrange(0, num_iter):\n          value = dropout.eval(feed_dict={keep_prob_placeholder: keep_prob})\n          final_count += np.count_nonzero(value)\n          # Verifies that there are only two values: 0 and 1/keep_prob.\n          sorted_value = np.unique(np.sort(value))\n          self.assertEqual(0, sorted_value[0])\n          self.assertAllClose(1 / keep_prob, sorted_value[1])\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testShapedDropoutUnknownShape(self):\n    x_dim = 40\n    y_dim = 30\n    keep_prob = 0.5\n    x = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    dropout_x = nn_ops.dropout(\n        x, keep_prob, noise_shape=array_ops.placeholder(dtypes.int32))\n    self.assertEqual(x.get_shape(), dropout_x.get_shape())\n\n  def testPartialShapedDropout(self):\n    x_dim = 40 * 30\n    y_dim = 3\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      with self.test_session():\n        t = constant_op.constant(\n            1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n        # Set noise_shape=[None, 1] which means [x_dim, 1].\n        dropout = nn_ops.dropout(t, keep_prob, noise_shape=[None, 1])\n        self.assertEqual([x_dim, y_dim], dropout.get_shape())\n        final_count = 0\n        for _ in xrange(0, num_iter):\n          value = dropout.eval()\n          final_count += np.count_nonzero(value)\n          # Verifies that there are only two values: 0 and 1/keep_prob.\n          sorted_value = np.unique(np.sort(value))\n          self.assertEqual(0, sorted_value[0])\n          self.assertAllClose(1 / keep_prob, sorted_value[1])\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testInvalidKeepProb(self):\n    x_dim = 40\n    y_dim = 30\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, -1.0)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, 1.1)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, [0.0, 1.0])\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, array_ops.placeholder(dtypes.float64))\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, array_ops.placeholder(dtypes.float32, shape=[2]))\n\n  def testShapedDropoutShapeError(self):\n    # Runs shaped dropout and verifies an error is thrown on misshapen noise.\n    x_dim = 40\n    y_dim = 30\n    keep_prob = 0.5\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, y_dim + 10])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, y_dim, 5])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim + 3])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim])\n    # test that broadcasting proceeds\n    _ = nn_ops.dropout(t, keep_prob, noise_shape=[y_dim])\n    _ = nn_ops.dropout(t, keep_prob, noise_shape=[1, y_dim])\n    _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, 1])\n    _ = nn_ops.dropout(t, keep_prob, noise_shape=[1, 1])\n\n  def testNoDropoutFast(self):\n    x = array_ops.zeros((5,))\n    for p in 1, constant_op.constant(1.0):\n      y = nn_ops.dropout(x, keep_prob=p)\n      self.assertTrue(x is y)\n\n  def testDropoutWithIntegerInputs(self):\n    x = constant_op.constant([1, 1, 1, 1, 1])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(x, 0.5)\n\n\nclass ComputeSampledLogitsTest(test_lib.TestCase):\n\n  def setUp(self):\n    self._eps = 1e-3\n\n  def _GenerateTestData(self, num_classes, dim, batch_size, num_true, labels,\n                        sampled, subtract_log_q):\n    \"\"\"Randomly generates input/output data for a single test case.\n\n    This function returns numpy constants for use in a test case.\n\n    Args:\n      num_classes: An int. The number of embedding classes in the test case.\n      dim: An int. The dimension of the embedding.\n      batch_size: An int. The batch size.\n      num_true: An int. The number of target classes per training example.\n      labels: A list of batch_size * num_true ints. The target classes.\n      sampled: A list of indices in [0, num_classes).\n      subtract_log_q: A bool corresponding to the parameter in\n          _compute_sampled_logits().\n\n    Returns:\n      weights: Embedding weights to use as test input. It is a numpy array\n          of shape [num_classes, dim]\n      biases: Embedding biases to use as test input. It is a numpy array\n          of shape [num_classes].\n      hidden_acts: Forward activations of the network to use as test input.\n          It is a numpy array of shape [batch_size, dim].\n      sampled_vals: A tuple based on `sampled` to use as test input in the\n          format returned by a *_candidate_sampler function.\n      exp_logits: The output logits expected from _compute_sampled_logits().\n          It is a numpy array of shape [batch_size, num_true + len(sampled)].\n      exp_labels: The output labels expected from _compute_sampled_logits().\n          It is a numpy array of shape [batch_size, num_true + len(sampled)].\n    \"\"\"\n    weights = np.random.randn(num_classes, dim).astype(np.float32)\n    biases = np.random.randn(num_classes).astype(np.float32)\n    hidden_acts = np.random.randn(batch_size, dim).astype(np.float32)\n\n    true_exp = np.full([batch_size, 1], fill_value=0.5, dtype=np.float32)\n    sampled_exp = np.full([len(sampled)], fill_value=0.5, dtype=np.float32)\n    sampled_vals = (sampled, true_exp, sampled_exp)\n\n    sampled_w, sampled_b = weights[sampled], biases[sampled]\n    true_w, true_b = weights[labels], biases[labels]\n\n    true_logits = np.sum(\n        hidden_acts.reshape((batch_size, 1, dim)) * true_w.reshape(\n            (batch_size, num_true, dim)),\n        axis=2)\n    true_b = true_b.reshape((batch_size, num_true))\n    true_logits += true_b\n    sampled_logits = np.dot(hidden_acts, sampled_w.T) + sampled_b\n\n    if subtract_log_q:\n      true_logits -= np.log(true_exp)\n      sampled_logits -= np.log(sampled_exp[np.newaxis, :])\n\n    exp_logits = np.concatenate([true_logits, sampled_logits], axis=1)\n    exp_labels = np.hstack((np.ones_like(true_logits) / num_true,\n                            np.zeros_like(sampled_logits)))\n\n    return weights, biases, hidden_acts, sampled_vals, exp_logits, exp_labels\n\n  def _ShardTestEmbeddings(self, weights, biases, num_shards):\n    \"\"\"Shards the weights and biases returned by _GenerateTestData.\n\n    Args:\n      weights: The weights returned by _GenerateTestData.\n      biases: The biases returned by _GenerateTestData.\n      num_shards: The number of shards to create.\n\n    Returns:\n      sharded_weights: A list of size `num_shards` containing all the weights.\n      sharded_biases: A list of size `num_shards` containing all the biases.\n    \"\"\"\n    with ops.Graph().as_default() as g:\n      sharded_weights = variable_scope.get_variable(\n          \"w\",\n          partitioner=partitioned_variables.fixed_size_partitioner(num_shards),\n          initializer=constant_op.constant(weights))\n      sharded_biases = variable_scope.get_variable(\n          \"b\",\n          partitioner=partitioned_variables.fixed_size_partitioner(num_shards),\n          initializer=constant_op.constant(biases))\n      with self.test_session(graph=g) as sess:\n        variables.global_variables_initializer().run()\n        return sess.run([list(sharded_weights), list(sharded_biases)])\n\n  def testShapes(self):\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    with self.test_session() as sess:\n      for num_true in range(1, 5):\n        labels = np.random.randint(\n            low=0, high=num_classes, size=batch_size * num_true)\n        (weights, biases, hidden_acts, sampled_vals, exp_logits,\n         exp_labels) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=[1, 0, 2, 3],\n             subtract_log_q=False)\n        logits_tensor, labels_tensor = _compute_sampled_logits(\n            weights=constant_op.constant(weights),\n            biases=constant_op.constant(biases),\n            labels=constant_op.constant(\n                labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n            inputs=constant_op.constant(hidden_acts),\n            num_sampled=4,\n            num_classes=num_classes,\n            num_true=num_true,\n            sampled_values=sampled_vals,\n            subtract_log_q=False,\n            remove_accidental_hits=False,\n            partition_strategy=\"div\",\n            name=\"sampled_logits_basic_num_true_%d\" % num_true)\n        got_logits, got_labels = sess.run([logits_tensor, labels_tensor])\n        self.assertEqual(exp_logits.shape, got_logits.shape, self._eps)\n        self.assertEqual(exp_labels.shape, got_labels.shape, self._eps)\n\n  def testBasic(self):\n    \"\"\"Without accidental hit removal or subtract_log_q.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    with self.test_session() as sess:\n      for num_true in range(1, 5):\n        labels = np.random.randint(\n            low=0, high=num_classes, size=batch_size * num_true)\n        (weights, biases, hidden_acts, sampled_vals, exp_logits,\n         exp_labels) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=[1, 0, 2, 3],\n             subtract_log_q=False)\n        logits_tensor, labels_tensor = _compute_sampled_logits(\n            weights=constant_op.constant(weights),\n            biases=constant_op.constant(biases),\n            labels=constant_op.constant(\n                labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n            inputs=constant_op.constant(hidden_acts),\n            num_sampled=4,\n            num_classes=num_classes,\n            num_true=num_true,\n            sampled_values=sampled_vals,\n            subtract_log_q=False,\n            remove_accidental_hits=False,\n            partition_strategy=\"div\",\n            name=\"sampled_logits_basic_num_true_%d\" % num_true)\n        got_logits, got_labels = sess.run([logits_tensor, labels_tensor])\n        self.assertAllClose(exp_logits, got_logits, self._eps)\n        self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testAccidentalHitRemoval(self):\n    \"\"\"With accidental hit removal, no subtract_log_q.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    sampled = [1, 0, 2, 3]\n    with self.test_session():\n      for num_true in range(1, 5):\n        labels = np.random.randint(\n            low=0, high=num_classes, size=batch_size * num_true)\n        (weights, biases, hidden_acts, sampled_vals, _,\n         _) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=sampled,\n             subtract_log_q=False)\n        logits_tensor, _ = _compute_sampled_logits(\n            weights=constant_op.constant(weights),\n            biases=constant_op.constant(biases),\n            labels=constant_op.constant(\n                labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n            inputs=constant_op.constant(hidden_acts),\n            num_sampled=len(sampled),\n            num_classes=num_classes,\n            num_true=num_true,\n            sampled_values=sampled_vals,\n            subtract_log_q=False,\n            remove_accidental_hits=True,\n            partition_strategy=\"div\",\n            name=\"sampled_logits_accidental_hit_removal_num_true_%d\" % num_true)\n        # Test that the exponentiated logits of accidental hits are near 0.\n        # First we need to find the hits in this random test run:\n        labels_reshape = labels.reshape((batch_size, num_true))\n        got_logits = logits_tensor.eval()\n        for row in xrange(batch_size):\n          row_labels = labels_reshape[row, :]\n          for col in xrange(len(sampled)):\n            if sampled[col] in row_labels:\n              # We need to add the num_true_test offset into logits_*\n              self.assertNear(\n                  np.exp(got_logits[row, col + num_true]), 0., self._eps)\n\n  def testSubtractLogQ(self):\n    \"\"\"With subtract_log_q, no accidental hit removal.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    with self.test_session() as sess:\n      for num_true in range(1, 5):\n        labels = np.random.randint(\n            low=0, high=num_classes, size=batch_size * num_true)\n        (weights, biases, hidden_acts, sampled_vals, exp_logits,\n         exp_labels) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=[1, 0, 2, 3],\n             subtract_log_q=True)\n        logits_tensor, labels_tensor = _compute_sampled_logits(\n            weights=constant_op.constant(weights),\n            biases=constant_op.constant(biases),\n            labels=constant_op.constant(\n                labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n            inputs=constant_op.constant(hidden_acts),\n            num_sampled=4,\n            num_classes=num_classes,\n            num_true=num_true,\n            sampled_values=sampled_vals,\n            subtract_log_q=True,\n            remove_accidental_hits=False,\n            partition_strategy=\"div\",\n            name=\"sampled_logits_subtract_log_q_num_true_%d\" % num_true)\n        got_logits, got_labels = sess.run([logits_tensor, labels_tensor])\n        self.assertAllClose(exp_logits, got_logits, self._eps)\n        self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testSharded(self):\n    \"\"\"With sharded weights and sharded biases.\"\"\"\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    with self.test_session() as sess:\n      for num_true in range(1, 5):\n        labels = np.random.randint(\n            low=0, high=num_classes, size=batch_size * num_true)\n        (weights, biases, hidden_acts, sampled_vals, exp_logits,\n         exp_labels) = self._GenerateTestData(\n             num_classes=num_classes,\n             dim=10,\n             batch_size=batch_size,\n             num_true=num_true,\n             labels=labels,\n             sampled=[1, 0, 2, 3],\n             subtract_log_q=False)\n        weight_shards, bias_shards = self._ShardTestEmbeddings(\n            weights, biases, num_shards=3)\n        logits_tensor, labels_tensor = _compute_sampled_logits(\n            weights=[constant_op.constant(shard) for shard in weight_shards],\n            biases=[constant_op.constant(shard) for shard in bias_shards],\n            labels=constant_op.constant(\n                labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n            inputs=constant_op.constant(hidden_acts),\n            num_sampled=4,\n            num_classes=num_classes,\n            num_true=num_true,\n            sampled_values=sampled_vals,\n            subtract_log_q=False,\n            remove_accidental_hits=False,\n            partition_strategy=\"div\",\n            name=\"sampled_logits_sharded_num_true_%d\" % num_true)\n        got_logits, got_labels = sess.run([logits_tensor, labels_tensor])\n        self.assertAllClose(exp_logits, got_logits, self._eps)\n        self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testNCELoss(self):\n    # A simple test to verify the numerics.\n\n    def _SigmoidCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      pred = 1. / (1. + np.exp(-logits))\n      eps = 0.0001\n      pred = np.minimum(np.maximum(pred, eps), 1 - eps)\n      return -targets * np.log(pred) - (1. - targets) * np.log(1. - pred)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_nce_loss = np.sum(\n        _SigmoidCrossEntropyWithLogits(exp_logits, exp_labels), 1)\n\n    with self.test_session():\n      got_nce_loss = nn_impl.nce_loss(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(labels, shape=(batch_size, 1)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=1,\n          sampled_values=sampled_vals,\n          partition_strategy=\"div\")\n\n      self.assertAllClose(exp_nce_loss, got_nce_loss.eval(), 1e-4)\n\n      # Test with sharded weights and sharded biases.\n      weight_shards, bias_shards = self._ShardTestEmbeddings(\n          weights, biases, num_shards=3)\n      got_nce_loss = nn_impl.nce_loss(\n          weights=[constant_op.constant(shard) for shard in weight_shards],\n          biases=[constant_op.constant(shard) for shard in bias_shards],\n          labels=constant_op.constant(labels, shape=(batch_size, 1)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=1,\n          sampled_values=sampled_vals,\n          partition_strategy=\"div\")\n\n      self.assertAllClose(exp_nce_loss, got_nce_loss.eval(), 1e-4)\n\n  def testSampledSoftmaxLoss(self):\n    # A simple test to verify the numerics.\n\n    def _SoftmaxCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      stable_exp_logits = np.exp(\n          logits - np.amax(logits, axis=1, keepdims=True))\n      pred = stable_exp_logits / np.sum(stable_exp_logits, 1, keepdims=True)\n      return -np.sum(targets * np.log(pred + 1.0e-20), axis=1)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_sampled_softmax_loss = _SoftmaxCrossEntropyWithLogits(\n        exp_logits, exp_labels)\n\n    with self.test_session():\n      got_sampled_softmax_loss = nn_impl.sampled_softmax_loss(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(labels, shape=(batch_size, 1)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=1,\n          sampled_values=sampled_vals,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\")\n\n      self.assertAllClose(exp_sampled_softmax_loss,\n                          got_sampled_softmax_loss.eval(), 1e-4)\n\n      # Test with sharded weights and sharded biases.\n      weight_shards, bias_shards = self._ShardTestEmbeddings(\n          weights, biases, num_shards=3)\n      got_sampled_softmax_loss = nn_impl.sampled_softmax_loss(\n          weights=[constant_op.constant(shard) for shard in weight_shards],\n          biases=[constant_op.constant(shard) for shard in bias_shards],\n          labels=constant_op.constant(labels, shape=(batch_size, 1)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=1,\n          sampled_values=sampled_vals,\n          remove_accidental_hits=False,\n          partition_strategy=\"div\")\n\n      self.assertAllClose(exp_sampled_softmax_loss,\n                          got_sampled_softmax_loss.eval(), 1e-4)\n\n\nclass CReluTest(test_lib.TestCase):\n\n  def test(self):\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = np.concatenate([x * (x > 0), -x * (x < 0)], axis=1)\n    with self.test_session():\n      z = nn_ops.crelu(constant_op.constant(x)).eval()\n      self.assertAllClose(y, z, 1e-4)\n\n\nclass ReluTest(test_lib.TestCase):\n\n  def test(self):\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = np.maximum(x, 0.0)\n    with self.test_session():\n      z = nn_ops.relu(constant_op.constant(x)).eval()\n      self.assertAllEqual(y, z)\n\n  def testNaNs(self):\n    # Test that relu(nan) = nan for various sizes.\n    for i in range(18):\n      x = np.zeros(i) + np.nan\n      with self.test_session():\n        z = nn_ops.relu(constant_op.constant(x)).eval()\n        self.assertTrue(np.isnan(z).all())\n\n\nclass LeakyReluTest(test_lib.TestCase):\n\n  def testRange(self):\n    batch_size = 3\n    height, width = 4, 4\n    np.random.seed(1)  # Make it reproducible.\n    inputs = np.random.uniform(size=(batch_size, height, width, 3)).astype(\n        np.float32)\n    inputs = constant_op.constant(inputs)\n\n    outputs = nn_ops.leaky_relu(inputs)\n    self.assertEquals(inputs.shape, outputs.shape)\n    with self.test_session() as sess:\n      inputs, outputs = sess.run([inputs, outputs])\n    self.assertGreaterEqual(outputs.min(), 0.0)\n    self.assertLessEqual(outputs.max(), 1.0)\n    self.assertAllClose(inputs, outputs)\n\n  def testValues(self):\n    for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n      np_values = np.array([-2, -1, 0, 1, 2], dtype=dtype)\n      outputs = nn_ops.leaky_relu(constant_op.constant(np_values))\n      with self.test_session() as sess:\n        outputs = sess.run(outputs)\n      tol = 2e-3 if dtype == np.float16 else 1e-6\n      self.assertAllClose(\n          outputs, [-0.4, -0.2, 0.0, 1.0, 2.0], rtol=tol, atol=tol)\n\n\nclass SwishTest(test_lib.TestCase):\n\n  def testValues(self):\n    np_values = np.array(\n        [np.linspace(-10.0, 0.0, 100),\n         np.linspace(0.0, 10.0, 100)],\n        dtype=np.float32)\n    tf_values = constant_op.constant(np_values)\n    actual_tf_outputs = nn_impl.swish(tf_values)\n    expected_tf_outputs = tf_values * math_ops.sigmoid(tf_values)\n    with self.test_session() as sess:\n      actual_outputs, expected_outputs = sess.run(\n          [actual_tf_outputs, expected_tf_outputs])\n    self.assertAllClose(actual_outputs, expected_outputs)\n\n  def testGradients(self):\n    shape = [5, 3, 4]\n    sigma = 5\n    input_values = np.random.randn(*shape) * sigma\n    x_tf = constant_op.constant(input_values)\n    y_tf = nn_impl.swish(x_tf)\n    with self.test_session():\n      err = gradient_checker.compute_gradient_error(x_tf, shape, y_tf, shape)\n    self.assertLess(err, 1e-4)\n\n\nclass MomentsTest(test_lib.TestCase):\n\n  def doOutputTest(self,\n                   input_shape,\n                   moments_axes,\n                   tol=1e-4,\n                   check_gradients=False):\n    for mu in [0.0, 1.0, 1e3]:\n      for sigma in [1.0, 0.1]:\n        for keep_dims in [True, False]:\n          input_values = np.random.rand(*input_shape) * sigma + mu\n          expected_mean = np.mean(\n              input_values, axis=moments_axes, keepdims=keep_dims)\n          expected_var = np.var(\n              input_values, axis=moments_axes, keepdims=keep_dims)\n          with ops.Graph().as_default() as g:\n            with self.test_session(graph=g) as sess:\n              inputs = constant_op.constant(\n                  input_values, shape=input_shape, dtype=dtypes.float32)\n              mean, variance = nn_impl.moments(\n                  inputs, moments_axes, keep_dims=keep_dims)\n\n              if check_gradients:\n                err = gradient_checker.compute_gradient_error(\n                    inputs, input_shape, mean, mean.shape.as_list())\n                self.assertLess(err, 1e-3)\n                err = gradient_checker.compute_gradient_error(\n                    inputs, input_shape, variance, variance.shape.as_list())\n                self.assertLess(err, 1e-3)\n\n              # Evaluate.\n              [mean, variance] = sess.run([mean, variance])\n              # Make sure that there are no NaNs\n              self.assertFalse(np.isnan(mean).any())\n              self.assertFalse(np.isnan(variance).any())\n              self.assertAllClose(mean, expected_mean, rtol=tol, atol=tol)\n              self.assertAllClose(variance, expected_var, rtol=tol, atol=tol)\n\n  def testOutputAndGradient2DInput0(self):\n    self.doOutputTest((10, 10), (0,), check_gradients=True)\n\n  def testOutputAndGradient2DInput01(self):\n    self.doOutputTest((10, 10), (0, 1), check_gradients=True)\n\n  def testOutput2DInput0(self):\n    self.doOutputTest((10, 300), (0,))\n\n  def testOutput2DInput1(self):\n    self.doOutputTest((10, 300), (1,))\n\n  def testOutput2DInput01(self):\n    self.doOutputTest((10, 300), (0, 1))\n\n  def testOutput4DInput0(self):\n    self.doOutputTest((10, 10, 10, 30), (0,))\n\n  def testOutput4DInput1(self):\n    self.doOutputTest((10, 10, 10, 30), (1,))\n\n  def testOutput4DInput3(self):\n    self.doOutputTest((10, 10, 10, 30), (3,))\n\n  def testOutput4DInput012(self):\n    self.doOutputTest((10, 10, 10, 30), (0, 1, 2))\n\n  def testOutput4DInput123(self):\n    self.doOutputTest((10, 10, 10, 30), (1, 2, 3))\n\n\nclass DataFormatDimMapTest(test_lib.TestCase):\n\n  def _test(self, x_val, y_val_expected):\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x)\n    with self.test_session(use_gpu=test_lib.is_gpu_available()) as sess:\n      y_val = sess.run(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def test(self):\n    self._test(0, 0)\n    self._test(1, 2)\n    self._test(2, 3)\n    self._test(3, 1)\n    self._test(-1, 1)\n    self._test(-2, 3)\n    self._test(-3, 2)\n    self._test(-4, 0)\n    self._test([1, 3], [2, 1])\n    self._test([1, 3, -2], [2, 1, 3])\n    self._test([1, -3, -2], [2, 2, 3])\n    self._test([[1, -3], [1, -1]], [[2, 2], [2, 1]])\n\n\nclass DataFormatVectorPermuteTest(test_lib.TestCase):\n\n  def testNHWCToNCHW(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with self.test_session(use_gpu=test_lib.is_gpu_available()) as sess:\n      y_val = sess.run(y)\n      self.assertAllEqual(y_val, [7, 3, 4, 9])\n\n  def testNCHWToNHWC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NCHW\", dst_format=\"NHWC\")\n    with self.test_session(use_gpu=test_lib.is_gpu_available()) as sess:\n      y_val = sess.run(y)\n      self.assertAllEqual(y_val, [7, 9, 3, 4])\n\n  def testNHWCToHWNC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"HWNC\")\n    with self.test_session(use_gpu=test_lib.is_gpu_available()) as sess:\n      y_val = sess.run(y)\n      self.assertAllEqual(y_val, [4, 9, 7, 3])\n\n  def testHWNCToNHWC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"HWNC\", dst_format=\"NHWC\")\n    with self.test_session(use_gpu=test_lib.is_gpu_available()) as sess:\n      y_val = sess.run(y)\n      self.assertAllEqual(y_val, [9, 7, 4, 3])\n\n  def testNHWCToNCHW2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with self.test_session(use_gpu=test_lib.is_gpu_available()) as sess:\n      y_val = sess.run(y)\n      self.assertAllEqual(y_val, [[7, 4], [5, 1], [9, 3], [4, 5]])\n\n  def testNHWCToHWNC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"HWNC\")\n    with self.test_session(use_gpu=test_lib.is_gpu_available()) as sess:\n      y_val = sess.run(y)\n      self.assertAllEqual(y_val, [[9, 3], [4, 5], [7, 4], [5, 1]])\n\n  def testHWNCToNHWC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"HWNC\", dst_format=\"NHWC\")\n    with self.test_session(use_gpu=test_lib.is_gpu_available()) as sess:\n      y_val = sess.run(y)\n      self.assertAllEqual(y_val, [[4, 5], [7, 4], [9, 3], [5, 1]])\n\n  def testNCHWToNHWC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=\"NCHW\", dst_format=\"NHWC\")\n    with self.test_session(use_gpu=test_lib.is_gpu_available()) as sess:\n      y_val = sess.run(y)\n      self.assertAllEqual(y_val, [[7, 4], [4, 5], [5, 1], [9, 3]])\n\n\nif __name__ == \"__main__\":\n  test_lib.main()\n"
}